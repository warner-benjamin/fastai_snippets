{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "> Additional loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "\n",
    "import torch.nn.modules.loss as TL\n",
    "\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BCE with Batch Mean -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class BCEWithLogitsLoss(TL._Loss):\n",
    "    \"\"\"\n",
    "    Like `nn.BCEWithLogitsLoss`, but with 'batchmean' reduction from MosiacML. `batchmean` scales\n",
    "    loss by the batch size which results in larger loss values more similar to `nn.CrossEntropy`\n",
    "    then `mean` reduction.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        weight:Tensor|None=None, # Rescaling weieght for each class\n",
    "        reduction:str='mean', # Reduction to apply to loss output. Also supports 'batchmean'.\n",
    "        pos_weight:Tensor|None=None # Weight of positive examples\n",
    "    ) -> None:\n",
    "        super().__init__(None, None, reduction)\n",
    "        self.register_buffer('weight', weight)\n",
    "        self.register_buffer('pos_weight', pos_weight)\n",
    "        self.weight:Tensor|None\n",
    "        self.pos_weight:Tensor|None\n",
    "\n",
    "    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n",
    "        if self.reduction == 'batchmean':\n",
    "            return F.binary_cross_entropy_with_logits(input, target,\n",
    "                                                      self.weight,\n",
    "                                                      pos_weight=self.pos_weight,\n",
    "                                                      reduction='sum')/torch.tensor(input.shape[0])\n",
    "        else:\n",
    "            return F.binary_cross_entropy_with_logits(input, target,\n",
    "                                                      self.weight,\n",
    "                                                      pos_weight=self.pos_weight,\n",
    "                                                      reduction=self.reduction)\n",
    "\n",
    "    def decodes(self, x:Tensor) -> Tensor:\n",
    "        \"Converts model output to target format\"\n",
    "        return x>self.thresh\n",
    "\n",
    "    def activation(self, x:Tensor) -> Tensor:\n",
    "        \"`nn.BCEWithLogitsLoss`'s fused activation function applied to model output\"\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Balanced -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "# Refactored from https://github.com/vandit15/Class-balanced-loss-pytorch\n",
    "# Class-balanced-loss-pytorch - MIT License - Copyright (c) 2019 Vandit Jain\n",
    "\n",
    "class ClassBalanced(TL._WeightedLoss):\n",
    "    \"Class Balanced weight calculation, from https://arxiv.org/abs/1901.05555.\"\n",
    "    def __init__(self,\n",
    "        samples_per_class:Tensor,\n",
    "        beta:float=0.99,\n",
    "        reduction:str='mean',\n",
    "    ):\n",
    "        num_classes = len(samples_per_class)\n",
    "        if not isinstance(samples_per_class, Tensor):\n",
    "            samples_per_class = Tensor(samples_per_class)\n",
    "\n",
    "        effective_num = 1.0 - torch.pow(beta, samples_per_class)\n",
    "        weight = (1.0 - beta) / effective_num\n",
    "        weight = weight / weight.sum() * num_classes\n",
    "\n",
    "        super().__init__(weight, None, None, reduction)\n",
    "\n",
    "    @delegates(nn.Module.to)\n",
    "    def to(self, *args, **kwargs):\n",
    "        device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(*args, **kwargs)\n",
    "        if self.weight.device != device: self.weight = self.weight.to(device=device)\n",
    "        super().to(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ClassBalancedCrossEntropyLoss(ClassBalanced):\n",
    "    \"Class Balanced Cross Entropy Loss, from https://arxiv.org/abs/1901.05555.\"\n",
    "    __constants__ = ['ignore_index', 'reduction', 'label_smoothing']\n",
    "    ignore_index: int\n",
    "    label_smoothing: float\n",
    "\n",
    "    def __init__(self,\n",
    "        samples_per_class:Tensor,\n",
    "        beta:float=0.99,\n",
    "        ignore_index:int=-100,\n",
    "        reduction:str='mean',\n",
    "        label_smoothing:float=0.0,\n",
    "        axis:int=-1\n",
    "    ):\n",
    "        super().__init__(samples_per_class, beta, reduction)\n",
    "        self.ignore_index = ignore_index\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.axis=axis\n",
    "\n",
    "    def forward(self, input:Tensor, target:Tensor) -> Tensor:\n",
    "        return F.cross_entropy(input, target, weight=self.weight,\n",
    "                               ignore_index=self.ignore_index, reduction=self.reduction,\n",
    "                               label_smoothing=self.label_smoothing)\n",
    "\n",
    "    def decodes(self, x:Tensor):\n",
    "        \"Converts model output to target format\"\n",
    "        return x.argmax(dim=self.axis)\n",
    "\n",
    "    def activation(self, x:Tensor):\n",
    "        \"`nn.CrossEntropyLoss`'s fused activation function applied to model output\"\n",
    "        return F.softmax(x, dim=self.axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ClassBalancedBCEWithLogitsLoss(ClassBalanced):\n",
    "    \"Class Balanced BCE With Logits Loss, from https://arxiv.org/abs/1901.05555 with 'batchmean' reduction\"\n",
    "\n",
    "    def __init__(self,\n",
    "        samples_per_class:Tensor,\n",
    "        beta:float=0.99,\n",
    "        reduction:str='mean',\n",
    "        thresh:float=0.5, # Threshold for `decodes`\n",
    "    ):\n",
    "        super().__init__(samples_per_class, beta, reduction)\n",
    "        self.thresh=thresh\n",
    "\n",
    "    def forward(self, input:Tensor, target:Tensor) -> Tensor:\n",
    "        if self.reduction == 'batchmean':\n",
    "            return F.binary_cross_entropy_with_logits(input, target,\n",
    "                                                      self.weight,\n",
    "                                                      pos_weight=self.pos_weight,\n",
    "                                                      reduction='sum')/torch.tensor(input.shape[0])\n",
    "        else:\n",
    "            return F.binary_cross_entropy_with_logits(input, target,\n",
    "                                                      self.weight,\n",
    "                                                      pos_weight=self.pos_weight,\n",
    "                                                      reduction=self.reduction)\n",
    "\n",
    "    def decodes(self, x:Tensor) -> Tensor:\n",
    "        \"Converts model output to target format\"\n",
    "        return x>self.thresh\n",
    "\n",
    "    def activation(self, x:Tensor) -> Tensor:\n",
    "        \"`nn.BCEWithLogitsLoss`'s fused activation function applied to model output\"\n",
    "        return torch.sigmoid(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
