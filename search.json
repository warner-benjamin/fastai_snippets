[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "fastxtend",
    "section": "",
    "text": "fastxtend accelerates fastai\nTrain fastai models faster with fastxtend’s fused optimizers, Progressive Resizing callback, integrated FFCV DataLoader, and integrated PyTorch Compile support."
  },
  {
    "objectID": "index.html#feature-overview",
    "href": "index.html#feature-overview",
    "title": "fastxtend",
    "section": "Feature overview",
    "text": "Feature overview\nTrain Models Faster\n\nDrop in fused optimizers, which are 21 to 293 percent faster then fastai native optimizers.\nUp to 75% optimizer memory savings with integrated bitsandbytes 8-bit optimizers.\nIncrease GPU throughput and decrease training time with the Progressive Resizing callback.\nUse the highly optimized FFCV DataLoader, fully integrated with fastai.\nIntegrated support for torch.compile via the Compile callbacks.\n\nGeneral Features\n\nFused implementations of modern optimizers, such as Adan, Lion, & StableAdam.\nHugging Face Transformers compatibility with fastai\nFlexible metrics which can log on train, valid, or both. Backwards compatible with fastai metrics.\nEasily use multiple losses and log each individual loss on train and valid.\nMultiple profilers for profiling training and identifying bottlenecks.\nA fast Exponential Moving Average callback for smoother training.\n\nVision\n\nApply MixUp, CutMix, or Augmentations at once with CutMixUp or CutMixUpAugment.\nAdditional image augmentations.\nSupport for running fastai batch transforms on CPU.\nMore attention and pooling modules\nA flexible implementation of fastai’s XResNet.\n\nCheck out the documentation for additional splitters, callbacks, schedulers, utilities, and more."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "fastxtend",
    "section": "Install",
    "text": "Install\nfastxtend is avalible on pypi:\npip install fastxtend\nfastxtend can be installed with task-specific dependencies for vision, ffcv, text, audio, or all:\npip install \"fastxtend[all]\"\nTo easily install most prerequisites for all fastxtend features, use Conda or Miniconda:\nconda create -n fastxtend python=3.11 \"pytorch&gt;=2.1\" torchvision torchaudio \\\npytorch-cuda=12.1 fastai nbdev pkg-config libjpeg-turbo opencv tqdm psutil \\\nterminaltables numpy \"numba&gt;=0.57\" librosa timm kornia rich typer wandb \\\n\"transformers&gt;=4.34\" \"tokenizers&gt;=0.14\" \"datasets&gt;=2.14\" ipykernel ipywidgets \\\n\"matplotlib&lt;3.8\" -c pytorch -c nvidia -c fastai -c huggingface -c conda-forge\n\nconda activate fastxtend\n\npip install \"fastxtend[all]\"\nreplacing pytorch-cuda=12.1 with your preferred supported version of Cuda.\nTo create an editable development install:\ngit clone https://github.com/warner-benjamin/fastxtend.git\ncd fastxtend\npip install -e \".[dev]\""
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "fastxtend",
    "section": "Usage",
    "text": "Usage\nLike fastai, fastxtend provides safe wildcard imports using python’s __all__.\nfrom fastai.vision.all import *\nfrom fastxtend.vision.all import *\nfrom fastxtend.ffcv.all import *\nIn general, import fastxtend after all fastai imports, as fastxtend modifies fastai. Any method modified by fastxtend is backwards compatible with the original fastai code."
  },
  {
    "objectID": "index.html#examples",
    "href": "index.html#examples",
    "title": "fastxtend",
    "section": "Examples",
    "text": "Examples\nUse a fused ForEach optimizer:\nLearner(..., opt_func=adam(foreach=True))\nOr a bitsandbytes 8-bit optimizer:\nLearner(..., opt_func=adam(eightbit=True))\nSpeed up image training using Progressive Resizing:\nLearner(... cbs=ProgressiveResize())\nLog an accuracy metric on the training set as a smoothed metric and validation set like normal:\nLearner(..., metrics=[Accuracy(log_metric=LogMetric.Train, metric_type=MetricType.Smooth),\n                      Accuracy()])\nLog multiple losses as individual metrics on train and valid:\nmloss = MultiLoss(loss_funcs=[nn.MSELoss, nn.L1Loss],\n                  weights=[1, 3.5], loss_names=['mse_loss', 'l1_loss'])\n\nLearner(..., loss_func=mloss, metrics=RMSE(), cbs=MultiLossCallback)\nCompile a model with torch.compile:\nfrom fastxtend.callback import compiler\n\nlearn = Learner(...).compile()\nProfile a fastai training loop:\nfrom fastxtend.callback import simpleprofiler\n\nlearn = Learner(...).profile()\nlearn.fit_one_cycle(2, 3e-3)"
  },
  {
    "objectID": "index.html#benchmark",
    "href": "index.html#benchmark",
    "title": "fastxtend",
    "section": "Benchmark",
    "text": "Benchmark\nTo run the benchmark on your own machine, see the example scripts for details on how to replicate."
  },
  {
    "objectID": "callback.progresize.html",
    "href": "callback.progresize.html",
    "title": "Progressive Resizing",
    "section": "",
    "text": "ProgressiveResize is inspired by MosaicML’s Progressive Resizing algorithm for Composer which in turn was inspired by fastai’s manual progressive resizing.\nProgressive Resizing decreases model training time by training on smaller images then gradually increasing to the full image size. This allows training on more samples for the same compute budget, often leading to higher performance then training on full sized images.\nsource"
  },
  {
    "objectID": "callback.progresize.html#example",
    "href": "callback.progresize.html#example",
    "title": "Progressive Resizing",
    "section": "Example",
    "text": "Example\nIn this example1, a xresnext50 is trained for 20 & 25 epochs on Imagenette at an image size of 224 pixels. Due to the short training run and small dataset, ProgressiveResize in batch mode is set to increase_by=16.\nProgressiveResize yields significant training time savings compared to training at full size. At a normalized compute budget of roughly 6.5 minutes, Progressive Resizing results with 92.7% accuracy compared to 92% accuracy with full sized training.\n\n\n\nMode\nEpochs\nTime (Mins)\nAccuracy\n\n\n\n\nFull Size\n20\n6.5\n92.0%\n\n\nProgressive Batch\n20\n5.2\n92.3%\n\n\nProgressive Epoch\n20\n5.2\n91.8%\n\n\nProgressive Batch\n25\n6.5\n92.7%\n\n\n\nDue to the regularization effect of training on different sized images, Progressive Resizing with increase_by=16 outperforms full sized training by 0.3% in 25 percent less timeon the same number of epochs2 .\n\nProgressive Resizing\nThere are two Progressive Resizing IncreaseMode types:\n\nincrease_mode=IncreaseMode.Batch\nincrease_mode=IncreaseMode.Epoch\n\nthis example will show both.\n\nBatch Resizing\nProgressiveResize with the default increase_mode=IncreaseMode.Batch.\n\nimagenette = untar_data(URLs.IMAGENETTE_320)\n\nwith less_random():\n    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                       splitter=GrandparentSplitter(valid_name='val'),\n                       get_items=get_image_files, get_y=parent_label,\n                       item_tfms=Resize(224),\n                       batch_tfms=[*aug_transforms(),\n                                   Normalize.from_stats(*imagenet_stats)])\n\n    dls = dblock.dataloaders(imagenette, bs=64,\n                             num_workers=num_cpus(), pin_memory=True)\n\n    learn = Learner(dls, xresnext50(n_out=dls.c), opt_func=ranger(foreach=True),\n                    loss_func=nn.CrossEntropyLoss(label_smoothing=0.1),\n                    metrics=Accuracy()).to_channelslast()\n\n    start = time.perf_counter()\n    learn.fit_flat_cos(20, 8e-3, cbs=ProgressiveResize(increase_by=16))\n    total = time.perf_counter() - start\n    print(f'Total training time: {scale_time(total)}')\n\nProgressively increase the initial image size of [112, 112] by 16 pixels every 0.8333 epochs for 7 resizes. \nStarting at epoch 10 and finishing at epoch 15 for a final training size of [224, 224].\nTotal training time: 311.8 s\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.670977\n1.883999\n0.454268\n00:13\n\n\n1\n1.403678\n1.226364\n0.710573\n00:13\n\n\n2\n1.251599\n1.446574\n0.626497\n00:13\n\n\n3\n1.136825\n1.079901\n0.768662\n00:13\n\n\n4\n1.062239\n1.250891\n0.718981\n00:13\n\n\n5\n1.006945\n0.955187\n0.820127\n00:13\n\n\n6\n0.957047\n1.238453\n0.703439\n00:13\n\n\n7\n0.910177\n0.900485\n0.842548\n00:13\n\n\n8\n0.889880\n0.963289\n0.816560\n00:13\n\n\n9\n0.860453\n0.881689\n0.849936\n00:13\n\n\n10\n0.839035\n0.952776\n0.828535\n00:14\n\n\n11\n0.815573\n0.857969\n0.863439\n00:14\n\n\n12\n0.791430\n0.863359\n0.858089\n00:15\n\n\n13\n0.770404\n0.786469\n0.887898\n00:16\n\n\n14\n0.772790\n0.848503\n0.866242\n00:17\n\n\n15\n0.745845\n0.774377\n0.890955\n00:19\n\n\n16\n0.704179\n0.769880\n0.891974\n00:19\n\n\n17\n0.640226\n0.731178\n0.914395\n00:19\n\n\n18\n0.604006\n0.706209\n0.920764\n00:19\n\n\n19\n0.584977\n0.698062\n0.922548\n00:19\n\n\n\n\n\n\n\nEpoch Resizing\nProgressiveResize with increase_mode=IncreaseMode.Epoch.\n\nimagenette = untar_data(URLs.IMAGENETTE_320)\n\nwith less_random():\n    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                       splitter=GrandparentSplitter(valid_name='val'),\n                       get_items=get_image_files, get_y=parent_label,\n                       item_tfms=Resize(224),\n                       batch_tfms=[*aug_transforms(),\n                                   Normalize.from_stats(*imagenet_stats)])\n\n    dls = dblock.dataloaders(imagenette, bs=64,\n                             num_workers=num_cpus(), pin_memory=True)\n\n    learn = Learner(dls, xresnext50(n_out=dls.c), opt_func=ranger(foreach=True),\n                    loss_func=nn.CrossEntropyLoss(label_smoothing=0.1),\n                    metrics=Accuracy()).to_channelslast()\n\n    start = time.perf_counter()\n    learn.fit_flat_cos(20, 8e-3,\n                       cbs=ProgressiveResize(increase_mode=IncreaseMode.Epoch))\n    total = time.perf_counter() - start\n    print(f'Total training time: {scale_time(total)}')\n\nProgressively increase the initial image size of [112, 112] by 28 pixels every 1 epoch for 4 resizes.\nStarting at epoch 12 and finishing at epoch 15 for a final training size of [224, 224].\nTotal training time: 309.3 s\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.670977\n1.883999\n0.454268\n00:13\n\n\n1\n1.403678\n1.226364\n0.710573\n00:13\n\n\n2\n1.251599\n1.446574\n0.626497\n00:13\n\n\n3\n1.136825\n1.079901\n0.768662\n00:13\n\n\n4\n1.062239\n1.250891\n0.718981\n00:13\n\n\n5\n1.006945\n0.955187\n0.820127\n00:13\n\n\n6\n0.957047\n1.238453\n0.703439\n00:13\n\n\n7\n0.910177\n0.900485\n0.842548\n00:13\n\n\n8\n0.889880\n0.963289\n0.816560\n00:13\n\n\n9\n0.860453\n0.881689\n0.849936\n00:14\n\n\n10\n0.839285\n0.916867\n0.835159\n00:13\n\n\n11\n0.817720\n0.837916\n0.866242\n00:13\n\n\n12\n0.792356\n0.844864\n0.869045\n00:14\n\n\n13\n0.780980\n0.811714\n0.878471\n00:15\n\n\n14\n0.780541\n0.870851\n0.853758\n00:17\n\n\n15\n0.766215\n0.788430\n0.888153\n00:19\n\n\n16\n0.709244\n0.788267\n0.887134\n00:19\n\n\n17\n0.649643\n0.732368\n0.915159\n00:19\n\n\n18\n0.611495\n0.717171\n0.915414\n00:19\n\n\n19\n0.590308\n0.708605\n0.918471\n00:19\n\n\n\n\n\n\n\n\nNormal Training\nfastai model training without Progressive Resizing.\n\nimagenette = untar_data(URLs.IMAGENETTE_320)\n\nwith less_random():\n    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                       splitter=GrandparentSplitter(valid_name='val'),\n                       get_items=get_image_files, get_y=parent_label,\n                       item_tfms=Resize(224),\n                       batch_tfms=[*aug_transforms(),\n                                   Normalize.from_stats(*imagenet_stats)])\n\n    dls = dblock.dataloaders(imagenette, bs=64,\n                             num_workers=num_cpus(), pin_memory=True)\n\n    learn = Learner(dls, xresnext50(n_out=dls.c), opt_func=ranger(foreach=True),\n                    loss_func=nn.CrossEntropyLoss(label_smoothing=0.1),\n                    metrics=Accuracy()).to_channelslast()\n\n    start = time.perf_counter()\n    learn.fit_flat_cos(20, 8e-3)\n    total = time.perf_counter() - start\n    print(f'Total training time: {scale_time(total)}')\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.693837\n1.660484\n0.539873\n00:19\n\n\n1\n1.425402\n1.288508\n0.682548\n00:19\n\n\n2\n1.249855\n1.231204\n0.726879\n00:19\n\n\n3\n1.107746\n1.027718\n0.794904\n00:19\n\n\n4\n1.046856\n1.113385\n0.782420\n00:19\n\n\n5\n0.974740\n1.055205\n0.800255\n00:19\n\n\n6\n0.933220\n1.195850\n0.756688\n00:19\n\n\n7\n0.880307\n0.905752\n0.845096\n00:19\n\n\n8\n0.854195\n1.113956\n0.772229\n00:19\n\n\n9\n0.839854\n0.838828\n0.868790\n00:19\n\n\n10\n0.815095\n0.868798\n0.861146\n00:19\n\n\n11\n0.786958\n0.839955\n0.867771\n00:19\n\n\n12\n0.763221\n0.884713\n0.853758\n00:19\n\n\n13\n0.751375\n0.780010\n0.890955\n00:19\n\n\n14\n0.740009\n0.835440\n0.872102\n00:19\n\n\n15\n0.710022\n0.793270\n0.888408\n00:19\n\n\n16\n0.683471\n0.743397\n0.909809\n00:19\n\n\n17\n0.630301\n0.735851\n0.910828\n00:19\n\n\n18\n0.596394\n0.715150\n0.916688\n00:19\n\n\n19\n0.577142\n0.701738\n0.918981\n00:19\n\n\n\n\n\nTotal training time: 387.2 s\n\n\n\n\nProgressive Resizing with Normalized Compute Budget\nProgressiveResize with the default increase_mode=IncreaseMode.Batch trained to match the Normal Training’s compute.\n\nimagenette = untar_data(URLs.IMAGENETTE_320)\n\nwith less_random():\n    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                       splitter=GrandparentSplitter(valid_name='val'),\n                       get_items=get_image_files, get_y=parent_label,\n                       item_tfms=Resize(224),\n                       batch_tfms=[*aug_transforms(),\n                                   Normalize.from_stats(*imagenet_stats)])\n\n    dls = dblock.dataloaders(imagenette, bs=64,\n                             num_workers=num_cpus(), pin_memory=True)\n\n    learn = Learner(dls, xresnext50(n_out=dls.c), opt_func=ranger(foreach=True),\n                    loss_func=nn.CrossEntropyLoss(label_smoothing=0.1),\n                    metrics=Accuracy()).to_channelslast()\n\n    start = time.perf_counter()\n    learn.fit_flat_cos(25, 8e-3, cbs=ProgressiveResize(increase_by=16))\n    total = time.perf_counter() - start\n    print(f'Total training time: {scale_time(total)}')\n\nProgressively increase the initial image size of [112, 112] by 16 pixels every 1.042 epochs for 7 resizes. \nStarting at epoch 12.5 and finishing at epoch 18.75 for a final training size of [224, 224].\nTotal training time: 390.5 s\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.670977\n1.883999\n0.454268\n00:13\n\n\n1\n1.403678\n1.226364\n0.710573\n00:13\n\n\n2\n1.251599\n1.446574\n0.626497\n00:13\n\n\n3\n1.136825\n1.079901\n0.768662\n00:13\n\n\n4\n1.062239\n1.250891\n0.718981\n00:13\n\n\n5\n1.006945\n0.955187\n0.820127\n00:13\n\n\n6\n0.957047\n1.238453\n0.703439\n00:13\n\n\n7\n0.910177\n0.900485\n0.842548\n00:13\n\n\n8\n0.889880\n0.963289\n0.816560\n00:13\n\n\n9\n0.860453\n0.881689\n0.849936\n00:13\n\n\n10\n0.839285\n0.916867\n0.835159\n00:13\n\n\n11\n0.817720\n0.837916\n0.866242\n00:13\n\n\n12\n0.806093\n0.869887\n0.850701\n00:14\n\n\n13\n0.780977\n0.805412\n0.877962\n00:14\n\n\n14\n0.766974\n0.899283\n0.839490\n00:14\n\n\n15\n0.757296\n0.811422\n0.878726\n00:15\n\n\n16\n0.736302\n0.855174\n0.853758\n00:15\n\n\n17\n0.723357\n0.769306\n0.901401\n00:16\n\n\n18\n0.714021\n0.765733\n0.895287\n00:18\n\n\n19\n0.697444\n0.736115\n0.911847\n00:19\n\n\n20\n0.663537\n0.790711\n0.881783\n00:19\n\n\n21\n0.617896\n0.712593\n0.919745\n00:19\n\n\n22\n0.583567\n0.710089\n0.918471\n00:19\n\n\n23\n0.562689\n0.685103\n0.927643\n00:19\n\n\n24\n0.551753\n0.686037\n0.926879\n00:19"
  },
  {
    "objectID": "callback.progresize.html#footnotes",
    "href": "callback.progresize.html#footnotes",
    "title": "Progressive Resizing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAll models are trained on a GeForce 3080 Ti using PyTorch 1.13.1 and Cuda 11.7. Results may differ with other datasets, hardware, and across runs.↩︎\nWhile Progressive Resizing can sometimes outperform full sized trained model in the same number of epochs, it is just as likely to perform worse, depending on setup.↩︎"
  },
  {
    "objectID": "audio.03_augment.html",
    "href": "audio.03_augment.html",
    "title": "Audio Data Augmentation",
    "section": "",
    "text": "Item transforms which run on the CPU.\n\nsource\n\n\n\n Flip (p:float=0.5)\n\nRandomly flip TensorAudio with probability p\n\nwith less_random():\n    _,axs = plt.subplots(1,2,figsize=(8,4))\n    f = Flip()\n    for ax in axs: f(audio, split_idx=0).show(ctx=ax, hear=False)\n\n\n\n\n\nsource\n\n\n\n\n Roll (p:float=0.5, max_roll:float=0.5)\n\nRandomly shift TensorAudio with rollover\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nfloat\n0.5\nPer-item probability\n\n\nmax_roll\nfloat\n0.5\nMaximum shift\n\n\n\n\nwith less_random():\n    _,axs = plt.subplots(1,3,figsize=(12,4))\n    f = Roll(p=1,max_roll=0.5)\n    for ax in axs: f(audio, split_idx=0).show(ctx=ax, hear=False)\n\n\n\n\n\nsource\n\n\n\n\n AudioPadMode (value, names=None, module=None, qualname=None, type=None,\n               start=1)\n\nAn enumeration.\n\nsource\n\n\n\n\n RandomCropPad (duration:Numeric|None=None, samples:int|None=None,\n                padmode:AudioPadMode=&lt;AudioPadMode.Repeat: 4&gt;,\n                constant:Numeric=0)\n\nRandomly resize TensorAudio to specified length. Center crop during valid.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nduration\nNumeric | None\nNone\nCrop length in seconds\n\n\nsamples\nint | None\nNone\nCrop length in samples\n\n\npadmode\nAudioPadMode\nAudioPadMode.Repeat\nHow to pad if necessary\n\n\nconstant\nNumeric\n0\nValue for AudioPadMode.Constant\n\n\n\nRandomCropPad will pad using one of the following five modes if the input length is less than duration or samples.\n\nwith less_random():\n    _,axs = plt.subplots(1,4,figsize=(18,4))\n    for ax,padmode in zip(axs.flatten(), [AudioPadMode.Constant, AudioPadMode.ConstantPre,\n                                          AudioPadMode.ConstantPost, AudioPadMode.Repeat]\n    ):\n        rcp = RandomCropPad(4, padmode=padmode)\n        rcp(audio, split_idx=1).show(ctx=ax, title=padmode, hear=False)\n        plt.tight_layout()\n\n\n\n\nDuring training, RandomCropPad will randomly crop if the input length is greater than duration or samples.\n\nwith less_random():\n    _,axs = plt.subplots(1,3,figsize=(12,4))\n    rcp = RandomCropPad(1.2)\n    for ax in axs: rcp(audio, split_idx=0).show(ctx=ax, hear=False)\n\n\n\n\nDuring validation or prediction, RandomCropPad will center crop if the input length is greater than duration or samples.\n\nwith less_random():\n    _,axs = plt.subplots(1,3,figsize=(12,4))\n    rcp = RandomCropPad(1.2)\n    for ax in axs: rcp(audio, split_idx=1).show(ctx=ax, hear=False)\n\n\n\n\n\nsource\n\n\n\n\n VolumeMode (value, names=None, module=None, qualname=None, type=None,\n             start=1)\n\nAn enumeration.\n\nsource\n\n\n\n\n Volume (p:float=0.75, gain:Numeric|None=None,\n         gain_range:tuple[Numeric,Numeric]=(-18, 6),\n         volmode:VolumeMode=&lt;VolumeMode.DB: 1&gt;)\n\nRandomly change TensorAudio’s volume\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nfloat\n0.75\nPer-item probability\n\n\ngain\nNumeric | None\nNone\nGain is a positive amplitude ratio, a power (voltage squared), or in decibels. If none, randomly select from gain_range.\n\n\ngain_range\ntuple[Numeric, Numeric]\n(-18, 6)\nRandom range for gain\n\n\nvolmode\nVolumeMode\nVolumeMode.DB\nOne of VolumeMode.DB, VolumeMode.Amplitude, or VolumeMode.Power\n\n\n\n\n_,axs = plt.subplots(1,4,figsize=(18,4))\nfor ax,gain in zip(axs.flatten(), [-18, -6, 0, 6]):\n    vol = Volume(p=1., gain=gain)\n    vol(audio, split_idx=0).show(ctx=ax, title=f'{gain} DB', hear=False)\n\n\n\n\n\nsource\n\n\n\n\n PeakNorm (p:float=0.1)\n\nRandomly apply constant gain so TensorAudio’s loudest level is between -1 and 1 with probability p\n\n_,axs = plt.subplots(1,1,figsize=(4,4))\nPeakNorm(p=1.)(audio, split_idx=0).show(ctx=axs, hear=False)\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nsource\n\n\n\n\n VolumeOrPeakNorm (p:float=0.75, peak_p:float=0.1, gain:Numeric|None=None,\n                   gain_range:tuple[Numeric,Numeric]=(-18, 6),\n                   volmode:VolumeMode=&lt;VolumeMode.DB: 1&gt;)\n\nRandomly apply Volume or Peak to TensorAudio in one transform\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nfloat\n0.75\nPer-item probability\n\n\npeak_p\nfloat\n0.1\nProbability of applying Peak\n\n\ngain\nNumeric | None\nNone\nGain is a positive amplitude ratio, a power (voltage squared), or in decibels. If none, randomly select from gain_range.\n\n\ngain_range\ntuple[Numeric, Numeric]\n(-18, 6)\nRandom range for gain\n\n\nvolmode\nVolumeMode\nVolumeMode.DB\nOne of VolumeMode.DB, VolumeMode.Amplitude, or VolumeMode.Power\n\n\n\n\nsource\n\n\n\n\n Noise (p=0.25, noise_level:float|None=None,\n        noise_range:tuple[float,float]=(0.01, 0.1),\n        color:NoiseColor|None=None)\n\nAdds noise of specified color and level to TensorAudio relative to mean audio level\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nfloat\n0.25\nPer-item probability\n\n\nnoise_level\nfloat | None\nNone\nLoudness of noise, if None randomly selects from noise_range\n\n\nnoise_range\ntuple[float, float]\n(0.01, 0.1)\nRange of noise loudness values\n\n\ncolor\nNoiseColor | None\nNone\nColor of noise to add, if None randomly selects from NoiseColor\n\n\n\n\nwith less_random():\n    cn = NoiseColor.White\n    _,axs = plt.subplots(1,4,figsize=(22,4))\n    plt.suptitle(f'{cn}')\n    for ax,nl in zip(axs.flatten(), [0.02, 0.05, 0.1, 0.2]):\n        ps = Noise(p=1., noise_level=nl, color=cn)\n        audio = TensorAudio.create(TEST_AUDIO)\n        ps(audio, split_idx=0).show(ctx=ax, title=f'noise_level: {nl}', hear=False)"
  },
  {
    "objectID": "audio.03_augment.html#tensoraudio-item-transforms",
    "href": "audio.03_augment.html#tensoraudio-item-transforms",
    "title": "Audio Data Augmentation",
    "section": "",
    "text": "Item transforms which run on the CPU.\n\nsource\n\n\n\n Flip (p:float=0.5)\n\nRandomly flip TensorAudio with probability p\n\nwith less_random():\n    _,axs = plt.subplots(1,2,figsize=(8,4))\n    f = Flip()\n    for ax in axs: f(audio, split_idx=0).show(ctx=ax, hear=False)\n\n\n\n\n\nsource\n\n\n\n\n Roll (p:float=0.5, max_roll:float=0.5)\n\nRandomly shift TensorAudio with rollover\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nfloat\n0.5\nPer-item probability\n\n\nmax_roll\nfloat\n0.5\nMaximum shift\n\n\n\n\nwith less_random():\n    _,axs = plt.subplots(1,3,figsize=(12,4))\n    f = Roll(p=1,max_roll=0.5)\n    for ax in axs: f(audio, split_idx=0).show(ctx=ax, hear=False)\n\n\n\n\n\nsource\n\n\n\n\n AudioPadMode (value, names=None, module=None, qualname=None, type=None,\n               start=1)\n\nAn enumeration.\n\nsource\n\n\n\n\n RandomCropPad (duration:Numeric|None=None, samples:int|None=None,\n                padmode:AudioPadMode=&lt;AudioPadMode.Repeat: 4&gt;,\n                constant:Numeric=0)\n\nRandomly resize TensorAudio to specified length. Center crop during valid.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nduration\nNumeric | None\nNone\nCrop length in seconds\n\n\nsamples\nint | None\nNone\nCrop length in samples\n\n\npadmode\nAudioPadMode\nAudioPadMode.Repeat\nHow to pad if necessary\n\n\nconstant\nNumeric\n0\nValue for AudioPadMode.Constant\n\n\n\nRandomCropPad will pad using one of the following five modes if the input length is less than duration or samples.\n\nwith less_random():\n    _,axs = plt.subplots(1,4,figsize=(18,4))\n    for ax,padmode in zip(axs.flatten(), [AudioPadMode.Constant, AudioPadMode.ConstantPre,\n                                          AudioPadMode.ConstantPost, AudioPadMode.Repeat]\n    ):\n        rcp = RandomCropPad(4, padmode=padmode)\n        rcp(audio, split_idx=1).show(ctx=ax, title=padmode, hear=False)\n        plt.tight_layout()\n\n\n\n\nDuring training, RandomCropPad will randomly crop if the input length is greater than duration or samples.\n\nwith less_random():\n    _,axs = plt.subplots(1,3,figsize=(12,4))\n    rcp = RandomCropPad(1.2)\n    for ax in axs: rcp(audio, split_idx=0).show(ctx=ax, hear=False)\n\n\n\n\nDuring validation or prediction, RandomCropPad will center crop if the input length is greater than duration or samples.\n\nwith less_random():\n    _,axs = plt.subplots(1,3,figsize=(12,4))\n    rcp = RandomCropPad(1.2)\n    for ax in axs: rcp(audio, split_idx=1).show(ctx=ax, hear=False)\n\n\n\n\n\nsource\n\n\n\n\n VolumeMode (value, names=None, module=None, qualname=None, type=None,\n             start=1)\n\nAn enumeration.\n\nsource\n\n\n\n\n Volume (p:float=0.75, gain:Numeric|None=None,\n         gain_range:tuple[Numeric,Numeric]=(-18, 6),\n         volmode:VolumeMode=&lt;VolumeMode.DB: 1&gt;)\n\nRandomly change TensorAudio’s volume\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nfloat\n0.75\nPer-item probability\n\n\ngain\nNumeric | None\nNone\nGain is a positive amplitude ratio, a power (voltage squared), or in decibels. If none, randomly select from gain_range.\n\n\ngain_range\ntuple[Numeric, Numeric]\n(-18, 6)\nRandom range for gain\n\n\nvolmode\nVolumeMode\nVolumeMode.DB\nOne of VolumeMode.DB, VolumeMode.Amplitude, or VolumeMode.Power\n\n\n\n\n_,axs = plt.subplots(1,4,figsize=(18,4))\nfor ax,gain in zip(axs.flatten(), [-18, -6, 0, 6]):\n    vol = Volume(p=1., gain=gain)\n    vol(audio, split_idx=0).show(ctx=ax, title=f'{gain} DB', hear=False)\n\n\n\n\n\nsource\n\n\n\n\n PeakNorm (p:float=0.1)\n\nRandomly apply constant gain so TensorAudio’s loudest level is between -1 and 1 with probability p\n\n_,axs = plt.subplots(1,1,figsize=(4,4))\nPeakNorm(p=1.)(audio, split_idx=0).show(ctx=axs, hear=False)\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nsource\n\n\n\n\n VolumeOrPeakNorm (p:float=0.75, peak_p:float=0.1, gain:Numeric|None=None,\n                   gain_range:tuple[Numeric,Numeric]=(-18, 6),\n                   volmode:VolumeMode=&lt;VolumeMode.DB: 1&gt;)\n\nRandomly apply Volume or Peak to TensorAudio in one transform\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nfloat\n0.75\nPer-item probability\n\n\npeak_p\nfloat\n0.1\nProbability of applying Peak\n\n\ngain\nNumeric | None\nNone\nGain is a positive amplitude ratio, a power (voltage squared), or in decibels. If none, randomly select from gain_range.\n\n\ngain_range\ntuple[Numeric, Numeric]\n(-18, 6)\nRandom range for gain\n\n\nvolmode\nVolumeMode\nVolumeMode.DB\nOne of VolumeMode.DB, VolumeMode.Amplitude, or VolumeMode.Power\n\n\n\n\nsource\n\n\n\n\n Noise (p=0.25, noise_level:float|None=None,\n        noise_range:tuple[float,float]=(0.01, 0.1),\n        color:NoiseColor|None=None)\n\nAdds noise of specified color and level to TensorAudio relative to mean audio level\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nfloat\n0.25\nPer-item probability\n\n\nnoise_level\nfloat | None\nNone\nLoudness of noise, if None randomly selects from noise_range\n\n\nnoise_range\ntuple[float, float]\n(0.01, 0.1)\nRange of noise loudness values\n\n\ncolor\nNoiseColor | None\nNone\nColor of noise to add, if None randomly selects from NoiseColor\n\n\n\n\nwith less_random():\n    cn = NoiseColor.White\n    _,axs = plt.subplots(1,4,figsize=(22,4))\n    plt.suptitle(f'{cn}')\n    for ax,nl in zip(axs.flatten(), [0.02, 0.05, 0.1, 0.2]):\n        ps = Noise(p=1., noise_level=nl, color=cn)\n        audio = TensorAudio.create(TEST_AUDIO)\n        ps(audio, split_idx=0).show(ctx=ax, title=f'noise_level: {nl}', hear=False)"
  },
  {
    "objectID": "audio.03_augment.html#tensoraudio-batch-transforms",
    "href": "audio.03_augment.html#tensoraudio-batch-transforms",
    "title": "Audio Data Augmentation",
    "section": "TensorAudio Batch Transforms",
    "text": "TensorAudio Batch Transforms\nBatch transforms which run on the GPU.\n\nsource\n\nVolumeBatch\n\n VolumeBatch (p:float=0.5, gain:Numeric|None=None,\n              gain_range:tuple[Numeric,Numeric]=(-18, 6),\n              volmode:VolumeMode=&lt;VolumeMode.DB: 1&gt;)\n\nRandomly change TensorAudio’s volume on the GPU\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nfloat\n0.5\nPer-item probability\n\n\ngain\nNumeric | None\nNone\nGain is a positive amplitude ratio, a power (voltage squared), or in decibels. If none, randomly select from gain_range.\n\n\ngain_range\ntuple[Numeric, Numeric]\n(-18, 6)\nRandom range for gain\n\n\nvolmode\nVolumeMode\nVolumeMode.DB\nOne of VolumeMode.DB, VolumeMode.Amplitude, or VolumeMode.Power\n\n\n\n\nsource\n\n\nPitchShift\n\n PitchShift (p:float=0.2, semitones:tuple[float,float]=(-4.0, 4.0),\n             bins_per_octave:int=12,\n             padmode:AudioPadMode=&lt;AudioPadMode.Repeat: 4&gt;,\n             constant:Numeric=0, split:int|None=None)\n\nFast shift of TensorAudio’s pitch.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nfloat\n0.2\nPer-item probability\n\n\nsemitones\ntuple[float, float]\n(-4.0, 4.0)\nRandom pitch shift range in semitones to compute efficient shifts\n\n\nbins_per_octave\nint\n12\nNumber of steps per octave\n\n\npadmode\nAudioPadMode\nAudioPadMode.Repeat\nHow to pad if necessary\n\n\nconstant\nNumeric\n0\nValue for AudioPadMode.Constant\n\n\nsplit\nint | None\nNone\nApply transform to split items at a time. Use to prevent GPU OOM.\n\n\n\n\nwith less_random():\n    _,axs = plt.subplots(1,4,figsize=(22,4))\n    ps = PitchShift(p=1.)\n    for ax in axs.flatten():\n        audio = TensorAudio.create(TEST_AUDIO)\n        ps(audio.unsqueeze(0), split_idx=0).squeeze(0).show(ctx=ax, title=f'shift: {ps.shift}', hear=False)\n\n\n\n\n\nsource\n\n\nPitchShiftTA\n\n PitchShiftTA (p:float=0.2, n_steps:int|None=None,\n               n_step_range:tuple[int,int]=(2, 6), bins_per_octave:int=12,\n               n_fft:int=512, win_length:int|None=None,\n               hop_length:int|None=None,\n               window_fn:Callable[...,Tensor]=&lt;built-in method hann_window\n               of type object at 0x7f95e7c335c0&gt;, wkwargs:dict|None=None)\n\nShift the TensorAudio’s pitch using TorchAudio. Can be slower than PitchShift\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nfloat\n0.2\nPer-item probability\n\n\nn_steps\nint | None\nNone\nThe (fractional) steps to shift waveform\n\n\nn_step_range\ntuple[int, int]\n(2, 6)\nRandom n_steps range if n_steps is None\n\n\nbins_per_octave\nint\n12\nNumber of steps per octave\n\n\nn_fft\nint\n512\nSize of FFT, creates n_fft // 2 + 1 bins\n\n\nwin_length\nint | None\nNone\nWindow size. Defaults to n_fft\n\n\nhop_length\nint | None\nNone\nLength of hop between STFT windows. Defaults to win_length // 4\n\n\nwindow_fn\nCallable[…, Tensor]\nhann_window\nWindow tensor applied/multiplied to each frame/window\n\n\nwkwargs\ndict | None\nNone\nArgs for window_fn\n\n\n\n\n_,axs = plt.subplots(1,4,figsize=(22,4))\nfor ax,steps in zip(axs.flatten(), [2, 3, 5, 7]):\n    ps = PitchShiftTA(p=1., n_steps=steps)\n    audio = TensorAudio.create(TEST_AUDIO)\n    ps(audio.unsqueeze(0), split_idx=0).squeeze(0).show(ctx=ax, title=f'n_steps: {steps}', hear=False)\n\n\n\n\n\nsource\n\n\nTimeStretch\n\n TimeStretch (p:float=0.2, stretch_rates:tuple[float,float]=(0.5, 2.0),\n              padmode:AudioPadMode=&lt;AudioPadMode.Repeat: 4&gt;,\n              constant:Numeric=0, split:int|None=None)\n\nFast time stretch of TensorAudio\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nfloat\n0.2\nPer-item probability\n\n\nstretch_rates\ntuple[float, float]\n(0.5, 2.0)\nRandom time stretch range to compute efficient stretches. Defaults to 50%-200% speed\n\n\npadmode\nAudioPadMode\nAudioPadMode.Repeat\nHow to pad if necessary\n\n\nconstant\nNumeric\n0\nValue for AudioPadMode.Constant\n\n\nsplit\nint | None\nNone\nApply transform to split items at a time. Use to prevent GPU OOM.\n\n\n\n\nwith less_random():\n    _,axs = plt.subplots(1,4,figsize=(22,4))\n    ts = TimeStretch(p=1.)\n    for ax in axs.flatten():\n        audio = TensorAudio.create(TEST_AUDIO)\n        ts(audio.unsqueeze(0), split_idx=0).squeeze(0).show(ctx=ax, title=f'stretch: {ts.stretch}', hear=False)\n\n\n\n\n\nsource\n\n\nPitchShiftOrTimeStretch\n\n PitchShiftOrTimeStretch (p:float=0.4, semitones:tuple[float,float]=(-4.0,\n                          4.0), bins_per_octave:int=12,\n                          stretch_rates:tuple[float,float]=(0.5, 2.0),\n                          padmode:AudioPadMode=&lt;AudioPadMode.Repeat: 4&gt;,\n                          constant:Numeric=0, split:int|None=None)\n\nRandomly apply either PitchShift or TimeStretch to TensorAudio to minimize distortion\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nfloat\n0.4\nPer-item probability\n\n\nsemitones\ntuple[float, float]\n(-4.0, 4.0)\nRandom pitch shift range in semitones to compute efficient shifts\n\n\nbins_per_octave\nint\n12\nNumber of steps per octave\n\n\nstretch_rates\ntuple[float, float]\n(0.5, 2.0)\nRandom time stretch range to compute efficient stretches. Defaults to 50%-200% speed\n\n\npadmode\nAudioPadMode\nAudioPadMode.Repeat\nHow to pad if necessary\n\n\nconstant\nNumeric\n0\nValue for AudioPadMode.Constant\n\n\nsplit\nint | None\nNone\nApply transform to split items at a time. Use to prevent OOM.\n\n\n\n\nwith less_random(2998):\n    _,axs = plt.subplots(1,4,figsize=(22,4))\n    psts = PitchShiftOrTimeStretch(p=1.)\n    for ax in axs.flatten():\n        audio = TensorAudio.create(TEST_AUDIO)\n        psts(audio.unsqueeze(0), split_idx=0).squeeze(0).show(ctx=ax, title='stretch' if sum(psts.stretch_idxs) else 'pitch', hear=False)"
  },
  {
    "objectID": "audio.03_augment.html#tensormelspec-transforms",
    "href": "audio.03_augment.html#tensormelspec-transforms",
    "title": "Audio Data Augmentation",
    "section": "TensorMelspec Transforms",
    "text": "TensorMelspec Transforms\nBatch transforms which run on the GPU.\n\n# audio = TensorAudio.create(TEST_AUDIO)\nmel = MelSpectrogram(audio.sr, hop_length=1024, n_fft=1024, n_mels=112)(audio)\nmel.show(to_db=True)\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nsource\n\nTimeMasking\n\n TimeMasking (p:float=0.25, max_mask:float=0.2, iid_masks:bool=True,\n              mask_value:int|None=0)\n\nRandomly apply masking to a TensorSpec or TensorMelSpec in the time domain\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nfloat\n0.25\nPer-item probability\n\n\nmax_mask\nfloat\n0.2\nMaximum possible length of the mask [0, max_mask)\n\n\niid_masks\nbool\nTrue\nApply different masks to each example/channel in the batch\n\n\nmask_value\nint | None\n0\nIf None, random value between batch min and max\n\n\n\n\nwith less_random():\n    _,axs = plt.subplots(1,3,figsize=(12,4))\n    tm = TimeMasking(p=1., max_mask=0.5)\n    for ax in axs:\n        mel = MelSpectrogram(audio.sr, hop_length=1024, n_fft=1024, n_mels=112)(audio)\n        tm(mel.unsqueeze(0), split_idx=0).squeeze(0).show(ctx=ax, to_db=True)\n\n\n\n\n\nsource\n\n\nFrequencyMasking\n\n FrequencyMasking (p:float=0.25, max_mask:float=0.2, iid_masks:bool=True,\n                   mask_value:int|None=0)\n\nRandpmly apply masking to a TensorSpec or TensorMelSpec in the frequency domain\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nfloat\n0.25\nPer-item probability\n\n\nmax_mask\nfloat\n0.2\nMaximum possible length of the mask [0, max_mask)\n\n\niid_masks\nbool\nTrue\nApply different masks to each example/channel in the batch\n\n\nmask_value\nint | None\n0\nIf None, random value between batch min and max\n\n\n\n\nwith less_random():\n    _,axs = plt.subplots(1,3,figsize=(12,4))\n    fm = FrequencyMasking(p=1., max_mask=0.5)\n    for ax in axs:\n        mel = MelSpectrogram(audio.sr, hop_length=1024, n_fft=1024, n_mels=112)(audio)\n        fm(mel.unsqueeze(0), split_idx=0).squeeze(0).show(ctx=ax, to_db=True)\n\n\n\n\n\nsource\n\n\nAmplitudeToDBMode\n\n AmplitudeToDBMode (value, names=None, module=None, qualname=None,\n                    type=None, start=1)\n\nAn enumeration.\n\nsource\n\n\nAmplitudeToDB\n\n AmplitudeToDB (top_db:float|None=None,\n                mode:AmplitudeToDBMode=&lt;AmplitudeToDBMode.Power: 'power'&gt;)\n\nTurn a TensorSpec or TensorMelSpec from the power/amplitude scale to the decibel scale\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntop_db\nfloat | None\nNone\nMinimum negative cut-off in decibels. A reasonable number is 80.\n\n\nmode\nAmplitudeToDBMode\nAmplitudeToDBMode.Power\nPower is elementwise square of magnitude\n\n\n\nThis output depends on the maximum value in the input tensor, and so may return different values for an audio clip split into snippets vs. a a full clip. See torchaudio docs for details.\n\nwith less_random():\n    _,axs = plt.subplots(1,2,figsize=(12,4))\n    adb = AmplitudeToDB(top_db=80)\n    for i, ax in enumerate(axs):\n        if i==1:\n            mel = MelSpectrogram(audio.sr, hop_length=1024, n_fft=1024, n_mels=112)(audio)\n            adb(mel.unsqueeze(0), split_idx=0).squeeze(0).show(ctx=ax)\n        else:\n           mel = MelSpectrogram(audio.sr, hop_length=1024, n_fft=1024, n_mels=112)(audio)\n           mel.show(ctx=ax)\n\n\n\n\n\nsource\n\n\nAudioNormalize\n\n AudioNormalize (mean=None, std=None, n_spec=1, axes=(0, 2, 3))\n\nNormalize/denorm batch of TensorSpec or TensorMelSpec."
  },
  {
    "objectID": "vision.models.pooling.html",
    "href": "vision.models.pooling.html",
    "title": "Pooling",
    "section": "",
    "text": "source\n\nBlurPool\n\n BlurPool (stride:int=2, ks:int=3, padding:int=0, ndim:int=2,\n           ceil_mode:bool=False)\n\nCompute blur (anti-aliasing) and downsample a given feature map.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nstride\nint\n2\nThe stride size for pooling\n\n\nks\nint\n3\nThe kernel size for pooling\n\n\npadding\nint\n0\nUnused, for fastai compatibility\n\n\nndim\nint\n2\nUnused, for fastai compatibility\n\n\nceil_mode\nbool\nFalse\nUnused, for fastai compatibility\n\n\nReturns\nBlurPool2D\n\n\n\n\n\nStride and ks are reversed to match Average Pooling inputs in XResNet, where AveragePool2D(ks=2, stride=None) results in same output shape as BlurPool2D(ks=3, stride=2).\n\nsource\n\n\nMaxBlurPool\n\n MaxBlurPool (stride:int=2, ks:int=3, padding:int=0, ndim:int=2,\n              ceil_mode:int=True, max_ks:int=2)\n\nCompute pools and blurs and downsample a given feature map. Equivalent to nn.Sequential(nn.MaxPool2d(...), BlurPool2D(...))\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nstride\nint\n2\nThe stride size for blur pooling\n\n\nks\nint\n3\nThe kernel size for blur pooling\n\n\npadding\nint\n0\nUnused, for fastai compatibility\n\n\nndim\nint\n2\nUnused, for fastai compatibility\n\n\nceil_mode\nint\nTrue\nIf True, output size matches conv2d with same kernel size\n\n\nmax_ks\nint\n2\nThe kernel size for max pooling\n\n\nReturns\nMaxBlurPool2D\n\n\n\n\n\nStride and ks are reversed to match Average Pooling inputs in XResNet, where AveragePool2D(ks=2, stride=None) results in same output shape as MaxBlurPool2D(ks=3, stride=2)."
  },
  {
    "objectID": "optimizer.torchscript.html",
    "href": "optimizer.torchscript.html",
    "title": "TorchScript Optimizers",
    "section": "",
    "text": "fastxtend TorchScript optimizers are adapted from fastai optimizers and are modified to be compiled with TorchScript. They are 10 to 137 percent faster relative to fastai native optimizers depending on the model and optimizer, with complex optimizers like QHAdam recieving the largest performance increase.\nUnlike fastai optimizers, which are made of multiple stepper callbacks, TorchScript optimizers require a per-optimizer step so TorchScript can fuse the operation into as few CUDA calls as possible. All fastai optimizers have TorchScript implementations.\n\n\n\n\n\n\nImportant\n\n\n\nTorchScript optimizers have only been tested on PyTorch 1.12+ with NVFuser and are not guaranteed to work on older versions.\n\n\nTorchScript optimizers are faster due to vertical fusion across multiple Cuda calls. Using xresnet50 and SGD with momentum as an example, a TorchScript fused SGD step would (hopefully) fuse all three Cuda calls (mul, add, and add) into one or two Cuda kernels resulting in 167 or 334 Cuda calls.\n@torch.jit.script\ndef sgd_momentum_jit(param:Tensor, grad:Tensor, grad_avg:Tensor, lr:float):\n    grad_avg = grad_avg.mul(mom).add(grad)\n    param = param.add(grad_avg, alpha=-lr)\nIn contrast, a standard PyTorch optimizer would call the SGD with momentum step 167 times for a total of 501 inplace Cuda kernel calls:\ndef simple_momentum_standard(param:Tensor, grad_avg:Tensor, lr:float):\n    grad_avg.mul_(mom).add_(grad)\n    param.add_(param.grad, alpha=-lr)\nTorchScript optimizers are tested to be equal to fastai optimizers for 25 steps using nbdev’s GitHub CI.\n\nsource\n\nJitOptimizer\n\n JitOptimizer (params:Listified[Tensor], opt_step:Callable,\n               decouple_wd:bool=False, **defaults)\n\nAn Optimizer with a modified step for TorchScript optimizers\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparams\nListified[Tensor]\n\nModel parameters\n\n\nopt_step\nCallable\n\nJitOptimizer optimizer step\n\n\ndecouple_wd\nbool\nFalse\nUse decoupled weight decay or L2 regularization, if applicable\n\n\ndefaults\n\n\n\n\n\n\n\nsource\n\n\nsgd_jit_step\nsgd_jit_step\n\nsource\n\n\nrmsprop_jit_step\nrmsprop_jit_step\n\nsource\n\n\nadam_jit_step\nadam_jit_step\n\nsource\n\n\nradam_jit_step\nradam_jit_step\n\nsource\n\n\nqhadam_jit_step\nqhadam_jit_step\n\nsource\n\n\nlarc_jit_step\nlarc_jit_step\n\nsource\n\n\nlamb_jit_step\nlamb_jit_step\n\nsource\n\n\nJitLookahead\n\n JitLookahead (params:Listified[Tensor], opt_step:Callable,\n               decouple_wd:bool=False, **defaults)\n\nAn JitOptimizer with a modified step for Lookahead TorchScript optimizers\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparams\nListified[Tensor]\n\nModel parameters\n\n\nopt_step\nCallable\n\nJitLookahead optimizer step\n\n\ndecouple_wd\nbool\nFalse\nUse decoupled weight decay or L2 regularization, if applicable\n\n\ndefaults\n\n\n\n\n\n\n\nsource\n\n\nranger_jit_step\nranger_jit_step"
  },
  {
    "objectID": "data.loader.html",
    "href": "data.loader.html",
    "title": "DataLoader",
    "section": "",
    "text": "fastxtend’s DataLoaderMixin allows adding fastai functionality to non-fastai DataLoaders. DataLoaderMixin supports batch transforms, one_batch, show_batch, and show_results, although inputs will need to be converted to fastai typed tensors for show methods to work.\nFor an example of using DataLoaderMixin, look at the source code for Loader.\n\nsource\n\nDataLoaderMixin.one_batch\n\n DataLoaderMixin.one_batch ()\n\nReturn one processed batch of input(s) and target(s)\n\nsource\n\n\nDataLoaderMixin.show_batch\n\n DataLoaderMixin.show_batch (b:Optional[Tuple[torch.Tensor,...]]=None,\n                             max_n:int=9, ctxs=None, show:bool=True,\n                             unique:bool=False, **kwargs)\n\nShow max_n input(s) and target(s) from the batch.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nb\nTuple[Tensor, …] | None\nNone\nBatch to show. If None calls one_batch\n\n\nmax_n\nint\n9\nMaximum number of items to show\n\n\nctxs\nNoneType\nNone\nList of ctx objects to show data. Could be matplotlib axis, DataFrame etc\n\n\nshow\nbool\nTrue\nIf False, return decoded batch instead of showing\n\n\nunique\nbool\nFalse\nWhether to show only one\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\nDataLoaderMixin.show_results\n\n DataLoaderMixin.show_results (b, out, max_n:int=9, ctxs=None,\n                               show:bool=True, **kwargs)\n\nShow max_n results with input(s), target(s) and prediction(s).\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nb\n\n\nBatch to show results for\n\n\nout\n\n\nPredicted output from model for the batch\n\n\nmax_n\nint\n9\nMaximum number of items to show\n\n\nctxs\nNoneType\nNone\nList of ctx objects to show data. Could be matplotlib axis, DataFrame etc\n\n\nshow\nbool\nTrue\nIf False, return decoded batch instead of showing\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\nDataLoaderMixin.to\n\n DataLoaderMixin.to (device:Union[int,str,torch.device])\n\nSets self.device=device.\n\nsource\n\n\nDataLoaderMixin.n_inp\n\n DataLoaderMixin.n_inp ()\n\nNumber of elements in a batch for model input\n\nsource\n\n\nDataLoaderMixin.split_idx\n\n DataLoaderMixin.split_idx ()\n\n\nsource\n\n\nDataLoaderMixin.decode\n\n DataLoaderMixin.decode (b:Tuple[torch.Tensor,...])\n\nDecode batch b\n\nsource\n\n\nDataLoaderMixin.decode_batch\n\n DataLoaderMixin.decode_batch (b:Tuple[torch.Tensor,...], max_n:int=9)\n\nDecode up to max_n input(s) from batch b"
  },
  {
    "objectID": "vision.data.html",
    "href": "vision.data.html",
    "title": "Vision Data",
    "section": "",
    "text": "With enough DataLoader workers and fast CPU cores, using fastai’s GPU augmentations can lead to slower training then using CPU augmentations.\nImageCPUBlock and MaskCPUBlock are convenience methods for executing fastai batch transforms on CPU.\n\nsource\n\n\n\n PreBatchAsItem (enc=None, dec=None, split_idx=None, order=None)\n\nConverts Tensor from CHW to BCHW by adding a fake B dim\n\nsource\n\n\n\n\n PostBatchAsItem (enc=None, dec=None, split_idx=None, order=None)\n\nConverts Tensor from BCHW to CHW by removing the fake B dim\nPyTorch 2D image transforms expect tensors to be in BCHW format. PreBatchAsItem and PostBatchAsItem add and remove a B dim to a single Tensor Image so fastai batch transforms will work on individual items.\n\nsource\n\n\n\n\n ImageCPUBlock (cls:fastai.vision.core.PILBase=&lt;class\n                'fastai.vision.core.PILImage'&gt;)\n\nA TransformBlock for images of cls for running batch_tfms on CPU\n\nsource\n\n\n\n\n MaskCPUBlock (codes:Union[Iterable[~T],MutableSequence[~T],fastcore.found\n               ation.L,fastcore.basics.fastuple,NoneType]=None)\n\nA TransformBlock for segmentation masks, potentially with codes, for running batch_tfms on CPU\n\n\n\n\nwith no_random():\n    imagenette = untar_data(URLs.IMAGENETTE_320)\n\n    dblock = DataBlock(blocks=(ImageCPUBlock, CategoryBlock),\n                        splitter=GrandparentSplitter(valid_name='val'),\n                        get_items=get_image_files, get_y=parent_label,\n                        item_tfms=aug_transforms(size=128),\n                        batch_tfms=Normalize.from_stats(*imagenet_stats))\n    dls = dblock.dataloaders(imagenette, bs=9, num_workers=num_cpus())\n\n    dls.show_batch()"
  },
  {
    "objectID": "vision.data.html#batch-transforms-on-cpu",
    "href": "vision.data.html#batch-transforms-on-cpu",
    "title": "Vision Data",
    "section": "",
    "text": "With enough DataLoader workers and fast CPU cores, using fastai’s GPU augmentations can lead to slower training then using CPU augmentations.\nImageCPUBlock and MaskCPUBlock are convenience methods for executing fastai batch transforms on CPU.\n\nsource\n\n\n\n PreBatchAsItem (enc=None, dec=None, split_idx=None, order=None)\n\nConverts Tensor from CHW to BCHW by adding a fake B dim\n\nsource\n\n\n\n\n PostBatchAsItem (enc=None, dec=None, split_idx=None, order=None)\n\nConverts Tensor from BCHW to CHW by removing the fake B dim\nPyTorch 2D image transforms expect tensors to be in BCHW format. PreBatchAsItem and PostBatchAsItem add and remove a B dim to a single Tensor Image so fastai batch transforms will work on individual items.\n\nsource\n\n\n\n\n ImageCPUBlock (cls:fastai.vision.core.PILBase=&lt;class\n                'fastai.vision.core.PILImage'&gt;)\n\nA TransformBlock for images of cls for running batch_tfms on CPU\n\nsource\n\n\n\n\n MaskCPUBlock (codes:Union[Iterable[~T],MutableSequence[~T],fastcore.found\n               ation.L,fastcore.basics.fastuple,NoneType]=None)\n\nA TransformBlock for segmentation masks, potentially with codes, for running batch_tfms on CPU\n\n\n\n\nwith no_random():\n    imagenette = untar_data(URLs.IMAGENETTE_320)\n\n    dblock = DataBlock(blocks=(ImageCPUBlock, CategoryBlock),\n                        splitter=GrandparentSplitter(valid_name='val'),\n                        get_items=get_image_files, get_y=parent_label,\n                        item_tfms=aug_transforms(size=128),\n                        batch_tfms=Normalize.from_stats(*imagenet_stats))\n    dls = dblock.dataloaders(imagenette, bs=9, num_workers=num_cpus())\n\n    dls.show_batch()"
  },
  {
    "objectID": "audio.02_data.html",
    "href": "audio.02_data.html",
    "title": "Audio Data",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "audio.02_data.html#transformblocks-for-audio",
    "href": "audio.02_data.html#transformblocks-for-audio",
    "title": "Audio Data",
    "section": "TransformBlocks for audio",
    "text": "TransformBlocks for audio\nAudio data blocks for using with the fastai data block API.\n\nsource\n\nAudioBlock\n\n AudioBlock (cls=&lt;class 'fastxtend.audio.core.TensorAudio'&gt;)\n\nA TransformBlock for audio of cls\n\nsource\n\n\nSpecBlock\n\n SpecBlock (cls=&lt;class 'fastxtend.audio.core.TensorAudio'&gt;, n_fft:Union[in\n            t,Iterable[int],MutableSequence[int],fastcore.foundation.L,fas\n            tcore.basics.fastuple]=1024, win_length:Union[int,Iterable[int\n            ],MutableSequence[int],fastcore.foundation.L,fastcore.basics.f\n            astuple,NoneType]=None, hop_length:Union[int,Iterable[int],Mut\n            ableSequence[int],fastcore.foundation.L,fastcore.basics.fastup\n            le,NoneType]=None, pad:Union[int,Iterable[int],MutableSequence\n            [int],fastcore.foundation.L,fastcore.basics.fastuple]=0, windo\n            w_fn:Union[Callable[...,torch.Tensor],Iterable[Callable[...,to\n            rch.Tensor]],MutableSequence[Callable[...,torch.Tensor]],fastc\n            ore.foundation.L,fastcore.basics.fastuple]=&lt;built-in method\n            hann_window of type object at 0x7f95e7c335c0&gt;, power:Union[flo\n            at,Iterable[float],MutableSequence[float],fastcore.foundation.\n            L,fastcore.basics.fastuple]=2.0, normalized:Union[bool,Iterabl\n            e[bool],MutableSequence[bool],fastcore.foundation.L,fastcore.b\n            asics.fastuple]=False, wkwargs:Union[dict,Iterable[dict],Mutab\n            leSequence[dict],fastcore.foundation.L,fastcore.basics.fastupl\n            e,NoneType]=None, center:Union[bool,Iterable[bool],MutableSequ\n            ence[bool],fastcore.foundation.L,fastcore.basics.fastuple]=Tru\n            e, pad_mode:Union[str,Iterable[str],MutableSequence[str],fastc\n            ore.foundation.L,fastcore.basics.fastuple]='reflect', norm:Uni\n            on[str,Iterable[str],MutableSequence[str],fastcore.foundation.\n            L,fastcore.basics.fastuple,NoneType]=None)\n\nA TransformBlock to read TensorAudio and then use the GPU to turn audio into one or more Spectrograms\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncls\n_TensorMeta\nTensorAudio\n\n\n\nn_fft\nListified[int]\n1024\nSpectrogram args\n\n\nwin_length\nListified[int] | None\nNone\n\n\n\nhop_length\nListified[int] | None\nNone\n\n\n\npad\nListified[int]\n0\n\n\n\nwindow_fn\nListified[Callable[…, Tensor]]\nhann_window\n\n\n\npower\nListified[float]\n2.0\n\n\n\nnormalized\nListified[bool]\nFalse\n\n\n\nwkwargs\nListified[dict] | None\nNone\n\n\n\ncenter\nListified[bool]\nTrue\n\n\n\npad_mode\nListified[str]\nreflect\n\n\n\nnorm\nListified[str] | None\nNone\n\n\n\n\n\nsource\n\n\nMelSpecBlock\n\n MelSpecBlock (cls=&lt;class 'fastxtend.audio.core.TensorAudio'&gt;, sr:Union[in\n               t,Iterable[int],MutableSequence[int],fastcore.foundation.L,\n               fastcore.basics.fastuple]=16000, n_fft:Union[int,Iterable[i\n               nt],MutableSequence[int],fastcore.foundation.L,fastcore.bas\n               ics.fastuple]=1024, win_length:Union[int,Iterable[int],Muta\n               bleSequence[int],fastcore.foundation.L,fastcore.basics.fast\n               uple,NoneType]=None, hop_length:Union[int,Iterable[int],Mut\n               ableSequence[int],fastcore.foundation.L,fastcore.basics.fas\n               tuple,NoneType]=None, f_min:Union[float,Iterable[float],Mut\n               ableSequence[float],fastcore.foundation.L,fastcore.basics.f\n               astuple]=0.0, f_max:Union[float,Iterable[float],MutableSequ\n               ence[float],fastcore.foundation.L,fastcore.basics.fastuple,\n               NoneType]=None, pad:Union[int,Iterable[int],MutableSequence\n               [int],fastcore.foundation.L,fastcore.basics.fastuple]=0, n_\n               mels:Union[int,Iterable[int],MutableSequence[int],fastcore.\n               foundation.L,fastcore.basics.fastuple]=128, window_fn:Union\n               [Callable[...,torch.Tensor],Iterable[Callable[...,torch.Ten\n               sor]],MutableSequence[Callable[...,torch.Tensor]],fastcore.\n               foundation.L,fastcore.basics.fastuple]=&lt;built-in method\n               hann_window of type object at 0x7f95e7c335c0&gt;, power:Union[\n               float,Iterable[float],MutableSequence[float],fastcore.found\n               ation.L,fastcore.basics.fastuple]=2.0, normalized:Union[boo\n               l,Iterable[bool],MutableSequence[bool],fastcore.foundation.\n               L,fastcore.basics.fastuple]=False, wkwargs:Union[dict,Itera\n               ble[dict],MutableSequence[dict],fastcore.foundation.L,fastc\n               ore.basics.fastuple,NoneType]=None, center:Union[bool,Itera\n               ble[bool],MutableSequence[bool],fastcore.foundation.L,fastc\n               ore.basics.fastuple]=True, pad_mode:Union[str,Iterable[str]\n               ,MutableSequence[str],fastcore.foundation.L,fastcore.basics\n               .fastuple]='reflect', norm:Union[str,Iterable[str],MutableS\n               equence[str],fastcore.foundation.L,fastcore.basics.fastuple\n               ,NoneType]=None, mel_scale:Union[str,Iterable[str],MutableS\n               equence[str],fastcore.foundation.L,fastcore.basics.fastuple\n               ]='htk')\n\nA TransformBlock to read TensorAudio and then use the GPU to turn audio into one or more MelSpectrograms\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncls\n_TensorMeta\nTensorAudio\n\n\n\nsr\nListified[int]\n16000\nMelSpectrogram args\n\n\nn_fft\nListified[int]\n1024\n\n\n\nwin_length\nListified[int] | None\nNone\n\n\n\nhop_length\nListified[int] | None\nNone\n\n\n\nf_min\nListified[float]\n0.0\n\n\n\nf_max\nListified[float] | None\nNone\n\n\n\npad\nListified[int]\n0\n\n\n\nn_mels\nListified[int]\n128\n\n\n\nwindow_fn\nListified[Callable[…, Tensor]]\nhann_window\n\n\n\npower\nListified[float]\n2.0\n\n\n\nnormalized\nListified[bool]\nFalse\n\n\n\nwkwargs\nListified[dict] | None\nNone\n\n\n\ncenter\nListified[bool]\nTrue\n\n\n\npad_mode\nListified[str]\nreflect\n\n\n\nnorm\nListified[str] | None\nNone\n\n\n\nmel_scale\nListified[str]\nhtk"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utility",
    "section": "",
    "text": "source\n\nfree_gpu_memory\n\n free_gpu_memory (learn:fastai.learner.Learner,\n                  dls:fastai.data.core.DataLoaders=None)\n\nFrees GPU memory using gc.collect and torch.cuda.empty_cache\n\nsource\n\n\nless_random\n\n less_random (seed:int=42, deterministic:Optional[bool]=None,\n              benchmark:Optional[bool]=None)\n\nStores and retrieves state of random number generators. Sets random seed for random, torch, and numpy.\nDoes not set torch.backends.cudnn.benchmark or torch.backends.cudnn.deterministic by default.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseed\nint\n42\nSeed for random, torch, and numpy\n\n\ndeterministic\nbool | None\nNone\nSet torch.backends.cudnn.deterministic if not None\n\n\nbenchmark\nbool | None\nNone\nSet torch.backends.cudnn.benchmark if not None\n\n\n\nA random state manager which provides some reproducibility without sacrificing potential training speed.\nUnlike fastai.torch_core.no_random, less_random does not set torch.backends.cudnn.benchmark or torch.backends.cudnn.deterministic by default.\nless_random training runs on the same GPU, PyTorch, & Cuda setup should be close to no_random reproducibility, but different hardware/software setup will have less reproducibility than using no_random.\n\nsource\n\n\nscale_time\n\n scale_time (val:float, spec:str='#0.4G')\n\nScale fractional second time values and return formatted to spec\n\nsource\n\n\npil_to_numpy\n\n pil_to_numpy (img:PIL.Image.Image)\n\nFast conversion of Pillow Image to NumPy NDArray\n\nsource\n\n\nconvert_to_int\n\n convert_to_int (s)"
  },
  {
    "objectID": "optimizer.stableadam.html",
    "href": "optimizer.stableadam.html",
    "title": "StableAdam Optimizer",
    "section": "",
    "text": "StableAdamW was introduced by Wortsman et al in Stable and low-precision training for large-scale vision-language models. StableAdamW is a AdamW-Adafactor hybrid, porting Adafactor’s update clipping into AdamW as a per parameter learning rate modification. StableAdamW’s update clipping outperforms gradient clipping on downstream tasks while avoiding model training instability.\nIn addition to a fastai native implementation, StableAdam has a fused ForEach implementation. See the Fused Optimizer documentation for more details.\nsource"
  },
  {
    "objectID": "optimizer.stableadam.html#hyperparameters",
    "href": "optimizer.stableadam.html#hyperparameters",
    "title": "StableAdam Optimizer",
    "section": "Hyperparameters",
    "text": "Hyperparameters\nHyperparameter notes from Wortsman et al:\nStableAdamW should be the same as AdamW, with β2, or sqr_mom for fastai optimizers, set to higher values such as 0.99 for best performance."
  },
  {
    "objectID": "optimizer.fused.html",
    "href": "optimizer.fused.html",
    "title": "Fused Optimizers",
    "section": "",
    "text": "fastxtend’s fused optimizers are 21 to 293 percent faster, drop-in replacements for fastai native optimizers.\nLike fastai optimizers, fastxtend fused optimizers support both discriminative learning rates across multiple parameter groups and per-parameter weight decay without any extra setup.\nWhile all fastai optimizers have vertically fused TorchScript implementations, only a subset have horizontally fused ForEach1 implementations. These optimizers, SGD, Adam, RAdam, LAMB, and Ranger, usually outperform their TorchScript counterparts in all but the tiniest models. fastxtend also has ForEach implementatons of Adan, Lion, Sophia, and StableAdam.\nfastxtend also adds full fastai support for bitsandbytes 8-bit optimizers2. 8-bit optimizers can reduce optimizer memory usage up to 75% compared to 32-bit optimizers. A subset of optimizers are supported: SGD, Adam, LARS, LAMB, and Lion."
  },
  {
    "objectID": "optimizer.fused.html#fused-performance",
    "href": "optimizer.fused.html#fused-performance",
    "title": "Fused Optimizers",
    "section": "Fused Performance",
    "text": "Fused Performance\nAs shown in Table 1, ForEach Optimizers are 21 to 293 percent faster3 in AdamW optimizer step performance relative to fastai implementations across benchmarked models. Complex optimizers without ForEach implementations, such as QHAdam, are up to 137 percent faster using TorchScript implementations.\n\n\nTable 1: Increase in AdamW opt_step Speed vs fastai Native Optimizer\n\n\n\n\n\n\n\n\n\n\nModel\nfastai Step\nForEach Step\nForEach Speedup\nJIT Step\nJIT Speedup\n\n\n\n\nXResNet18\n26ms\n12ms\n109%\n20ms\n29%\n\n\nXResNet50\n56ms\n32ms\n74%\n46ms\n20%\n\n\nXSE-ResNeXt50\n72ms\n43ms\n68%\n61ms\n18%\n\n\nXResNet101\n88ms\n47ms\n84%\n68ms\n30%\n\n\nDeBERTa Base\n27ms\n6.9ms\n293%\n19ms\n46%\n\n\n\n\nThis speedup persists with single or multiple parameter groups. Although more groups can lead to a small decrease in optimizer step speed, as shown by DeBERTa in Table 2.\n\n\nTable 2: Increase in AdamW opt_step Speed With Multiple Param Groups vs fastai Native Optimizer\n\n\n\n\n\n\n\n\n\n\n\nModel\nLayers\nfastai Step\nForEach Step\nForEach Speedup\nJIT Step\nJIT Speedup\n\n\n\n\nXResNet18\n2\n25ms\n12ms\n103%\n19ms\n30%\n\n\nXResNet50\n2\n56ms\n32ms\n76%\n46ms\n24%\n\n\nXSE-ResNeXt50\n2\n72ms\n45ms\n85%\n61ms\n29%\n\n\nXResNet101\n2\n87ms\n47ms\n60%\n67ms\n17%\n\n\nConvNeXt Tiny\n2\n125ms\n102ms\n22%\n115ms\n9.4%\n\n\nConvNeXt Small\n2\n200ms\n165ms\n21%\n181ms\n10%\n\n\nViT Patch16 Small\n2\n62ms\n38ms\n62%\n52ms\n20%\n\n\nDeBERTa Base\n4\n27ms\n7.7ms\n254%\n19ms\n47%"
  },
  {
    "objectID": "optimizer.fused.html#examples",
    "href": "optimizer.fused.html#examples",
    "title": "Fused Optimizers",
    "section": "Examples",
    "text": "Examples\nFor backwards compatibility, all fastxtend optimizers return a fastai native optimizer by default. To use a fused version set foreach=True or jit=True.\nfrom fastai.vision.all import *\nfrom fastxtend.vision.all import *\n\n# Use ForEach AdamW\nopt_func = adam(foreach=True)\n\n# Or use TorchScript AdamW\nopt_func = adam(jit=True)\n\n# Or use bitsandbytes' 8-bit AdamW\nopt_func = adam(eightbit=True)\n\nLearner(..., opt_func=opt_func)\nOr import fused optimizers independent of other fastxtend features.\nfrom fastai.vision.all import *\nfrom fastxtend.optimizer.all import *\n\nLearner(..., opt_func=partial(Adam, foreach=True))\n\n\n\n\n\n\nNote\n\n\n\nadam(...) is a fastxtend convenience method equivalent to partial(Adam, ...). fastextend adds lowercase convenience methods for all fastai optimizers."
  },
  {
    "objectID": "optimizer.fused.html#sgd-optimizer",
    "href": "optimizer.fused.html#sgd-optimizer",
    "title": "Fused Optimizers",
    "section": "SGD Optimizer",
    "text": "SGD Optimizer\nStochastic gradient descent, optionally with momentum.\nOptional weight decay of wd is applied, as true weight decay (decay the weights directly) if decouple_wd=True else as L2 regularization (add the decay to the gradients).\n8-bit SGD only supports L2 weight decay: decouple_wd=False, and requires momentum: mom&gt;0.\n\nsource\n\nSGD\n\n SGD (params:Union[torch.Tensor,Iterable[torch.Tensor],MutableSequence[tor\n      ch.Tensor],fastcore.foundation.L,fastcore.basics.fastuple],\n      lr:float, mom:float=0.0, wd:float=0.0, decouple_wd:bool=True,\n      foreach:bool=False, jit:bool=False, eightbit:bool=False,\n      **eightbitargs)\n\nA fastai SGD/SGDW optimizer with fused ForEach, TorchScript, & 8-bit implementations\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparams\nListified[Tensor]\n\nModel parameters or parameter groups\n\n\nlr\nfloat\n\nDefault learning rate\n\n\nmom\nfloat\n0.0\nGradient moving average (β1) coefficient\n\n\nwd\nfloat\n0.0\nOptional weight decay (true or L2)\n\n\ndecouple_wd\nbool\nTrue\nApply true weight decay (SGDW) or L2 regularization (SGD)\n\n\nforeach\nbool\nFalse\nUse fused ForEach implementation\n\n\njit\nbool\nFalse\nUse fused TorchScript implementation\n\n\neightbit\nbool\nFalse\nUse fused 8-bit implementation\n\n\neightbitargs\n\n\n\n\n\nReturns\nOptimizer | SGDForEachOptimizer | JitOptimizer | SGD8bitOptimizer\n\n\n\n\n\n\nsource\n\n\nsgd\n\n sgd (mom:float=0.0, wd:float=0.0, decouple_wd:bool=True,\n      foreach:bool=False, jit:bool=False, eightbit:bool=False,\n      **eightbitargs)\n\nPartial function for the SGD/SGDW optimizer with fused ForEach, TorchScript, & 8-bit implementations\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmom\nfloat\n0.0\nGradient moving average (β1) coefficient\n\n\nwd\nfloat\n0.0\nOptional weight decay (true or L2)\n\n\ndecouple_wd\nbool\nTrue\nApply true weight decay (SGDW) or L2 regularization (SGD)\n\n\nforeach\nbool\nFalse\nUse fused ForEach implementation\n\n\njit\nbool\nFalse\nUse fused TorchScript implementation\n\n\neightbit\nbool\nFalse\nUse fused 8-bit implementation\n\n\neightbitargs\n\n\n\n\n\nReturns\nOptimizer | SGDForEachOptimizer | JitOptimizer | SGD8bitOptimizer"
  },
  {
    "objectID": "optimizer.fused.html#rmsprop-optimizer",
    "href": "optimizer.fused.html#rmsprop-optimizer",
    "title": "Fused Optimizers",
    "section": "RMSProp Optimizer",
    "text": "RMSProp Optimizer\nRMSProp was introduced by Geoffrey Hinton in his course. What is named sqr_mom here is the alpha in the course.\nOptional weight decay of wd is applied as true weight decay (decay the weights directly) if decouple_wd=True else as L2 regularization (add the decay to the gradients).\n8-bit RMSProp only supports L2 weight decay: decouple_wd=False, and does not support momentum: mom=0.\n\n\n\n\n\n\nNote\n\n\n\nThe order of the mom and sqr_mom hyperparameters has been swapped from fastai to follow the order of all the other fastai and fastxtend optimizers.\n\n\n\nsource\n\nRMSProp\n\n RMSProp (params:Union[torch.Tensor,Iterable[torch.Tensor],MutableSequence\n          [torch.Tensor],fastcore.foundation.L,fastcore.basics.fastuple],\n          lr:float, mom:float=0.0, sqr_mom:float=0.99, eps:float=1e-08,\n          wd:float=0.0, decouple_wd:bool=True, jit:bool=False,\n          eightbit:bool=False, **eightbitargs)\n\nA fastai RMSProp/RMSPropW optimizer with fused TorchScript and 8-bit implementations\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparams\nListified[Tensor]\n\nModel parameters or parameter groups\n\n\nlr\nfloat\n\nDefault learning rate\n\n\nmom\nfloat\n0.0\nGradient moving average (β1) coefficient\n\n\nsqr_mom\nfloat\n0.99\nGradient squared moving average (β2) coefficient\n\n\neps\nfloat\n1e-08\nAdded for numerical stability\n\n\nwd\nfloat\n0.0\nOptional weight decay (true or L2)\n\n\ndecouple_wd\nbool\nTrue\nApply true weight decay or L2 regularization. Ignored if eightbit=True\n\n\njit\nbool\nFalse\nUse fused TorchScript implementation\n\n\neightbit\nbool\nFalse\nUse fused 8-bit implementation\n\n\neightbitargs\n\n\n\n\n\nReturns\nOptimizer | JitOptimizer | RMSProp8bitOptimizer\n\n\n\n\n\n\nsource\n\n\nrmsprop\n\n rmsprop (mom:float=0.0, sqr_mom:float=0.99, eps:float=1e-08,\n          wd:float=0.0, decouple_wd:bool=True, jit:bool=False,\n          eightbit:bool=False, **eightbitargs)\n\nPartial function for the RMSProp/RMSPropW optimizer with fused TorchScript and 8-bit implementations\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmom\nfloat\n0.0\nGradient moving average (β1) coefficient\n\n\nsqr_mom\nfloat\n0.99\nGradient squared moving average (β2) coefficient\n\n\neps\nfloat\n1e-08\nAdded for numerical stability\n\n\nwd\nfloat\n0.0\nOptional weight decay (true or L2)\n\n\ndecouple_wd\nbool\nTrue\nApply true weight decay (RMSPropW) or L2 regularization (RMSProp)\n\n\njit\nbool\nFalse\nUse fused TorchScript implementation\n\n\neightbit\nbool\nFalse\nUse fused 8-bit implementation\n\n\neightbitargs\n\n\n\n\n\nReturns\nOptimizer | JitOptimizer | RMSProp8bitOptimizer"
  },
  {
    "objectID": "optimizer.fused.html#adam-optimizer",
    "href": "optimizer.fused.html#adam-optimizer",
    "title": "Fused Optimizers",
    "section": "Adam Optimizer",
    "text": "Adam Optimizer\nAdam was introduced by Diederik P. Kingma and Jimmy Ba in Adam: A Method for Stochastic Optimization. For consistency across optimizers, fastai renamed beta1 and beta2 in the paper to mom and sqr_mom. Note that the defaults also differ from the paper (0.99 for sqr_mom or beta2, 1e-5 for eps). Those values seem to be better from experimentation in a wide range of situations.\nOptional weight decay of wd is applied, as true weight decay (decay the weights directly) if decouple_wd=True else as L2 regularization (add the decay to the gradients).\n8-bit Adam only supports true weight decay: decouple_wd=True.\n\n\n\n\n\n\nNote\n\n\n\nDon’t forget that eps is an hyper-parameter you can change. Some models won’t train without a very high eps like 0.1 (intuitively, the higher eps is, the closer Adam is to normal SGD). The usual default of 1e-8 is often too extreme in the sense Adam does’t manage to get as good results as with SGD.\n\n\n\nsource\n\nAdam\n\n Adam (params:Union[torch.Tensor,Iterable[torch.Tensor],MutableSequence[to\n       rch.Tensor],fastcore.foundation.L,fastcore.basics.fastuple],\n       lr:float, mom:float=0.9, sqr_mom:float=0.99, eps:float=1e-05,\n       wd:float=0.01, decouple_wd:bool=True, foreach:bool=False,\n       jit:bool=False, eightbit:bool=False, **eightbitargs)\n\nA fastai Adam/AdamW optimizer with fused ForEach, TorchScript, & 8-bit implementations\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparams\nListified[Tensor]\n\nModel parameters or parameter groups\n\n\nlr\nfloat\n\nDefault learning rate\n\n\nmom\nfloat\n0.9\nGradient moving average (β1) coefficient\n\n\nsqr_mom\nfloat\n0.99\nGradient squared moving average (β2) coefficient\n\n\neps\nfloat\n1e-05\nAdded for numerical stability\n\n\nwd\nfloat\n0.01\nOptional weight decay (true or L2)\n\n\ndecouple_wd\nbool\nTrue\nApply true weight decay (AdamW) or L2 regularization (Adam)\n\n\nforeach\nbool\nFalse\nUse fused ForEach implementation\n\n\njit\nbool\nFalse\nUse fused TorchScript implementation\n\n\neightbit\nbool\nFalse\nUse fused 8-bit implementation\n\n\neightbitargs\n\n\n\n\n\nReturns\nOptimizer | AdamForEachOptimizer | JitOptimizer | AdamW8bitOptimizer\n\n\n\n\n\n\nsource\n\n\nadam\n\n adam (mom:float=0.9, sqr_mom:float=0.99, eps:float=1e-05, wd:float=0.01,\n       decouple_wd:bool=True, foreach:bool=False, jit:bool=False,\n       eightbit:bool=False, **eightbitargs)\n\nPartial function for the Adam/AdamW optimizer with fused ForEach, TorchScript, & 8-bit implementations\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmom\nfloat\n0.9\nGradient moving average (β1) coefficient\n\n\nsqr_mom\nfloat\n0.99\nGradient squared moving average (β2) coefficient\n\n\neps\nfloat\n1e-05\nAdded for numerical stability\n\n\nwd\nfloat\n0.01\nOptional weight decay (true or L2)\n\n\ndecouple_wd\nbool\nTrue\nApply true weight decay (AdamW) or L2 regularization (Adam)\n\n\nforeach\nbool\nFalse\nUse fused ForEach implementation\n\n\njit\nbool\nFalse\nUse fused TorchScript implementation\n\n\neightbit\nbool\nFalse\nUse fused 8-bit implementation\n\n\neightbitargs\n\n\n\n\n\nReturns\nOptimizer | AdamForEachOptimizer | JitOptimizer | AdamW8bitOptimizer"
  },
  {
    "objectID": "optimizer.fused.html#radam-optimizer",
    "href": "optimizer.fused.html#radam-optimizer",
    "title": "Fused Optimizers",
    "section": "RAdam Optimizer",
    "text": "RAdam Optimizer\nRAdam (for rectified Adam) was introduced by Zhang et al. in On the Variance of the Adaptive Default learning rate and Beyond to slightly modify the Adam optimizer to be more stable at the beginning of training (and thus not require a long warmup). They use an estimate of the variance of the moving average of the squared gradients (the term in the denominator of traditional Adam) and rescale this moving average by this term before performing the update.\nThe native fastai implementation also incorporates SAdam; set beta to enable this (definition same as in the paper).\n\n\n\n\n\n\nNote\n\n\n\nfastxtend ForEach and TorchScript implementations do not support beta and SAdam.\n\n\nOptional weight decay of wd is applied, as true weight decay (decay the weights directly) if decouple_wd=True else as L2 regularization (add the decay to the gradients).\n\nsource\n\nRAdam\n\n RAdam (params:Union[torch.Tensor,Iterable[torch.Tensor],MutableSequence[t\n        orch.Tensor],fastcore.foundation.L,fastcore.basics.fastuple],\n        lr:float, mom:float=0.9, sqr_mom:float=0.99, eps:float=1e-05,\n        wd:float=0.0, beta:float=0.0, decouple_wd:bool=True,\n        foreach:bool=False, jit:bool=False)\n\nA fastai RAdam/RAdamW optimizer with fused ForEach and TorchScript implementations\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparams\nListified[Tensor]\n\nModel parameters or parameter groups\n\n\nlr\nfloat\n\nDefault learning rate\n\n\nmom\nfloat\n0.9\nGradient moving average (β1) coefficient\n\n\nsqr_mom\nfloat\n0.99\nGradient squared moving average (β2) coefficient\n\n\neps\nfloat\n1e-05\nAdded for numerical stability\n\n\nwd\nfloat\n0.0\nOptional weight decay (true or L2)\n\n\nbeta\nfloat\n0.0\nSet to enable SAdam with native fastai RAdam\n\n\ndecouple_wd\nbool\nTrue\nApply true weight decay (RAdamW) or L2 regularization (RAdam)\n\n\nforeach\nbool\nFalse\nUse fused ForEach implementation\n\n\njit\nbool\nFalse\nUse fused TorchScript implementation\n\n\nReturns\nOptimizer | RAdamForEachOptimizer | JitOptimizer\n\n\n\n\n\n\nsource\n\n\nradam\n\n radam (mom:float=0.9, sqr_mom:float=0.99, eps:float=1e-05, wd:float=0.0,\n        beta:float=0.0, decouple_wd:bool=True, foreach:bool=False,\n        jit:bool=False)\n\nPartial function for the RAdam/RAdamW optimizer with fused ForEach and TorchScript implementations\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmom\nfloat\n0.9\nGradient moving average (β1) coefficient\n\n\nsqr_mom\nfloat\n0.99\nGradient squared moving average (β2) coefficient\n\n\neps\nfloat\n1e-05\nAdded for numerical stability\n\n\nwd\nfloat\n0.0\nOptional weight decay (true or L2)\n\n\nbeta\nfloat\n0.0\nSet to enable SAdam with native fastai RAdam\n\n\ndecouple_wd\nbool\nTrue\nApply true weight decay (RAdamW) or L2 regularization (RAdam)\n\n\nforeach\nbool\nFalse\nUse fused ForEach implementation\n\n\njit\nbool\nFalse\nUse fused TorchScript implementation\n\n\nReturns\nOptimizer | RAdamForEachOptimizer | JitOptimizer"
  },
  {
    "objectID": "optimizer.fused.html#qhadam-optimizer",
    "href": "optimizer.fused.html#qhadam-optimizer",
    "title": "Fused Optimizers",
    "section": "QHAdam Optimizer",
    "text": "QHAdam Optimizer\nQHAdam (for Quasi-Hyperbolic Adam) was introduced by Ma & Yarats in Quasi-Hyperbolic Momentum and Adam for Deep Learning as a “computationally cheap, intuitive to interpret, and simple to implement” optimizer. Additional code can be found in their qhoptim repo. QHAdam is based on QH-Momentum, which introduces the immediate discount factor nu, encapsulating plain SGD (nu = 0) and momentum (nu = 1). QH-Momentum is defined below, where g_t+1 is the update of the moment. An interpretation of QHM is as a nu-weighted average of the momentum update step and the plain SGD update step.\nθ_t+1 ← θ_t − lr * [(1 − nu) · ∇L_t(θ_t) + nu · g_t+1]\nQHAdam takes the concept behind QHM above and applies it to Adam, replacing both of Adam’s moment estimators with quasi-hyperbolic terms.\nThe paper’s suggested default parameters are mom = 0.999, sqr_mom = 0.999, nu_1 = 0.7 and and nu_2 = 1.0. When training is not stable, it is possible that setting nu_2 &lt; 1 can improve stability by imposing a tighter step size bound. Note that QHAdam recovers Adam when nu_1 = nu_2 = 1.0. QHAdam recovers RMSProp (Hinton et al., 2012) when nu_1 = 0 and nu_2 = 1, and NAdam (Dozat, 2016) when nu_1 = mom and nu_2 = 1.\nOptional weight decay of wd is applied, as true weight decay (decay the weights directly) if decouple_wd=True else as L2 regularization (add the decay to the gradients).\n\nsource\n\nQHAdam\n\n QHAdam (params:Union[torch.Tensor,Iterable[torch.Tensor],MutableSequence[\n         torch.Tensor],fastcore.foundation.L,fastcore.basics.fastuple],\n         lr:float, mom:float=0.999, sqr_mom:float=0.999, nu_1:float=0.7,\n         nu_2:float=1.0, eps:float=1e-08, wd:float=0.0,\n         decouple_wd:bool=True, jit:bool=False)\n\nA fastai QHAdam/QHAdamW optimizer with a fused TorchScript implementation\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparams\nListified[Tensor]\n\nModel parameters or parameter groups\n\n\nlr\nfloat\n\nDefault learning rate\n\n\nmom\nfloat\n0.999\nGradient moving average (β1) coefficient\n\n\nsqr_mom\nfloat\n0.999\nGradient squared moving average (β2) coefficient\n\n\nnu_1\nfloat\n0.7\nQH immediate discount factor\n\n\nnu_2\nfloat\n1.0\nQH momentum discount factor\n\n\neps\nfloat\n1e-08\nAdded for numerical stability\n\n\nwd\nfloat\n0.0\nOptional weight decay (true or L2)\n\n\ndecouple_wd\nbool\nTrue\nApply true weight decay (QHAdamW) or L2 regularization (QHAdam)\n\n\njit\nbool\nFalse\nUse fused TorchScript implementation\n\n\nReturns\nOptimizer | JitOptimizer\n\n\n\n\n\n\nsource\n\n\nqhadam\n\n qhadam (mom:float=0.999, sqr_mom:float=0.999, nu_1:float=0.7,\n         nu_2:float=1.0, eps:float=1e-08, wd:float=0.0,\n         decouple_wd:bool=True, jit:bool=False)\n\nPartial function for the QHAdam/QHAdamW optimizer with a fused TorchScript implementation\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmom\nfloat\n0.999\nGradient moving average (β1) coefficient\n\n\nsqr_mom\nfloat\n0.999\nGradient squared moving average (β2) coefficient\n\n\nnu_1\nfloat\n0.7\nQH immediate discount factor\n\n\nnu_2\nfloat\n1.0\nQH momentum discount factor\n\n\neps\nfloat\n1e-08\nAdded for numerical stability\n\n\nwd\nfloat\n0.0\nOptional weight decay (true or L2)\n\n\ndecouple_wd\nbool\nTrue\nApply true weight decay (QHAdamW) or L2 regularization (QHAdam)\n\n\njit\nbool\nFalse\nUse fused TorchScript implementation\n\n\nReturns\nOptimizer | JitOptimizer"
  },
  {
    "objectID": "optimizer.fused.html#larslarc-optimizer",
    "href": "optimizer.fused.html#larslarc-optimizer",
    "title": "Fused Optimizers",
    "section": "LARS/LARC Optimizer",
    "text": "LARS/LARC Optimizer\nThe LARS optimizer was first introduced in Large Batch Training of Convolutional Networks then refined in its LARC variant (original LARS is with clip=False). A Default learning rate is computed for each individual layer with a certain trust_coefficient, then clipped to be always less than lr.\nOptional weight decay of wd is applied, as true weight decay (decay the weights directly) if decouple_wd=True else as L2 regularization (add the decay to the gradients).\n8-bit implementation is for LARS: clip=False, and only supports L2 weight decay: decouple_wd=False.\n\nsource\n\nLarc\n\n Larc (params:Union[torch.Tensor,Iterable[torch.Tensor],MutableSequence[to\n       rch.Tensor],fastcore.foundation.L,fastcore.basics.fastuple],\n       lr:float, mom:float=0.9, clip:bool=True, trust_coeff:float=0.02,\n       eps:float=1e-08, wd:float=0.0, decouple_wd:bool=True,\n       jit:bool=False, eightbit:bool=False, **eightbitargs)\n\nA fastai LARC/LARS optimizer with fused TorchScript & 8-bit implementations\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparams\nListified[Tensor]\n\nModel parameters or parameter groups\n\n\nlr\nfloat\n\nDefault learning rate\n\n\nmom\nfloat\n0.9\nGradient moving average (β1) coefficient\n\n\nclip\nbool\nTrue\nLARC if clip=True, LARS if clip=False\n\n\ntrust_coeff\nfloat\n0.02\nTrust coeffiecnet for calculating layerwise LR\n\n\neps\nfloat\n1e-08\nAdded for numerical stability\n\n\nwd\nfloat\n0.0\nOptional weight decay (true or L2)\n\n\ndecouple_wd\nbool\nTrue\nApply true weight decay or L2 regularization. Ignored if eightbit=True\n\n\njit\nbool\nFalse\nUse fused TorchScript implementation\n\n\neightbit\nbool\nFalse\nUse fused 8-bit implementation. Only supports LARS: clip=False\n\n\neightbitargs\n\n\n\n\n\nReturns\nOptimizer | JitOptimizer | LARS8bitOptimizer\n\n\n\n\n\n\nsource\n\n\nlarc\n\n larc (mom:float=0.9, clip:bool=True, trust_coeff:float=0.02,\n       eps:float=1e-08, wd:float=0.0, decouple_wd:bool=True,\n       jit:bool=False, eightbit:bool=False, **eightbitargs)\n\nPartial function for the LARC/LARS optimizer with fused TorchScript & 8-bit implementations\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmom\nfloat\n0.9\nGradient moving average (β1) coefficient\n\n\nclip\nbool\nTrue\nLARC if clip=True, LARS if clip=False\n\n\ntrust_coeff\nfloat\n0.02\nTrust coeffiecnet for calculating layerwise LR\n\n\neps\nfloat\n1e-08\nAdded for numerical stability\n\n\nwd\nfloat\n0.0\nOptional weight decay (true or L2)\n\n\ndecouple_wd\nbool\nTrue\nApply true weight decay or L2 regularization\n\n\njit\nbool\nFalse\nUse fused TorchScript implementation\n\n\neightbit\nbool\nFalse\nUse fused 8-bit implementation. Only supports LARS\n\n\neightbitargs\n\n\n\n\n\nReturns\nOptimizer | JitOptimizer | LARS8bitOptimizer"
  },
  {
    "objectID": "optimizer.fused.html#lamb-optimizer",
    "href": "optimizer.fused.html#lamb-optimizer",
    "title": "Fused Optimizers",
    "section": "LAMB Optimizer",
    "text": "LAMB Optimizer\nLAMB was introduced in Large Batch Optimization for Deep Learning: Training BERT in 76 minutes. Intuitively, it’s LARC applied to Adam. As in Adam, beta1 and beta2 in the paper is renamed to mom and sqr_mom. Note that the defaults also differ from the paper (0.99 for sqr_mom or beta2, 1e-5 for eps). Those values seem to be better from experimentation in a wide range of situations.\nOptional weight decay of wd is applied, as true weight decay (decay the weights directly) if decouple_wd=True else as L2 regularization (add the decay to the gradients).\n8-bit LAMB only supports true weight decay: decouple_wd=True.\n\nsource\n\nLamb\n\n Lamb (params:Union[torch.Tensor,Iterable[torch.Tensor],MutableSequence[to\n       rch.Tensor],fastcore.foundation.L,fastcore.basics.fastuple],\n       lr:float, mom:float=0.9, sqr_mom:float=0.99, eps:float=1e-05,\n       wd:float=0.0, decouple_wd:bool=True, foreach:bool=False,\n       jit:bool=False, eightbit:bool=False, **eightbitargs)\n\nA fastai LAMB optimizer with fused ForEach, TorchScript, & 8-bit implementations\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparams\nListified[Tensor]\n\nModel parameters or parameter groups\n\n\nlr\nfloat\n\nDefault learning rate\n\n\nmom\nfloat\n0.9\nGradient moving average (β1) coefficient\n\n\nsqr_mom\nfloat\n0.99\nGradient squared moving average (β2) coefficient\n\n\neps\nfloat\n1e-05\nAdded for numerical stability\n\n\nwd\nfloat\n0.0\nOptional weight decay (true or L2)\n\n\ndecouple_wd\nbool\nTrue\nApply true weight decay or L2 regularization. Ignored if eightbit=True\n\n\nforeach\nbool\nFalse\nUse fused ForEach implementation\n\n\njit\nbool\nFalse\nUse fused TorchScript implementation\n\n\neightbit\nbool\nFalse\nUse fused 8-bit implementation. Only supports true weight decay\n\n\neightbitargs\n\n\n\n\n\nReturns\nOptimizer | LambForEachOptimizer | JitOptimizer | LAMB8bitOptimizer\n\n\n\n\n\n\nsource\n\n\nlamb\n\n lamb (mom:float=0.9, sqr_mom:float=0.99, eps:float=1e-05, wd:float=0.0,\n       decouple_wd:bool=True, foreach:bool=False, jit:bool=False,\n       eightbit:bool=False, **eightbitargs)\n\nPartial function for the LAMB optimizer with fused ForEach, TorchScript, & 8-bit implementations\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmom\nfloat\n0.9\nGradient moving average (β1) coefficient\n\n\nsqr_mom\nfloat\n0.99\nGradient squared moving average (β2) coefficient\n\n\neps\nfloat\n1e-05\nAdded for numerical stability\n\n\nwd\nfloat\n0.0\nOptional weight decay (true or L2)\n\n\ndecouple_wd\nbool\nTrue\nApply true weight decay or L2 regularization\n\n\nforeach\nbool\nFalse\nUse fused ForEach implementation\n\n\njit\nbool\nFalse\nUse fused TorchScript implementation\n\n\neightbit\nbool\nFalse\nUse fused 8-bit implementation. Only supports true weight decay\n\n\neightbitargs\n\n\n\n\n\nReturns\nOptimizer | LambForEachOptimizer | JitOptimizer | LAMB8bitOptimizer"
  },
  {
    "objectID": "optimizer.fused.html#ranger-optimizer",
    "href": "optimizer.fused.html#ranger-optimizer",
    "title": "Fused Optimizers",
    "section": "Ranger Optimizer",
    "text": "Ranger Optimizer\n\n\n\n\n\n\nWarning\n\n\n\nRanger is the only non-backward compatible fastxtend Optimizer. Ranger is equivalent fastai’s ranger while fastxtend’s ranger is a partial function which returns Ranger. Most fastai code should be uneffected by this change.\n\n\nLookahead was introduced by Zhang et al. in Lookahead Optimizer: k steps forward, 1 step back. With Lookahead, the final weights (slow weights) are a moving average of the normal weights (fast weights). Every k steps, Lookahead modifieds the current weights by a moving average of the fast weights (normal weights) with the slow weights (the copy of old weights k steps ago). Those slow weights act like a stability mechanism.\nRanger was introduced by Less Wright in New Deep Learning Optimizer, Ranger: Synergistic combination of RAdam + Lookahead for the best of both. It combines RAdam and Lookahead together in one optimizer and reduces the need for hyperparameter tuning due to a combination of RAdam’s warmup heuristic and Lookahead’s interpolation of parameter weights.\nRanger performs best on vision tasks when paired with the fit_flat_cos or fit_flat_varied schedulers.\nOptional weight decay of wd is applied, as true weight decay (decay the weights directly) if decouple_wd=True else as L2 regularization (add the decay to the gradients).\n\n\n\n\n\n\nImportant\n\n\n\nWhile fastai’s Lookahead can be applied to any optimizer, fastxtend’s JitLookahead must have a custom written TorchScript callback and ForEachOptimizer a custom Lookahead optimizer step. Currently ranger with RAdam is the only TorchScript and ForEach optimizer with Lookahead support.\n\n\n\nsource\n\nRanger\n\n Ranger (params:Union[torch.Tensor,Iterable[torch.Tensor],MutableSequence[\n         torch.Tensor],fastcore.foundation.L,fastcore.basics.fastuple],\n         lr:float, mom:float=0.95, sqr_mom:float=0.99, eps:float=1e-06,\n         wd:float=0.01, k:int=6, alpha:float=0.5, decouple_wd:bool=True,\n         foreach:bool=False, jit:bool=False)\n\nConvenience method for Lookahead with RAdam fused ForEach and TorchScript implementations\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparams\nListified[Tensor]\n\nModel parameters or parameter groups\n\n\nlr\nfloat\n\nDefault learning rate\n\n\nmom\nfloat\n0.95\nGradient moving average (β1) coefficient\n\n\nsqr_mom\nfloat\n0.99\nGradient squared moving average (β2) coefficient\n\n\neps\nfloat\n1e-06\nAdded for numerical stability\n\n\nwd\nfloat\n0.01\nOptional weight decay (true or L2)\n\n\nk\nint\n6\nHow often to conduct Lookahead step\n\n\nalpha\nfloat\n0.5\nSlow weight moving average coefficient\n\n\ndecouple_wd\nbool\nTrue\nApply true weight decay (RAdamW) or L2 regularization (RAdam)\n\n\nforeach\nbool\nFalse\nUse fused ForEach implementation\n\n\njit\nbool\nFalse\nUse fused TorchScript implementation\n\n\nReturns\nLookahead | RangerForEachOptimizer | JitLookahead\n\n\n\n\n\n\nsource\n\n\nranger\n\n ranger (mom:float=0.95, sqr_mom:float=0.99, eps:float=1e-06,\n         wd:float=0.01, k:int=6, alpha:float=0.5, decouple_wd:bool=True,\n         foreach:bool=False, jit:bool=False)\n\nPartial function of the onvenience method for Lookahead with RAdam fused ForEach and TorchScript implementations\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmom\nfloat\n0.95\nGradient moving average (β1) coefficient\n\n\nsqr_mom\nfloat\n0.99\nGradient squared moving average (β2) coefficient\n\n\neps\nfloat\n1e-06\nAdded for numerical stability\n\n\nwd\nfloat\n0.01\nOptional weight decay (true or L2)\n\n\nk\nint\n6\nHow often to conduct Lookahead step\n\n\nalpha\nfloat\n0.5\nSlow weight moving average coefficient\n\n\ndecouple_wd\nbool\nTrue\nApply true weight decay (RAdamW) or L2 regularization (RAdam)\n\n\nforeach\nbool\nFalse\nUse fused ForEach implementation\n\n\njit\nbool\nFalse\nUse fused TorchScript implementation\n\n\nReturns\nLookahead | RangerForEachOptimizer | JitLookahead"
  },
  {
    "objectID": "optimizer.fused.html#footnotes",
    "href": "optimizer.fused.html#footnotes",
    "title": "Fused Optimizers",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nfastxtend ForEach optimizers are adapted from the PyTorch ForEach _multi_tensor implementations, but seamlessly work with fastai features.↩︎\nWhile it is possible to use bitsandbytes optimizers with fastai via fastai.optimizer.OptimWrapper, this doesn’t provide compatibility with all fastai optimizer features. fastxtend adds full fastai compatibility to bitsandbytes 8-bit optimizers, including per-parameter weight decay, automatic weight decay exclusion for normalization and bias terms, and discriminative learning rate support.↩︎\nAll optimizers benchmarked on a GeForce 3080 Ti using PyTorch 1.12.1, Cuda 11.6, Mixed Precision, Channels Last (except ViT and DeBERTa), and fastxtend’s Simple Profiler Callback. Results may differ with other optimizers, models, hardware, and across benchmarking runs. Speedup is calculated from the total time spent on the optimization step.↩︎"
  },
  {
    "objectID": "multiloss.html",
    "href": "multiloss.html",
    "title": "MultiLoss",
    "section": "",
    "text": "MultiLoss and MultiTargetLoss are two simple multiple loss wrappers which allow logging individual losses as metrics using MultiLossCallback. Jump to the example to see how to use with fastai.\nVia MixHandlerX, both are compatible with MixUp, CutMix, CutMixUp, and CutMixUpAugment.\nsource"
  },
  {
    "objectID": "multiloss.html#example",
    "href": "multiloss.html#example",
    "title": "MultiLoss",
    "section": "Example",
    "text": "Example\n\nwith no_random():\n    mloss = MultiLoss(loss_funcs=[nn.MSELoss, nn.L1Loss],\n                      weights=[1, 3.5],\n                      loss_names=['mse_loss', 'l1_loss'])\n\n\n    learn = synth_learner(n_trn=5, loss_func=mloss, metrics=RMSE(), cbs=MultiLossCallback)\n    learn.fit(5)\n\n\n\n\nepoch\ntrain_loss\ntrain_mse_loss\ntrain_l1_loss\nvalid_loss\nvalid_mse_loss\nvalid_l1_loss\nvalid_rmse\ntime\n\n\n\n\n0\n23.598301\n12.719514\n10.878788\n17.910727\n9.067028\n8.843699\n3.011151\n00:00\n\n\n1\n22.448792\n11.937573\n10.511218\n15.481797\n7.464430\n8.017367\n2.732111\n00:00\n\n\n2\n20.827835\n10.837888\n9.989948\n12.756706\n5.756156\n7.000550\n2.399199\n00:00\n\n\n3\n19.028177\n9.657351\n9.370827\n10.031281\n4.145008\n5.886274\n2.035929\n00:00\n\n\n4\n17.167393\n8.481768\n8.685625\n7.581020\n2.787561\n4.793459\n1.669599\n00:00"
  },
  {
    "objectID": "callback.tracker.html",
    "href": "callback.tracker.html",
    "title": "Tracking Callbacks",
    "section": "",
    "text": "source\n\nTerminateOnTrainNaN\n\n TerminateOnTrainNaN (after_create=None, before_fit=None,\n                      before_epoch=None, before_train=None,\n                      before_batch=None, after_pred=None, after_loss=None,\n                      before_backward=None, after_cancel_backward=None,\n                      after_backward=None, before_step=None,\n                      after_cancel_step=None, after_step=None,\n                      after_cancel_batch=None, after_batch=None,\n                      after_cancel_train=None, after_train=None,\n                      before_validate=None, after_cancel_validate=None,\n                      after_validate=None, after_cancel_epoch=None,\n                      after_epoch=None, after_cancel_fit=None,\n                      after_fit=None)\n\nA Callback that terminates training if the training loss is NaN and ignores valid loss.\n\nsource\n\n\nSaveModelAtEnd\n\n SaveModelAtEnd (fname='model', with_opt=False)\n\nA SaveModelCallback which only saves the model at the end so loggers can find it.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfname\nstr\nmodel\nModel filename\n\n\nwith_opt\nbool\nFalse\nInclude optimizer state\n\n\n\n\nsource\n\n\nLastMetricCallback\n\n LastMetricCallback (metrics:Listified[str]|None=None)\n\nA Callback which stores the last metric(s) value by name (or all if None) in the Learner.lastmetric dictionary"
  },
  {
    "objectID": "ffcv.utils.html",
    "href": "ffcv.utils.html",
    "title": "FFCV Utilities",
    "section": "",
    "text": "fastxtend provides the rgb_dataset_to_ffcv convenience method for easy FFCV image dataset creation.\nrgb_dataset_to_ffcv uses fastxtend’s Writer for dataset interoperability with FFCV.\n\nsource\n\nLabelField\n\n LabelField (value, names=None, module=None, qualname=None, type=None,\n             start=1)\n\nAn enumeration.\n\nsource\n\n\nrgb_dataset_to_ffcv\n\n rgb_dataset_to_ffcv (dataset:Union[torch.utils.data.dataset.Dataset,fasta\n                      i.data.core.Datasets],\n                      write_path:Union[str,pathlib.Path],\n                      max_resolution:Optional[int]=None,\n                      min_resolution:Optional[int]=None,\n                      write_mode:str='raw',\n                      smart_threshold:Optional[int]=None,\n                      compress_probability:float=0.5,\n                      jpeg_quality:float=90, interpolation=3,\n                      resample=&lt;Resampling.LANCZOS: 1&gt;,\n                      num_workers:int=-1, chunk_size:int=100,\n                      pillow_resize:bool=True,\n                      label_field:__main__.LabelField=&lt;LabelField.int:\n                      'int'&gt;)\n\nWrites PyTorch/fastai compatible dataset into FFCV format at filepath write_path.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataset\nDataset | Datasets\n\nA PyTorch Dataset or single fastai Datasets\n\n\nwrite_path\nstr | Path\n\nFile name to store dataset in FFCV beton format\n\n\nmax_resolution\nint | None\nNone\nIf maximum side length is greater than max_resolution, resize so maximum side length equals max_resolution\n\n\nmin_resolution\nint | None\nNone\nIf minimum side length is greater than min_resolution, resize so minimum side length equals min_resolution\n\n\nwrite_mode\nstr\nraw\nRGBImageField write mode: ‘raw’, ‘jpg’, ‘smart’, ‘proportion’\n\n\nsmart_threshold\nint | None\nNone\nIf write_mode='smart', JPEG-compress RAW bytes is larger than smart_threshold\n\n\ncompress_probability\nfloat\n0.5\nProbability with which image is JPEG-compressed\n\n\njpeg_quality\nfloat\n90\nQuality to use for jpeg compression if write_mode='proportion'\n\n\ninterpolation\nint\n3\nOpenCV interpolation flag for resizing images with OpenCV\n\n\nresample\nResampling\nResampling.LANCZOS\nPillow resampling filter for resizing images with Pillow\n\n\nnum_workers\nint\n-1\nNumber of workers to use. Defaults to number of CPUs\n\n\nchunk_size\nint\n100\nSize of chunks processed by each worker\n\n\npillow_resize\nbool\nTrue\nUse Pillow to resize images instead of OpenCV\n\n\nlabel_field\nLabelField\nLabelField.int\nUse FFCV IntField or FloatField for labels\n\n\n\nwrite_mode should be one of:\n\n‘raw’: write uint8 pixel values\n‘jpg’: compress to JPEG format\n‘smart’: decide between saving pixel values and JPEG compressing based on image size\n‘proportion’: JPEG compress a random subset of the data with size specified by the compress_probability argument"
  },
  {
    "objectID": "optimizer.adan.html",
    "href": "optimizer.adan.html",
    "title": "Adan: ADAptive Nesterov Momentum Optimizer",
    "section": "",
    "text": "Adan was introduced by Xie et al in Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models. Adan uses a efficient Nesterov momentum estimation method to avoid the extra computation and memory overhead of calculating the extrapolation point gradient.\nNadam also estimates Nesterov momentum, but in contrast it only estimates the first-order gradient moment while Adan estimates both first- and second-order movements.\nFor consistency with other fastai optimizers, the coefficients beta1, beta2, and beta3 have been inversed from the paper values, e.g. β1=0.98 instead of β1=0.02.\nIn addition to a fastai native implementation, Adan has fused ForEach and Torchscript implementations. See the Fused Optimizer documentation for more details.\nsource"
  },
  {
    "objectID": "optimizer.adan.html#hyperparameters",
    "href": "optimizer.adan.html#hyperparameters",
    "title": "Adan: ADAptive Nesterov Momentum Optimizer",
    "section": "Hyperparameters",
    "text": "Hyperparameters\nHyperparameter notes from Xie et al:\n\nbeta2 is the least sensitive Adan hyperparameter, default of 0.92 works for majority of tasks\nXie et al primarily tune beta3 (between 0.9-0.999) before beta1 (between 0.9-0.98) for different tasks\nAdan pairs well with large learning rates. Paper and GitHub report up to 3x larger than Lamb and up to 5-10x larger than AdamW\nXie et al use the default weight decay of 0.02 for all tasks except fine-tuning BERT (wd=0.01) and reinforcement learning (wd=0)\n\n\n\n\n\n\n\nNote\n\n\n\nWith paper_init=True, fastxtend’s Adan matches Xie et al’s Adan implementation."
  },
  {
    "objectID": "optimizer.adan.html#training-speed",
    "href": "optimizer.adan.html#training-speed",
    "title": "Adan: ADAptive Nesterov Momentum Optimizer",
    "section": "Training Speed",
    "text": "Training Speed\n\n\n\n\n\n\nImportant\n\n\n\nForEach and TorchScript optimizers have only been tested on PyTorch 1.12+ and are not guaranteed to work on older versions.\n\n\nOne critique of Adan is the original PyTorch implementation was significantly slower than AdamW. Between 41 to 97 percent slower on tested models. However, Xie et al’s implementation has been refactored to decrease memory usage and Cuda operations. These improvements have been ported to the fastxtend versions. This improved Adan implementation is benchmarked in Table 2 below.\nAs shown in Table 1, fastxtend’s fused ForEach Adan is 36 to 401 percent faster1 then a standard PyTorch implementation.\n\n\nTable 1: Increase in Adan opt_step Speed vs Native Optimizer\n\n\n\n\n\n\n\n\n\n\n\nModel\nLayers\nNative Step\nForEach Step\nForEach Speedup\nJIT Step\nJIT Speedup\n\n\n\n\nXResNet18\n1\n31ms\n13ms\n150%\n27ms\n18%\n\n\nXResNet50\n1\n69ms\n33ms\n108%\n56ms\n24%\n\n\nXSE-ResNeXt50\n1\n94ms\n45ms\n110%\n75ms\n25%\n\n\nXResNet101\n1\n115ms\n46ms\n148%\n89ms\n29%\n\n\nConvNeXt Tiny\n2\n139ms\n102ms\n36%\n124ms\n11%\n\n\nConvNeXt Small\n2\n225ms\n162ms\n39%\n198ms\n13%\n\n\nViT Patch16 Small\n2\n76ms\n46ms\n65%\n62ms\n21%\n\n\nDeBERTa Base\n1\n42ms\n8.4ms\n401%\n28ms\n46%\n\n\n\n\nNow the Adan ForEach steps are only 1.6 to 17 percent slower than the AdamW ForEach steps, and the difference in performance and as a percentage of total training time is significantly smaller. An Adan ForEach step is 0.6ms to 4ms slower than an AdamW ForEach step across measured models, instead of 6ms to 29ms with Adan native as shown in Table 2.\n\n\nTable 2: AdamW vs Adan Training Speed\n\n\n\n\n(a) Native Implementation\n\n\nModel\nAdamW Step\nAdan Step\nSlowdown\n\n\n\n\nXResNet18\n25ms\n31ms\n27%\n\n\nXResNet50\n53ms\n69ms\n28%\n\n\nXSE-ResNeXt50\n71ms\n94ms\n35%\n\n\nXResNet101\n85ms\n115ms\n33%\n\n\nConvNeXt Tiny\n124ms\n139ms\n12%\n\n\nConvNeXt Small\n196ms\n225ms\n15%\n\n\nViT Patch16 Small\n63ms\n76ms\n20%\n\n\nDeBERTa Base\n26ms\n42ms\n62%\n\n\n\n\n\n\n(b) Fused ForEach Implementation\n\n\nModel\nAdamW Step\nAdan Step\nSlowdown\n\n\n\n\nXResNet18\n13ms\n13ms\n5.0%\n\n\nXResNet50\n31ms\n33ms\n2.3%\n\n\nXSE-ResNeXt50\n43ms\n45ms\n5.9%\n\n\nXResNet101\n43ms\n46ms\n8.4%\n\n\nConvNeXt Tiny\n100ms\n102ms\n1.6%\n\n\nConvNeXt Small\n159ms\n162ms\n1.6%\n\n\nViT Patch16 Small\n44ms\n46ms\n2.9%\n\n\nDeBERTa Base\n7.2ms\n8.4ms\n17%"
  },
  {
    "objectID": "optimizer.adan.html#footnotes",
    "href": "optimizer.adan.html#footnotes",
    "title": "Adan: ADAptive Nesterov Momentum Optimizer",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBenchmarked on a GeForce 3080 Ti using PyTorch 1.13.1, Cuda 11.7, Mixed Precision, Channels Last (except DeBERTa and ViT), and fastxtend’s Simple Profiler Callback. Results may differ on other models, hardware, and across benchmarking runs. Speedup and slowdown are calculated from the total time spent on the optimization step.↩︎"
  },
  {
    "objectID": "basics.html",
    "href": "basics.html",
    "title": "Basics",
    "section": "",
    "text": "source\n\nis_listish\n\n is_listish (x)\n\nSubset of is_listy: (tuple,list,L)\n\nsource\n\n\nlistify_store_attr\n\n listify_store_attr (names=None, but='', cast=False, store_args=None,\n                     **attrs)\n\nMaybe listify, then store params named in comma-separated names from calling context into attrs in self"
  },
  {
    "objectID": "ffcv.fields.html",
    "href": "ffcv.fields.html",
    "title": "FFCV Fields",
    "section": "",
    "text": "fastxtend provides multiple FFCV fields, including existing FFCV fields as a reference, and a modified RGBImageField with Pillow support for dataset creation."
  },
  {
    "objectID": "ffcv.fields.html#ffcv-field-reference",
    "href": "ffcv.fields.html#ffcv-field-reference",
    "title": "FFCV Fields",
    "section": "FFCV Field Reference",
    "text": "FFCV Field Reference\nThese fields are from FFCV. You can find the original documentation at the FFCV API Reference.\nEach field has at least one decoder to use in a FFCV Pipline.\n\n\nBytesField\n\n BytesField ()\n\nA subclass of :class:~ffcv.fields.Field supporting variable-length byte arrays.\nIntended for use with data such as text or raw data which may not have a fixed size. Data is written sequentially while saving pointers and read by pointer lookup.\nThe writer expects to be passed a 1D uint8 numpy array of variable length for each sample.\nPair with the BytesDecoder.\n\n\n\nIntField\n\n IntField ()\n\nA subclass of :class:~ffcv.fields.Field supporting (scalar) integer values.\nPair with the IntDecoder.\n\n\n\nFloatField\n\n FloatField ()\n\nA subclass of :class:~ffcv.fields.Field supporting (scalar) floating-point (float64) values.\nPair with the FloatDecoder.\n\n\n\nNDArrayField\n\n NDArrayField (dtype:numpy.dtype, shape:Tuple[int,...])\n\nA subclass of :class:~ffcv.fields.Field supporting multi-dimensional fixed size matrices of any numpy type.\nPair with the NDArrayDecoder.\n\n\n\nJSONField\n\n JSONField ()\n\nA subclass of :class:~ffcv.fields.BytesField that encodes JSON data.\nThe writer expects to be passed a dict that is compatible with the JSON specification.\n.. warning :: Because FFCV is based on tensors/ndarrays the reader and therefore the loader can’t give return JSON to the user. This is why we provide :class:~ffcv.fields.JSONField.unpack which does the conversion. It’s up to the user to call it in the main body of the loop\nPair with the BytesDecoder.\n\n\n\n\n\n\nWarning\n\n\n\n\n\nBecause FFCV is based on tensors/ndarrays the reader and therefore the loader can’t give return JSON to the user. This is why we provide JSONField.unpack which does the conversion. It’s up to the user to call it in the main body of the loop.\n\n\n\n\n\n\nJSONField.unpack\n\n JSONField.unpack (batch)\n\nConvert back the output of a :class:~ffcv.fields.JSONField field produced by :class:~ffcv.Loader into an actual JSON.\nIt works both on an entire batch and will return an array of python dicts or a single sample and will simply return a dict.\n\n\n\nTorchTensorField\n\n TorchTensorField (dtype:torch.dtype, shape:Tuple[int,...])\n\nA subclass of :class:~ffcv.fields.Field supporting multi-dimensional fixed size matrices of any torch type.\nPair with the NDArrayDecoder."
  },
  {
    "objectID": "ffcv.fields.html#modified-fields",
    "href": "ffcv.fields.html#modified-fields",
    "title": "FFCV Fields",
    "section": "Modified Fields",
    "text": "Modified Fields\nFFCV’s RGBImageField with Pillow support for dataset creation.\n\nsource\n\nRGBImageField\n\n RGBImageField (write_mode='raw', max_resolution:int=None,\n                min_resolution:int=None, smart_threshold:int=None,\n                jpeg_quality:int=90, compress_probability:float=0.5,\n                interpolation=3, resample=&lt;Resampling.LANCZOS: 1&gt;,\n                pillow_resize:bool=False)\n\nA subclass of :class:~ffcv.fields.Field supporting RGB image data.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nwrite_mode\nstr\nraw\nHow to write the image data to the dataset file. Should be either ‘raw’(uint8 pixel values), ‘jpg’ (compress to JPEG format), ‘smart’(decide between saving pixel values and JPEG compressing based on imagesize), and ‘proportion’ (JPEG compress a random subset of the data withsize specified by the compress_probability argument). By default: ‘raw’.\n\n\nmax_resolution\nint\nNone\nIf specified, resize images to have maximum side length equal to this value if maximum side length is larger. By default: None\n\n\nmin_resolution\nint\nNone\nIf specified, resize images to have minimum side length equal to this value if minimum side length is larger. By default: None\n\n\nsmart_threshold\nint\nNone\nWhen write_mode='smart, will compress an image if RAW byte size islarger than smart_threshold.\n\n\njpeg_quality\nint\n90\nThe quality parameter for JPEG encoding (ignored for write_mode='raw'). By default 90\n\n\ncompress_probability\nfloat\n0.5\nIgnored unless write_mode='proportion'; in the latter case it is theprobability with which image is JPEG-compressed. By default 0.5.\n\n\ninterpolation\nint\n3\nThe OpenCV interpolation flag for resizing images with OpenCV. By default INTER_AREA.\n\n\nresample\nResampling\nResampling.LANCZOS\nThe Pillow resampling filter for resizing images with Pillow. By default LANCZOS.\n\n\npillow_resize\nbool\nFalse\nUse Pillow to resize images instead of OpenCV. By default False (OpenCV).\n\n\nReturns\nNone\n\n\n\n\n\nPair with one of the following three decoders:\n\nSimpleRGBImageDecoder\nRandomResizedCropRGBImageDecoder\nCenterCropRGBImageDecoder"
  },
  {
    "objectID": "ffcv.inference.html",
    "href": "ffcv.inference.html",
    "title": "FFCV Inference Using fastai",
    "section": "",
    "text": "FFCV is currently hardcoded to use OpenCV’s INTER_AREA when resizing images, while fastai uses Pillow.\nThis module provides OpenCV versions of fastai DataBlock items, prefixed with FFAI, for inference after training with Loader."
  },
  {
    "objectID": "ffcv.inference.html#ffai-image-loading",
    "href": "ffcv.inference.html#ffai-image-loading",
    "title": "FFCV Inference Using fastai",
    "section": "FFAI Image Loading",
    "text": "FFAI Image Loading\nFFAIBase is the OpenCV version of fastai.vision.core.PILBase. It uses OpenCV for image loading and type dispatch transforms.\nUnlike OpenCV, these images load in RGB format.\n\nsource\n\nFFAIBase\n\n FFAIBase (input_array)\n\nBase class for a FFCV image that can show itself and convert to a Tensor\n\nsource\n\n\nFFAIImage\n\n FFAIImage (input_array)\n\nA RGB FFCV image that can show itself and converts to TensorImage\nUnlike OpenCV images, FFCV images and FFAIImage are in RGB format.\n\nsource\n\n\nFFAIImageBW\n\n FFAIImageBW (input_array)\n\nA BW FFCV image that can show itself and converts to TensorImageBW\n\nsource\n\n\nFFAIMask\n\n FFAIMask (input_array)\n\nA FFCV image mask that can show itself and converts to TensorMask\n\nsource\n\n\nencodes\n\n encodes (o:__main__.FFAIMask)\n\n\nsource\n\n\nencodes\n\n encodes (o:__main__.FFAIMask)\n\nAny data augmentation transform that runs on FFAI Images must be run before the fastai.data.transform.ToTensor transform, which has an order of 5."
  },
  {
    "objectID": "ffcv.inference.html#ffai-imageblocks",
    "href": "ffcv.inference.html#ffai-imageblocks",
    "title": "FFCV Inference Using fastai",
    "section": "FFAI ImageBlocks",
    "text": "FFAI ImageBlocks\nFFAIImageBlock is identical to fastai.vision.data.ImageBlock except it uses FFAIImage by default.\n\nsource\n\nFFAIImageBlock\n\n FFAIImageBlock (cls:__main__.FFAIBase=&lt;class '__main__.FFAIImage'&gt;)\n\nA TransformBlock for images of cls"
  },
  {
    "objectID": "ffcv.inference.html#basic-ffai-item-transforms",
    "href": "ffcv.inference.html#basic-ffai-item-transforms",
    "title": "FFCV Inference Using fastai",
    "section": "Basic FFAI Item Transforms",
    "text": "Basic FFAI Item Transforms\nCurrently only FFAICenterCrop is implemented for inference.\n\nsource\n\nFFAICenterCrop\n\n FFAICenterCrop (output_size:int, ratio:float, interpolation=(3, 0))\n\nA transform with a __repr__ that shows its attrs\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\noutput_size\nint\n\nSquare size to resize and crop image to\n\n\nratio\nfloat\n\nRatio of (crop size) / (min side length)\n\n\ninterpolation\ntuple\n(3, 0)\nOpenCV interpolation mode, resamples[1] for mask"
  },
  {
    "objectID": "audio.04_learner.html",
    "href": "audio.04_learner.html",
    "title": "Audio Learner",
    "section": "",
    "text": "source\n\nStackSpecCallback\n\n StackSpecCallback (after_create=None, before_fit=None, before_epoch=None,\n                    before_train=None, before_batch=None, after_pred=None,\n                    after_loss=None, before_backward=None,\n                    after_cancel_backward=None, after_backward=None,\n                    before_step=None, after_cancel_step=None,\n                    after_step=None, after_cancel_batch=None,\n                    after_batch=None, after_cancel_train=None,\n                    after_train=None, before_validate=None,\n                    after_cancel_validate=None, after_validate=None,\n                    after_cancel_epoch=None, after_epoch=None,\n                    after_cancel_fit=None, after_fit=None)\n\nStacks tuples of TensorSpec or TensorMelSpec. ToDo: add resizing\n\nsource\n\n\naudio_learner\n\n audio_learner (dls:fastai.data.core.DataLoaders,\n                model:torch.nn.modules.module.Module, loss_func:Union[torc\n                h.nn.modules.module.Module,Callable[...,torch.Tensor],None\n                Type]=None, opt_func:Union[fastai.optimizer.Optimizer,fast\n                ai.optimizer.OptimWrapper]=&lt;function Adam&gt;,\n                lr:Union[float,slice]=0.001, splitter:Callable[[torch.nn.m\n                odules.module.Module],list[torch.Tensor]]=&lt;function\n                trainable_params&gt;, cbs:Union[fastai.callback.core.Callback\n                ,Iterable[fastai.callback.core.Callback],MutableSequence[f\n                astai.callback.core.Callback],fastcore.foundation.L,fastco\n                re.basics.fastuple,NoneType]=None, metrics:Union[fastai.le\n                arner.Metric,Iterable[fastai.learner.Metric],MutableSequen\n                ce[fastai.learner.Metric],fastcore.foundation.L,fastcore.b\n                asics.fastuple,NoneType]=None,\n                path:Union[str,pathlib.Path,NoneType]=None,\n                model_dir:Union[str,pathlib.Path]='models',\n                wd:Optional[float]=None, wd_bn_bias:bool=False,\n                train_bn:bool=True, moms:tuple[float,float,float]=(0.95,\n                0.85, 0.95), default_cbs:bool=True)\n\nAn Audio specific Learner that stacks tuples of TensorSpec or TensorMelSpec\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndls\nDataLoaders\n\nDataLoaders containing fastai or PyTorch DataLoaders\n\n\nmodel\nnn.Module\n\nPyTorch model for training or inference\n\n\nloss_func\nnn.Module | Callable[…, Tensor] | None\nNone\nLoss function. Defaults to dls loss\n\n\nopt_func\nOptimizer | OptimWrapper\nAdam\nOptimization function for training\n\n\nlr\nfloat | slice\n0.001\nDefault learning rate\n\n\nsplitter\nCallable[[nn.Module], list[Tensor]]\ntrainable_params\nSplit model into parameter groups. Defaults to one parameter group\n\n\ncbs\nListified[Callback] | None\nNone\nCallbacks to add to Learner\n\n\nmetrics\nListified[Metric] | None\nNone\nMetrics to calculate on validation set\n\n\npath\nstr | Path | None\nNone\nParent directory to save, load, and export models. Defaults to dls path\n\n\nmodel_dir\nstr | Path\nmodels\nSubdirectory to save and load models\n\n\nwd\nfloat | None\nNone\nDefault weight decay\n\n\nwd_bn_bias\nbool\nFalse\nApply weight decay to normalization and bias parameters\n\n\ntrain_bn\nbool\nTrue\nTrain frozen normalization layers\n\n\nmoms\ntuple[float, float, float]\n(0.95, 0.85, 0.95)\nDefault momentum for schedulers\n\n\ndefault_cbs\nbool\nTrue\nInclude default Callbacks\n\n\nReturns\nLearner"
  },
  {
    "objectID": "schedulers.html",
    "href": "schedulers.html",
    "title": "Schedulers",
    "section": "",
    "text": "source\n\nLearner.fit_flat_warmup\n\n Learner.fit_flat_warmup (n_epoch:int, lr:Optional[float]=None,\n                          div:Union[int,float]=25.0,\n                          div_final:Union[int,float]=100000.0,\n                          pct_start:float=0.75, warm_pct:float=0.2,\n                          warm_epoch:int=5, warm_mode:str='auto', warm_sch\n                          ed:Callable[...,fastai.callback.schedule._Anneal\n                          er]=&lt;function SchedCos&gt;,\n                          wd:Optional[float]=None, cbs:Union[fastai.callba\n                          ck.core.Callback,Iterable[fastai.callback.core.C\n                          allback],MutableSequence[fastai.callback.core.Ca\n                          llback],fastcore.foundation.L,fastcore.basics.fa\n                          stuple,NoneType]=None, reset_opt:bool=False)\n\nFit self.model for n_epoch at flat lr with a warmup and ending with cosine annealing.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_epoch\nint\n\nNumber of epochs\n\n\nlr\nfloat | None\nNone\nMaximum learning rate\n\n\ndiv\nNumeric\n25.0\nInitial learning rate: lr/div\n\n\ndiv_final\nNumeric\n100000.0\nFinal learning rate: lr/div_final\n\n\npct_start\nfloat\n0.75\nStart learning rate cosine annealing\n\n\nwarm_pct\nfloat\n0.2\nLearning rate warmup in percent\n\n\nwarm_epoch\nint\n5\nLearning rate warmup in epochs\n\n\nwarm_mode\nstr\nauto\nWarmup using ‘epoch’, ‘pct’, or min of epoch/pct if ‘auto’\n\n\nwarm_sched\nCallable[…, _Annealer]\nSchedCos\nLearning rate warmup schedule\n\n\nwd\nfloat | None\nNone\nWeight decay, defaults to Optimizer weight decay\n\n\ncbs\nListified[Callback] | None\nNone\nTemporary Callbacks to apply during fit\n\n\nreset_opt\nbool\nFalse\nReset Optimizer before fit\n\n\n\nfit_flat_warmup is identical to fastai’s fit_flat_cos, except with an added learning rate warmup phase.\nBy default, fit_flat_warmup will apply learning rate warmup for a minimum of warm_pct percent of training steps or warm_epoch number of training epochs. Set warm_mode='pct' to warmup the learning rate for warm_pct percent of training steps or set warm_mode='epoch' to warmup the learning rate for warm_epoch number of epochs.\nwarm_sched can be one of SchedCos (the default), SchedLin,SchedExp, SchedPoly, or a custom fastai annealer based schedule. SchedPoly must be passed as partial function: partial(SchedPoly, power=0.5).\n\n\n\n\n\n\nsource\n\n\nLearner.fit_cos_anneal\n\n Learner.fit_cos_anneal (n_epoch:int, lr:Optional[float]=None,\n                         div:Union[int,float]=25.0,\n                         div_final:Union[int,float]=100000.0,\n                         warm_pct:float=0.2, warm_epoch:int=5,\n                         warm_mode:str='auto', warm_sched:Callable[...,fas\n                         tai.callback.schedule._Annealer]=&lt;function\n                         SchedCos&gt;, wd:Optional[float]=None, cbs:Union[fas\n                         tai.callback.core.Callback,Iterable[fastai.callba\n                         ck.core.Callback],MutableSequence[fastai.callback\n                         .core.Callback],fastcore.foundation.L,fastcore.ba\n                         sics.fastuple,NoneType]=None,\n                         reset_opt:bool=False)\n\nFit self.model for n_epoch using a with cosine annealing schedule with a max lr and optional warmup.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_epoch\nint\n\nNumber of epochs\n\n\nlr\nfloat | None\nNone\nMaximum learning rate\n\n\ndiv\nNumeric\n25.0\nInitial learning rate: lr/div\n\n\ndiv_final\nNumeric\n100000.0\nFinal learning rate: lr/div_final\n\n\nwarm_pct\nfloat\n0.2\nLearning rate warmup in percent\n\n\nwarm_epoch\nint\n5\nLearning rate warmup in epochs\n\n\nwarm_mode\nstr\nauto\nWarmup using ‘epoch’, ‘pct’, or min of epoch/pct ‘auto’\n\n\nwarm_sched\nCallable[…, _Annealer]\nSchedCos\nLearning rate warmup schedule\n\n\nwd\nfloat | None\nNone\nWeight decay, defaults to Optimizer weight decay\n\n\ncbs\nListified[Callback] | None\nNone\nTemporary Callbacks to apply during fit\n\n\nreset_opt\nbool\nFalse\nReset Optimizer before fit\n\n\n\nTo disable learning rate warmup, set warm_pct=0.\nBy default, fit_cos_anneal will apply learning rate warmup for a minimum of warm_pct percent of training steps or warm_epoch number of training epochs. Set warm_mode='pct' to warmup the learning rate for warm_pct percent of training steps or set warm_mode='epoch' to warmup the learning rate for warm_epoch number of epochs.\nwarm_sched can be one of SchedCos (the default), SchedLin,SchedExp, SchedPoly, or a custom fastai annealer based schedule. SchedPoly must be passed as partial function: partial(SchedPoly, power=0.5).\nWith optional learning rate warmup:\n\n\n\n\n\nAnd without learning ratewarmup:\n\n\n\n\n\n\nsource\n\n\nLearner.fit_flat_varied\n\n Learner.fit_flat_varied (n_epoch:int, start_lr:Optional[float]=None,\n                          div_final:Union[int,float]=100000.0,\n                          pct_start:float=0.75, wd:Optional[float]=None, n\n                          ext_lr:Union[float,Iterable[float],MutableSequen\n                          ce[float],fastcore.foundation.L,fastcore.basics.\n                          fastuple,slice,NoneType]=None, change_by:Union[i\n                          nt,Iterable[int],MutableSequence[int],fastcore.f\n                          oundation.L,fastcore.basics.fastuple,float,Itera\n                          ble[float],MutableSequence[float],NoneType]=None\n                          , change_time:Union[int,Iterable[int],MutableSeq\n                          uence[int],fastcore.foundation.L,fastcore.basics\n                          .fastuple,float,Iterable[float],MutableSequence[\n                          float]]=1, change_sched:Union[Callable[...,fasta\n                          i.callback.schedule._Annealer],Iterable[Callable\n                          [...,fastai.callback.schedule._Annealer]],Mutabl\n                          eSequence[Callable[...,fastai.callback.schedule.\n                          _Annealer]],fastcore.foundation.L,fastcore.basic\n                          s.fastuple,NoneType]=None, cbs:Union[fastai.call\n                          back.core.Callback,Iterable[fastai.callback.core\n                          .Callback],MutableSequence[fastai.callback.core.\n                          Callback],fastcore.foundation.L,fastcore.basics.\n                          fastuple,NoneType]=None, reset_opt:bool=False)\n\nFit self.model for n_epoch at flat start_lr, then change to flat next_lr at change_by, optionally with cosine annealing or custom change_sched over change_time. Final cosine annealing at pct_start.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_epoch\nint\n\nNumber of epochs\n\n\nstart_lr\nfloat | None\nNone\nInitial learning rate\n\n\ndiv_final\nNumeric\n100000.0\nFinal learning rate: lr/div_final\n\n\npct_start\nfloat\n0.75\nStart learning rate cosine annealing\n\n\nwd\nfloat | None\nNone\nWeight decay, defaults to Optimizer weight decay\n\n\nnext_lr\nListified[float] | slice | None\nNone\nLearning rates to switch to at change_by. Must be same length as change_by\n\n\nchange_by\nListified[int] | Listified[float] | None\nNone\nEpochs or percent of steps to switch to next_lr by. Must be same length as next_lr\n\n\nchange_time\nListified[int] | Listified[float]\n1\nIf greater than 0 (percent of steps or epochs), how long to change to next_lr. Must be same length as next_lr\n\n\nchange_sched\nListified[Callable[…, _Annealer]] | None\nNone\nSchedule(s) for change. Defaults to SchedCos. Must be same length as next_lr\n\n\ncbs\nListified[Callback] | None\nNone\nTemporary Callbacks to apply during fit\n\n\nreset_opt\nbool\nFalse\nReset Optimizer before fit\n\n\n\nchange_sched can be one of SchedLin, SchedCos (the default), SchedExp, SchedPoly, or a custom fastai annealer based schedule. SchedPoly must be passed as partial function: partial(SchedPoly, power=0.5).\n\n\nExample Fit Flat Varied Schedules\nDiscriminative Linear Warmup:\nlearn.fit_flat_varied(4, slice(3e-5, 3e-3), next_lr=3e-3, change_by=1, change_time=1, change_sched=SchedLin)\n\n\n\ndiscriminative linear warmup\n\n\nMultiple Cosine Annealing:\nlearn.fit_flat_varied((15, 8e-3, next_lr=[6e-3, 4e-3], change_by=[4, 8], change_time=2)\n\n\n\nmultiple cosine annealing\n\n\nImmediate Change:\nlearn.fit_flat_varied((10, 8e-3, next_lr=[6e-3, 4e-3], change_by=[0.25, 0.5], change_time=0)\n\n\n\nimmediate change"
  },
  {
    "objectID": "losses.html",
    "href": "losses.html",
    "title": "Loss Functions",
    "section": "",
    "text": "source\n\nBCEWithLogitsLoss\n\n BCEWithLogitsLoss (weight:Tensor|None=None, reduction:str='mean',\n                    pos_weight:Tensor|None=None, thresh:float=0.5)\n\nLike nn.BCEWithLogitsLoss, but with ‘batchmean’ reduction from MosiacML. batchmean scales loss by the batch size which results in larger loss values more similar to nn.CrossEntropy then mean reduction.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nweight\nTensor | None\nNone\nRescaling weight for each class\n\n\nreduction\nstr\nmean\nPytorch reduction to apply to loss output. Also supports ‘batchmean’.\n\n\npos_weight\nTensor | None\nNone\nWeight of positive examples\n\n\nthresh\nfloat\n0.5\nThreshold for decodes\n\n\n\n\nsource\n\n\nClassBalancedCrossEntropyLoss\n\n ClassBalancedCrossEntropyLoss (samples_per_class:Tensor|Listy[int],\n                                beta:float=0.99, ignore_index:int=-100,\n                                reduction:str='mean',\n                                label_smoothing:float=0.0, axis:int=-1)\n\nClass Balanced Cross Entropy Loss, from https://arxiv.org/abs/1901.05555.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsamples_per_class\nTensor | Listy[int]\n\nNumber of samples per class\n\n\nbeta\nfloat\n0.99\nRebalance factor, usually between [0.9, 0.9999]\n\n\nignore_index\nint\n-100\nTarget value which is ignored and doesn’t contribute to gradient\n\n\nreduction\nstr\nmean\nPytorch reduction to apply to loss output\n\n\nlabel_smoothing\nfloat\n0.0\nConvert hard targets to soft targets, defaults to no smoothing\n\n\naxis\nint\n-1\nArgMax axis for fastai `decodes``\n\n\n\n\nsource\n\n\nClassBalancedBCEWithLogitsLoss\n\n ClassBalancedBCEWithLogitsLoss (samples_per_class:Tensor|Listy[int],\n                                 beta:float=0.99, reduction:str='mean',\n                                 pos_weight:Tensor|None=None,\n                                 thresh:float=0.5)\n\nClass Balanced BCE With Logits Loss, from https://arxiv.org/abs/1901.05555 with ‘batchmean’ reduction\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsamples_per_class\nTensor | Listy[int]\n\nNumber of samples per class\n\n\nbeta\nfloat\n0.99\nRebalance factor, usually between [0.9, 0.9999]\n\n\nreduction\nstr\nmean\nPytorch reduction to apply to loss output. Also supports ‘batchmean’.\n\n\npos_weight\nTensor | None\nNone\nBCE Weight of positive examples\n\n\nthresh\nfloat\n0.5\nThreshold for fastai decodes"
  },
  {
    "objectID": "optimizer.eightbit.html",
    "href": "optimizer.eightbit.html",
    "title": "8-Bit Optimizers",
    "section": "",
    "text": "bitsandbytes 8-bit optimizers can reduce optimizer memory usage up to 75% compared to 32-bit optimizers.\nWhile it is possible to use bitsandbytes optimizers1 with fastai via fastai.optimizer.OptimWrapper, this doesn’t provide compatibility with all fastai optimizer features. fastxtend adds full fastai compatibility to bitsandbytes 8-bit optimizers, including per-parameter weight decay, automatic weight decay exclusion for normalization and bias terms, and discriminative learning rate support.\nTo use 8-bit optimizers, install bitsandbytes on a machine with a Cuda device\nthen import fastxtend optimizers after importing fastai\nIf training NLP models, you may need to replace the PyTorch embedding layer with a bitsandbytes layer : torch.nn.Embedding(..) -&gt; bnb.nn.Embedding(..).\nCheck out the bitsandbytes readme for more details on using 8-bit optimizers."
  },
  {
    "objectID": "optimizer.eightbit.html#fastai-and-bitsandbytes-compatibility",
    "href": "optimizer.eightbit.html#fastai-and-bitsandbytes-compatibility",
    "title": "8-Bit Optimizers",
    "section": "fastai and bitsandbytes Compatibility",
    "text": "fastai and bitsandbytes Compatibility\n\nsource\n\nEightBitFastaiAdapter\n\n EightBitFastaiAdapter ()\n\nBase for adding fastai optimizer functionality to EightBit Optimizers\n\nsource\n\n\nEightBitCommon\n\n EightBitCommon ()\n\nCommon changes to EightBit Optimizers\n\nsource\n\n\nEightBit1StateOptimizer\n\n EightBit1StateOptimizer (optimizer_name, params, lr=0.001, mom=0.9,\n                          sqr_mom=0.0, eps=1e-08, wd=0.0, optim_bits=8,\n                          args=None, min_8bit_size=4096,\n                          percentile_clipping=100, block_wise=True,\n                          max_unorm=0.0, skip_zeros=False, is_paged=False,\n                          sync_each_step=False)\n\nAdds fastai optimizer functionality & compatability to Optimizer1State\n\nsource\n\n\nEightBit2StateOptimizer\n\n EightBit2StateOptimizer (optimizer_name, params, lr=0.001, mom=0.9,\n                          sqr_mom=0.999, eps=1e-08, wd=0.0, optim_bits=8,\n                          args=None, min_8bit_size=4096,\n                          percentile_clipping=100, block_wise=True,\n                          max_unorm=0.0, skip_zeros=False, is_paged=False,\n                          sync_each_step=False)\n\nAdds fastai optimizer functionality & compatability to Optimizer2State"
  },
  {
    "objectID": "optimizer.eightbit.html#bit-optimizers",
    "href": "optimizer.eightbit.html#bit-optimizers",
    "title": "8-Bit Optimizers",
    "section": "8-bit Optimizers",
    "text": "8-bit Optimizers\n\nsource\n\nSGD8bitOptimizer\n\n SGD8bitOptimizer (params, lr, mom, wd=0, args=None, min_8bit_size=4096,\n                   percentile_clipping=100, block_wise=True,\n                   sync_each_step=False)\n\nA fastai-compatible bitsandbytes 8-bit SGD optimizer\n\nsource\n\n\nRMSProp8bitOptimizer\n\n RMSProp8bitOptimizer (params, lr=0.01, sqr_mom=0.99, eps=1e-08, wd=0,\n                       args=None, min_8bit_size=4096,\n                       percentile_clipping=100, block_wise=True,\n                       sync_each_step=False)\n\nA fastai-compatible bitsandbytes 8-bit RMSProb optimizer\n\nsource\n\n\nAdamW8bitOptimizer\n\n AdamW8bitOptimizer (params, lr=0.001, mom=0.9, sqr_mom=0.99, eps=1e-08,\n                     wd=0.01, args=None, min_8bit_size=4096,\n                     percentile_clipping=100, block_wise=True,\n                     is_paged=False, sync_each_step=False)\n\nA fastai-compatible bitsandbytes 8-bit AdamW optimizer\n\nsource\n\n\nLARS8bitOptimizer\n\n LARS8bitOptimizer (params, lr, mom=0, wd=0, args=None,\n                    min_8bit_size=4096, percentile_clipping=100,\n                    trust_coeff=0.02, sync_each_step=False)\n\nA fastai-compatible bitsandbytes 8-bit LARS optimizer\n\nsource\n\n\nLAMB8bitOptimizer\n\n LAMB8bitOptimizer (params, lr=0.001, mom=0.9, sqr_mom=0.999, eps=1e-08,\n                    wd=0, args=None, min_8bit_size=4096,\n                    percentile_clipping=100, block_wise=False,\n                    sync_each_step=False)\n\nA fastai-compatible bitsandbytes 8-bit LAMB optimizer\n\nsource\n\n\nLion8bitOptimizer\n\n Lion8bitOptimizer (params, lr=0.0001, beta1=0.9, beta2=0.99, wd=0,\n                    args=None, min_8bit_size=4096,\n                    percentile_clipping=100, block_wise=True,\n                    is_paged=False, sync_each_step=False)\n\nA fastai-compatible bitsandbytes 8-bit Lion optimizer"
  },
  {
    "objectID": "optimizer.eightbit.html#footnotes",
    "href": "optimizer.eightbit.html#footnotes",
    "title": "8-Bit Optimizers",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOr any PyTorch-compatible optimizer.↩︎"
  },
  {
    "objectID": "test_utils.html",
    "href": "test_utils.html",
    "title": "Test Utils",
    "section": "",
    "text": "As of this comment, nbdev cannot export _all_ if it is on more then one line\n\nsource\n\ntst_param\n\n tst_param (val, grad=None)\n\nCreate a tensor with val and a gradient of grad for testing\n\nsource\n\n\ntst_params\n\n tst_params ()"
  },
  {
    "objectID": "ffcv.loader.html",
    "href": "ffcv.loader.html",
    "title": "FFCV Loader",
    "section": "",
    "text": "fastxtend’s Loader adds fastai features to FFCV’s Loader, including one_batch, show_batch, show_results, and support for GPU batch transforms, to name a few.\n\nsource\n\nLoader\n\n Loader (fname:str|Path, batch_size:int, num_workers:int=-1,\n         os_cache:bool=True, order:ORDER_TYPE=&lt;OrderOption.SEQUENTIAL: 1&gt;,\n         distributed:bool=False, seed:int|None=None,\n         indices:Sequence[int]|None=None,\n         pipelines:Mapping[str,Sequence[Operation|nn.Module]]={},\n         custom_fields:Mapping[str,Field]={}, drop_last:bool|None=None,\n         batches_ahead:int=2, recompile:bool=False,\n         device:str|int|torch.device|None=None, async_tfms:bool=False,\n         n_inp:int|None=None, split_idx:int|None=None, do_setup:bool=True,\n         **kwargs)\n\nFFCV Loader with fastai Transformed DataLoader TfmdDL batch transforms\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfname\nstr | Path\n\nPath to the location of the dataset (FFCV beton format)\n\n\nbatch_size\nint\n\nBatch size\n\n\nnum_workers\nint\n-1\nNumber of CPU cores to use in parallel (default: All available up to 16)\n\n\nos_cache\nbool\nTrue\nLeverage the OS for caching. Beneficial when there is enough memory to cache the dataset\n\n\norder\nORDER_TYPE\nOrderOption.SEQUENTIAL\nDataset traversal order, one of: SEQEUNTIAL, RANDOM, QUASI_RANDOM\n\n\ndistributed\nbool\nFalse\nEmulates the behavior of PyTorch’s DistributedSampler for distributed training\n\n\nseed\nint | None\nNone\nRandom seed for batch ordering\n\n\nindices\nSequence[int] | None\nNone\nSubset dataset by returning only these indices\n\n\npipelines\nMapping[str, Sequence[Operation | nn.Module]]\n{}\nDictionary defining for each field the sequence of Decoders and transforms to apply\n\n\ncustom_fields\nMapping[str, Field]\n{}\nDictonary informing Loader of the types associated to fields that are using a custom type\n\n\ndrop_last\nbool | None\nNone\nDrop non-full batch in each epoch. Defaults to True if order is SEQEUNTIAL\n\n\nbatches_ahead\nint\n2\nNumber of batches prepared in advance; balances latency and memory\n\n\nrecompile\nbool\nFalse\nRecompile at every epoch. Required if FFCV augmentations change during training\n\n\ndevice\nstr | int | torch.device | None\nNone\nDevice to place batch. Defaults to fastai’s default_device\n\n\nasync_tfms\nbool\nFalse\nAsynchronously run batch_tfms before batch is drawn.\n\n\nn_inp\nint | None\nNone\nNumber of inputs to the model. Defaults to pipelines length minus 1\n\n\nsplit_idx\nint | None\nNone\nApply batch transform(s) to training (0) or validation (1) set. Defaults to valid if order is SEQEUNTIAL\n\n\ndo_setup\nbool\nTrue\nRun setup() for batch transform(s)\n\n\nkwargs\n\n\n\n\n\n\nImportant Loader arguments:\n\norder: Controls how much memory is used for dataset caching and whether the dataset is randomly shuffled. Can be one of RANDOM, QUASI_RANDOM, or SEQUENTIAL. See the note below for more details. Defaults to SEQUENTIAL, which is unrandomized.\nos_cache: By default, FFCV will attempt to cache the entire dataset into RAM using the operating system’s caching. This can be changed by setting os_cache=False or setting the enviroment variable ‘FFCV_DEFAULT_CACHE_PROCESS’ to “True” or “1”. If os_cache=False then order must be set to QUASI_RANDOM for the training Loader.\nnum_workers: If not set, will use all CPU cores up to 16 by default.\nbatches_ahead: Controls the number of batches ahead the Loader works. Increasing uses more RAM, both CPU and GPU. Defaults to 2.\nn_inp: Controls which inputs to pass to the model. By default, set to number of pipelines minus 1.\ndrop_last: Whether to drop the last partial batch. By default, will set to True if order is RANDOM or QUASI_RANDOM, False if SEQUENTIAL.\ndevice: The device to place the processed batches of data on. Defaults to fastai.torch_core.default_device if not set.\nasync_tfms: Asynchronously apply batch_tfms before the batch is drawn. Can accelerate training if GPU compute isn’t fully saturated (95% or less) or if only using IntToFloatTensor and Normalize.\nsplit_idx: This tells the fastai batch transforms what dataset they are operating on. By default will use 0 (train) if order is RANDOM or QUASI_RANDOM, 1 (valid) if SEQUENTIAL.\ndistributed: For distributed training on multiple GPUs. Emulates the behavior of PyTorch’s DistributedSampler. QUASI_RANDOM is unavailable with distributed training.\n\n\n\n\n\n\n\nNote: Order Memory Usage\n\n\n\n\n\nEach order option requires differing amounts of system memory.\n\nRANDOM caches the entire dataset in memory for fast random sampling. RANDOM uses the most memory.\nQUASI_RANDOM caches a subset of the dataset at a time in memory and randomly samples from the subset. Use when the entire dataset cannot fit into memory.\nSEQUENTIAL requires least memory. It loads a few samples ahead of time. As the name suggests, it is not random, and primarly is for validation.\n\n\n\n\nAsynchronous batch transforms can accelerate training by decreasing the draw time at the expense of slightly longer batch step. If the GPU isn’t fully saturated, usually 95% or less compute use, this will be a net gain in training performance. async_tfms=True pairs well with ProgressiveResize, as the GPU is almost never saturated when training on smaller then full sized images. When near or fully saturated, asynchronous batch transforms usually result a wash in training time.\n\nsource\n\n\nLoader.one_batch\n\n Loader.one_batch (batches_ahead:bool=False)\n\nReturn one processed batch of input(s) and target(s), optionally loading batches_ahead\n\nsource\n\n\nDataLoaderMixin.show_batch\n\n DataLoaderMixin.show_batch (b:Optional[Tuple[torch.Tensor,...]]=None,\n                             max_n:int=9, ctxs=None, show:bool=True,\n                             unique:bool=False, **kwargs)\n\nShow max_n input(s) and target(s) from the batch.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nb\nTuple[Tensor, …] | None\nNone\nBatch to show. If None calls one_batch\n\n\nmax_n\nint\n9\nMaximum number of items to show\n\n\nctxs\nNoneType\nNone\nList of ctx objects to show data. Could be matplotlib axis, DataFrame etc\n\n\nshow\nbool\nTrue\nIf False, return decoded batch instead of showing\n\n\nunique\nbool\nFalse\nWhether to show only one\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\nDataLoaderMixin.show_results\n\n DataLoaderMixin.show_results (b, out, max_n:int=9, ctxs=None,\n                               show:bool=True, **kwargs)\n\nShow max_n results with input(s), target(s) and prediction(s).\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nb\n\n\nBatch to show results for\n\n\nout\n\n\nPredicted output from model for the batch\n\n\nmax_n\nint\n9\nMaximum number of items to show\n\n\nctxs\nNoneType\nNone\nList of ctx objects to show data. Could be matplotlib axis, DataFrame etc\n\n\nshow\nbool\nTrue\nIf False, return decoded batch instead of showing\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\nDataLoaderMixin.to\n\n DataLoaderMixin.to (device:Union[int,str,torch.device])\n\nSets self.device=device.\n\nsource\n\n\nDataLoaderMixin.n_inp\n\n DataLoaderMixin.n_inp ()\n\nNumber of elements in a batch for model input\n\nsource\n\n\nDataLoaderMixin.decode\n\n DataLoaderMixin.decode (b:Tuple[torch.Tensor,...])\n\nDecode batch b\n\nsource\n\n\nDataLoaderMixin.decode_batch\n\n DataLoaderMixin.decode_batch (b:Tuple[torch.Tensor,...], max_n:int=9)\n\nDecode up to max_n input(s) from batch b"
  },
  {
    "objectID": "callback.compiler.html",
    "href": "callback.compiler.html",
    "title": "PyTorch Compile",
    "section": "",
    "text": "The CompilerCallback and DynamoExplainCallback provide an easy to use torch.compile integration for fastai.\nFor more information on torch.compile please read PyTorch’s getting started guide. For troubleshooting torch.compile refer to this PyTorch Nightly guide.\nThis module is not imported via any fastxtend all imports. You must import it separately after importing fastai and fastxtend as it modifies model saving, loading, and training:\nTo use, create a fastai.learner.Learner with a torch.compile compatible model and call compile on the Learner or pass CompilerCallback to the Learner of fit method callbacks.\nsource"
  },
  {
    "objectID": "callback.compiler.html#convenience-method",
    "href": "callback.compiler.html#convenience-method",
    "title": "PyTorch Compile",
    "section": "Convenience Method",
    "text": "Convenience Method\nfastxtend adds a convenience method to fastai.learner.Learner to easily enable torch.compile.\n\nsource\n\nLearner.compile\n\n Learner.compile (fullgraph:bool=False, dynamic:bool=False,\n                  backend:Union[str,Callable]='inductor',\n                  mode:Union[str,__main__.CompileMode,NoneType]=None,\n                  options:Optional[Dict[str,Union[str,int,bool]]]=None, ma\n                  tmul_precision:Union[str,__main__.MatMulPrecision]='high\n                  ', recompile:bool=False, verbose:bool=False)\n\nSet Learner to compile model using torch.compile via CompilerCallback\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfullgraph\nbool\nFalse\nPrevent breaking model into subgraphs\n\n\ndynamic\nbool\nFalse\nUse dynamic shape tracing. Sets to False if PyTorch &lt; 2.1\n\n\nbackend\nstr | Callable\ninductor\ntorch.compile backend to use\n\n\nmode\nstr | CompileMode | None\nNone\ntorch.compile mode to use\n\n\noptions\nDict[str, Union[str, int, bool]] | None\nNone\nExtra options to pass to compile backend\n\n\nmatmul_precision\nstr | MatMulPrecision\nhigh\nSet Ampere and newer matmul precision\n\n\nrecompile\nbool\nFalse\nForce a compiled model to recompile. Use when freezing/unfreezing a compiled model.\n\n\nverbose\nbool\nFalse\nVerbose output\n\n\n\ncompile only sets dynamic if using PyTorch 2.1 or later, for PyTorch 2.0 it’s hardcoded to False. You can override this by directly setting via CompilerCallback.\nTo use, call the compile method after initalizing a fastai.learner.Learner.\nlearn = Learner(...).compile()\nlearn.fine_tune(1)"
  },
  {
    "objectID": "callback.compiler.html#compatability-patches",
    "href": "callback.compiler.html#compatability-patches",
    "title": "PyTorch Compile",
    "section": "Compatability Patches",
    "text": "Compatability Patches\nThese patches integrate torch.compile with fastai exporting, loading, freezing, unfreezing, and fine tuning.\n\nExporting and Loading\n\nsource\n\n\nLearner.export\n\n Learner.export\n                 (fname:Union[str,os.PathLike,BinaryIO,IO[bytes]]='export.\n                 pkl', pickle_module:Any=&lt;module 'pickle' from '/opt/hoste\n                 dtoolcache/Python/3.9.18/x64/lib/python3.9/pickle.py'&gt;,\n                 pickle_protocol:int=2)\n\nExport the content of self without the items and the optimizer state for inference\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfname\nFILE_LIKE\nexport.pkl\nLearner export file name, path, bytes, or IO\n\n\npickle_module\nAny\npickle\nModule used for pickling metadata and objects\n\n\npickle_protocol\nint\n2\nPickle protocol used\n\n\n\n\nsource\n\n\nload_learner\n\n load_learner (fname:Union[str,os.PathLike,BinaryIO,IO[bytes]],\n               cpu:bool=True, pickle_module=&lt;module 'pickle' from '/opt/ho\n               stedtoolcache/Python/3.9.18/x64/lib/python3.9/pickle.py'&gt;)\n\nLoad a Learner object in fname, by default putting it on the cpu\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfname\nFILE_LIKE\n\nFile name, path, bytes, or IO\n\n\ncpu\nbool\nTrue\nLoad model to CPU\n\n\npickle_module\nmodule\npickle\nModule used for unpickling metadata and objects\n\n\n\nBy default, load_learner will remove the CompilerCallback.\n\n\nFreezing and Unfreezing\n\nsource\n\n\nLearner.freeze_to\n\n Learner.freeze_to (n:int)\n\nFreeze parameter groups up to n\nFreezing and unfreezing models works, but they need to be recompiled after. freeze_to will set CompilerCallback to recompile the model or warn users they need to manually recompile.\n\n\nTraining\n\nsource\n\n\nLearner.fine_tune\n\n Learner.fine_tune (epochs:int, base_lr:float=0.002, freeze_epochs:int=1,\n                    lr_mult:Union[int,float]=100, pct_start:float=0.3,\n                    div:Union[int,float]=5.0, compile_frozen:bool=False,\n                    lr_max=None, div_final=100000.0, wd=None, moms=None,\n                    cbs=None, reset_opt=False, start_epoch=0)\n\nFine tune with Learner.freeze for freeze_epochs, then with Learner.unfreeze for epochs, using discriminative LR.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nepochs\nint\n\nNumber of unfrozen epochs to train\n\n\nbase_lr\nfloat\n0.002\nBase learning rate, model head unfrozen learning rate\n\n\nfreeze_epochs\nint\n1\nNumber of frozen epochs to train\n\n\nlr_mult\nNumeric\n100\nModel stem unfrozen learning rate: base_lr/lr_mult\n\n\npct_start\nfloat\n0.3\nStart unfrozen learning rate cosine annealing\n\n\ndiv\nNumeric\n5.0\nInitial unfrozen learning rate: base_lr/div\n\n\ncompile_frozen\nbool\nFalse\nCompile model during frozen finetuning if CompilerCallback is used\n\n\nlr_max\nNoneType\nNone\n\n\n\ndiv_final\nfloat\n100000.0\n\n\n\nwd\nNoneType\nNone\n\n\n\nmoms\nNoneType\nNone\n\n\n\ncbs\nNoneType\nNone\n\n\n\nreset_opt\nbool\nFalse\n\n\n\nstart_epoch\nint\n0\n\n\n\n\nBy default, fine_tune will not compile the freeze_epochs, but this can be overridden by passing freeze_compile=True. If the model is already compiled, this will have no effect."
  },
  {
    "objectID": "callback.compiler.html#footnotes",
    "href": "callback.compiler.html#footnotes",
    "title": "PyTorch Compile",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUnless using the FFCV Loader, then it uses the training dataloader. This doesn’t effect seeded training as FFCV dataloaders do not seed transforms, only dataset order.↩︎"
  },
  {
    "objectID": "ffcv.epoch_iterator.html",
    "href": "ffcv.epoch_iterator.html",
    "title": "fastxtend",
    "section": "",
    "text": "source\n\nEpochIterator\n\n EpochIterator (loader, order:Sequence[int])\n\nA class that represents a thread of control.\nThis class can be safely subclassed in a limited fashion. There are two ways to specify the activity: by passing a callable object to the constructor, or by overriding the run() method in a subclass.\n\nsource\n\n\nAsyncEpochIterator\n\n AsyncEpochIterator (loader, order:Sequence[int],\n                     after_batch:fastcore.transform.Pipeline)\n\nA class that represents a thread of control.\nThis class can be safely subclassed in a limited fashion. There are two ways to specify the activity: by passing a callable object to the constructor, or by overriding the run() method in a subclass."
  },
  {
    "objectID": "metrics.html",
    "href": "metrics.html",
    "title": "Metrics Extended",
    "section": "",
    "text": "fastxtend’s Metrics Extended is an enhancement of fastai metrics and is backward compatible with fastai metrics. You can mix and match fastxtend and fastai metrics in the same Learner.\nfastxtend metrics add the following features to fastai metrics:\nThere are three main metric types: AvgMetricX, AccumMetricX, and AvgSmoothMetricX. These correspond one-to-one with fastai.learner.AvgMetric, fastai.metrics.AccumMetric, and fastai.learner.AvgSmoothLoss. fastxtend metrics inherit from fastai.learner.Metric and run on fastai.learner.Learner via a modified fastai.learner.Recorder callback.\nTo jump to the fastxtend metrics reference, click here."
  },
  {
    "objectID": "metrics.html#using-a-metric",
    "href": "metrics.html#using-a-metric",
    "title": "Metrics Extended",
    "section": "Using a Metric",
    "text": "Using a Metric\nTo use the accuracy metric, or any fastxtend metrics detailed below, create a Learner like normal (or task specific learner such as vision_learner, text_classifier_learner, etc) and add the metric(s) to the metrics argument:\nfrom fastai.vision.all import *\nfrom fastxtend.vision.all import *\n\nLearner(..., metrics=Accuracy())\nFastxtend metrics can be mixed with fastai metrics:\nLearner(..., metrics=[accuracy, Accuracy()])\nFastxtend metrics can be logged during training, validation, or both by setting the log_metric argument to LogMetric.Train, LogMetric.Valid, or LogMetric.Both. The sole exception is AvgSmoothMetricX which only logs during training.\n\n\n\n\n\n\nNote\n\n\n\nBy default, a fastxtend metric will log during validation. fastai metrics can only log during validation.\n\n\nTo log a fastxtend metric during training pass LogMetric.Train to log_metric:\nLearner(..., metrics=Accuracy(log_metric=LogMetric.Train))\nNon-scikit-learn metrics can have the log type set via the metric_type argument to one of MetricType.Avg, MetricType.Accum, MetricType.Smooth, corresponding to AvgMetricX, AccumMetricX, and AvgSmoothMetricX, respectively.\nTo log a smooth metric on the training set and normal metric on the valid set:\nLearner(..., \n        metrics=[Accuracy(log_metric=LogMetric.Train, metric_type=MetricType.Smooth), \n                 Accuracy()])\nFastxtend metrics also support custom names via the name argument:\nLearner(..., metrics=Accuracy(name='metric_name'))\nwhich will result in Accuracy logging under “metric_name” instead of the default “accuracy”.\nIf a fastxtend metric is logged with multiple MetricTypes, the fastxtend Recorder will automatically deduplication the metric names. Unless the metric’s name argument is set. Then fastxtend will not deduplicate any metric names."
  },
  {
    "objectID": "metrics.html#creating-a-metric",
    "href": "metrics.html#creating-a-metric",
    "title": "Metrics Extended",
    "section": "Creating a Metric",
    "text": "Creating a Metric\nAvgMetricX, AccumMetricX, and AvgSmoothMetricX all require func, which is a funcational implementation of the metric. The signature of func should be inp,targ (where inp are the predictions of the model and targ the corresponding labels).\nFastxtend metrics can be logged during training, validation, or both by setting the log_metric argument to LogMetric.Train, LogMetric.Valid, or LogMetric.Both. The sole exception is AvgSmoothMetricX which only computes during training.\nAvgMetricX, AccumMetricX, and AvgSmoothMetricX will automatically recognize and pass any func’s unique arguments to func.\n\n\n\n\n\n\nImportant\n\n\n\nSome metrics, like Root Mean Squared Error, will have incorrect results if passed to AvgMetricX via MetricType.Avg, as the mean of multiple batches of RMSE isn’t equal to the RMSE of the whole dataset. For these metrics use AccumMetricX via MetricType.Accum.\n\n\nAn example of creating a fastxtend metric from a functional implementation:\ndef example_accuracy(inp, targ):\n    return (inp == targ).float().mean()\n\ndef ExampleAccuracy(dim_argmax=-1, log_metric=LogMetric.Valid, **kwargs):\n    return AvgMetricX(example_accuracy, dim_argmax=dim_argmax, log_metric=log_metric, **kwargs)\nAlternatively, use the func_to_metric convenience method to create the metric:\ndef ExampleAccuracy(axis=-1, log_metric=LogMetric.Valid, **kwargs):\n    return func_to_metric(example_accuracy, MetricType.Avg, True, axis=axis, log_metric=log_metric, **kwargs)\nIt is also possible to inherit directly from MetricX to create a fastxtend metric.\nclass ExampleAccuracy(MetricX):\n    def __init__(self, dim_argmax=-1, log_metric=LogMetric.Valid, **kwargs):\n    super().__init__(dim_argmax=dim_argmax, log_metric=log_metric, **kwargs)\n\n    def reset(self): self.preds,self.targs = [],[]\n\n    def accumulate(self, learn):\n        super().accumulate(learn)\n        self.preds.append(learn.to_detach(self.pred))\n        self.targs.append(learn.to_detach(self.targ))\n\n    @property\n    def value(self):\n        if len(self.preds) == 0: return\n        preds,targs = torch.cat(self.preds),torch.cat(self.targs)\n        return (preds == targs).float().mean()\n\n\n\n\n\n\nImportant\n\n\n\nIf your custom MetricX has state depending on tensors, don’t forget to store it on the CPU to avoid any potential memory leaks."
  },
  {
    "objectID": "metrics.html#additional-metrics-functionality",
    "href": "metrics.html#additional-metrics-functionality",
    "title": "Metrics Extended",
    "section": "Additional Metrics Functionality",
    "text": "Additional Metrics Functionality\nMetricX, and classes which inherit from MetricX such as AvgMetricX, AccumMetricX, and AvgSmoothMetricX, have optional helper functionality in MetricX.accumulate to assist in developing metrics.\nFor classification problems with single label, predictions need to be transformed with a softmax then an argmax before being compared to the targets. Since a softmax doesn’t change the order of the numbers, apply the argmax. Pass along dim_argmax to have this done by MetricX (usually -1 will work pretty well). If the metric implementation requires probabilities and not predictions, use softmax=True.\nFor classification problems with multiple labels, or if targets are one-hot encoded, predictions may need to pass through a sigmoid (if it wasn’t included in in the model) then be compared to a given threshold (to decide between 0 and 1), this is done by MetricX by passing sigmoid=True and/or a value for thresh.\nAvgMetricX, AccumMetricX, and AvgSmoothMetricX have two additional arguments to assist in creating metrics: to_np and invert_arg.\nFor example, if using a functional metric from sklearn.metrics, predictions and labels will need to be converted to numpy arrays with to_np=True. Also, scikit-learn metrics adopt the convention y_true, y_preds which is the opposite from fastai, so pass invert_arg=True to make AvgMetricX, AccumMetricX, and AvgSmoothMetricX do the inversion. Alternatively, use the skm_to_fastxtend convenience method to handle sklearn.metrics automatically.\n\nsource\n\nLogMetric\n\n LogMetric (value, names=None, module=None, qualname=None, type=None,\n            start=1)\n\nAll logging types for MetricX\n\nsource\n\n\nMetricType\n\n MetricType (value, names=None, module=None, qualname=None, type=None,\n             start=1)\n\nAll types of MetricX\n\nsource\n\n\nActivationType\n\n ActivationType (value, names=None, module=None, qualname=None, type=None,\n                 start=1)\n\nAll activation classes for `MetricX\n\nsource\n\n\nMetricX\n\n MetricX (dim_argmax=None, activation=&lt;ActivationType.No: 1&gt;, thresh=None,\n          log_metric=None, name=None)\n\nBlueprint for defining an extended metric with accumulate\n\n\n\n\n\n\nNote\n\n\n\nBy default, a MetricX will only log during validation. Metrics can individually set to run during training, validation, or both by passing LogMetric.Train, LogMetric.Valid, or LogMetric.Both to log_metric, respectively.\n\n\nFor classification problems with single label, predictions need to be transformed with a softmax then an argmax before being compared to the targets. Since a softmax doesn’t change the order of the numbers, apply the argmax. Pass along dim_argmax to have this done by MetricX (usually -1 will work pretty well). If the metric implementation requires probabilities and not predictions, use softmax=True.\nFor classification problems with multiple labels, or if targets are one-hot encoded, predictions may need to pass through a sigmoid (if it wasn’t included in in the model) then be compared to a given threshold (to decide between 0 and 1), this is done by MetricX by passing sigmoid=True and/or a value for thresh.\nMetrics can be simple averages (like accuracy) but sometimes their computation is a little bit more complex and can’t be averaged over batches (like precision or recall), which is why we need a special AccumMetricX class for them. For simple functions that can be computed as averages over batches, we can use the class AvgMetricX, otherwise you’ll need to implement the following methods.\n\n\n\n\n\n\nNote\n\n\n\nIf your custom MetricX has state depending on tensors, don’t forget to store it on the CPU to avoid any potential memory leaks.\n\n\n\nsource\n\n\nMetricX.reset\n\n MetricX.reset ()\n\nReset inner state to prepare for new computation\n\nsource\n\n\nMetricX.accumulate\n\n MetricX.accumulate (learn)\n\nStore targs and preds from learn, using activation function and argmax as appropriate\n\nsource\n\n\nMetricX.value\n\n MetricX.value ()\n\nThe value of the metric\n\nsource\n\n\nMetricX.name\n\n MetricX.name ()\n\nName of the Metric, camel-cased and with Metric removed. Or custom name if provided\n\nsource\n\n\nAvgMetricX\n\n AvgMetricX (func, to_np=False, invert_arg=False, dim_argmax=None,\n             activation=&lt;ActivationType.No: 1&gt;, thresh=None,\n             log_metric=None, name=None)\n\nAverage the values of func taking into account potential different batch sizes\nfunc is applied to each batch of predictions/targets and then averaged when value attribute is asked for.The signature of func should be inp,targ (where inp are the predictions of the model and targ the corresponding labels).\n\n\n\n\n\n\nImportant\n\n\n\nSome metrics, like Root Mean Squared Error, will have incorrect results if passed to AvgMetricX, as the mean of multiple batches of RMSE isn’t equal to the RMSE of the whole dataset. For these metrics use AccumMetricX.\n\n\nIf using a functional metric from sklearn.metrics, predictions and labels will need to be converted to numpy arrays with to_np=True. Also, scikit-learn metrics adopt the convention y_true, y_preds which is the opposite from fastai, so pass invert_arg=True to make AvgMetricX, AccumMetricX, and AvgSmoothMetricX do the inversion. Alternatively, use the skm_to_fastxtend convenience method to handle sklearn.metrics automatically.\nBy default, fastxtend’s scikit-learn metrics use AccumMetricX.\n\nsource\n\n\nAccumMetricX\n\n AccumMetricX (func, to_np=False, invert_arg=False, flatten=True,\n               dim_argmax=None, activation=&lt;ActivationType.No: 1&gt;,\n               thresh=None, log_metric=None, name=None)\n\nStores predictions and targets on CPU in accumulate to perform final calculations with func.\nfunc is only applied to the accumulated predictions/targets when the value attribute is asked for (so at the end of a validation/training phase, in use with Learner and its Recorder).The signature of func should be inp,targ (where inp are the predictions of the model and targ the corresponding labels).\nIf using a functional metric from sklearn.metrics, predictions and labels will need to be converted to numpy arrays with to_np=True. Also, scikit-learn metrics adopt the convention y_true, y_preds which is the opposite from fastai, so pass invert_arg=True to make AvgMetricX, AccumMetricX, and AvgSmoothMetricX do the inversion. Alternatively, use the skm_to_fastxtend convenience method to handle sklearn.metrics automatically.\nBy default, fastai’s scikit-learn metrics use AccumMetricX.\n\nsource\n\n\nAvgSmoothMetricX\n\n AvgSmoothMetricX (func, beta=0.98, to_np=False, invert_arg=False,\n                   dim_argmax=None, activation=&lt;ActivationType.No: 1&gt;,\n                   thresh=None, name=None)\n\nSmooth average the values of func (exponentially weighted with beta). Only computed on training set.\nfunc is only applied to the accumulated predictions/targets when the value attribute is asked for (so at the end of a validation/training phase, in use with Learner and its Recorder).The signature of func should be inp,targ (where inp are the predictions of the model and targ the corresponding labels).\nIf using a functional metric from sklearn.metrics, predictions and labels will need to be converted to numpy arrays with to_np=True. Also, scikit-learn metrics adopt the convention y_true, y_preds which is the opposite from fastai, so pass invert_arg=True to make AvgMetricX, AccumMetricX, and AvgSmoothMetricX do the inversion. Alternatively, use the skm_to_fastxtend convenience method to handle sklearn.metrics automatically.\n\nsource\n\n\nAvgLossX\n\n AvgLossX (dim_argmax=None, activation=&lt;ActivationType.No: 1&gt;,\n           thresh=None, log_metric=None, name=None)\n\nAverage the losses taking into account potential different batch sizes\n\nsource\n\n\nAvgSmoothLossX\n\n AvgSmoothLossX (beta=0.98)\n\nSmooth average of the losses (exponentially weighted with beta)\n\nsource\n\n\nValueMetricX\n\n ValueMetricX (func, name=None, log_metric=None)\n\nUse to include a pre-calculated metric value (for instance calculated in a Callback) and returned by func"
  },
  {
    "objectID": "metrics.html#metrics",
    "href": "metrics.html#metrics",
    "title": "Metrics Extended",
    "section": "Metrics",
    "text": "Metrics\n\nCustom Metric Creation\nfastxtend provides two convenience methods for creating custom metrics from functions: func_to_metric and skm_to_fastxtend.\n\nsource\n\n\nfunc_to_metric\n\n func_to_metric (func, metric_type, is_class, thresh=None, axis=-1,\n                 activation=None, log_metric=&lt;LogMetric.Valid: 2&gt;,\n                 dim_argmax=None, name=None)\n\nConvert func metric to a fastai metric\nThis is the quickest way to use a functional metric as a fastxtend metric.\nmetric_type is one of MetricType.Avg, MetricType.Accum, or MetricType.Smooth which set the metric to use AvgMetricX, AccumMetricX, or AvgSmoothMetricX, respectively.\nis_class indicates if you are in a classification problem or not. In this case: - leaving thresh to None indicates it’s a single-label classification problem and predictions will pass through an argmax over axis before being compared to the targets - setting a value for thresh indicates it’s a multi-label classification problem and predictions will pass through a sigmoid (can be deactivated with sigmoid=False) and be compared to thresh before being compared to the targets\nIf is_class=False, it indicates you are in a regression problem, and predictions are compared to the targets without being modified. In all cases, kwargs are extra keyword arguments passed to func.\n\n\n\n\n\n\nImportant\n\n\n\nSome metrics, like Root Mean Squared Error, will have incorrect results if passed to AvgMetricX via MetricType.Avg, as the mean of multiple batches of RMSE isn’t equal to the RMSE of the whole dataset. For these metrics use AccumMetricX by setting metric_type to MetricType.Accum.\n\n\n\nsource\n\n\nskm_to_fastxtend\n\n skm_to_fastxtend (func, is_class=True, thresh=None, axis=-1,\n                   activation=None, log_metric=&lt;LogMetric.Valid: 2&gt;,\n                   dim_argmax=None, name=None)\n\nConvert func from sklearn.metrics to a fastai metric\nThis is the quickest way to use a scikit-learn metric using fastxtend metrics. It is the same as func_to_metric except it defaults to using AccumMetricX."
  },
  {
    "objectID": "metrics.html#single-label-classification",
    "href": "metrics.html#single-label-classification",
    "title": "Metrics Extended",
    "section": "Single-label classification",
    "text": "Single-label classification\n\n\n\n\n\n\nWarning\n\n\n\nAll functions defined in this section are intended for single-label classification and targets that are not one-hot encoded. For multi-label problems or one-hot encoded targets, use the version suffixed with multi.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nMany metrics in fastxtend are thin wrappers around sklearn functionality. However, sklearn metrics can handle python list strings, amongst other things, whereas fastxtend metrics work with PyTorch, and thus require tensors. The arguments that are passed to metrics are after all transformations, such as categories being converted to indices, have occurred. This means that when you pass a label of a metric, for instance, that you must pass indices, not strings. This can be converted with vocab.map_obj.\n\n\n\nsource\n\nAccuracy\n\n Accuracy (axis=-1, metric_type=&lt;MetricType.Avg: 1&gt;,\n           log_metric=&lt;LogMetric.Valid: 2&gt;, **kwargs)\n\nCompute accuracy with targ when pred is bs * n_classes\n\nsource\n\n\nErrorRate\n\n ErrorRate (axis=-1, metric_type=&lt;MetricType.Avg: 1&gt;,\n            log_metric=&lt;LogMetric.Valid: 2&gt;, **kwargs)\n\nCompute 1 - accuracy with targ when pred is bs * n_classes\n\nsource\n\n\nTopKAccuracy\n\n TopKAccuracy (k=5, axis=-1, metric_type=&lt;MetricType.Avg: 1&gt;,\n               log_metric=&lt;LogMetric.Valid: 2&gt;, **kwargs)\n\nComputes the Top-k accuracy (targ is in the top k predictions of inp)\n\nsource\n\n\nAPScoreBinary\n\n APScoreBinary (axis=-1, average='macro', pos_label=1, sample_weight=None,\n                log_metric=&lt;LogMetric.Valid: 2&gt;, **kwargs)\n\nAverage Precision for single-label binary classification problems\nSee the scikit-learn documentation for more details.\n\nsource\n\n\nBalancedAccuracy\n\n BalancedAccuracy (axis=-1, sample_weight=None, adjusted=False,\n                   log_metric=&lt;LogMetric.Valid: 2&gt;, **kwargs)\n\nBalanced Accuracy for single-label binary classification problems\nSee the scikit-learn documentation for more details.\n\nsource\n\n\nBrierScore\n\n BrierScore (axis=-1, sample_weight=None, pos_label=None,\n             log_metric=&lt;LogMetric.Valid: 2&gt;, **kwargs)\n\nBrier score for single-label classification problems\nSee the scikit-learn documentation for more details.\n\nsource\n\n\nCohenKappa\n\n CohenKappa (axis=-1, labels=None, weights=None, sample_weight=None,\n             log_metric=&lt;LogMetric.Valid: 2&gt;, **kwargs)\n\nCohen kappa for single-label classification problems\nSee the scikit-learn documentation for more details.\n\nsource\n\n\nF1Score\n\n F1Score (axis=-1, labels=None, pos_label=1, average='binary',\n          sample_weight=None, log_metric=&lt;LogMetric.Valid: 2&gt;, **kwargs)\n\nF1 score for single-label classification problems\nSee the scikit-learn documentation for more details.\n\nsource\n\n\nFBeta\n\n FBeta (beta, axis=-1, labels=None, pos_label=1, average='binary',\n        sample_weight=None, log_metric=&lt;LogMetric.Valid: 2&gt;, **kwargs)\n\nFBeta score with beta for single-label classification problems\nSee the scikit-learn documentation for more details.\n\nsource\n\n\nHammingLoss\n\n HammingLoss (axis=-1, sample_weight=None, log_metric=&lt;LogMetric.Valid:\n              2&gt;, **kwargs)\n\nHamming loss for single-label classification problems\nSee the scikit-learn documentation for more details.\n\nsource\n\n\nJaccard\n\n Jaccard (axis=-1, labels=None, pos_label=1, average='binary',\n          sample_weight=None, log_metric=&lt;LogMetric.Valid: 2&gt;, **kwargs)\n\nJaccard score for single-label classification problems\nSee the scikit-learn documentation for more details.\n\nsource\n\n\nPrecision\n\n Precision (axis=-1, labels=None, pos_label=1, average='binary',\n            sample_weight=None, log_metric=&lt;LogMetric.Valid: 2&gt;, **kwargs)\n\nPrecision for single-label classification problems\nSee the scikit-learn documentation for more details.\n\nsource\n\n\nRecall\n\n Recall (axis=-1, labels=None, pos_label=1, average='binary',\n         sample_weight=None, log_metric=&lt;LogMetric.Valid: 2&gt;, **kwargs)\n\nRecall for single-label classification problems\nSee the scikit-learn documentation for more details.\n\nsource\n\n\nRocAuc\n\n RocAuc (axis=-1, average='macro', sample_weight=None, max_fpr=None,\n         multi_class='ovr', log_metric=&lt;LogMetric.Valid: 2&gt;, **kwargs)\n\nArea Under the Receiver Operating Characteristic Curve for single-label multiclass classification problems\nSee the scikit-learn documentation for more details.\n\nsource\n\n\nRocAucBinary\n\n RocAucBinary (axis=-1, average='macro', sample_weight=None, max_fpr=None,\n               multi_class='raise', log_metric=&lt;LogMetric.Valid: 2&gt;,\n               **kwargs)\n\nArea Under the Receiver Operating Characteristic Curve for single-label binary classification problems\nSee the scikit-learn documentation for more details.\n\nsource\n\n\nMatthewsCorrCoef\n\n MatthewsCorrCoef (sample_weight=None, log_metric=&lt;LogMetric.Valid: 2&gt;,\n                   **kwargs)\n\nMatthews correlation coefficient for single-label classification problems\nSee the scikit-learn documentation for more details."
  },
  {
    "objectID": "metrics.html#multi-label-classification",
    "href": "metrics.html#multi-label-classification",
    "title": "Metrics Extended",
    "section": "Multi-label classification",
    "text": "Multi-label classification\n\nsource\n\nAccuracyMulti\n\n AccuracyMulti (thresh=0.5, sigmoid=True, metric_type=&lt;MetricType.Avg: 1&gt;,\n                log_metric=&lt;LogMetric.Valid: 2&gt;, **kwargs)\n\nCompute accuracy when inp and targ are the same size.\n\nsource\n\n\nAPScoreMulti\n\n APScoreMulti (sigmoid=True, average='macro', pos_label=1,\n               sample_weight=None, log_metric=&lt;LogMetric.Valid: 2&gt;,\n               **kwargs)\n\nAverage Precision for multi-label classification problems\nSee the scikit-learn documentation for more details.\n\nsource\n\n\nBrierScoreMulti\n\n BrierScoreMulti (thresh=0.5, sigmoid=True, sample_weight=None,\n                  pos_label=None, log_metric=&lt;LogMetric.Valid: 2&gt;,\n                  **kwargs)\n\nBrier score for multi-label classification problems\nSee the scikit-learn documentation for more details.\n\nsource\n\n\nF1ScoreMulti\n\n F1ScoreMulti (thresh=0.5, sigmoid=True, labels=None, pos_label=1,\n               average='macro', sample_weight=None,\n               log_metric=&lt;LogMetric.Valid: 2&gt;, **kwargs)\n\nF1 score for multi-label classification problems\nSee the scikit-learn documentation for more details.\n\nsource\n\n\nFBetaMulti\n\n FBetaMulti (beta, thresh=0.5, sigmoid=True, labels=None, pos_label=1,\n             average='macro', sample_weight=None,\n             log_metric=&lt;LogMetric.Valid: 2&gt;, **kwargs)\n\nFBeta score with beta for multi-label classification problems\nSee the scikit-learn documentation for more details.\n\nsource\n\n\nHammingLossMulti\n\n HammingLossMulti (thresh=0.5, sigmoid=True, labels=None,\n                   sample_weight=None, log_metric=&lt;LogMetric.Valid: 2&gt;,\n                   **kwargs)\n\nHamming loss for multi-label classification problems\nSee the scikit-learn documentation for more details.\n\nsource\n\n\nJaccardMulti\n\n JaccardMulti (thresh=0.5, sigmoid=True, labels=None, pos_label=1,\n               average='macro', sample_weight=None,\n               log_metric=&lt;LogMetric.Valid: 2&gt;, **kwargs)\n\nJaccard score for multi-label classification problems\nSee the scikit-learn documentation for more details.\n\nsource\n\n\nMatthewsCorrCoefMulti\n\n MatthewsCorrCoefMulti (thresh=0.5, sigmoid=True, sample_weight=None,\n                        log_metric=&lt;LogMetric.Valid: 2&gt;, **kwargs)\n\nMatthews correlation coefficient for multi-label classification problems\nSee the scikit-learn documentation for more details.\n\nsource\n\n\nPrecisionMulti\n\n PrecisionMulti (thresh=0.5, sigmoid=True, labels=None, pos_label=1,\n                 average='macro', sample_weight=None,\n                 log_metric=&lt;LogMetric.Valid: 2&gt;, **kwargs)\n\nPrecision for multi-label classification problems\nSee the scikit-learn documentation for more details.\n\nsource\n\n\nRecallMulti\n\n RecallMulti (thresh=0.5, sigmoid=True, labels=None, pos_label=1,\n              average='macro', sample_weight=None,\n              log_metric=&lt;LogMetric.Valid: 2&gt;, **kwargs)\n\nRecall for multi-label classification problems\nSee the scikit-learn documentation for more details.\n\nsource\n\n\nRocAucMulti\n\n RocAucMulti (sigmoid=True, average='macro', sample_weight=None,\n              max_fpr=None, log_metric=&lt;LogMetric.Valid: 2&gt;, **kwargs)\n\nArea Under the Receiver Operating Characteristic Curve for multi-label binary classification problems\nSee the scikit-learn documentation for more details."
  },
  {
    "objectID": "metrics.html#regression",
    "href": "metrics.html#regression",
    "title": "Metrics Extended",
    "section": "Regression",
    "text": "Regression\n\nsource\n\nMSE\n\n MSE (metric_type=&lt;MetricType.Avg: 1&gt;, log_metric=&lt;LogMetric.Valid: 2&gt;,\n      **kwargs)\n\nMean squared error between inp and targ.\n\nsource\n\n\nRMSE\n\n RMSE (log_metric=&lt;LogMetric.Valid: 2&gt;, **kwargs)\n\nMean squared error between inp and targ.\n\nsource\n\n\nMAE\n\n MAE (metric_type=&lt;MetricType.Avg: 1&gt;, log_metric=&lt;LogMetric.Valid: 2&gt;,\n      **kwargs)\n\nMean absolute error between inp and targ.\n\nsource\n\n\nMSLE\n\n MSLE (metric_type=&lt;MetricType.Avg: 1&gt;, log_metric=&lt;LogMetric.Valid: 2&gt;,\n       **kwargs)\n\nMean squared logarithmic error between inp and targ.\n\nsource\n\n\nExpRMSE\n\n ExpRMSE (log_metric=&lt;LogMetric.Valid: 2&gt;, **kwargs)\n\nRoot mean square percentage error of the exponential of predictions and targets\n\nsource\n\n\nExplainedVariance\n\n ExplainedVariance (sample_weight=None, log_metric=&lt;LogMetric.Valid: 2&gt;,\n                    **kwargs)\n\nExplained variance between predictions and targets\nSee the scikit-learn documentation for more details.\n\nsource\n\n\nR2Score\n\n R2Score (sample_weight=None, log_metric=&lt;LogMetric.Valid: 2&gt;, **kwargs)\n\nR2 score between predictions and targets\nSee the scikit-learn documentation for more details.\n\nsource\n\n\nPearsonCorrCoef\n\n PearsonCorrCoef (dim_argmax=None, log_metric=&lt;LogMetric.Valid: 2&gt;,\n                  **kwargs)\n\nPearson correlation coefficient for regression problem\nSee the scipy documentation for more details.\n\nsource\n\n\nSpearmanCorrCoef\n\n SpearmanCorrCoef (dim_argmax=None, axis=0, nan_policy='propagate',\n                   log_metric=&lt;LogMetric.Valid: 2&gt;, **kwargs)\n\nSpearman correlation coefficient for regression problem\nSee the scipy documentation for more details."
  },
  {
    "objectID": "metrics.html#segmentation",
    "href": "metrics.html#segmentation",
    "title": "Metrics Extended",
    "section": "Segmentation",
    "text": "Segmentation\n\nsource\n\nForegroundAcc\n\n ForegroundAcc (bkg_idx=0, axis=1, metric_type=&lt;MetricType.Avg: 1&gt;,\n                log_metric=&lt;LogMetric.Valid: 2&gt;, **kwargs)\n\nComputes non-background accuracy for multiclass segmentation\n\nsource\n\n\nDice\n\n Dice (axis=1, log_metric=&lt;LogMetric.Valid: 2&gt;, **kwargs)\n\nDice coefficient metric for binary target in segmentation\n\nsource\n\n\nDiceMulti\n\n DiceMulti (axis=1, log_metric=&lt;LogMetric.Valid: 2&gt;, **kwargs)\n\nAveraged Dice metric (Macro F1) for multiclass target in segmentation\nThe DiceMulti method implements the “Averaged F1: arithmetic mean over harmonic means” described in this publication: https://arxiv.org/pdf/1911.03347.pdf\n\nsource\n\n\nJaccardCoeff\n\n JaccardCoeff (axis=1, log_metric=&lt;LogMetric.Valid: 2&gt;, **kwargs)\n\nImplementation of the Jaccard coefficient that is lighter in RAM"
  },
  {
    "objectID": "metrics.html#nlp",
    "href": "metrics.html#nlp",
    "title": "Metrics Extended",
    "section": "NLP",
    "text": "NLP\n\nsource\n\nCorpusBLEUMetric\n\n CorpusBLEUMetric (vocab_sz=5000, axis=-1, log_metric=&lt;LogMetric.Valid:\n                   2&gt;, name='CorpusBLEU', **kwargs)\n\nBLEU Metric calculated over the validation corpus\nThe BLEU metric was introduced in this article to come up with a way to evaluate the performance of translation models. It’s based on the precision of n-grams in your prediction compared to your target. See the fastai NLP course BLEU notebook for a more detailed description of BLEU.\nThe smoothing used in the precision calculation is the same as in SacreBLEU, which in turn is “method 3” from the Chen & Cherry, 2014 paper.\n\nsource\n\n\nPerplexity\n\n Perplexity (dim_argmax=None, activation=&lt;ActivationType.No: 1&gt;,\n             thresh=None, log_metric=None, name=None)\n\nPerplexity (exponential of cross-entropy loss) for Language Models\n\nsource\n\n\nLossMetric\n\n LossMetric (func, to_np=False, invert_arg=False, dim_argmax=None,\n             activation=&lt;ActivationType.No: 1&gt;, thresh=None,\n             log_metric=None, name=None)\n\nCreate a metric from loss_func.attr named nm\n\nsource\n\n\nLossMetrics\n\n LossMetrics (attrs, nms=None)\n\nList of LossMetric for each of attrs and nms"
  },
  {
    "objectID": "metrics.html#logging",
    "href": "metrics.html#logging",
    "title": "Metrics Extended",
    "section": "Logging",
    "text": "Logging\nMetrics Extended is compatible with logging to Weights and Biases and TensorBoard using fastai’s WandbCallback and TensorBoardCallback."
  },
  {
    "objectID": "callback.channelslast.html",
    "href": "callback.channelslast.html",
    "title": "Channels Last",
    "section": "",
    "text": "With MixedPrecision, image models trained in channels last format on Tensor Cores can increase training throughput over contiguous format. PyTorch observed a 22% improvment in ResNet50 training speed using channels last and 8-35% improvement across a selection of models tested on a V100.\nChannels last format is compatible with modern GPUs (Volta, Turing, or newer) and modern CPUs (Ice Lake or newer).\nChannels last memory format currently is implemented for NCHW Tensors. Not all PyTorch operators have been converted to support channels last. See (Beta) Channels Last Memory Format in PyTorch tutorial for more details.\nsource"
  },
  {
    "objectID": "callback.channelslast.html#convenience-methods",
    "href": "callback.channelslast.html#convenience-methods",
    "title": "Channels Last",
    "section": "Convenience Methods",
    "text": "Convenience Methods\nfastxtend adds two convenience methods to Learner to easily activate and disable channels last format.\nfastxend’s Learner.to_channelslast is a drop in replacement for fastai.callback.channelslast.Learner.to_channelslast with the same defaults. It additionally supports AMPMode for selecting either float16 or bfloat16 mixed precision.\n\nsource\n\nLearner.to_channelslast\n\n Learner.to_channelslast (use_amp:bool=True,\n                          amp_mode:Union[str,fastxtend.callback.amp.AMPMod\n                          e]=&lt;AMPMode.FP16: 'fp16'&gt;, init_scale=65536.0,\n                          growth_factor=2.0, backoff_factor=0.5,\n                          growth_interval=2000, enabled=True)\n\nSet Learner and inputs to channels_last format and float16 Mixed Precision by default\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nuse_amp\nbool\nTrue\nAdd MixedPrecision with amp_mode. Recommended for full channels last performance\n\n\namp_mode\nstr | AMPMode\nAMPMode.FP16\nMixed Precision training mode. Supports fp16 and bf16.\n\n\ninit_scale\nfloat\n65536.0\n\n\n\ngrowth_factor\nfloat\n2.0\n\n\n\nbackoff_factor\nfloat\n0.5\n\n\n\ngrowth_interval\nint\n2000\n\n\n\nenabled\nbool\nTrue\n\n\n\n\nBy default, setting Learner.to_channelslast(True) will train in float16 mixed precision to match the fastai method. Set amp_mode=False to train in float32 channels last format (not recommended).\nPassing GradScaler arguments to Learner.to_channelslast when training in bfloat16 has no effect, as bfloat16 mixed precision does not used a gradient scaler.\n\nsource\n\n\nLearner.to_contiguous\n\n Learner.to_contiguous (to_fp32=False)\n\nSet Learner and inputs to contiguous_format (default format), optionally to single precision"
  },
  {
    "objectID": "transform.html",
    "href": "transform.html",
    "title": "Transform",
    "section": "",
    "text": "source\n\nBatchRandTransform\n\n BatchRandTransform (p:Numeric=1.0,\n                     before_call:Callable[[Tensor|tuple[Tensor,...],int],N\n                     one]|None=None, **kwargs)\n\nRandomly selects a subset of batch b to apply transform with per item probability p in before_call\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nNumeric\n1.0\nProbability of applying Transform to each batch item\n\n\nbefore_call\nCallable[[Tensor | tuple[Tensor, …], int], None] | None\nNone\nBatch preprocessing function\n\n\nkwargs"
  },
  {
    "objectID": "vision.augment.batch.html",
    "href": "vision.augment.batch.html",
    "title": "Additional Batch Augmentations",
    "section": "",
    "text": "source\n\nGrayscaleMode\n\n GrayscaleMode (value, names=None, module=None, qualname=None, type=None,\n                start=1)\n\nGrayscaleModes for Grayscale\n\nsource\n\n\nGrayscale\n\n Grayscale (p:float=0.1, mode:GrayscaleMode=&lt;GrayscaleMode.Random: 3&gt;)\n\nConvert RGB image into grayscale using luma_bt.601, luma_bt.709, averaging, or randomly selected\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nfloat\n0.1\nPer-item probability\n\n\nmode\nGrayscaleMode\nGrayscaleMode.Random\nGrayScaleMode to apply to images. Random applies all three element-wise with equal probability\n\n\n\n\nwith less_random():\n    _,axs = plt.subplots(1,3,figsize=(20,4))\n    for ax, mode in zip(axs, [GrayscaleMode.Luma601, GrayscaleMode.Luma709, GrayscaleMode.Average]): \n        f = partial(Grayscale(p=1, mode=mode), split_idx=0)\n        f(_batch(img.clone())).squeeze().show(ctx=ax, title=f'mode={mode}')\n\n\n\n\n\nsource\n\n\nChannelDrop\n\n ChannelDrop (p:float=0.1, replace:float|None=None)\n\nDrop entire channel by replacing it with random solid value [0,1)\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nfloat\n0.1\nPer-item probability\n\n\nreplace\nfloat | None\nNone\nSet constant replacement value. Defaults to element-wise random value [0,1)\n\n\n\n\nwith less_random():\n    _,axs = plt.subplots(1,3,figsize=(20,4))\n    f = ChannelDrop(p=1)\n    for ax in axs: f(_batch(img.clone()), split_idx=0).squeeze().show(ctx=ax)\n\n\n\n\n\nsource\n\n\nRandomNoise\n\n RandomNoise (p:float=0.25, stdev:float|tuple=(0.1, 0.25),\n              random:bool=True)\n\nAdd random guassian noise based on stdev\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nfloat\n0.25\nPer-item probability\n\n\nstdev\nfloat | tuple\n(0.1, 0.25)\nMaximum or range of the standard deviation of added noise\n\n\nrandom\nbool\nTrue\nRandomize standard deviation of added noise between [stdev[0], stdev[1])\n\n\n\nLarger images can use higher stdev as seen with this 600x400 pixel example:\n\nwith less_random():\n    _,axs = plt.subplots(1,4,figsize=(20,4))\n    for ax, stdev in zip(axs, [0.1, 0.25, 0.5, 0.75]):\n        f = partial(RandomNoise(p=1, stdev=stdev, random=False), split_idx=0)\n        norm_apply_denorm(_batch(img.clone()), f, nrm).squeeze().show(ctx=ax, title=f'stdev={stdev}')\n\n\n\n\nBut smaller images should use lower stdev as seen with this 150x100 pixel example:\n\nwith less_random():\n    _,axs = plt.subplots(1,4,figsize=(20,4))\n    r = Resize((100, 150))\n    for ax, stdev in zip(axs, [0.1, 0.2, 0.3, 0.5]):\n        f = partial(RandomNoise(p=1, stdev=stdev, random=False), split_idx=0)\n        norm_apply_denorm(_batch(r(img.clone(), )), f, nrm).squeeze().show(ctx=ax, title=f'stdev={stdev}')\n\n\n\n\n\nsource\n\n\nRandomErasingBatch\n\n RandomErasingBatch (p:float=0.25, sl:float=0.0, sh:float=0.3,\n                     min_aspect:float=0.3, max_count:int=1,\n                     element:bool=False)\n\nRandomly selects a rectangle region in an image and randomizes its pixels.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nfloat\n0.25\nPer-item probability\n\n\nsl\nfloat\n0.0\nMinimum proportion of erased area\n\n\nsh\nfloat\n0.3\nMaximum proportion of erased area\n\n\nmin_aspect\nfloat\n0.3\nMinimum aspect ratio of erased area\n\n\nmax_count\nint\n1\nMaximum number of erasing blocks per image, area per box is scaled by count\n\n\nelement\nbool\nFalse\nLoop over through batch and apply element-wise unique erasing\n\n\n\n\nwith less_random():\n    _,axs = plt.subplots(1,4,figsize=(18,4))\n    for ax, area in zip(axs, [0.05, 0.1, 0.2, 0.3]):\n        f = partial(RandomErasingBatch(p=1, sl=area, sh=area), split_idx=0)\n        norm_apply_denorm(_batch(img.clone()), f, nrm).squeeze().show(ctx=ax, title=f'area={area}')\n\n\n\n\n\nsource\n\n\naffine_transforms\n\n affine_transforms (mult:float=1.0, do_flip:bool=True,\n                    flip_vert:bool=False, max_rotate:float=10.0,\n                    min_zoom:float=1.0, max_zoom:float=1.1,\n                    max_warp:float=0.2, p_affine:float=0.75,\n                    xtra_tfms:list=None, size:Union[int,tuple]=None,\n                    mode:str='bilinear', pad_mode='reflection',\n                    align_corners=True, batch=False, min_scale=1.0)\n\nUtility function to easily create a list of affine transforms: flip, rotate, zoom, and warp.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmult\nfloat\n1.0\nMultiplication applying to max_rotate,max_warp\n\n\ndo_flip\nbool\nTrue\nRandom flipping\n\n\nflip_vert\nbool\nFalse\nFlip vertically and horizontally\n\n\nmax_rotate\nfloat\n10.0\nMaximum degree of rotation\n\n\nmin_zoom\nfloat\n1.0\nMinimum zoom\n\n\nmax_zoom\nfloat\n1.1\nMaximum zoom\n\n\nmax_warp\nfloat\n0.2\nMaximum warp\n\n\np_affine\nfloat\n0.75\nProbability of applying affine transformation\n\n\nxtra_tfms\nlist\nNone\nCustom Transformations\n\n\nsize\nint | tuple\nNone\nOutput size, duplicated if one value is specified\n\n\nmode\nstr\nbilinear\nPyTorch F.grid_sample interpolation\n\n\npad_mode\nstr\nreflection\nA PadMode\n\n\nalign_corners\nbool\nTrue\nPyTorch F.grid_sample align_corners\n\n\nbatch\nbool\nFalse\nApply identical transformation to entire batch\n\n\nmin_scale\nfloat\n1.0\nMinimum scale of the crop, in relation to image area\n\n\n\naffine_transforms identical to fastai.vision.augmentation.aug_transforms, except with the lighting transforms removed. It’s intended for use with the fastai+FFCV Loader, using FFCV Numba transforms for lighting."
  },
  {
    "objectID": "callback.lr_finder.html",
    "href": "callback.lr_finder.html",
    "title": "Learning Rate Finder",
    "section": "",
    "text": "When finished running, fastai’s learning rate finder only restores the model weights and optimizer to the initial state.\nBy default, fastxtend’s learning rate finder additionally restores the dataloader and random state to their inital state, so running Learner.lr_find has no effect on model training.\n\nsource\n\nLRFinder\n\n LRFinder (start_lr=1e-07, end_lr=10, num_it=100, stop_div=True,\n           restore_state=True)\n\nTraining with exponentially growing learning rate\n\nsource\n\n\nLRFinder.before_fit\n\n LRFinder.before_fit ()\n\nInitialize container for hyper-parameters and save the model & optimizer, optionally saving dataloader & random state\n\nsource\n\n\nLRFinder.before_batch\n\n LRFinder.before_batch ()\n\nSet the proper hyper-parameters in the optimizer\n\nsource\n\n\nLRFinder.after_batch\n\n LRFinder.after_batch ()\n\nRecord hyper-parameters of this batch and potentially stop training\n\nsource\n\n\nLRFinder.before_validate\n\n LRFinder.before_validate ()\n\nSkip the validation part of training\n\nsource\n\n\nLRFinder.after_fit\n\n LRFinder.after_fit ()\n\nSave the hyper-parameters in the recorder if there is one and load the original model & optimizer, optionally restoring dataloader & random state\n\nsource\n\n\nLearner.lr_find\n\n Learner.lr_find (start_lr=1e-07, end_lr=10, num_it=100, stop_div=True,\n                  show_plot=True, suggest_funcs=&lt;function valley&gt;,\n                  restore_state=True)\n\nLaunch a mock training to find a good learning rate and return suggestions based on suggest_funcs as a named tuple.\nUse restore_state to reset dataloaders and random state after running.\nWithout restore_state, running lr_find advances both the random state and DataLoaders and behaves the same way as fastai’s lr_find. Which means the following two code blocks will result with different training output.\nwith no_random():\n    dls = get_dls()\n    learn = Learner(dls, xresnet18(n_out=dls.c))\n\nwith no_random():\n    learn.lr_find(restore_state=False)\n    learn.fit_one_cycle(2, 3e-3)\nwith no_random():\n    dls = get_dls()\n    learn = Learner(dls, xresnet18(n_out=dls.c))\n\nwith no_random():\n    learn.fit_one_cycle(2, 3e-3)\nWhile the default of restore_state=True prevents this from occurring, it has the potential downside of showing less variance in learning rate results due to every call to lr_find will be over the same first n_iter items using the same random state."
  },
  {
    "objectID": "callback.gradaccum.html",
    "href": "callback.gradaccum.html",
    "title": "Gradient Accumulation",
    "section": "",
    "text": "Gradient accumulation allows training on batch sizes which are too large to fit into memory by splitting each batch into mini-batches and skipping the optimizer step until gradients have been accumulated from all mini-batches.\nBecause fastxtend gradient accumulation modifies the default fastai gradient accumulation behavior, it is not included in any all imports. You must import fastxtend gradient accumulation after fastai and fastxend imports:"
  },
  {
    "objectID": "callback.gradaccum.html#differences-from-fastai",
    "href": "callback.gradaccum.html#differences-from-fastai",
    "title": "Gradient Accumulation",
    "section": "Differences from fastai",
    "text": "Differences from fastai\nfastai gradient accumulation works by treating each dataloader batch as a micro-batch, and then accumulating the micro-batches across multiple forward and backward steps into one larger macro-batch before performing an optimizer step.\n\nTraining Loss Logging\nBy default, GradientAccumulation and GradientAccumulationSchedule record and log accumulated batches instead of micro-batches. This effects training losses and Weights and Biases training steps.\nRecorded and logged training losses are the accumulated loss used for the optimization step, not the last micro-batch loss as fastai records. Weights and Biases training steps will be reduced by ratio of micro-batches to accumulated batches, while TensorBoard training steps will be uneffected.\nTo revert to fastai behavor of recording micro-batches, set log_accum_batch=False.\n\n\nDrop Last Batch\nBy default, GradientAccumulation and GradientAccumulationSchedule also drop the entire last macro-batch if there are not enough mini-batches in the epoch. This behavior matches training on the large batch size with the standard PyTorch DataLoader setting of drop_last=True. In contrast, fastai will accumulate mini-batches across epochs to form the full sized macro-batch.\nTo revert to fastai micro-batching behavior, set drop_last=False.\n\nsource\n\n\nGradientAccumulation\n\n GradientAccumulation (accum_bs:int|None, n_acc:int=32,\n                       micro_batch_size:int|None=None,\n                       log_accum_batch:bool=True, drop_last:bool=True)\n\nAccumulate gradients before updating weights\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\naccum_bs\nint | None\n\nAccumulation batch size. Defaults to n_acc if not set\n\n\nn_acc\nint\n32\nDefault accum_bs value. Used for compatability with fastai\n\n\nmicro_batch_size\nint | None\nNone\nManually set micro-batch size if using non-fastai or non-fastxtend dataloader\n\n\nlog_accum_batch\nbool\nTrue\nLog each accumulated batch (True) or micro batch (False). False is default fastai behavior\n\n\ndrop_last\nbool\nTrue\nDrop last incomplete macro-batch. If False, macro-batch can be accumulated across two epochs (fastai default)\n\n\n\n\nsource\n\n\nGradientAccumulationSchedule\n\n GradientAccumulationSchedule (start_accum_bs:int, final_accum_bs:int,\n                               start:Numeric=0, finish:Numeric=0.3,\n                               schedule:Callable[...,_Annealer]=&lt;function\n                               SchedCos&gt;, micro_batch_size:int|None=None,\n                               log_accum_batch:bool=True,\n                               drop_last:bool=True)\n\nGradient accumulation with a schedulable batch size\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nstart_accum_bs\nint\n\nInitial gradient accumulation batch size\n\n\nfinal_accum_bs\nint\n\nFinal gradient accumulation batch size\n\n\nstart\nNumeric\n0\nStart batch size schedule in percent of training steps (float) or epochs (int, index 0)\n\n\nfinish\nNumeric\n0.3\nFinish batch size schedule in percent of training steps (float) or epochs (int, index 0)\n\n\nschedule\nCallable[…, _Annealer]\nSchedCos\nBatch size schedule type\n\n\nmicro_batch_size\nint | None\nNone\nManually set micro-batch size if using non-fastai or non-fastxtend dataloader\n\n\nlog_accum_batch\nbool\nTrue\nLog each accumulated batch (True) or micro batch (False). False is default fastai behavior\n\n\ndrop_last\nbool\nTrue\nDrop last incomplete macro-batch. If False, macro-batch can be accumulated across two epochs (fastai default)"
  },
  {
    "objectID": "optimizer.sophia.html",
    "href": "optimizer.sophia.html",
    "title": "Sophia: Second-order Clipped Stochastic Optimization",
    "section": "",
    "text": "Sophia was introduced by Liu et al in Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training. Sophia is a second-order optimizer that leverages a light-weight Hessian estimate as a pre-conditioner, which is supposed to handle the Large Language Model (LLM) loss landscape better than AdamW. The Hessian pre-conditioner is more aggressive than AdamW, with stronger update penalties sharp dimensions, which can lead to a more uniform loss decrease across parameters and faster convergence. Additionally, Sophia applies element-wise clipping to updates which allows infrequent and stochastic updates to the Hessian estimate, reducing optimizer wall-clock time.\nIn addition to a fastai native implementation, Sophia has a fused ForEach implementation. See the Fused Optimizer documentation for more details.\nsource"
  },
  {
    "objectID": "optimizer.sophia.html#hyperparameters",
    "href": "optimizer.sophia.html#hyperparameters",
    "title": "Sophia: Second-order Clipped Stochastic Optimization",
    "section": "Hyperparameters",
    "text": "Hyperparameters\nHyperparameter notes from Liu et al:\n\nSophia hyperparameters should be similar to AdamW\n\\rho (rho) should be in [0.01, 0.1]. A larger \\rho means more aggressive updates\nSophia may benefit from slightly higher weight decay and learning rate compared to AdamW"
  },
  {
    "objectID": "vision.models.attention_modules.html",
    "href": "vision.models.attention_modules.html",
    "title": "Attention Modules",
    "section": "",
    "text": "source\n\nECA\n\n ECA (nf, ks:int=None, gamma:int=2, beta:int=1)\n\nEfficient Channel Attention, from https://arxiv.org/abs/1910.03151.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnf\n\n\nnumber of input features\n\n\nks\nint\nNone\nif set, Cov1D uses a fixed kernel size instead of adaptive kernel size\n\n\ngamma\nint\n2\nused for adaptive kernel size, see paper for more details\n\n\nbeta\nint\n1\nused for adaptive kernel size, see paper for more details\n\n\n\nEfficient Channel Attention modified from https://github.com/BangguWu/ECANet\n\nsource\n\n\nShuffleAttention\n\n ShuffleAttention (nf, groups=64)\n\nImplementation of Shuffle Attention, from https://arxiv.org/abs/2102.00240\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnf\n\n\nnumber of input features\n\n\ngroups\nint\n64\nnumber of subfeature groups, usually 32 or 64\n\n\n\n\nsource\n\n\nTripletAttention\n\n TripletAttention (nf, ks:int=7, no_spatial=False)\n\nLightly modified implementation of Triplet Attention, from http://arxiv.org/abs/2010.03045\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnf\n\n\nunused input features, for compatibility\n\n\nks\nint\n7\nkernel size for AttentionGate\n\n\nno_spatial\nbool\nFalse\nexclude Spatial attention as third attention\n\n\n\nTriplet Attention lightly modified from https://github.com/landskape-ai/triplet-attention"
  },
  {
    "objectID": "callback.overview.html",
    "href": "callback.overview.html",
    "title": "Getting Started with fastxtend Callbacks",
    "section": "",
    "text": "As an extensions library for fastai, fastxend contains multiple types of callbacks. This guide will briefly go over all types of callbacks in fastxtend. Every callback in this guide will link to the full callback documentation."
  },
  {
    "objectID": "callback.overview.html#new-and-replacement-callbacks",
    "href": "callback.overview.html#new-and-replacement-callbacks",
    "title": "Getting Started with fastxtend Callbacks",
    "section": "New and Replacement Callbacks",
    "text": "New and Replacement Callbacks\nThe first type of callback in fastxtend are new and replacement callbacks. These callbacks either add new features to fastai or augment existing fastai functionality but are fully backwards compatible. These callbacks are imported via task specific all imports such as fastxtend.vision.all or fastxtend.text.all.\nNew Callbacks:\n\nExponential Moving Average Callbacks\n\nEMACallback\nEMASchedule\n\nTracking Callbacks\n\nTerminateOnTrainNaN\nSaveModelAtEnd\nLastMetricCallback\n\n\nReplacement Callbacks:\n\nCutMixUp Callbacks\n\nMixUp\nCutMix\n\nLearning Rate Finder Callback\n\nLRFinder"
  },
  {
    "objectID": "callback.overview.html#task-specific-callbacks",
    "href": "callback.overview.html#task-specific-callbacks",
    "title": "Getting Started with fastxtend Callbacks",
    "section": "Task Specific Callbacks",
    "text": "Task Specific Callbacks\nThese callbacks are only imported with task specific imports.\nVision callbacks imported from fastxtend.vision.all:\n\nCutMixUp Callbacks\n\nCutMixUp\nCutMixUpAugment\n\nProgressive Resizing Callback\n\nProgressiveResize\n\n\nText callbacks imported from fastxend.text.all:\n\nTransformers Compatibility Callback\n\nHuggingFaceCallback\n\n\nAudio callbacks imported from fastxtend.auido.all:\n\nAudio Cutmixup Callbacks\n\nAudioMixUp\nAudioCutMixUp\nAudioCutMixUpAugment"
  },
  {
    "objectID": "callback.overview.html#modification-callbacks",
    "href": "callback.overview.html#modification-callbacks",
    "title": "Getting Started with fastxtend Callbacks",
    "section": "Modification Callbacks",
    "text": "Modification Callbacks\nThese callbacks modify either existing fastai functionality or change the fastai defaults and must be imported separately.\n\nPyTorch Compile Callbacks imported via fastxtend.callback.compiler\n\nCompilerCallback\nDynamoExplainCallback\n\nGradient Accumulation Callbacks imported via fastxtend.callback.gradaccum\n\nGradientAccumulation\nGradientAccumulationSchedule\n\nProfiling Callbacks imported via fastxtend.callback.profiler\n\nThroughputCallback\nSimpleProfilerCallback"
  },
  {
    "objectID": "callback.overview.html#utility-callbacks",
    "href": "callback.overview.html#utility-callbacks",
    "title": "Getting Started with fastxtend Callbacks",
    "section": "Utility Callbacks",
    "text": "Utility Callbacks\nThese callbacks add tools for developing other callbacks. - LogDispatch - CallbackScheduler"
  },
  {
    "objectID": "callback.overview.html#feature-porting-callbacks",
    "href": "callback.overview.html#feature-porting-callbacks",
    "title": "Getting Started with fastxtend Callbacks",
    "section": "Feature Porting Callbacks",
    "text": "Feature Porting Callbacks\nThese callbacks backport current fastai features to older versions of fastai. These callbacks started out as fastxtend callbacks and then were upstreamed into fastai. If you are using the latest version of fastai you will be using the fastai versions of these callbacks.\n\nChannelsLast\nMixedPrecision"
  },
  {
    "objectID": "audio.01_core.html",
    "href": "audio.01_core.html",
    "title": "Audio Core",
    "section": "",
    "text": "Audio Core is heavily inspired by and contains code from the no longer maintained FastAudio package.\n\nsource\n\nshow_audio_signal\n\n show_audio_signal (at, ctx, ax=None, title='', sep=0.03, **kwargs)\n\n\nsource\n\n\nTensorAudio\n\n TensorAudio (x, sr=None, **kwargs)\n\nTensor for audio. Can be created from files and has extra properties. Also knows how to show itself.\n\nsource\n\n\nTensorAudio.create\n\n TensorAudio.create (fn:Union[str,pathlib.Path,BinaryIO], **kwargs)\n\nCreates TensorAudio from file fn\n\nsource\n\n\nTensorAudio.hear\n\n TensorAudio.hear ()\n\nListen to audio clip. Creates a html player.\n\nsource\n\n\nTensorAudio.show\n\n TensorAudio.show (ctx=None, hear=True, ax=None, title='', sep=0.03)\n\nShow audio clip using librosa. Pass hear=True to also display a html player to listen.\n\nsource\n\n\nTensorAudio.save\n\n TensorAudio.save (fn:pathlib.Path, overwrite=True)\n\nSave the audio into the specfied path\n\nsource\n\n\nshow_spectrogram\n\n show_spectrogram (aspec, title='', ax=None, ctx=None, sep=0.025,\n                   to_db=False, **kwargs)\n\n\nsource\n\n\nTensorSpec\n\n TensorSpec (x, **kwargs)\n\nTensor for Audio Spectrograms. Has extra properties and knows how to show itself.\nThe best way to create a TensorSpec is to use the Spectrogram transform or SpecBlock which uses the Spectrogram transform.\n\nsource\n\n\nTensorSpec.create\n\n TensorSpec.create (ta:torch.Tensor, settings:Optional[dict]=None)\n\nCreate an TensorSpec from a torch tensor\n\nsource\n\n\nTensorSpec.show\n\n TensorSpec.show (ctx=None, ax=None, title='', sep=0.025, to_db=False)\n\nShow spectrogram using librosa\n\nsource\n\n\nTensorMelSpec\n\n TensorMelSpec (x, **kwargs)\n\nTensor for Audio MelSpectrograms. Has extra properties and knows how to show itself.\nThe best way to create a TensorMelSpec is to use the MelSpectrogram transform or MelSpecBlock which uses the MelSpectrogram transform.\n\nsource\n\n\nTensorMelSpec.create\n\n TensorMelSpec.create (ta:torch.Tensor, settings:Optional[dict]=None)\n\nCreate an TensorMelSpec from a torch tensor"
  },
  {
    "objectID": "ffcv.tutorial.html",
    "href": "ffcv.tutorial.html",
    "title": "Getting Started with fastxtend and FFCV",
    "section": "",
    "text": "fastxtend integrates FFCV with fastai. You can now use the speed of the highly optimized FFCV DataLoader natively with fastai batch transforms, callbacks, and other DataLoader features.\nFFCV is “a drop-in data loading system that dramatically increases data throughput in model training.” It accelerates1 DataLoader throughput by combining Numba compiled item transforms with a custom data format and cached data loading.\nFFCV has a getting started tutorial which pairs well with this guide, providing additional context and depth.\nWhile fastxtend’s FFCV integration is currently in beta, it is fully functional. Expect new features and quality of life improvements in future fastxtend releases."
  },
  {
    "objectID": "ffcv.tutorial.html#installing-fastai-ffcv-and-fastxtend",
    "href": "ffcv.tutorial.html#installing-fastai-ffcv-and-fastxtend",
    "title": "Getting Started with fastxtend and FFCV",
    "section": "Installing fastai, FFCV, and fastxtend",
    "text": "Installing fastai, FFCV, and fastxtend\nThe easiest way to install fastai, fastxtend, and FFCV is to use Conda or Miniconda2 on Linux (or WSL):\nconda create -n fastxtend python=3.11 \"pytorch&gt;=2.1\" \\\ntorchvision pytorch-cuda=12.1 fastai pkg-config \\\nlibjpeg-turbo opencv tqdm terminaltables psutil numpy \\\n\"numba&gt;=0.57\" timm kornia -c pytorch -c nvidia \\\n-c fastai -c huggingface -c conda-forge\n\n# Switch to the newly created conda environment\nconda activate fastxtend\nreplacing pytorch-cuda=12.13 with your prefered supported version of Cuda. In rare4 cases, you may need to add the compilers package to the conda install.\nAnd then install fastxtend via pip:\n# Install fastxtend with Vision & FFCV support\npip install fastxtend[ffcv]\nOr to install with all of fastxtend’s features:\nconda create -n fastxtend python=3.11 \"pytorch&gt;=2.1\" torchvision \\\ntorchaudio pytorch-cuda=12.1 fastai nbdev pkg-config libjpeg-turbo \\\nopencv tqdm psutil terminaltables numpy \"numba&gt;=0.57\" librosa timm \\\nkornia rich typer wandb \"transformers&gt;=4.34\" \"tokenizers&gt;=0.14\" \\\n\"datasets&gt;=2.14\" ipykernel ipywidgets \"matplotlib&lt;3.8\" -c pytorch \\\n-c nvidia -c fastai -c huggingface -c conda-forge\n\n# Switch to the newly created conda environment\nconda activate fastxtend\n\n# install fastxtend and FFCV\npip install fastxtend[all]\n\n\n\n\n\n\nNote: Windows Installation\n\n\n\n\n\nIf you are using Windows5, please follow the FFCV Windows installation guide, then install fastxtend via pip.\n\n\n\n\n\n\n\n\n\nNote: Suggested Optional Packages\n\n\n\n\n\nIf you want to use notebooks with fastxtend, it’s recommended to add one ofjupyter or jupyterlab along with ipykernel to the conda install packages.\ncupy is not listed in the conda packages as it’s only needed if you want to use FFCV’s NormalizeImage on the GPU. It’s recommended to use fastai’s Normalize instead. See adding batch transforms section for more details."
  },
  {
    "objectID": "ffcv.tutorial.html#importing-ffcv-via-fastxtend",
    "href": "ffcv.tutorial.html#importing-ffcv-via-fastxtend",
    "title": "Getting Started with fastxtend and FFCV",
    "section": "Importing FFCV via fastxtend",
    "text": "Importing FFCV via fastxtend\nfastxtend’s FFCV integration has been designed to use __all__ to safely import everything needed to use FFCV with fastai and fastxtend.\nRun:\n\nfrom fastai.vision.all import *\nfrom fastxtend.vision.all import *\nfrom fastxtend.ffcv.all import *\n\nand you are ready to go.\n\n\n\n\n\n\nNote: Importing Transforms & Operations\n\n\n\n\n\nfastxtend’s FFCV transforms and operations are imported under the fx prefix, since they sometimes overlap with fastai batch transforms.\nFor example, the FFCV augmenation RandomErasing is an Numba FFCV version of the batch transform fastai.vision.augment.RandomErasing.\n\n\n\nYou can also import the FFCV integration individually. This will require you to mix imports from fastxtend.ffcv and ffcv.\nfrom ffcv.fields.decoders import CenterCropDecoder\nfrom fastxtend.ffcv.loader import Loader, OrderOption\nfrom fastxtend.ffcv.transforms import RandomHorizontalFlip\n# etc\nHowever, during the beta it’s recommended to use from fastxtend.ffcv.all import *, as imports may change between fastxtend releases."
  },
  {
    "objectID": "ffcv.tutorial.html#creating-a-ffcv-dataset",
    "href": "ffcv.tutorial.html#creating-a-ffcv-dataset",
    "title": "Getting Started with fastxtend and FFCV",
    "section": "Creating a FFCV Dataset",
    "text": "Creating a FFCV Dataset\nBefore we can start training with FFCV, our dataset needs to be converted into FFCV’s custom beaton format. This can be done using the DatasetWriter.\nfastxtend provides the rgb_dataset_to_ffcv convenience method for easy FFCV image dataset creation. rgb_dataset_to_ffcv expects a PyTorch compatible Dataset or any Python iterator.\nFirst, create an Imagenette dataset using the fastai DataBlock API.\n\npath = URLs.IMAGENETTE_320\nsource = untar_data(path)\n\ndblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   splitter=GrandparentSplitter(valid_name='val'),\n                   get_items=get_image_files, get_y=parent_label)\ndset = dblock.datasets(source)\n\nNext, use rgb_dataset_to_ffcv to create two FFCV files: one for the training dataset and one for the validation dataset.\n\npath = Path.home()/'.cache/fastxtend'\npath.mkdir(exist_ok=True)\n\nrgb_dataset_to_ffcv(dset.train, path/'imagenette_320_train.ffcv')\n\nrgb_dataset_to_ffcv(dset.valid, path/'imagenette_320_valid.ffcv')\n\n\n\n\n\n\n\nTip: Change Chunk Size\n\n\n\n\n\nIf you have more (or less) memory, you can increase (or decrease) DatasetWriter’s chunk_size from the default of 100.\n\n\n\nIf Imagenette was not already resized, we could pass max_resolution or min_resolution to resize the images. To recreate Imagenette 320 from the full size dataset, pass min_resolution=320:\nrgb_dataset_to_ffcv(\n    dset.train, path/'imagenette_320_valid.ffcv', min_resolution=320)\nBy default, rgb_dataset_to_ffcv will use Pillow and the LANCZOS resample method to resize the image, and DatasetWriter will use OpenCV with INTER_AREA.\n\n\n\n\n\n\nTip: Pillow-SIMD\n\n\n\n\n\nTo accelerate image resizing, you’ll probably want Pillow-SIMD installed."
  },
  {
    "objectID": "ffcv.tutorial.html#creating-a-fastxtend-loader",
    "href": "ffcv.tutorial.html#creating-a-fastxtend-loader",
    "title": "Getting Started with fastxtend and FFCV",
    "section": "Creating a fastxtend Loader",
    "text": "Creating a fastxtend Loader\nfastxtend adds fastai features to FFCV’s Loader, including one_batch, show_batch, show_results, and support for batch transforms, to name a few.\nCurrently fastai.data.block.DataBlock is unsupported for creating a fastxtend Loader, so we’ll have to create it from scratch.\nFor reference, here is the fastai DataBlock we’ll be recreating6 using the fastxtend Loader.\ndblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   splitter=GrandparentSplitter(valid_name='val'),\n                   get_items=get_image_files, get_y=parent_label,\n                   item_tfms=[RandomResizedCrop(224), FlipItem(0.5)],\n                   batch_tfms=[*aug_transforms(do_flip=False), Normalize(*imagenet_stats)])\ndls = dblock.dataloaders(source, bs=64, num_workers=num_cpus())\n\n\n\n\n\n\nImportant: Loader Transform Type & Order Matters\n\n\n\n\n\nUnlike fastai, fastxtend’s FFCV integration, and FFCV itself, does not automatically select between training and validation versions of transforms.\nNeither does it automatically create a validation pipeline or automatically reorder transforms and operations.\nYou are responsible for adding the correct decoders, transforms, and operations to the correct pipelines in the correct order.\n\n\n\n\nSetting Up Pipelines\nFFCV uses pipelines to declare what input fields to read, how to decode them, and which operations and transforms to apply on them.\nThe dataloader will need three pipelines: one for the training images, validation images, and a shared pipeline for labels.\nWe need to make sure that decoders, transforms, and operations are all in the correct order, as they will be executed sequentially.\n\nTraining Pipeline\nReading a FFCV dataset requires a FFCV decoder. FFCV has multiple decoders, but we’ll use RandomResizedCropRGBImageDecoder which integrates a random resizing crop into image loading.\nAnd we’ll add RandomHorizontalFlip to flip the image.\ntrain_pipe = [\n    RandomResizedCropRGBImageDecoder(output_size=(224,224)),\n    ft.RandomHorizontalFlip(0.5)\n]\n\n\n\n\n\n\nTip: fastxtend has Image Transforms\n\n\n\n\n\nfastxtend provides multiple FFCV transforms, including existing FFCV transforms with harmonized arguments, fastai transforms implemented as FFCV transforms, and additional FFCV transforms.\n\n\n\nAfter passing through FFCV’s Numba compiled transforms, the Imagenette images are still in CPU memory as NumPy arrays. Before we can pass them to our model, they need to be converted to fastai.torch_core.TensorImage and moved to the GPU.\nWe’ll extend our training pipeline by adding the ToTensorImage and ToDevice operations.\ntrain_pipe.extend([ft.ToTensorImage(), ft.ToDevice()])\nLoader will now asynchronously transfer each training image batch to the GPU.\n\n\n\n\n\n\nWarning: Don’t use FFCV’s Tensor Operations\n\n\n\n\n\nFFCV has ToTensor, ToTorchImage, and ToDevice operations for converting NumPy arrays to PyTorch Tensors and moving to the GPU. These are compatible with PyTorch dataloaders but they are not compatible with fastai as they will strip the required types for fastai features, such as batch transforms, callbacks, plotting, etc.\nUse fastxtend’s ToTensorImage and ToDevice for compatibility with fastai features.\n\n\n\n\n\nValidation Pipeline\nWith the training pipeline finalized, it’s time to create the validation pipeline.\nFFCV and fastxtend currently have one validation image decoder: CenterCropRGBImageDecoder, which resizes and center crops the validation image. This is identical to fastai.vision.augment.Resize valdiation behavior.\nvalid_pipe = [\n    CenterCropRGBImageDecoder(output_size=(224,224), ratio=1),\n    ft.ToTensorImage(),\n    ft.ToDevice()\n]\nLike train_pipe, we use ToTensorImage to convert to the correct tensor type and ToDevice asynchronously transfer each batch to the GPU.\n\n\nLabel Pipeline\nNow we have our image pipeline for our training and validaton datasets but we need to create our label pipeline.\nSince this is a single label dataset, we have integers as labels, so we’ll use an IntDecoder and convert to TensorCategory, followed by squeezing the extra dimension7 with Squeeze and using ToDevice to transfer to the GPU.\nlabel_pipe = [\n    IntDecoder(), ft.ToTensorCategory(),\n    ft.Squeeze(), ft.ToDevice()\n]\n\n\n\nAdding Required & Optional Batch Transforms\nAfter Loader finishes processing the pipelines, the images are batched on the GPU, but will be in uint8 format and unnormalized.\nFFCV has operations to handle both8, but using them will convert the images from TensorImage to Tensor, limiting compatibility with other fastai features, such as callbacks, plotting, etc.\nSince fastxtend’s Loader supports fastai GPU batch transforms, we’ll use them instead.\n\nRequired Batch Transform\nTo convert the uint8 tensors to float and normalize the images, we’ll use fastai.data.transforms.IntToFloatTensor and fastai.data.transforms.Normalize to preserve tensor types and metadata.\n\n\n\n\n\n\nImportant: IntToFloatTensor is a Required Batch Transform\n\n\n\n\n\nfastai.data.transforms.IntToFloatTensor is a required batch transform (batch_tfms) when training on image data for fastai feature compatibility.\n\n\n\nUnlike the FFCV transforms we’ve used so far, the fastai transforms in Loader will automatically reorder themselves into and use type dispatch to apply to the correct tensor types.\nbatch_tfms = [\n    IntToFloatTensor,\n    Normalize.from_stats(*imagenet_stats)\n]\nIt is recommended, but not required, to normalize an image batch.\n\n\nOptional Batch Transforms\nIt’s also possible to add any fastai batch transform to Loader’s batch_tfms, such as fastai.vision.augment.aug_transforms or affine_transforms:\nbatch_tfms = [\n    IntToFloatTensor,\n    *aug_transforms(),\n    Normalize.from_stats(*imagenet_stats)\n]\n\n\n\nCreating the Loader\nWith the image pipelines, label pipeline, and batch transforms set up, we can now create our dataloaders, one Loader for train and one for valid.\n\nTraining Loader\nStarting with the training Loader, we can manually set all the training specific arguments:\nLoader(path/'imagenette_320_train.ffcv',\n    batch_size=64,\n    num_workers=num_cpus(),\n    os_cache=True,\n    order=OrderOption.RANDOM,\n    drop_last=True,\n    pipelines={'image': train_pipeline, 'label': label_pipeline},\n    batch_tfms=batch_tfms,\n    batches_ahead=2,\n    device='cuda',\n    split_idx=0,\n    n_inp=1\n)\n\n\nLoader Arguments\nThere are a handful of important Loader arguments which need further explanation:\n\norder: Controls how much memory is used for dataset caching and whether the dataset is randomly shuffled. Can be one of RANDOM, QUASI_RANDOM, or SEQUENTIAL. See the note below for more details. Defaults to SEQUENTIAL, which is unrandomized.\nos_cache: By default, FFCV will attempt to cache the entire dataset into RAM using the operating system’s caching. This can be changed by setting os_cache=False or setting the enviroment variable ‘FFCV_DEFAULT_CACHE_PROCESS’ to “True” or “1”. If os_cache=False then order must be set to QUASI_RANDOM for the training Loader.\nnum_workers: If not set, will use all CPU cores up to 16 by default.\nbatches_ahead: Controls the number of batches ahead the Loader works. Increasing uses more RAM, both CPU and GPU. Defaults to 2.\nn_inp: Controls which inputs to pass to the model. By default, set to number of pipelines minus 1.\ndrop_last: Whether to drop the last partial batch. By default, will set to True if order is RANDOM or QUASI_RANDOM, False if SEQUENTIAL.\nasync_tfms: Asynchronously apply batch_tfms before the batch is drawn. Can accelerate training if GPU compute isn’t fully saturated (95% or less) or if only using IntToFloatTensor and Normalize.\ndevice: The device to place the processed batches of data on. Defaults to fastai.torch_core.default_device if not set.\nsplit_idx: This tells the fastai batch transforms what dataset they are operating on. By default will use 0 (train) if order is RANDOM or QUASI_RANDOM, 1 (valid) if SEQUENTIAL.\n\n\n\n\n\n\n\nNote: Order Memory Usage\n\n\n\n\n\nEach order option requires differing amounts of system memory.\n\nRANDOM caches the entire dataset in memory for fast random sampling. RANDOM uses the most memory.\nQUASI_RANDOM caches a subset of the dataset at a time in memory and randomly samples from the subset. Use when the entire dataset cannot fit into memory.\nSEQUENTIAL requires least memory. It loads a few samples ahead of time. As the name suggests, it is not random, and primarly is for validation.\n\n\n\n\n\n\n\n\n\n\nTip: Asynchronous Transforms Can Increase Training Speed\n\n\n\n\n\nWith Loader, fastai batch transforms can either be computed synchronously or asynchronously.\nSynchronous transforms behave like the fastai dataloader: a batch is called for by the training loop, batch transforms are computed and applied to the batch, and then the batch is passed to the model for training. Asynchronous batch transforms are computed and applied to batches ahead of time, in parallel with model training. With asynchronous batch transforms there isn’t any delay between calling for a batch and training on the batch.\nBecause asynchronous transforms are computed in parallel with the training loop they can slow down the model training loop. However, due to eliminating the delay for computing batch transforms, asynchronous transforms increase overall training speed.\nTry asynchronous transforms by setting async_tfms = True if GPU compute isn’t fully saturated (95% or less) or if only using the required transforms IntToFloatTensor and Normalize.\n\n\n\n\n\nSimplified Training Loader\nSince Imagenette 320 is small enough to load both the training and validation images into RAM9, the only arguments for the training Loader that must be set are:\nLoader(path/'imagenette_320_train.ffcv',\n    batch_size=64,\n    order=OrderOption.RANDOM,\n    pipelines={'image': train_pipeline, 'label': label_pipeline},\n    batch_tfms=batch_tfms,\n    batches_ahead=2\n)\n\n\nValidation Loader\nNext is the validation Loader.\nLoader(path/'imagenette_320_valid.ffcv',\n    batch_size=64,\n    pipelines={'image': valid_pipeline, 'label': label_pipeline},\n    batch_tfms=batch_tfms,\n    batches_ahead=2\n)\nThis example only sets the required arguments, relying on the Loader defaults for the rest.\n\n\n\nPutting it all Together\nThe last step is to wrap both the training and validation Loader in a fastai.data.core.DataLoaders.\nThe example below shows all the steps covered so far in a single codeblock.\n\nloaders = {}\nfor name in ['train', 'valid']:\n    label_pipe = [\n        IntDecoder(), fx.ToTensorCategory(),\n        fx.Squeeze(), fx.ToDevice()\n    ]\n\n    if name=='train':\n        image_pipe = [\n            RandomResizedCropRGBImageDecoder(output_size=(224,224), scale=(0.35, 1)),\n            fx.RandomHorizontalFlip(), fx.ToTensorImage(), fx.ToDevice()\n        ]\n        order = OrderOption.RANDOM\n    else:\n        image_pipe = [\n            CenterCropRGBImageDecoder(output_size=(224,224), ratio=1),\n            fx.ToTensorImage(), fx.ToDevice()\n        ]\n        order = OrderOption.SEQUENTIAL\n\n    batch_tfms = [IntToFloatTensor, *aug_transforms(), Normalize.from_stats(*imagenet_stats)]\n\n    loaders[name] = Loader(path/f'imagenette_320_{name}.ffcv',\n                        batch_size=64 if name=='train' else 128,\n                        order=order,\n                        pipelines={'image': image_pipe, 'label': label_pipe},\n                        batch_tfms=batch_tfms,\n                        batches_ahead=1,\n                        seed=42\n                    )\n\ndls = DataLoaders(loaders['train'], loaders['valid'])"
  },
  {
    "objectID": "ffcv.tutorial.html#training-with-a-fastxtend-loader",
    "href": "ffcv.tutorial.html#training-with-a-fastxtend-loader",
    "title": "Getting Started with fastxtend and FFCV",
    "section": "Training with a fastxtend Loader",
    "text": "Training with a fastxtend Loader\nWith the DataLoaders created, the only thing left to do is create a fastai.learner.Learner with our model, optimizer, loss function, metrics, and callbacks of choice.\nHere we create the setup which should train Imagenette to ~92.5% in ~226 seconds on a 3080 Ti, depending on randomness and hardware.\n\nwith less_random():\n    learn = Learner(dls, xresnext50(n_out=10), opt_func=ranger(foreach=True),\n                    loss_func=nn.CrossEntropyLoss(label_smoothing=0.1), metrics=Accuracy(),\n                    cbs=ProgressiveResize(increase_by=16)).to_channelslast()\n\nThe first batch will be slower, as Numba needs to compile and FFCV needs to allocate memory for each transform and operation.\nOnce the compilation is over, we will benefit from FFCV’s accelerated data loading.\n\nwith less_random():\n    learn.fit_flat_cos(20, 8e-3)\n\nProgressively increase the initial image size of [112, 112] by 16 pixels every 0.8333 epochs for 7 resizes. \nStarting at epoch 10 and finishing at epoch 15 for a final training size of [224, 224].\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.739842\n1.804805\n0.515924\n00:12\n\n\n1\n1.483479\n1.330434\n0.662930\n00:08\n\n\n2\n1.317108\n1.193714\n0.724076\n00:08\n\n\n3\n1.215840\n1.404873\n0.640255\n00:08\n\n\n4\n1.114017\n1.206947\n0.715924\n00:08\n\n\n5\n1.068557\n0.985433\n0.817070\n00:08\n\n\n6\n1.017356\n1.111050\n0.760255\n00:08\n\n\n7\n0.985094\n1.069334\n0.767898\n00:08\n\n\n8\n0.960843\n0.889831\n0.847389\n00:08\n\n\n9\n0.911082\n1.085570\n0.755159\n00:08\n\n\n10\n0.895687\n0.927597\n0.836688\n00:09\n\n\n11\n0.867939\n0.798519\n0.885860\n00:09\n\n\n12\n0.870780\n0.957848\n0.807388\n00:11\n\n\n13\n0.843433\n0.894144\n0.846624\n00:13\n\n\n14\n0.820972\n0.793127\n0.884076\n00:15\n\n\n15\n0.797982\n0.948408\n0.824204\n00:17\n\n\n16\n0.766880\n0.769455\n0.896815\n00:17\n\n\n17\n0.716646\n0.749211\n0.906242\n00:17\n\n\n18\n0.671945\n0.715331\n0.923057\n00:17\n\n\n19\n0.649383\n0.699832\n0.926115\n00:17\n\n\n\n\n\n\nwith less_random():\n    learn.fit_flat_cos(5, 8e-3)"
  },
  {
    "objectID": "ffcv.tutorial.html#inference-with-ffcv-and-fastai",
    "href": "ffcv.tutorial.html#inference-with-ffcv-and-fastai",
    "title": "Getting Started with fastxtend and FFCV",
    "section": "Inference with FFCV and fastai",
    "text": "Inference with FFCV and fastai\nWhile the fastxtend Loader and custom FFCV file format are great for accelerated training, they are not as useful for inference. This tutorial we will use the fastai DataBlock for inference.\nSince FFCV uses OpenCV for resizing and fastai uses Pillow, we cannot use the default fastai pipeline.\n\n\n\n\n\n\nImportant: Unlike fastai, FFCV uses OpenCV\n\n\n\n\n\nFFCV is hardcoded to use OpenCV’s INTER_AREA when resizing images, while fastai uses Pillow. This means we cannot use fastai.vision.data.ImageBlock for inference.\n\n\n\nfastxtend’s FFCV Inference module provides a FFAIImageBlock and FFAICenterCrop item transform which use OpenCV for resizing images.\n\n\n\n\n\n\nTip: FFAI is for Inference\n\n\n\n\n\nAny class or method with the prefix FFAI is intended for inference after training with the fastxtend Loader.\n\n\n\nWe can create an inference dataloader using the fastai DataBlock API which will create images identically to the Loader pipeline we created earlier in this tutorial.\n\ninference_dblock = DataBlock(blocks=(FFAIImageBlock, CategoryBlock),\n                             splitter=GrandparentSplitter(valid_name='val'),\n                             get_items=get_image_files, get_y=parent_label,\n                             item_tfms=[FFAICenterCrop(224, ratio=1)],\n                             batch_tfms=[*aug_transforms(), Normalize(*imagenet_stats)])\ninference_dls = dblock.dataloaders(source, bs=64, num_workers=num_cpus())\n\nThen assuming we had a folder with test images in '/test', we’d set the learn dataloader to the new inference_dls and perform inference like normal.\nlearn.dls = inference_dls\ntest_dl = dls.test_dl('/test')\npreds, _ = learn.get_preds(dl=test_dl)\nFor more details on inference, check out my Inference with fastai tutorial."
  },
  {
    "objectID": "ffcv.tutorial.html#reproducibility-and-other-limitations",
    "href": "ffcv.tutorial.html#reproducibility-and-other-limitations",
    "title": "Getting Started with fastxtend and FFCV",
    "section": "Reproducibility and Other Limitations",
    "text": "Reproducibility and Other Limitations\nOne downside of FFCV is it provides less reproducibility than most dataloader solutions.\nWhile Loader has a seed argument, it currently only affects the order data is loaded. With a couple exceptions, the Numba transforms are neither seeded nor reproducible. These transforms are also independent across pipelines.\nThis means many image-to-image training tasks, such as image segmentation, cannot easily use Loader as the inputs and outputs will not be identically resized, flipped, etc.\nHowever, this FFCV limitation might be resolved in the near future. As MetaAI recently announced FFCV-SSL, a fork of FFCV which, among other things, has reproducible transforms."
  },
  {
    "objectID": "ffcv.tutorial.html#identical-fastai-and-ffcv-dataloaders",
    "href": "ffcv.tutorial.html#identical-fastai-and-ffcv-dataloaders",
    "title": "Getting Started with fastxtend and FFCV",
    "section": "Identical fastai and FFCV Dataloaders",
    "text": "Identical fastai and FFCV Dataloaders\nThe fastai dataloader and FFCV dataloader we created in this tutorial do not produce identical validation images10.\nThis is due to fastai.vision.augment.RandomResizedCrop adding padding and squishing validation images, while FFCV’s CenterCropRGBImageDecoder creates validation images via center crop.\nThere’s an easy way to create a fastai dataloader with identical behavior via the DataBlock API, we just have to create it twice.\nFirst, create a dataloader like we did before using RandomResizedCrop. Then create a second dataloader except with fastai.vision.augment.Resize. Resize creates a center crop during validation, just like CenterCropRGBImageDecoder with ratio=1. Finally, set the first DataLoaders’ valid dataloader as our second valid dataloader. Then we can ptionally delete the second DataLoaders.\n\ndblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   splitter=GrandparentSplitter(valid_name='val'),\n                   get_items=get_image_files, get_y=parent_label,\n                   item_tfms=[RandomResizedCrop(224), FlipItem(0.5)],\n                   batch_tfms=[*aug_transforms(do_flip=False),\n                               Normalize.from_stats(*imagenet_stats)])\ndls = dblock.dataloaders(source, bs=64, num_workers=num_cpus())\n\nvblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   splitter=GrandparentSplitter(valid_name='val'),\n                   get_items=get_image_files, get_y=parent_label,\n                   item_tfms=Resize(224),\n                   batch_tfms=[*aug_transforms(), Normalize.from_stats(*imagenet_stats)])\nvls = vblock.dataloaders(source, bs=64, num_workers=num_cpus())\n\ndls.valid = vls.valid\nvls = None"
  },
  {
    "objectID": "ffcv.tutorial.html#footnotes",
    "href": "ffcv.tutorial.html#footnotes",
    "title": "Getting Started with fastxtend and FFCV",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMosiacML found that using FFCV led to a ~1.85x increase in throughput, from ~17,800 images/sec to ~30,000 images/sec on a 2x 32-core CPU and 8x A100 system.↩︎\nMiniconda with the faster libmamba solver is recommended.↩︎\nIf you want to include a full Cuda install, I find that specifying the Cuda version label -c nvidia/label/cuda-12.1.0 usually results in better enviroment solving. Sometimes the label doesn’t have all the packages, so you’ll need to add them manually to the conda install packages nvidia::missing_pacakge.↩︎\nThe FFCV Linux installation guide states the compilers package is rarely needed.↩︎\nfastxtend Windows support is currently untested, but it should work. It is recommended to use WSL on Windows.↩︎\nThis will not be a one-to-one recreation, as fastai.vision.augment.RandomResizedCrop adds padding and squishes the validation images, while the fastxend Loader will use a standard center crop for validation. See the Identical fastai and FFCV Dataloaders section for how to create an identical fastai dataloader.↩︎\nSince this is a single label problem, we need to remove the added dimension after the batch is created. If this were a multi-labeled dataset, we’d skip the squeezing step.↩︎\nCuPy is required to use for FFCV’s NormalizeImage on the GPU. Add cupy to the conda installation script if using.↩︎\nAssuming your machine has more than 8GB of RAM.↩︎\nIgnoring the OpenCV vs Pillow resizing differences.↩︎"
  },
  {
    "objectID": "ffcv.writer.html",
    "href": "ffcv.writer.html",
    "title": "FFCV Writer",
    "section": "",
    "text": "fastxtend’s RGBImageField only differs in encoding from FFCV’s RGBImageField. Decoding is the same for both.\nThis module modifies FFCV’s DatasetWriter to write fastxtend’s RGBImageField as FFCV’s RGBImageField during dataset creation so both FFCV’s Loader and fastxtend’s Loader will read RGBImageField without requiring a custom field.\nFor dataset interoperability, use fastxtend’s DatasetWriter when creating FFCV datasets using fastxtend’s RGBImageField.\n\nsource\n\nDatasetWriter\n\n DatasetWriter (fname:str, fields:Mapping[str,ffcv.fields.base.Field],\n                page_size:int=8388608, num_workers:int=-1)\n\nWrites given dataset into FFCV format (.beton). Supports indexable objects (e.g., PyTorch Datasets) and webdataset.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfname\nstr\n\n\n\n\nfields\ntyping.Mapping[str, ffcv.fields.base.Field]\n\nMap from keys to Field’s (order matters!)\n\n\npage_size\nint\n8388608\nPage size used internally\n\n\nnum_workers\nint\n-1\nNumber of processes to use\n\n\n\n\n\n\nDatasetWriter.from_indexed_dataset\n\n DatasetWriter.from_indexed_dataset (dataset, indices:List[int]=None,\n                                     chunksize=100,\n                                     shuffle_indices:bool=False)\n\nRead dataset from an indexable dataset. See https://docs.ffcv.io/writing_datasets.html#indexable-dataset for sample usage.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataset\n\n\n\n\n\nindices\ntyping.List[int]\nNone\nUse a subset of the dataset specified by indices.\n\n\nchunksize\nint\n100\nSize of chunks processed by each worker during conversion.\n\n\nshuffle_indices\nbool\nFalse\nShuffle order of the dataset.\n\n\n\n\n\n\nDatasetWriter.from_webdataset\n\n DatasetWriter.from_webdataset (shards:List[str], pipeline:Callable)\n\nRead from webdataset-like format. See https://docs.ffcv.io/writing_datasets.html#webdataset for sample usage."
  },
  {
    "objectID": "text.huggingface.html",
    "href": "text.huggingface.html",
    "title": "Hugging Face Transformers Compatibility",
    "section": "",
    "text": "fastxtend provides basic compatibility for training Hugging Face Transformers models using the fastai.learner.Learner.\nTo use fastxend’s compatibility, setup the Hugging Face dataset, dataloader, and model per the Transformers documentation, exchanging the PyTorch Dataloader for the HuggingFaceLoader. Then wrap the dataloaders in fastai.data.core.DataLoaders and create a Learner with the Hugging Face model, HuggingFaceLoss, and HuggingFaceCallback. This will automatically setup the compatibility and use the Hugging Face model’s built in loss.\nJump to the example section for a full tutorial.\nTo train with a different loss, pass in a PyTorch compatible loss to Learner as normal, and HuggingFaceCallback will use it instead of the model’s built in loss.\nsource"
  },
  {
    "objectID": "text.huggingface.html#example",
    "href": "text.huggingface.html#example",
    "title": "Hugging Face Transformers Compatibility",
    "section": "Example",
    "text": "Example\nIn this example, we’ll use Hugging Face Transformers along with fastai & fastxtend to train a DistilRoBERTa on a IMDb subset.\n\n\n\n\n\n\nNote: Transformers Documentation\n\n\n\n\n\nThis example is based on the Transformers documentation sequence classification example.\n\n\n\n\nSetup Transformer Objects\nFirst, we’ll grab the DistilRoBERTa tokenizer and model from the Transformers Auto methods.\n\ntokenizer = AutoTokenizer.from_pretrained('distilroberta-base')\nmodel = AutoModelForSequenceClassification.from_pretrained('distilroberta-base', num_labels=2)\n\nNext, download IMDb using Dataset’s load_dataset. In this example, we’ll use a subset of IMDb.\n\nimdb = load_dataset('imdb')\nwith less_random():\n    imdb['train'] = imdb['train'].shuffle().select(range(5000))\n    imdb['test'] = imdb['test'].shuffle().select(range(1000))\n\nNext, we’ll tokenize the data using Dataset’s map method.\n\ndef tokenize_data(batch, tokenizer):\n    return tokenizer(batch['text'], truncation=True)\n\nimdb['train'] = imdb['train'].map(\n    partial(tokenize_data, tokenizer=tokenizer),\n    remove_columns='text', batched=True, batch_size=512, num_proc=num_cpus(),\n)\n\nimdb['test'] = imdb['test'].map(\n    partial(tokenize_data, tokenizer=tokenizer),\n    remove_columns='text', batched=True, batch_size=512, num_proc=num_cpus(),\n)\n\n\n\nDefine the DataLoader\nWe need to use fastxtend’s HuggingFaceLoader instead of the PyTorch DataLoader. HuggingFaceLoader is a simple wrapper around a PyTorch DataLoader which returns Transformer’s dictionary batches in tuples as the fastai.learner.Learner expects. It is otherwise identical to the PyTorch DataLoader.\nAfter creating the train and valid HuggingFaceLoader, we need to wrap them in fastai.data.core.DataLoaders.\n\nwith less_random():\n    train_dataloader = HuggingFaceLoader(\n        imdb['train'].with_format('torch'), batch_size=16,\n        collate_fn=DataCollatorWithPadding(tokenizer), shuffle=True,\n        drop_last=True, num_workers=num_cpus()\n    )\n\n    valid_dataloader = HuggingFaceLoader(\n        imdb['test'].with_format('torch'), batch_size=16,\n        collate_fn=DataCollatorWithPadding(tokenizer), shuffle=False,\n        drop_last=False, num_workers=num_cpus()\n    )\n\n    dls = DataLoaders(train_dataloader, valid_dataloader)\n\n\n\nCreate a Learner and Train\nFinally, we’ll create the Learner to train DistilRoBERTa on IMDb. We’ll pass in the HuggingFaceCallback to cbs to handle loss function compatibility between Transformers and fastai.\nTransformer models contain an internal loss method, which we’ll use by passing HuggingFaceLoss to loss_func.\n\n\n\n\n\n\nNote: Non-Default Transformers Keys\n\n\n\n\n\nHuggingFaceCallback expects the Transformer model to have and output logits and loss keys. If these exist but are named differently, you’ll need to pass the non-standard key names to HuggingFaceCallback.\nIf your input doesn’t have a label key, perhaps because you are pretraining a causal language model, you should set it to None.\n\n\n\nWe now can use any fastai and/or fastxtend callbacks, optimizers, or metrics to train our Transformers model like usual.\n\nwith less_random():\n    learn = Learner(dls, model, loss_func=HuggingFaceLoss(),\n                    opt_func=stableadam(foreach=True),\n                    metrics=Accuracy(), cbs=HuggingFaceCallback).to_bf16()\n\n    learn.fit_flat_warmup(3, lr=8e-4, wd=1e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.691708\n0.690203\n0.492000\n00:38\n\n\n1\n0.510412\n0.409681\n0.854000\n00:37\n\n\n2\n0.282954\n0.300484\n0.873000\n00:38\n\n\n\n\n\nIf we want to use our own loss, such as nn.CrossEntropyLoss with label smoothing, we could pass in any PyTorch compatible loss function to Learner and HuggingFaceCallback will automatically use it instead of DistilRoBERTa’s internal loss function.\nIn this example, we use fastxtend’s CompilerCallback via the Learner.compile convenience method to accelerate training throughput using torch.compile. After compiling the model in the first epoch, training speed is increased, and memory usage is reduced. In this small example it’s an overall loss, but we’d want to compile DistilRoBERTa if training on the entirety of IMDb.\n\n\n\n\n\n\nWarning: Dynamic Requires PyTorch 2.1+\n\n\n\n\n\nCompiling the model with compile(dynamic=True) requires a minimum of Pytorch 2.1. Dynamic shapes does not work in PyTorch 2.0.\n\n\n\n\nmodel = AutoModelForSequenceClassification.from_pretrained('distilroberta-base', num_labels=2)\n\nwith less_random():\n    learn = Learner(dls, model, loss_func=nn.CrossEntropyLoss(label_smoothing=0.1),\n                    opt_func=stableadam(foreach=True), metrics=Accuracy(),\n                    cbs=HuggingFaceCallback).to_bf16().compile(dynamic=True)\n\n    learn.fit_flat_warmup(3, lr=8e-4, wd=1e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.686346\n0.677865\n0.658000\n01:25\n\n\n1\n0.423131\n0.383354\n0.886000\n00:27\n\n\n2\n0.355547\n0.374400\n0.887000\n00:27\n\n\n\n\n\n\n\nAccessing and Saving the Model\nThe the original Transformers model is accessable via Learner.model.hf_model or Learner.hf_model (both point to the same object).\nWe can use any Transformers method to save the model, such as save_pretrained.\n\nlearn.hf_model.save_pretrained(model_path)"
  },
  {
    "objectID": "vision.models.xresnet.html",
    "href": "vision.models.xresnet.html",
    "title": "XResNet Extended",
    "section": "",
    "text": "fastxtend’s XResNet is backwards compatible with fastai.vision.models.xresnet.XResNet.\nIt adds the following features to XResNet:"
  },
  {
    "objectID": "vision.models.xresnet.html#resnet-blocks",
    "href": "vision.models.xresnet.html#resnet-blocks",
    "title": "XResNet Extended",
    "section": "ResNet Blocks",
    "text": "ResNet Blocks\n\nsource\n\nResBlock\n\n ResBlock (expansion, ni, nf, stride=1, groups=1, attn_mod=None, nh1=None,\n           nh2=None, dw=False, g2=1, sa=False, sym=False,\n           norm_type=&lt;NormType.Batch: 1&gt;, act_cls=&lt;class\n           'torch.nn.modules.activation.ReLU'&gt;, ndim=2, ks=3,\n           block_pool=&lt;function AvgPool&gt;, pool_first=True, stoch_depth=0,\n           padding=None, bias=None, bn_1st=True, transpose=False,\n           init='auto', xtra=None, bias_std=0.01,\n           dilation:Union[int,Tuple[int,int]]=1, padding_mode:str='zeros',\n           device=None, dtype=None)\n\nResnet block from ni to nh with stride\n\nsource\n\n\nResNeXtBlock\n\n ResNeXtBlock (expansion, ni, nf, groups=32, stride=1, base_width=4,\n               attn_mod=None, nh1=None, nh2=None, dw=False, g2=1,\n               sa=False, sym=False, norm_type=&lt;NormType.Batch: 1&gt;,\n               act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;, ndim=2,\n               ks=3, block_pool=&lt;function AvgPool&gt;, pool_first=True,\n               stoch_depth=0, padding=None, bias=None, bn_1st=True,\n               transpose=False, init='auto', xtra=None, bias_std=0.01,\n               dilation:Union[int,Tuple[int,int]]=1,\n               padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nexpansion\n\n\n\n\n\nni\n\n\n\n\n\nnf\n\n\n\n\n\ngroups\nint\n32\n\n\n\nstride\nint\n1\n\n\n\nbase_width\nint\n4\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsa\nbool\nFalse\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone"
  },
  {
    "objectID": "vision.models.xresnet.html#squeeze-excitation-blocks",
    "href": "vision.models.xresnet.html#squeeze-excitation-blocks",
    "title": "XResNet Extended",
    "section": "Squeeze & Excitation Blocks",
    "text": "Squeeze & Excitation Blocks\n\nsource\n\nSEBlock\n\n SEBlock (expansion, ni, nf, groups=1, se_reduction=16, stride=1,\n          se_act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;,\n          attn_mod=None, nh1=None, nh2=None, dw=False, g2=1, sa=False,\n          sym=False, norm_type=&lt;NormType.Batch: 1&gt;, act_cls=&lt;class\n          'torch.nn.modules.activation.ReLU'&gt;, ndim=2, ks=3,\n          block_pool=&lt;function AvgPool&gt;, pool_first=True, stoch_depth=0,\n          padding=None, bias=None, bn_1st=True, transpose=False,\n          init='auto', xtra=None, bias_std=0.01,\n          dilation:Union[int,Tuple[int,int]]=1, padding_mode:str='zeros',\n          device=None, dtype=None)\n\nA Squeeze and Excitation XResNet Block. Can set se_act_cls seperately.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nexpansion\n\n\n\n\n\nni\n\n\n\n\n\nnf\n\n\n\n\n\ngroups\nint\n1\n\n\n\nse_reduction\nint\n16\n\n\n\nstride\nint\n1\n\n\n\nse_act_cls\ntype\nReLU\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsa\nbool\nFalse\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nSEResNeXtBlock\n\n SEResNeXtBlock (expansion, ni, nf, groups=32, se_reduction=16, stride=1,\n                 base_width=4, se_act_cls=&lt;class\n                 'torch.nn.modules.activation.ReLU'&gt;, attn_mod=None,\n                 nh1=None, nh2=None, dw=False, g2=1, sa=False, sym=False,\n                 norm_type=&lt;NormType.Batch: 1&gt;, act_cls=&lt;class\n                 'torch.nn.modules.activation.ReLU'&gt;, ndim=2, ks=3,\n                 block_pool=&lt;function AvgPool&gt;, pool_first=True,\n                 stoch_depth=0, padding=None, bias=None, bn_1st=True,\n                 transpose=False, init='auto', xtra=None, bias_std=0.01,\n                 dilation:Union[int,Tuple[int,int]]=1,\n                 padding_mode:str='zeros', device=None, dtype=None)\n\nA Squeeze and Excitation XResNeXtBlock. Can set se_act_cls seperately.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nexpansion\n\n\n\n\n\nni\n\n\n\n\n\nnf\n\n\n\n\n\ngroups\nint\n32\n\n\n\nse_reduction\nint\n16\n\n\n\nstride\nint\n1\n\n\n\nbase_width\nint\n4\n\n\n\nse_act_cls\ntype\nReLU\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsa\nbool\nFalse\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone"
  },
  {
    "objectID": "vision.models.xresnet.html#efficient-channel-attention-blocks",
    "href": "vision.models.xresnet.html#efficient-channel-attention-blocks",
    "title": "XResNet Extended",
    "section": "Efficient Channel Attention Blocks",
    "text": "Efficient Channel Attention Blocks\n\nsource\n\nECABlock\n\n ECABlock (expansion, ni, nf, groups=1, eca_ks=None, stride=1,\n           attn_mod=None, nh1=None, nh2=None, dw=False, g2=1, sa=False,\n           sym=False, norm_type=&lt;NormType.Batch: 1&gt;, act_cls=&lt;class\n           'torch.nn.modules.activation.ReLU'&gt;, ndim=2, ks=3,\n           block_pool=&lt;function AvgPool&gt;, pool_first=True, stoch_depth=0,\n           padding=None, bias=None, bn_1st=True, transpose=False,\n           init='auto', xtra=None, bias_std=0.01,\n           dilation:Union[int,Tuple[int,int]]=1, padding_mode:str='zeros',\n           device=None, dtype=None)\n\nAn Efficient Channel Attention XResNet Block\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nexpansion\n\n\n\n\n\nni\n\n\n\n\n\nnf\n\n\n\n\n\ngroups\nint\n1\n\n\n\neca_ks\nNoneType\nNone\n\n\n\nstride\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsa\nbool\nFalse\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nECAResNeXtBlock\n\n ECAResNeXtBlock (expansion, ni, nf, groups=32, eca_ks=None, stride=1,\n                  base_width=4, attn_mod=None, nh1=None, nh2=None,\n                  dw=False, g2=1, sa=False, sym=False,\n                  norm_type=&lt;NormType.Batch: 1&gt;, act_cls=&lt;class\n                  'torch.nn.modules.activation.ReLU'&gt;, ndim=2, ks=3,\n                  block_pool=&lt;function AvgPool&gt;, pool_first=True,\n                  stoch_depth=0, padding=None, bias=None, bn_1st=True,\n                  transpose=False, init='auto', xtra=None, bias_std=0.01,\n                  dilation:Union[int,Tuple[int,int]]=1,\n                  padding_mode:str='zeros', device=None, dtype=None)\n\nAn Efficient Channel Attention XResNeXtBlock\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nexpansion\n\n\n\n\n\nni\n\n\n\n\n\nnf\n\n\n\n\n\ngroups\nint\n32\n\n\n\neca_ks\nNoneType\nNone\n\n\n\nstride\nint\n1\n\n\n\nbase_width\nint\n4\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsa\nbool\nFalse\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone"
  },
  {
    "objectID": "vision.models.xresnet.html#shuffle-attention-blocks",
    "href": "vision.models.xresnet.html#shuffle-attention-blocks",
    "title": "XResNet Extended",
    "section": "Shuffle Attention Blocks",
    "text": "Shuffle Attention Blocks\n\nsource\n\nSABlock\n\n SABlock (expansion, ni, nf, groups=1, sa_grps=64, stride=1,\n          attn_mod=None, nh1=None, nh2=None, dw=False, g2=1, sa=False,\n          sym=False, norm_type=&lt;NormType.Batch: 1&gt;, act_cls=&lt;class\n          'torch.nn.modules.activation.ReLU'&gt;, ndim=2, ks=3,\n          block_pool=&lt;function AvgPool&gt;, pool_first=True, stoch_depth=0,\n          padding=None, bias=None, bn_1st=True, transpose=False,\n          init='auto', xtra=None, bias_std=0.01,\n          dilation:Union[int,Tuple[int,int]]=1, padding_mode:str='zeros',\n          device=None, dtype=None)\n\nA Shuffle Attention XResNet Block\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nexpansion\n\n\n\n\n\nni\n\n\n\n\n\nnf\n\n\n\n\n\ngroups\nint\n1\n\n\n\nsa_grps\nint\n64\n\n\n\nstride\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsa\nbool\nFalse\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nSAResNeXtBlock\n\n SAResNeXtBlock (expansion, ni, nf, groups=32, sa_grps=64, stride=1,\n                 base_width=4, attn_mod=None, nh1=None, nh2=None,\n                 dw=False, g2=1, sa=False, sym=False,\n                 norm_type=&lt;NormType.Batch: 1&gt;, act_cls=&lt;class\n                 'torch.nn.modules.activation.ReLU'&gt;, ndim=2, ks=3,\n                 block_pool=&lt;function AvgPool&gt;, pool_first=True,\n                 stoch_depth=0, padding=None, bias=None, bn_1st=True,\n                 transpose=False, init='auto', xtra=None, bias_std=0.01,\n                 dilation:Union[int,Tuple[int,int]]=1,\n                 padding_mode:str='zeros', device=None, dtype=None)\n\nA Shuffle Attention XResNeXtBlock\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nexpansion\n\n\n\n\n\nni\n\n\n\n\n\nnf\n\n\n\n\n\ngroups\nint\n32\n\n\n\nsa_grps\nint\n64\n\n\n\nstride\nint\n1\n\n\n\nbase_width\nint\n4\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsa\nbool\nFalse\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone"
  },
  {
    "objectID": "vision.models.xresnet.html#triplet-attention-blocks",
    "href": "vision.models.xresnet.html#triplet-attention-blocks",
    "title": "XResNet Extended",
    "section": "Triplet Attention Blocks",
    "text": "Triplet Attention Blocks\n\nsource\n\nTABlock\n\n TABlock (expansion, ni, nf, groups=1, ta_ks=7, stride=1, attn_mod=None,\n          nh1=None, nh2=None, dw=False, g2=1, sa=False, sym=False,\n          norm_type=&lt;NormType.Batch: 1&gt;, act_cls=&lt;class\n          'torch.nn.modules.activation.ReLU'&gt;, ndim=2, ks=3,\n          block_pool=&lt;function AvgPool&gt;, pool_first=True, stoch_depth=0,\n          padding=None, bias=None, bn_1st=True, transpose=False,\n          init='auto', xtra=None, bias_std=0.01,\n          dilation:Union[int,Tuple[int,int]]=1, padding_mode:str='zeros',\n          device=None, dtype=None)\n\nA Triplet Attention XResNet Block\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nexpansion\n\n\n\n\n\nni\n\n\n\n\n\nnf\n\n\n\n\n\ngroups\nint\n1\n\n\n\nta_ks\nint\n7\n\n\n\nstride\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsa\nbool\nFalse\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nTAResNeXtBlock\n\n TAResNeXtBlock (expansion, ni, nf, groups=32, ta_ks=7, stride=1,\n                 base_width=4, attn_mod=None, nh1=None, nh2=None,\n                 dw=False, g2=1, sa=False, sym=False,\n                 norm_type=&lt;NormType.Batch: 1&gt;, act_cls=&lt;class\n                 'torch.nn.modules.activation.ReLU'&gt;, ndim=2, ks=3,\n                 block_pool=&lt;function AvgPool&gt;, pool_first=True,\n                 stoch_depth=0, padding=None, bias=None, bn_1st=True,\n                 transpose=False, init='auto', xtra=None, bias_std=0.01,\n                 dilation:Union[int,Tuple[int,int]]=1,\n                 padding_mode:str='zeros', device=None, dtype=None)\n\nA Triplet Attention XResNeXtBlock\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nexpansion\n\n\n\n\n\nni\n\n\n\n\n\nnf\n\n\n\n\n\ngroups\nint\n32\n\n\n\nta_ks\nint\n7\n\n\n\nstride\nint\n1\n\n\n\nbase_width\nint\n4\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsa\nbool\nFalse\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nXResNet\n\n XResNet (block, expansion, layers, p=0.0, c_in=3, n_out=1000,\n          stem_szs=(32, 32, 64), block_szs=[64, 128, 256, 512], widen=1.0,\n          sa=False, act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;,\n          ndim=2, ks=3, stride=2, stem_layer=&lt;class\n          'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n          head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n          pretrained=False, groups=1, attn_mod=None, nh1=None, nh2=None,\n          dw=False, g2=1, sym=False, norm_type=&lt;NormType.Batch: 1&gt;,\n          block_pool=&lt;function AvgPool&gt;, pool_first=True, stoch_depth=0,\n          padding=None, bias=None, bn_1st=True, transpose=False,\n          init='auto', xtra=None, bias_std=0.01,\n          dilation:Union[int,Tuple[int,int]]=1, padding_mode:str='zeros',\n          device=None, dtype=None)\n\nA flexible version of fastai’s XResNet\nFastxtend’s XResNet allows a custom_head, setting stem_pool, block_pool, and head_pool pooling layers on creation, per ResBlock stochastic depth stoch_depth, and support for more attention modules."
  },
  {
    "objectID": "vision.models.xresnet.html#xresnet-models",
    "href": "vision.models.xresnet.html#xresnet-models",
    "title": "XResNet Extended",
    "section": "XResNet Models",
    "text": "XResNet Models\nPredefined XResNet models\n\nsource\n\nxresnet101\n\n xresnet101 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n             block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n             act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;, ndim=2,\n             ks=3, stride=2, stem_layer=&lt;class 'fastai.layers.ConvLayer'&gt;,\n             stem_pool=&lt;function MaxPool&gt;, head_pool=&lt;function\n             AdaptiveAvgPool&gt;, custom_head=None, pretrained=False,\n             groups=1, attn_mod=None, nh1=None, nh2=None, dw=False, g2=1,\n             sym=False, norm_type=&lt;NormType.Batch: 1&gt;,\n             block_pool=&lt;function AvgPool&gt;, pool_first=True,\n             stoch_depth=0, padding=None, bias=None, bn_1st=True,\n             transpose=False, init='auto', xtra=None, bias_std=0.01,\n             dilation:Union[int,Tuple[int,int]]=1,\n             padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxresnet50\n\n xresnet50 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n            block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n            act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;, ndim=2,\n            ks=3, stride=2, stem_layer=&lt;class 'fastai.layers.ConvLayer'&gt;,\n            stem_pool=&lt;function MaxPool&gt;, head_pool=&lt;function\n            AdaptiveAvgPool&gt;, custom_head=None, pretrained=False,\n            groups=1, attn_mod=None, nh1=None, nh2=None, dw=False, g2=1,\n            sym=False, norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n            AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n            bias=None, bn_1st=True, transpose=False, init='auto',\n            xtra=None, bias_std=0.01,\n            dilation:Union[int,Tuple[int,int]]=1,\n            padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxresnet34\n\n xresnet34 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n            block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n            act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;, ndim=2,\n            ks=3, stride=2, stem_layer=&lt;class 'fastai.layers.ConvLayer'&gt;,\n            stem_pool=&lt;function MaxPool&gt;, head_pool=&lt;function\n            AdaptiveAvgPool&gt;, custom_head=None, pretrained=False,\n            groups=1, attn_mod=None, nh1=None, nh2=None, dw=False, g2=1,\n            sym=False, norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n            AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n            bias=None, bn_1st=True, transpose=False, init='auto',\n            xtra=None, bias_std=0.01,\n            dilation:Union[int,Tuple[int,int]]=1,\n            padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxresnet18\n\n xresnet18 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n            block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n            act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;, ndim=2,\n            ks=3, stride=2, stem_layer=&lt;class 'fastai.layers.ConvLayer'&gt;,\n            stem_pool=&lt;function MaxPool&gt;, head_pool=&lt;function\n            AdaptiveAvgPool&gt;, custom_head=None, pretrained=False,\n            groups=1, attn_mod=None, nh1=None, nh2=None, dw=False, g2=1,\n            sym=False, norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n            AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n            bias=None, bn_1st=True, transpose=False, init='auto',\n            xtra=None, bias_std=0.01,\n            dilation:Union[int,Tuple[int,int]]=1,\n            padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone"
  },
  {
    "objectID": "vision.models.xresnet.html#xresnext-models",
    "href": "vision.models.xresnet.html#xresnext-models",
    "title": "XResNet Extended",
    "section": "XResNeXt Models",
    "text": "XResNeXt Models\nPredefined XResNeXt models\n\nsource\n\nxresnext101\n\n xresnext101 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n              block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n              act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;, ndim=2,\n              ks=3, stride=2, stem_layer=&lt;class\n              'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n              head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n              pretrained=False, groups=1, attn_mod=None, nh1=None,\n              nh2=None, dw=False, g2=1, sym=False,\n              norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n              AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n              bias=None, bn_1st=True, transpose=False, init='auto',\n              xtra=None, bias_std=0.01,\n              dilation:Union[int,Tuple[int,int]]=1,\n              padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxresnext50\n\n xresnext50 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n             block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n             act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;, ndim=2,\n             ks=3, stride=2, stem_layer=&lt;class 'fastai.layers.ConvLayer'&gt;,\n             stem_pool=&lt;function MaxPool&gt;, head_pool=&lt;function\n             AdaptiveAvgPool&gt;, custom_head=None, pretrained=False,\n             groups=1, attn_mod=None, nh1=None, nh2=None, dw=False, g2=1,\n             sym=False, norm_type=&lt;NormType.Batch: 1&gt;,\n             block_pool=&lt;function AvgPool&gt;, pool_first=True,\n             stoch_depth=0, padding=None, bias=None, bn_1st=True,\n             transpose=False, init='auto', xtra=None, bias_std=0.01,\n             dilation:Union[int,Tuple[int,int]]=1,\n             padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxresnext34\n\n xresnext34 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n             block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n             act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;, ndim=2,\n             ks=3, stride=2, stem_layer=&lt;class 'fastai.layers.ConvLayer'&gt;,\n             stem_pool=&lt;function MaxPool&gt;, head_pool=&lt;function\n             AdaptiveAvgPool&gt;, custom_head=None, pretrained=False,\n             groups=1, attn_mod=None, nh1=None, nh2=None, dw=False, g2=1,\n             sym=False, norm_type=&lt;NormType.Batch: 1&gt;,\n             block_pool=&lt;function AvgPool&gt;, pool_first=True,\n             stoch_depth=0, padding=None, bias=None, bn_1st=True,\n             transpose=False, init='auto', xtra=None, bias_std=0.01,\n             dilation:Union[int,Tuple[int,int]]=1,\n             padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxresnext18\n\n xresnext18 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n             block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n             act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;, ndim=2,\n             ks=3, stride=2, stem_layer=&lt;class 'fastai.layers.ConvLayer'&gt;,\n             stem_pool=&lt;function MaxPool&gt;, head_pool=&lt;function\n             AdaptiveAvgPool&gt;, custom_head=None, pretrained=False,\n             groups=1, attn_mod=None, nh1=None, nh2=None, dw=False, g2=1,\n             sym=False, norm_type=&lt;NormType.Batch: 1&gt;,\n             block_pool=&lt;function AvgPool&gt;, pool_first=True,\n             stoch_depth=0, padding=None, bias=None, bn_1st=True,\n             transpose=False, init='auto', xtra=None, bias_std=0.01,\n             dilation:Union[int,Tuple[int,int]]=1,\n             padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone"
  },
  {
    "objectID": "vision.models.xresnet.html#xse-resnet-models",
    "href": "vision.models.xresnet.html#xse-resnet-models",
    "title": "XResNet Extended",
    "section": "XSE-ResNet Models",
    "text": "XSE-ResNet Models\nPredefined Squeeze and Excitation XResNet models\n\nsource\n\nxse_resnet101\n\n xse_resnet101 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n                block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n                act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;,\n                ndim=2, ks=3, stride=2, stem_layer=&lt;class\n                'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n                head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n                pretrained=False, groups=1, attn_mod=None, nh1=None,\n                nh2=None, dw=False, g2=1, sym=False,\n                norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n                AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n                bias=None, bn_1st=True, transpose=False, init='auto',\n                xtra=None, bias_std=0.01,\n                dilation:Union[int,Tuple[int,int]]=1,\n                padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxse_resnet50\n\n xse_resnet50 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n               block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n               act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;, ndim=2,\n               ks=3, stride=2, stem_layer=&lt;class\n               'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n               head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n               pretrained=False, groups=1, attn_mod=None, nh1=None,\n               nh2=None, dw=False, g2=1, sym=False,\n               norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n               AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n               bias=None, bn_1st=True, transpose=False, init='auto',\n               xtra=None, bias_std=0.01,\n               dilation:Union[int,Tuple[int,int]]=1,\n               padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxse_resnet34\n\n xse_resnet34 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n               block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n               act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;, ndim=2,\n               ks=3, stride=2, stem_layer=&lt;class\n               'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n               head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n               pretrained=False, groups=1, attn_mod=None, nh1=None,\n               nh2=None, dw=False, g2=1, sym=False,\n               norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n               AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n               bias=None, bn_1st=True, transpose=False, init='auto',\n               xtra=None, bias_std=0.01,\n               dilation:Union[int,Tuple[int,int]]=1,\n               padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxse_resnet18\n\n xse_resnet18 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n               block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n               act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;, ndim=2,\n               ks=3, stride=2, stem_layer=&lt;class\n               'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n               head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n               pretrained=False, groups=1, attn_mod=None, nh1=None,\n               nh2=None, dw=False, g2=1, sym=False,\n               norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n               AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n               bias=None, bn_1st=True, transpose=False, init='auto',\n               xtra=None, bias_std=0.01,\n               dilation:Union[int,Tuple[int,int]]=1,\n               padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone"
  },
  {
    "objectID": "vision.models.xresnet.html#xse-resnext-models",
    "href": "vision.models.xresnet.html#xse-resnext-models",
    "title": "XResNet Extended",
    "section": "XSE-ResNeXt Models",
    "text": "XSE-ResNeXt Models\nPredefined Squeeze and Excitation XResNeXt models\n\nsource\n\nxse_resnext101\n\n xse_resnext101 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n                 block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n                 act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;,\n                 ndim=2, ks=3, stride=2, stem_layer=&lt;class\n                 'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n                 head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n                 pretrained=False, groups=1, attn_mod=None, nh1=None,\n                 nh2=None, dw=False, g2=1, sym=False,\n                 norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n                 AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n                 bias=None, bn_1st=True, transpose=False, init='auto',\n                 xtra=None, bias_std=0.01,\n                 dilation:Union[int,Tuple[int,int]]=1,\n                 padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxse_resnext50\n\n xse_resnext50 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n                block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n                act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;,\n                ndim=2, ks=3, stride=2, stem_layer=&lt;class\n                'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n                head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n                pretrained=False, groups=1, attn_mod=None, nh1=None,\n                nh2=None, dw=False, g2=1, sym=False,\n                norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n                AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n                bias=None, bn_1st=True, transpose=False, init='auto',\n                xtra=None, bias_std=0.01,\n                dilation:Union[int,Tuple[int,int]]=1,\n                padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxse_resnext34\n\n xse_resnext34 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n                block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n                act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;,\n                ndim=2, ks=3, stride=2, stem_layer=&lt;class\n                'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n                head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n                pretrained=False, groups=1, attn_mod=None, nh1=None,\n                nh2=None, dw=False, g2=1, sym=False,\n                norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n                AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n                bias=None, bn_1st=True, transpose=False, init='auto',\n                xtra=None, bias_std=0.01,\n                dilation:Union[int,Tuple[int,int]]=1,\n                padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxse_resnext18\n\n xse_resnext18 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n                block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n                act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;,\n                ndim=2, ks=3, stride=2, stem_layer=&lt;class\n                'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n                head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n                pretrained=False, groups=1, attn_mod=None, nh1=None,\n                nh2=None, dw=False, g2=1, sym=False,\n                norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n                AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n                bias=None, bn_1st=True, transpose=False, init='auto',\n                xtra=None, bias_std=0.01,\n                dilation:Union[int,Tuple[int,int]]=1,\n                padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone"
  },
  {
    "objectID": "vision.models.xresnet.html#xeca-resnet-models",
    "href": "vision.models.xresnet.html#xeca-resnet-models",
    "title": "XResNet Extended",
    "section": "XECA-ResNet Models",
    "text": "XECA-ResNet Models\nPredefined Efficient Channel Attention XResNet models\n\nsource\n\nxeca_resnet101\n\n xeca_resnet101 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n                 block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n                 act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;,\n                 ndim=2, ks=3, stride=2, stem_layer=&lt;class\n                 'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n                 head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n                 pretrained=False, groups=1, attn_mod=None, nh1=None,\n                 nh2=None, dw=False, g2=1, sym=False,\n                 norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n                 AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n                 bias=None, bn_1st=True, transpose=False, init='auto',\n                 xtra=None, bias_std=0.01,\n                 dilation:Union[int,Tuple[int,int]]=1,\n                 padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxeca_resnet50\n\n xeca_resnet50 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n                block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n                act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;,\n                ndim=2, ks=3, stride=2, stem_layer=&lt;class\n                'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n                head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n                pretrained=False, groups=1, attn_mod=None, nh1=None,\n                nh2=None, dw=False, g2=1, sym=False,\n                norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n                AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n                bias=None, bn_1st=True, transpose=False, init='auto',\n                xtra=None, bias_std=0.01,\n                dilation:Union[int,Tuple[int,int]]=1,\n                padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxeca_resnet34\n\n xeca_resnet34 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n                block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n                act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;,\n                ndim=2, ks=3, stride=2, stem_layer=&lt;class\n                'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n                head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n                pretrained=False, groups=1, attn_mod=None, nh1=None,\n                nh2=None, dw=False, g2=1, sym=False,\n                norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n                AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n                bias=None, bn_1st=True, transpose=False, init='auto',\n                xtra=None, bias_std=0.01,\n                dilation:Union[int,Tuple[int,int]]=1,\n                padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxeca_resnet18\n\n xeca_resnet18 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n                block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n                act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;,\n                ndim=2, ks=3, stride=2, stem_layer=&lt;class\n                'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n                head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n                pretrained=False, groups=1, attn_mod=None, nh1=None,\n                nh2=None, dw=False, g2=1, sym=False,\n                norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n                AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n                bias=None, bn_1st=True, transpose=False, init='auto',\n                xtra=None, bias_std=0.01,\n                dilation:Union[int,Tuple[int,int]]=1,\n                padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone"
  },
  {
    "objectID": "vision.models.xresnet.html#xeca-resnext-models",
    "href": "vision.models.xresnet.html#xeca-resnext-models",
    "title": "XResNet Extended",
    "section": "XECA-ResNeXt Models",
    "text": "XECA-ResNeXt Models\nPredefined Efficient Channel Attention XResNeXt models\n\nsource\n\nxeca_resnext101\n\n xeca_resnext101 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n                  block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n                  act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;,\n                  ndim=2, ks=3, stride=2, stem_layer=&lt;class\n                  'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function\n                  MaxPool&gt;, head_pool=&lt;function AdaptiveAvgPool&gt;,\n                  custom_head=None, pretrained=False, groups=1,\n                  attn_mod=None, nh1=None, nh2=None, dw=False, g2=1,\n                  sym=False, norm_type=&lt;NormType.Batch: 1&gt;,\n                  block_pool=&lt;function AvgPool&gt;, pool_first=True,\n                  stoch_depth=0, padding=None, bias=None, bn_1st=True,\n                  transpose=False, init='auto', xtra=None, bias_std=0.01,\n                  dilation:Union[int,Tuple[int,int]]=1,\n                  padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxeca_resnext50\n\n xeca_resnext50 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n                 block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n                 act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;,\n                 ndim=2, ks=3, stride=2, stem_layer=&lt;class\n                 'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n                 head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n                 pretrained=False, groups=1, attn_mod=None, nh1=None,\n                 nh2=None, dw=False, g2=1, sym=False,\n                 norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n                 AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n                 bias=None, bn_1st=True, transpose=False, init='auto',\n                 xtra=None, bias_std=0.01,\n                 dilation:Union[int,Tuple[int,int]]=1,\n                 padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxeca_resnext34\n\n xeca_resnext34 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n                 block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n                 act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;,\n                 ndim=2, ks=3, stride=2, stem_layer=&lt;class\n                 'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n                 head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n                 pretrained=False, groups=1, attn_mod=None, nh1=None,\n                 nh2=None, dw=False, g2=1, sym=False,\n                 norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n                 AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n                 bias=None, bn_1st=True, transpose=False, init='auto',\n                 xtra=None, bias_std=0.01,\n                 dilation:Union[int,Tuple[int,int]]=1,\n                 padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxeca_resnext18\n\n xeca_resnext18 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n                 block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n                 act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;,\n                 ndim=2, ks=3, stride=2, stem_layer=&lt;class\n                 'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n                 head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n                 pretrained=False, groups=1, attn_mod=None, nh1=None,\n                 nh2=None, dw=False, g2=1, sym=False,\n                 norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n                 AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n                 bias=None, bn_1st=True, transpose=False, init='auto',\n                 xtra=None, bias_std=0.01,\n                 dilation:Union[int,Tuple[int,int]]=1,\n                 padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone"
  },
  {
    "objectID": "vision.models.xresnet.html#xsa-resnet-models",
    "href": "vision.models.xresnet.html#xsa-resnet-models",
    "title": "XResNet Extended",
    "section": "XSA-ResNet Models",
    "text": "XSA-ResNet Models\nPredefined Shuffle Attention XResNet models\n\nsource\n\nxsa_resnet101\n\n xsa_resnet101 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n                block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n                act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;,\n                ndim=2, ks=3, stride=2, stem_layer=&lt;class\n                'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n                head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n                pretrained=False, groups=1, attn_mod=None, nh1=None,\n                nh2=None, dw=False, g2=1, sym=False,\n                norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n                AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n                bias=None, bn_1st=True, transpose=False, init='auto',\n                xtra=None, bias_std=0.01,\n                dilation:Union[int,Tuple[int,int]]=1,\n                padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxsa_resnet50\n\n xsa_resnet50 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n               block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n               act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;, ndim=2,\n               ks=3, stride=2, stem_layer=&lt;class\n               'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n               head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n               pretrained=False, groups=1, attn_mod=None, nh1=None,\n               nh2=None, dw=False, g2=1, sym=False,\n               norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n               AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n               bias=None, bn_1st=True, transpose=False, init='auto',\n               xtra=None, bias_std=0.01,\n               dilation:Union[int,Tuple[int,int]]=1,\n               padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxsa_resnet34\n\n xsa_resnet34 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n               block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n               act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;, ndim=2,\n               ks=3, stride=2, stem_layer=&lt;class\n               'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n               head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n               pretrained=False, groups=1, attn_mod=None, nh1=None,\n               nh2=None, dw=False, g2=1, sym=False,\n               norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n               AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n               bias=None, bn_1st=True, transpose=False, init='auto',\n               xtra=None, bias_std=0.01,\n               dilation:Union[int,Tuple[int,int]]=1,\n               padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxsa_resnet18\n\n xsa_resnet18 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n               block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n               act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;, ndim=2,\n               ks=3, stride=2, stem_layer=&lt;class\n               'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n               head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n               pretrained=False, groups=1, attn_mod=None, nh1=None,\n               nh2=None, dw=False, g2=1, sym=False,\n               norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n               AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n               bias=None, bn_1st=True, transpose=False, init='auto',\n               xtra=None, bias_std=0.01,\n               dilation:Union[int,Tuple[int,int]]=1,\n               padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone"
  },
  {
    "objectID": "vision.models.xresnet.html#xsa-resnext-models",
    "href": "vision.models.xresnet.html#xsa-resnext-models",
    "title": "XResNet Extended",
    "section": "XSA-ResNeXt Models",
    "text": "XSA-ResNeXt Models\nPredefined Shuffle Attention XResNeXt models\n\nsource\n\nxsa_resnext101\n\n xsa_resnext101 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n                 block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n                 act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;,\n                 ndim=2, ks=3, stride=2, stem_layer=&lt;class\n                 'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n                 head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n                 pretrained=False, groups=1, attn_mod=None, nh1=None,\n                 nh2=None, dw=False, g2=1, sym=False,\n                 norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n                 AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n                 bias=None, bn_1st=True, transpose=False, init='auto',\n                 xtra=None, bias_std=0.01,\n                 dilation:Union[int,Tuple[int,int]]=1,\n                 padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxsa_resnext50\n\n xsa_resnext50 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n                block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n                act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;,\n                ndim=2, ks=3, stride=2, stem_layer=&lt;class\n                'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n                head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n                pretrained=False, groups=1, attn_mod=None, nh1=None,\n                nh2=None, dw=False, g2=1, sym=False,\n                norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n                AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n                bias=None, bn_1st=True, transpose=False, init='auto',\n                xtra=None, bias_std=0.01,\n                dilation:Union[int,Tuple[int,int]]=1,\n                padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxsa_resnext34\n\n xsa_resnext34 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n                block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n                act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;,\n                ndim=2, ks=3, stride=2, stem_layer=&lt;class\n                'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n                head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n                pretrained=False, groups=1, attn_mod=None, nh1=None,\n                nh2=None, dw=False, g2=1, sym=False,\n                norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n                AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n                bias=None, bn_1st=True, transpose=False, init='auto',\n                xtra=None, bias_std=0.01,\n                dilation:Union[int,Tuple[int,int]]=1,\n                padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxsa_resnext18\n\n xsa_resnext18 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n                block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n                act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;,\n                ndim=2, ks=3, stride=2, stem_layer=&lt;class\n                'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n                head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n                pretrained=False, groups=1, attn_mod=None, nh1=None,\n                nh2=None, dw=False, g2=1, sym=False,\n                norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n                AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n                bias=None, bn_1st=True, transpose=False, init='auto',\n                xtra=None, bias_std=0.01,\n                dilation:Union[int,Tuple[int,int]]=1,\n                padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone"
  },
  {
    "objectID": "vision.models.xresnet.html#xta-resnet-models",
    "href": "vision.models.xresnet.html#xta-resnet-models",
    "title": "XResNet Extended",
    "section": "XTA-ResNet Models",
    "text": "XTA-ResNet Models\nPredefined Triplet Attention XResNet models\n\nsource\n\nxta_resnet101\n\n xta_resnet101 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n                block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n                act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;,\n                ndim=2, ks=3, stride=2, stem_layer=&lt;class\n                'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n                head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n                pretrained=False, groups=1, attn_mod=None, nh1=None,\n                nh2=None, dw=False, g2=1, sym=False,\n                norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n                AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n                bias=None, bn_1st=True, transpose=False, init='auto',\n                xtra=None, bias_std=0.01,\n                dilation:Union[int,Tuple[int,int]]=1,\n                padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxta_resnet50\n\n xta_resnet50 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n               block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n               act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;, ndim=2,\n               ks=3, stride=2, stem_layer=&lt;class\n               'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n               head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n               pretrained=False, groups=1, attn_mod=None, nh1=None,\n               nh2=None, dw=False, g2=1, sym=False,\n               norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n               AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n               bias=None, bn_1st=True, transpose=False, init='auto',\n               xtra=None, bias_std=0.01,\n               dilation:Union[int,Tuple[int,int]]=1,\n               padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxta_resnet34\n\n xta_resnet34 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n               block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n               act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;, ndim=2,\n               ks=3, stride=2, stem_layer=&lt;class\n               'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n               head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n               pretrained=False, groups=1, attn_mod=None, nh1=None,\n               nh2=None, dw=False, g2=1, sym=False,\n               norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n               AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n               bias=None, bn_1st=True, transpose=False, init='auto',\n               xtra=None, bias_std=0.01,\n               dilation:Union[int,Tuple[int,int]]=1,\n               padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxta_resnet18\n\n xta_resnet18 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n               block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n               act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;, ndim=2,\n               ks=3, stride=2, stem_layer=&lt;class\n               'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n               head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n               pretrained=False, groups=1, attn_mod=None, nh1=None,\n               nh2=None, dw=False, g2=1, sym=False,\n               norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n               AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n               bias=None, bn_1st=True, transpose=False, init='auto',\n               xtra=None, bias_std=0.01,\n               dilation:Union[int,Tuple[int,int]]=1,\n               padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone"
  },
  {
    "objectID": "vision.models.xresnet.html#xta-resnext-models",
    "href": "vision.models.xresnet.html#xta-resnext-models",
    "title": "XResNet Extended",
    "section": "XTA-ResNeXt Models",
    "text": "XTA-ResNeXt Models\nPredefined Triplet Attention XResNeXt models\n\nsource\n\nxta_resnext101\n\n xta_resnext101 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n                 block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n                 act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;,\n                 ndim=2, ks=3, stride=2, stem_layer=&lt;class\n                 'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n                 head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n                 pretrained=False, groups=1, attn_mod=None, nh1=None,\n                 nh2=None, dw=False, g2=1, sym=False,\n                 norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n                 AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n                 bias=None, bn_1st=True, transpose=False, init='auto',\n                 xtra=None, bias_std=0.01,\n                 dilation:Union[int,Tuple[int,int]]=1,\n                 padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxta_resnext50\n\n xta_resnext50 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n                block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n                act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;,\n                ndim=2, ks=3, stride=2, stem_layer=&lt;class\n                'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n                head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n                pretrained=False, groups=1, attn_mod=None, nh1=None,\n                nh2=None, dw=False, g2=1, sym=False,\n                norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n                AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n                bias=None, bn_1st=True, transpose=False, init='auto',\n                xtra=None, bias_std=0.01,\n                dilation:Union[int,Tuple[int,int]]=1,\n                padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxta_resnext34\n\n xta_resnext34 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n                block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n                act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;,\n                ndim=2, ks=3, stride=2, stem_layer=&lt;class\n                'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n                head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n                pretrained=False, groups=1, attn_mod=None, nh1=None,\n                nh2=None, dw=False, g2=1, sym=False,\n                norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n                AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n                bias=None, bn_1st=True, transpose=False, init='auto',\n                xtra=None, bias_std=0.01,\n                dilation:Union[int,Tuple[int,int]]=1,\n                padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxta_resnext18\n\n xta_resnext18 (n_out=1000, p=0.0, c_in=3, stem_szs=(32, 32, 64),\n                block_szs=[64, 128, 256, 512], widen=1.0, sa=False,\n                act_cls=&lt;class 'torch.nn.modules.activation.ReLU'&gt;,\n                ndim=2, ks=3, stride=2, stem_layer=&lt;class\n                'fastai.layers.ConvLayer'&gt;, stem_pool=&lt;function MaxPool&gt;,\n                head_pool=&lt;function AdaptiveAvgPool&gt;, custom_head=None,\n                pretrained=False, groups=1, attn_mod=None, nh1=None,\n                nh2=None, dw=False, g2=1, sym=False,\n                norm_type=&lt;NormType.Batch: 1&gt;, block_pool=&lt;function\n                AvgPool&gt;, pool_first=True, stoch_depth=0, padding=None,\n                bias=None, bn_1st=True, transpose=False, init='auto',\n                xtra=None, bias_std=0.01,\n                dilation:Union[int,Tuple[int,int]]=1,\n                padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_out\nint\n1000\n\n\n\np\nfloat\n0.0\n\n\n\nc_in\nint\n3\n\n\n\nstem_szs\ntuple\n(32, 32, 64)\n\n\n\nblock_szs\nlist\n[64, 128, 256, 512]\n\n\n\nwiden\nfloat\n1.0\n\n\n\nsa\nbool\nFalse\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\nstride\nint\n2\n\n\n\nstem_layer\ntype\nConvLayer\n\n\n\nstem_pool\nfunction\nMaxPool\n\n\n\nhead_pool\nfunction\nAdaptiveAvgPool\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\npretrained\nbool\nFalse\n\n\n\ngroups\nint\n1\n\n\n\nattn_mod\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nblock_pool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\nstoch_depth\nint\n0\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone"
  },
  {
    "objectID": "data.transforms.html",
    "href": "data.transforms.html",
    "title": "Splitters",
    "section": "",
    "text": "source\n\nKFoldColSplitter\n\n KFoldColSplitter (fold:Union[int,Iterable[int],MutableSequence[int],fastc\n                   ore.foundation.L,fastcore.basics.fastuple]=0,\n                   col:Union[int,str]='folds')\n\nSplit items (supposed to be a dataframe) by fold in col\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfold\nListified[int]\n0\nValid set fold(s)\n\n\ncol\nint | str\nfolds\nColumn with folds\n\n\n\n\nsource\n\n\nParentSplitter\n\n ParentSplitter (train_name:str='train', valid_name:str='valid')\n\nSplit items from the parent folder names (train_name and valid_name).\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntrain_name\nstr\ntrain\nTrain set folder name\n\n\nvalid_name\nstr\nvalid\nValid set folder name\n\n\n\n\nsource\n\n\nGreatGrandparentSplitter\n\n GreatGrandparentSplitter (train_name:str='train', valid_name:str='valid')\n\nSplit items from the great grand parent folder names (train_name and valid_name).\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntrain_name\nstr\ntrain\nTrain set folder name\n\n\nvalid_name\nstr\nvalid\nValid set folder name"
  },
  {
    "objectID": "optimizer.foreach.html",
    "href": "optimizer.foreach.html",
    "title": "ForEach Optimizers",
    "section": "",
    "text": "fastxtend ForEach optimizers are adapted from the PyTorch ForEach _multi_tensor implementations. They are 21 to 293 percent faster relative to fastai native optimizers depending on the model.\nThe primary difference between PyTorch’s ForEach implementations and fastxtend is fastxtend’s ForEach optimizers apply per-parameter weight decay in one optimizer step instead of requiring a seperate weight decay parameter group and a non-weight decay parameter group. This also allows seamless support for fastai’s discriminative learning rates.\nUnlike fastai optimizers, which are made of multiple stepper callbacks and share one Optimizer, ForEach optimizers require an optimizer specific ForEachOptimizer implementation.\nCurrently SGD, Adam, RAdam, Lamb, and Ranger have ForEach implementations.\n::{.callout-important} ForEach optimizers have only been tested on PyTorch 1.12+ and are not guaranteed to work on older versions. ::\nForEach optimizers are faster due to horizontal fusion across multiple parameters. Using xresnet50 and the simplest form of SGD as an example, a ForEach optimizer would construct a list of all 167 params and their grads before performing one horizontally fused step.\ndef simple_sgd_foreach(params:list[Tensor], grads:list[Tensor], lr:float)\n    torch._foreach_add_(params, grads, alpha=-lr)\nIn contrast, a standard PyTorch optimizer would call the simple SGD step 167 times:\ndef simple_sgd_standard(param:Tensor, lr:float)\n    param.add_(param.grad, alpha=-lr)\nForEach optimizers are tested to be equal to fastai optimizers for 25 steps using nbdev’s GitHub CI.\n\nsource\n\nForEachOptimizer\n\n ForEachOptimizer (params:Listified[Tensor], opt_step:Callable,\n                   decouple_wd:bool=True, **defaults)\n\nBase foreach optimizer class, updating params with opt_step instead of Optimizer.cbs\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparams\nListified[Tensor]\n\nModel parameters\n\n\nopt_step\nCallable\n\nForEachOptimizer optimizer step\n\n\ndecouple_wd\nbool\nTrue\nUse true weight decay or L2 regularization, if applicable\n\n\ndefaults\n\n\n\n\n\n\n\nsource\n\n\nSGDForEachOptimizer\n\n SGDForEachOptimizer (params:Listified[Tensor], opt_step:Callable,\n                      decouple_wd:bool=True, **defaults)\n\nA ForEachOptimizer with a modified step for sgd_foreach_step\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparams\nListified[Tensor]\n\nModel parameters\n\n\nopt_step\nCallable\n\nForEachOptimizer optimizer step\n\n\ndecouple_wd\nbool\nTrue\nUse true weight decay or L2 regularization, if applicable\n\n\ndefaults\n\n\n\n\n\n\n\nsource\n\n\nAdamForEachOptimizer\n\n AdamForEachOptimizer (params:Listified[Tensor], opt_step:Callable,\n                       decouple_wd:bool=True, **defaults)\n\nAn ForEachOptimizer with a modified step for adam_foreach_step\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparams\nListified[Tensor]\n\nModel parameters\n\n\nopt_step\nCallable\n\nForEachOptimizer optimizer step\n\n\ndecouple_wd\nbool\nTrue\nUse true weight decay or L2 regularization, if applicable\n\n\ndefaults\n\n\n\n\n\n\n\nsource\n\n\nRAdamForEachOptimizer\n\n RAdamForEachOptimizer (params:Listified[Tensor], opt_step:Callable,\n                        decouple_wd:bool=True, **defaults)\n\nAn ForEachOptimizer with a modified step for radam_foreach_step\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparams\nListified[Tensor]\n\nModel parameters\n\n\nopt_step\nCallable\n\nForEachOptimizer optimizer step\n\n\ndecouple_wd\nbool\nTrue\nUse true weight decay or L2 regularization, if applicable\n\n\ndefaults\n\n\n\n\n\n\n\nsource\n\n\nLambForEachOptimizer\n\n LambForEachOptimizer (params:Listified[Tensor], opt_step:Callable,\n                       decouple_wd:bool=True, **defaults)\n\nAn ForEachOptimizer with a modified step for lamb_foreach_step\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparams\nListified[Tensor]\n\nModel parameters\n\n\nopt_step\nCallable\n\nForEachOptimizer optimizer step\n\n\ndecouple_wd\nbool\nTrue\nUse true weight decay or L2 regularization, if applicable\n\n\ndefaults\n\n\n\n\n\n\n\nsource\n\n\nRangerForEachOptimizer\n\n RangerForEachOptimizer (params:Listified[Tensor], opt_step:Callable,\n                         decouple_wd:bool=True, **defaults)\n\nAn ForEachOptimizer with a modified LookAhead step for ranger_foreach_step\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparams\nListified[Tensor]\n\nModel parameters\n\n\nopt_step\nCallable\n\nForEachOptimizer optimizer step\n\n\ndecouple_wd\nbool\nTrue\nUse true weight decay or L2 regularization, if applicable\n\n\ndefaults"
  },
  {
    "objectID": "ffcv.transforms.html",
    "href": "ffcv.transforms.html",
    "title": "FFCV Transforms",
    "section": "",
    "text": "fastxtend provides multiple FFCV transforms, including existing FFCV transforms with harmonized arguments, fastai transforms implemented as FFCV transforms, and additional FFCV transforms.\nBy default, these transforms are imported under ft if using from fastxtend.ffcv.all import *."
  },
  {
    "objectID": "ffcv.transforms.html#ffcv-transforms-reference",
    "href": "ffcv.transforms.html#ffcv-transforms-reference",
    "title": "FFCV Transforms",
    "section": "FFCV Transforms Reference",
    "text": "FFCV Transforms Reference\nThese FFCV Transforms have had their initialization arguments harmonized and are otherwise identical to their FFCV counterparts. You can find the original versions and documentation at the FFCV API Reference.\n\n\nRandomResizedCrop\n\n RandomResizedCrop (scale:Tuple[float,float], ratio:Tuple[float,float],\n                    size:int)\n\nCrop a random portion of image with random aspect ratio and resize it to a given size. Chances are you do not want to use this augmentation and instead want to include RRC as part of the decoder, by using the :cla:~ffcv.fields.rgb_image.ResizedCropRGBImageDecoder class.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nscale\ntyping.Tuple[float, float]\nLower and upper bounds for the ratio of random area of the crop.\n\n\nratio\ntyping.Tuple[float, float]\nLower and upper bounds for random aspect ratio of the crop.\n\n\nsize\nint\nSide length of the output.\n\n\n\nWhile you can use RandomResizedCrop by itself, it’s probably better to use RandomResizedCropRGBImageDecoder which integrates the transform with image loading.\n\nsource\n\n\nRandomHorizontalFlip\n\n RandomHorizontalFlip (prob:float=0.5)\n\nFlip the image horizontally with probability prob. Operates on raw arrays (not tensors).\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nprob\nfloat\n0.5\nThe probability with which to flip each image in the batch horizontally.\n\n\n\n\n\n\nCutout\n\n Cutout (crop_size:int, fill:Tuple[int,int,int]=(0, 0, 0))\n\nCutout data augmentation (https://arxiv.org/abs/1708.04552).\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncrop_size\nint\n\nSize of the random square to cut out.\n\n\nfill\ntyping.Tuple[int, int, int]\n(0, 0, 0)\nAn RGB color ((0, 0, 0) by default) to fill the cutout square with.Useful for when a normalization layer follows cutout, in which caseyou can set the fill such that the square is zeropost-normalization.\n\n\n\n\nsource\n\n\nTranslate\n\n Translate (padding:int, fill:Tuple[int,int,int]=(0, 0, 0))\n\nTranslate each image randomly in vertical and horizontal directions up to specified number of pixels.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npadding\nint\n\nMax number of pixels to translate in any direction.\n\n\nfill\nTuple[int, int, int]\n(0, 0, 0)\nAn RGB color ((0, 0, 0) by default) to fill the area outside the shifted image.\n\n\n\nfastxend’s Translate is FFCV’s RandomTranslate renamed, as it applies to all images in batch and not a random subset.\nfastxtend’s RandomTranslate applies a random translatation to a random subset of images in a batch.\n\n\n\nPoison\n\n Poison (mask:numpy.ndarray, alpha:numpy.ndarray, indices, clamp=(0, 255))\n\nPoison specified images by adding a mask with given opacity. Operates on raw arrays (not tensors).\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmask\nndarray\n\nThe mask to apply to each image.\n\n\nalpha\nndarray\n\n\n\n\nindices\nSequence[int]\n\nThe indices of images that should have the mask applied.\n\n\nclamp\ntuple\n(0, 255)\nClamps the final pixel values between these two values (default: (0, 255)).\n\n\n\n\n\n\nReplaceLabel\n\n ReplaceLabel (indices, new_label:int)\n\nReplace label of specified images.\n\n\n\n\nType\nDetails\n\n\n\n\nindices\nSequence[int]\nThe indices of images to relabel.\n\n\nnew_label\nint\nThe new label to assign.\n\n\n\n\n\n\nSqueeze\n\n Squeeze (*dims)\n\nRemove given dimensions of input of size 1. Operates on tensors."
  },
  {
    "objectID": "ffcv.transforms.html#color-transforms",
    "href": "ffcv.transforms.html#color-transforms",
    "title": "FFCV Transforms",
    "section": "Color Transforms",
    "text": "Color Transforms\nThese color transforms support both fastai-style logit space color transforms and TorchVision blending based color transforms.\n\n\n\n\n\n\nWarning\n\n\n\nDefaults for these transforms are for the fastai-style implementations.\n\n\nExcept for RandomHue1, these transforms use the slightly slower fastai implementations. Users should test the fastai FFCV, TorchVision FFCV, and fastai batch transforms to determine which are most performant.\nThese transforms are also compatible with FFCV.\n\nsource\n\nRandomBrightness\n\n RandomBrightness (prob:float=0.75, max_lighting:float=0.2,\n                   fastai:bool=True)\n\nRandomly adjust image brightness. Supports both TorchVision and fastai style brightness transforms.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nprob\nfloat\n0.75\nProbability of changing brightness\n\n\nmax_lighting\nfloat\n0.2\nMaximum brightness change. Randomly choose factor on [0.5(1-magnitude), 0.5(1+magnitude)], or [max(0, 1-magnitude), 1+magnitude] if fastai=False.\n\n\nfastai\nbool\nTrue\nfastai-style transform or TorchVision. Defaults to fastai.\n\n\n\n\nsource\n\n\nRandomContrast\n\n RandomContrast (prob:float=0.75, max_lighting:float=0.2,\n                 fastai:bool=True)\n\nRandomly adjust image contrast. Supports both TorchVision and fastai style contrast transforms.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nprob\nfloat\n0.75\nProbability of changing contrast\n\n\nmax_lighting\nfloat\n0.2\nMaximum contrast change. Randomly choose factor on [1-max_lighting, 1/(1-max_lighting)] in log space, or [max(0, 1-magnitude), 1+magnitude] if fastai=False.\n\n\nfastai\nbool\nTrue\nfastai-style transform or TorchVision. Defaults to fastai.\n\n\n\n\nsource\n\n\nRandomSaturation\n\n RandomSaturation (prob:float=0.75, max_lighting:float=0.2,\n                   fastai:bool=True)\n\nRandomly adjust image saturation. Supports both TorchVision and fastai style saturation transforms.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nprob\nfloat\n0.75\nProbability of changing saturation\n\n\nmax_lighting\nfloat\n0.2\nMaximum saturation change. Randomly choose factor on [1-max_lighting, 1/(1-max_lighting)] in log space, or [max(0, 1-magnitude), 1+magnitude] if fastai=False.\n\n\nfastai\nbool\nTrue\nfastai-style transform or TorchVision. Defaults to fastai.\n\n\n\n\nsource\n\n\nRandomLighting\n\n RandomLighting (prob:float=0.75, max_lighting:float=0.2,\n                 max_brightness:float|None=None,\n                 max_contrast:float|None=None,\n                 max_saturation:float|None=None,\n                 prob_brightness:float|None=None,\n                 prob_contrast:float|None=None,\n                 prob_saturation:float|None=None, fastai:bool=False)\n\nRandomly adjust image brightness, contrast, and saturation. Combines all three into single transform for speed. Supports both TorchVision and fastai style lighting transforms.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nprob\nfloat\n0.75\nDefault probability of changing brightness, contrast, and saturation. Individual probability overrides this value.\n\n\nmax_lighting\nfloat\n0.2\nDefault maximum lighting change. Individual lighting overrides this value. See max_brightness, max_contrast, and max_saturation for details.\n\n\nmax_brightness\nfloat | None\nNone\nMaximum brightness change. Randomly choose factor on [0.5(1-magnitude), 0.5(1+magnitude)], or [max(0, 1-magnitude), 1+magnitude] if fastai=False.\n\n\nmax_contrast\nfloat | None\nNone\nMaximum contrast change. Randomly choose factor on [1-max_lighting, 1/(1-max_lighting)] in log space, or [max(0, 1-magnitude), 1+magnitude] if fastai=False.\n\n\nmax_saturation\nfloat | None\nNone\nMaximum saturation change. Randomly choose factor on [1-max_lighting, 1/(1-max_lighting)] in log space, or [max(0, 1-magnitude), 1+magnitude] if fastai=False.\n\n\nprob_brightness\nfloat | None\nNone\nIndividual probability of changing brightness. Set to override prob.\n\n\nprob_contrast\nfloat | None\nNone\nIndividual probability of changing contrast. Set to override prob.\n\n\nprob_saturation\nfloat | None\nNone\nIndividual probability of changing saturation. Set to override prob.\n\n\nfastai\nbool\nFalse\nfastai-style transform or TorchVision. Defaults to fastai.\n\n\n\n\nsource\n\n\nRandomHue\n\n RandomHue (prob:float=0.75, max_hue:float=0.1, fastai:bool=True)\n\nRandomly adjust image Hue. Supports both TorchVision and fastai style contrast transforms.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nprob\nfloat\n0.75\nProbability of changing hue\n\n\nmax_hue\nfloat\n0.1\nMaximum hue change. Randomly choose factor on [1-max_hue, 1/(1-max_hue)] in log space, or [-magnitude, magnitude] clipped to [-0.5, 0.5] if fastai=False.\n\n\nfastai\nbool\nTrue\nIf True applies the slower, fastai-style transform. Defaults to TorchVision\n\n\n\nUnlike the other color transformations, RandomHue uses the same implementation for both fastai and TorchVision versions, so performance should be the same. The only difference is the max_hue selection."
  },
  {
    "objectID": "ffcv.transforms.html#additional-ffcv-transforms",
    "href": "ffcv.transforms.html#additional-ffcv-transforms",
    "title": "FFCV Transforms",
    "section": "Additional FFCV Transforms",
    "text": "Additional FFCV Transforms\nContains both transforms which pending acceptance and/or release for the next version of FFCV2 and fastxtend FFCV transforms.\nBoth are useable in fastxtend and FFCV."
  },
  {
    "objectID": "ffcv.transforms.html#randomgrayscale",
    "href": "ffcv.transforms.html#randomgrayscale",
    "title": "FFCV Transforms",
    "section": "RandomGrayscale",
    "text": "RandomGrayscale\nRandomly convert images to grayscale, using one of Luma601, Luma709, Average, or random per image choice of all three.\n\nsource\n\nGrayscaleType\n\n GrayscaleType (value, names=None, module=None, qualname=None, type=None,\n                start=1)\n\nGrayscale Types for RandomGrayscale for typo-proof and autocompletion\n\nsource\n\n\nRandomGrayscale\n\n RandomGrayscale (prob:float=0.1,\n                  grayscale:GrayscaleType=&lt;GrayscaleType.Random: 3&gt;)\n\nRandom grayscale conversion augmentation.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nprob\nfloat\n0.1\nProbability of applying on each image.\n\n\ngrayscale\nGrayscaleType\nGrayscaleType.Random\n\n\n\n\n\nsource\n\n\nRandomChannelDrop\n\n RandomChannelDrop (prob:float=0.1)\n\nRandomly replace image channel with random value.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nprob\nfloat\n0.1\nProbability of applying on each image.\n\n\n\n\nsource\n\n\nRandomCutout\n\n RandomCutout (prob:float, crop_size:int, fill:Tuple[int,int,int]=(0, 0,\n               0))\n\nRandom cutout data augmentation (https://arxiv.org/abs/1708.04552).\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nprob\nfloat\n\nProbability of applying on each image.\n\n\ncrop_size\nint\n\nSize of the random square to cut out.\n\n\nfill\nTuple[int, int, int]\n(0, 0, 0)\nAn RGB color ((0, 0, 0) by default) to fill the cutout square with.Useful for when a normalization layer follows cutout, in which caseyou can set the fill such that the square is zero post-normalization.\n\n\n\n\nsource\n\n\nRandomTranslate\n\n RandomTranslate (prob:float, padding:int, fill:Tuple[int,int,int]=(0, 0,\n                  0))\n\nTranslate each image randomly in vertical and horizontal directions up to specified number of pixels.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nprob\nfloat\n\nProbability of applying on each image.\n\n\npadding\nint\n\nMax number of pixels to translate in any direction.\n\n\nfill\nTuple[int, int, int]\n(0, 0, 0)\nAn RGB color ((0, 0, 0) by default) to fill the area outside the shifted image.\n\n\n\nUnlike FFCV’s RandomTranslate, which applies a random translatation to all images in a batch, fastxtend’s RandomTranslate randomly applies a random translatation to a subset of images in a batch with probability prob.\n\nsource\n\n\nRandomErasing\n\n RandomErasing (prob:float=0.25, min_area:float=0.02, max_area:float=0.3,\n                min_aspect:float=0.3, max_count:int=1,\n                fill_mean:Tuple[int,int,int]=(124, 116, 103),\n                fill_std:Tuple[int,int,int]=(58, 57, 57),\n                fast_fill:bool=True)\n\nRandom erasing data augmentation (https://arxiv.org/abs/1708.04896).\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nprob\nfloat\n0.25\nProbability of applying on each image.\n\n\nmin_area\nfloat\n0.02\nMinimum erased area as percentage of image size.\n\n\nmax_area\nfloat\n0.3\nMaximum erased area as percentage of image size.\n\n\nmin_aspect\nfloat\n0.3\nMinimum aspect ratio of erased area.\n\n\nmax_count\nint\n1\nMaximum number of erased blocks per image. Erased Area is scaled by max_count.\n\n\nfill_mean\nTuple[int, int, int]\n(124, 116, 103)\nThe RGB color mean (ImageNet’s (124, 116, 103) by default) to randomly fill theerased area with. Should be the mean of dataset or pretrained dataset.\n\n\nfill_std\nTuple[int, int, int]\n(58, 57, 57)\nThe RGB color standard deviation (ImageNet’s (58, 57, 57) by default) to randomlyfill the erased area with. Should be the st. dev of dataset or pretrained dataset.\n\n\nfast_fill\nbool\nTrue\nDefault of True is ~2X faster by generating noise once per batch and randomlyselecting slices of the noise instead of generating unique noise per each image.\n\n\n\nThe implementation for RandomErasing was inspired by fastai.vision.augment.RandomErasing."
  },
  {
    "objectID": "ffcv.transforms.html#footnotes",
    "href": "ffcv.transforms.html#footnotes",
    "title": "FFCV Transforms",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUnlike the other color transformations, RandomHue uses the same implementation for both fastai and TorchVision versions, so performance should be the same. The only difference is the max_hue selection.↩︎\nIf accepted, these will be converted to a documentation reference.↩︎"
  },
  {
    "objectID": "callback.utils.html",
    "href": "callback.utils.html",
    "title": "Callback Utilities",
    "section": "",
    "text": "fastxtend adds the ability to log additional values from callbacks via LogDispatch callback and Learner._log_values & Learner._log_dict.\nTo log additional values to a supported logger, import LogDispatch and then pass values as arguments to self.learn._log_values or as a dictionary to self.learn._log_dict in after_batch for training values and after_epoch for validation values.\nfrom fastxtend.callback.utils import *\n\ndef ExampleCallback(Callback)\n    def after_batch(self):\n        self.learn._log_values(train_value1=value1, train_value2=value2)\n\n    def after_epoch(self):\n        self.learn._log_dict({'valid/value1':value1, 'valid/value2':value2})\nLogDispatch will update a train and valid dictionary using the argument names as keys, and will call each supported logger’s update method after Recorder runs.\nIf imported LogDispatch will automatically be added to the default fastai callbacks.\nSupported loggers:\n\nfastai.callback.wandb.WandbCallback\nfastai.callback.tensorboard.TensorBoardCallback\n\nfastxtend uses LogDispatch across multiple callbacks, such as EMAWarmupCallback and ProgressiveResize, to log additional information.\nPRs to add additional supported loggers are welcome.\n\n\nLogDispatch.log_wandb_table allows logging a Weights and Biases Table in a callback.\ndef ExampleCallback(Callback)\n    def after_fit(self):\n        self.learn.log_dispatch.log_wandb_table(name='pandas_df', dataframe=pandas_df)\nLogDispatch.log_wandb_summary allows logging Weights and Biases Summary Metrics in a callback.\ndef ExampleCallback(Callback)\n    def after_fit(self):\n        self.learn.log_dispatch.log_wandb_summary(name='summary_metric', summary=summary_metric)\n\nsource\n\n\n\n\n LogDispatch (after_create=None, before_fit=None, before_epoch=None,\n              before_train=None, before_batch=None, after_pred=None,\n              after_loss=None, before_backward=None,\n              after_cancel_backward=None, after_backward=None,\n              before_step=None, after_cancel_step=None, after_step=None,\n              after_cancel_batch=None, after_batch=None,\n              after_cancel_train=None, after_train=None,\n              before_validate=None, after_cancel_validate=None,\n              after_validate=None, after_cancel_epoch=None,\n              after_epoch=None, after_cancel_fit=None, after_fit=None)\n\nA default callback for dispatching additional values to loggers\n\nsource\n\n\n\n\n LogDispatch.log_wandb_table (name:str, **kwargs)\n\nLog wandb.Table to Weights and Biases. See wandb.Table for details\n\nsource\n\n\n\n\n LogDispatch.log_wandb_summary (name:str, summary:Callable)\n\nLog Summary Metrics to Weights and Biases. See wandb.summary for details\n\nsource\n\n\n\n\n Learner._log_values (**kwargs)\n\nUpdate additional logging values from arguments using LogDispatch.\n\nsource\n\n\n\n\n Learner._log_dict (value_dict:dict)\n\nUpdate additional logging values from a dictionary using LogDispatch."
  },
  {
    "objectID": "callback.utils.html#log-values-from-callbacks",
    "href": "callback.utils.html#log-values-from-callbacks",
    "title": "Callback Utilities",
    "section": "",
    "text": "fastxtend adds the ability to log additional values from callbacks via LogDispatch callback and Learner._log_values & Learner._log_dict.\nTo log additional values to a supported logger, import LogDispatch and then pass values as arguments to self.learn._log_values or as a dictionary to self.learn._log_dict in after_batch for training values and after_epoch for validation values.\nfrom fastxtend.callback.utils import *\n\ndef ExampleCallback(Callback)\n    def after_batch(self):\n        self.learn._log_values(train_value1=value1, train_value2=value2)\n\n    def after_epoch(self):\n        self.learn._log_dict({'valid/value1':value1, 'valid/value2':value2})\nLogDispatch will update a train and valid dictionary using the argument names as keys, and will call each supported logger’s update method after Recorder runs.\nIf imported LogDispatch will automatically be added to the default fastai callbacks.\nSupported loggers:\n\nfastai.callback.wandb.WandbCallback\nfastai.callback.tensorboard.TensorBoardCallback\n\nfastxtend uses LogDispatch across multiple callbacks, such as EMAWarmupCallback and ProgressiveResize, to log additional information.\nPRs to add additional supported loggers are welcome.\n\n\nLogDispatch.log_wandb_table allows logging a Weights and Biases Table in a callback.\ndef ExampleCallback(Callback)\n    def after_fit(self):\n        self.learn.log_dispatch.log_wandb_table(name='pandas_df', dataframe=pandas_df)\nLogDispatch.log_wandb_summary allows logging Weights and Biases Summary Metrics in a callback.\ndef ExampleCallback(Callback)\n    def after_fit(self):\n        self.learn.log_dispatch.log_wandb_summary(name='summary_metric', summary=summary_metric)\n\nsource\n\n\n\n\n LogDispatch (after_create=None, before_fit=None, before_epoch=None,\n              before_train=None, before_batch=None, after_pred=None,\n              after_loss=None, before_backward=None,\n              after_cancel_backward=None, after_backward=None,\n              before_step=None, after_cancel_step=None, after_step=None,\n              after_cancel_batch=None, after_batch=None,\n              after_cancel_train=None, after_train=None,\n              before_validate=None, after_cancel_validate=None,\n              after_validate=None, after_cancel_epoch=None,\n              after_epoch=None, after_cancel_fit=None, after_fit=None)\n\nA default callback for dispatching additional values to loggers\n\nsource\n\n\n\n\n LogDispatch.log_wandb_table (name:str, **kwargs)\n\nLog wandb.Table to Weights and Biases. See wandb.Table for details\n\nsource\n\n\n\n\n LogDispatch.log_wandb_summary (name:str, summary:Callable)\n\nLog Summary Metrics to Weights and Biases. See wandb.summary for details\n\nsource\n\n\n\n\n Learner._log_values (**kwargs)\n\nUpdate additional logging values from arguments using LogDispatch.\n\nsource\n\n\n\n\n Learner._log_dict (value_dict:dict)\n\nUpdate additional logging values from a dictionary using LogDispatch."
  },
  {
    "objectID": "callback.utils.html#callback-scheduler",
    "href": "callback.utils.html#callback-scheduler",
    "title": "Callback Utilities",
    "section": "Callback Scheduler",
    "text": "Callback Scheduler\nCallbackScheduler is a utlitity method for callback developers to modifying callback values on a schedule.\nfastxtend uses it in EMAWarmupCallback to schedule the EMA decay rate.\n\nsource\n\nCallbackScheduler\n\n CallbackScheduler ()\n\nA mixin for scheduling values in a Callback\n\nsource\n\n\nCallbackScheduler.setup_schedule\n\n CallbackScheduler.setup_schedule (n_epoch:int, dls_len:int,\n                                   start_value:Union[int,float],\n                                   final_value:Union[int,float],\n                                   start:Union[int,float],\n                                   finish:Union[int,float], schedule:Calla\n                                   ble[...,fastai.callback.schedule._Annea\n                                   ler]=&lt;function SchedCos&gt;,\n                                   ndigits:Union[int,float,NoneType]=None,\n                                   callback_name:str='CallbackScheduler',\n                                   resume:bool=False)\n\nSetup the schedule for schedule_step. Call during Callback.begin_fit\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_epoch\nint\n\nNumber of training epochs. From a callback pass n_epoch\n\n\ndls_len\nint\n\nLength of the training dataset\n\n\nstart_value\nNumeric\n\nInitial scheduling value\n\n\nfinal_value\nNumeric\n\nFinal scheduling value\n\n\nstart\nNumeric\n\nStart schedule in percent of training steps (float) or epochs (int, index 0)\n\n\nfinish\nNumeric\n\nFinish schedule in percent of training steps (float) or epochs (int, index 0)\n\n\nschedule\nCallable[…, _Annealer]\nSchedCos\nSchedule type. Any fastai schedule annealer\n\n\nndigits\nNumeric | None\nNone\nRound return value to ndigits if set using Python’s round.\n\n\ncallback_name\nstr\nCallbackScheduler\nName of scheduler for warning & error messages\n\n\nresume\nbool\nFalse\nWhether training has resumed or not\n\n\n\n\nsource\n\n\nCallbackScheduler.schedule_step\n\n CallbackScheduler.schedule_step (value:Union[int,float], pct_train:float)\n\nTakes a scheduling step and returns updated value\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nvalue\nNumeric\nValue to schedule. Value is returned as the passed in type\n\n\npct_train\nfloat\nTraining progress in percent. From a callback pass self.pct_train"
  },
  {
    "objectID": "optimizer.lion.html",
    "href": "optimizer.lion.html",
    "title": "Lion: EvoLved Sign Momentum Optimizer",
    "section": "",
    "text": "Lion was introduced by Chen et al in Symbolic Discovery of Optimization Algorithms. Lion only keeps track of the gradient moving average (momentum) which reduces memory usage compared to AdamW. Lion uses two momentum EMA factors, one for tracking momentum and another for using momentum in the update step. Using default hyperparameters, this allows up to ten times longer history for momentum tracking while leveraging more of the current gradient for the model update. Unlike most optimizers, Lion uses the same magnitude for each parameter update calculated using the sign operation.\nIn addition to a fastai native implementation, Lion has a fused ForEach and bitsandbytes 8-bit implementations. See the Fused Optimizer and 8-bit Optimizer documentation for more details.\nsource"
  },
  {
    "objectID": "optimizer.lion.html#hyperparameters",
    "href": "optimizer.lion.html#hyperparameters",
    "title": "Lion: EvoLved Sign Momentum Optimizer",
    "section": "Hyperparameters",
    "text": "Hyperparameters\nHyperparameter notes from Chen et al:\n\nExcept for language modeling, beta1 and beta2 are held at 0.9 and 0.99, respectively. When traing T5 they set beta1=0.95 and beta2=0.98.\nDue to the larger update norm from the sign operation, the Lion learning rate is typically 10X smaller than AdamW, with 3X smaller sometimes performing better.\nSince the effective weight decay is multiplied by the learning rate, weight decay should be increased by the learning rate decrease (10X or 3X).\nThe optimal batch size for Lion is 4096 (vs AdamW’s 256), but Lion still performs well at a batch size of 64 and matches or exceeds AdamW on all tested batch sizes."
  },
  {
    "objectID": "optimizer.lion.html#training-speed",
    "href": "optimizer.lion.html#training-speed",
    "title": "Lion: EvoLved Sign Momentum Optimizer",
    "section": "Training Speed",
    "text": "Training Speed\n\n\n\n\n\n\nImportant\n\n\n\nThe ForEach optimizer has only been tested on PyTorch 1.12+ and are not guaranteed to work on older versions.\n\n\nAs shown in Table 1, fastxtend’s fused ForEach Lion is 13 to 195 percent faster1 then a standard PyTorch implementation. This training speed advantage could increase in a future PyTorch release, as PyTorch doesn’t have a ForEach implementation of sign2, so the implementation falls back to a for loop in the middle of the Lion update step.\n\n\nTable 1: Increase in Lion ForEach opt_step Speed vs Native Optimizer\n\n\n\n\n\n\n\n\n\nModel\nLayers\nNative Step\nForEach Step\nForEach Speedup\n\n\n\n\nXResNet18\n1\n23ms\n13ms\n73%\n\n\nXResNet50\n1\n50ms\n34ms\n47%\n\n\nXSE-ResNeXt50\n1\n66ms\n47ms\n41%\n\n\nXResNet101\n1\n76ms\n48ms\n59%\n\n\nConvNeXt Tiny\n2\n118ms\n104ms\n13%\n\n\nConvNeXt Small\n2\n189ms\n164ms\n16%\n\n\nViT Patch16 Small\n2\n57ms\n45ms\n26%\n\n\nDeBERTa Base\n1\n22ms\n7.5ms\n195%\n\n\n\n\nDue a simpler update and only tracking momentum, the native implementation of Lion is both faster than the native implementation of AdamW and uses less memory. However, since the ForEach implementation requires use of a for loop, Lion ForEach is equal or slower than AdamW ForEach. However, Lion ForEach should still use less memory than AdamW ForEach.\n\n\nTable 2: AdamW vs Lion Training Speed\n\n\n\n\n(a) Native Implementation\n\n\nModel\nAdamW Step\nLion Step\nSpeedup\n\n\n\n\nXResNet18\n26ms\n23ms\n15%\n\n\nXResNet50\n54ms\n50ms\n8.2%\n\n\nXSE-ResNeXt50\n72ms\n66ms\n8.3%\n\n\nXResNet101\n91ms\n76ms\n19%\n\n\nConvNeXt Tiny\n125ms\n118ms\n6.1%\n\n\nConvNeXt Small\n202ms\n189ms\n6.8%\n\n\nViT Patch16 Small\n63ms\n57ms\n9.4%\n\n\nDeBERTa Base\n26ms\n22ms\n25%\n\n\n\n\n\n\n(b) Fused ForEach Implementation\n\n\nModel\nAdamW Step\nLion Step\nSlowdown\n\n\n\n\nXResNet18\n13ms\n13ms\n0.9%\n\n\nXResNet50\n33ms\n34ms\n4.6%\n\n\nXSE-ResNeXt50\n42ms\n47ms\n12%\n\n\nXResNet101\n46ms\n48ms\n4.2%\n\n\nConvNeXt Tiny\n102ms\n104ms\n2.5%\n\n\nConvNeXt Small\n161ms\n164ms\n1.6%\n\n\nViT Patch16 Small\n42ms\n45ms\n4.9%\n\n\nDeBERTa Base\n7.4ms\n7.5ms\n1.5%"
  },
  {
    "objectID": "optimizer.lion.html#footnotes",
    "href": "optimizer.lion.html#footnotes",
    "title": "Lion: EvoLved Sign Momentum Optimizer",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBenchmarked on a GeForce 3080 Ti using PyTorch 1.13.1, Cuda 11.7, Mixed Precision, Channels Last (except DeBERTa and ViT), and fastxtend’s Simple Profiler Callback. Results may differ on other models, hardware, and across benchmarking runs. Speedup and slowdown are calculated from the total time spent on the optimization step.↩︎\nNumerically equivalent approximations of sign using ForEach operators ended up using more memory and were a wash on training speed.↩︎"
  },
  {
    "objectID": "callback.profiler.html",
    "href": "callback.profiler.html",
    "title": "Profiler Callbacks",
    "section": "",
    "text": "Since fastxtend profilers change the fastai data loading loop, they are not imported by any of the fastxtend all imports and need to be imported seperately:\nJump to usage examples."
  },
  {
    "objectID": "callback.profiler.html#events",
    "href": "callback.profiler.html#events",
    "title": "Profiler Callbacks",
    "section": "Events",
    "text": "Events\nfastai callbacks do not have an event which is called directly before drawing a batch. fastxtend profilers add a new callback event called before_draw.\nWith a fastxtend profiler imported, a callback can implement actions on the following events:\n\nafter_create: called after the Learner is created\nbefore_fit: called before starting training or inference, ideal for initial setup.\nbefore_epoch: called at the beginning of each epoch, useful for any behavior you need to reset at each epoch.\nbefore_train: called at the beginning of the training part of an epoch.\nbefore_draw: called at the beginning of each batch, just before drawing said batch.\nbefore_batch: called at the beginning of each batch, just after drawing said batch. It can be used to do any setup necessary for the batch (like hyper-parameter scheduling) or to change the input/target before it goes in the model (change of the input with techniques like mixup for instance).\nafter_pred: called after computing the output of the model on the batch. It can be used to change that output before it’s fed to the loss.\nafter_loss: called after the loss has been computed, but before the backward pass. It can be used to add any penalty to the loss (AR or TAR in RNN training for instance).\nbefore_backward: called after the loss has been computed, but only in training mode (i.e. when the backward pass will be used)\nbefore_step: called after the backward pass, but before the update of the parameters. It can be used to do any change to the gradients before said update (gradient clipping for instance).\nafter_step: called after the step and before the gradients are zeroed.\nafter_batch: called at the end of a batch, for any clean-up before the next one.\nafter_train: called at the end of the training phase of an epoch.\nbefore_validate: called at the beginning of the validation phase of an epoch, useful for any setup needed specifically for validation.\nafter_validate: called at the end of the validation part of an epoch.\nafter_epoch: called at the end of an epoch, for any clean-up before the next one.\nafter_fit: called at the end of training, for final clean-up."
  },
  {
    "objectID": "callback.profiler.html#throughput",
    "href": "callback.profiler.html#throughput",
    "title": "Profiler Callbacks",
    "section": "Throughput",
    "text": "Throughput\nThe Throughput profiler only measures the step, draw, and batch. To use, both ThroughputCallback and ThroughputPostCallback must be added to the Learner. The recommended way to use is via Learner.profile.\n\nsource\n\nThroughputCallback\n\n ThroughputCallback (show_report:bool=True, plain:bool=False,\n                     markdown:bool=False, save_csv:bool=False,\n                     csv_name:str='throughput.csv',\n                     rolling_average:int=10, drop_first_batch:bool=True)\n\nAdds a throughput profiler to the fastai Learner. Optionally showing formatted report or saving unformatted results as csv.\nPair with ThroughputPostCallback to profile training performance.\nPost fit, access report & results via Learner.profile_report & Learner.profile_results.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nshow_report\nbool\nTrue\nDisplay formatted report post profile\n\n\nplain\nbool\nFalse\nFor Jupyter Notebooks, display plain report\n\n\nmarkdown\nbool\nFalse\nDisplay markdown formatted report\n\n\nsave_csv\nbool\nFalse\nSave raw results to csv\n\n\ncsv_name\nstr\nthroughput.csv\nCSV save location\n\n\nrolling_average\nint\n10\nNumber of batches to average throughput over\n\n\ndrop_first_batch\nbool\nTrue\nDrop the first batch from profiling\n\n\n\n\nsource\n\n\nThroughputPostCallback\n\n ThroughputPostCallback ()\n\nRequired pair with ThroughputCallback to profile training performance. Removes itself after training is over."
  },
  {
    "objectID": "callback.profiler.html#simple-profiler",
    "href": "callback.profiler.html#simple-profiler",
    "title": "Profiler Callbacks",
    "section": "Simple Profiler",
    "text": "Simple Profiler\nTo use, both SimpleProfilerCallback and SimpleProfilerPostCallback must be added to the Learner. The recommended way to use is via Learner.profile.\n\nsource\n\nSimpleProfilerCallback\n\n SimpleProfilerCallback (show_report:bool=True, plain:bool=False,\n                         markdown:bool=False, save_csv:bool=False,\n                         csv_name:str='simpleprofiler.csv',\n                         rolling_average:int=10,\n                         drop_first_batch:bool=True)\n\nAdds a simple profiler to the fastai Learner. Optionally showing formatted report or saving unformatted results as csv.\nPair with SimpleProfilerPostCallback to profile training performance.\nPost fit, access report & results via Learner.profile_report & Learner.profile_results.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nshow_report\nbool\nTrue\nDisplay formatted report post profile\n\n\nplain\nbool\nFalse\nFor Jupyter Notebooks, display plain report\n\n\nmarkdown\nbool\nFalse\nDisplay markdown formatted report\n\n\nsave_csv\nbool\nFalse\nSave raw results to csv\n\n\ncsv_name\nstr\nsimpleprofiler.csv\nCSV save location\n\n\nrolling_average\nint\n10\nNumber of batches to average throughput over\n\n\ndrop_first_batch\nbool\nTrue\nDrop the first batch from profiling\n\n\n\n\nsource\n\n\nSimpleProfilerPostCallback\n\n SimpleProfilerPostCallback ()\n\nRequired pair with SimpleProfilerCallback to profile training performance. Removes itself after training is over."
  },
  {
    "objectID": "callback.profiler.html#convenience-method",
    "href": "callback.profiler.html#convenience-method",
    "title": "Profiler Callbacks",
    "section": "Convenience Method",
    "text": "Convenience Method\nLearner.profile is the easy and recommended way to use a fastxtend profiler.\n\nsource\n\nProfileMode\n\n ProfileMode (value, names=None, module=None, qualname=None, type=None,\n              start=1)\n\nProfile enum for Learner.profile\n\nsource\n\n\nLearner.profile\n\n Learner.profile (mode:__main__.ProfileMode=&lt;ProfileMode.Throughput:\n                  'throughput'&gt;, show_report:bool=True, plain:bool=False,\n                  markdown:bool=False, save_csv:bool=False,\n                  csv_name:str='profiler.csv', rolling_average:int=10,\n                  drop_first_batch:bool=True)\n\nRun a fastxtend profiler which removes itself when finished training.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmode\nProfileMode\nProfileMode.Throughput\nWhich profiler to use. Throughput or Simple.\n\n\nshow_report\nbool\nTrue\nDisplay formatted report post profile\n\n\nplain\nbool\nFalse\nFor Jupyter Notebooks, display plain report\n\n\nmarkdown\nbool\nFalse\nDisplay markdown formatted report\n\n\nsave_csv\nbool\nFalse\nSave raw results to csv\n\n\ncsv_name\nstr\nprofiler.csv\nCSV save location\n\n\nrolling_average\nint\n10\nNumber of batches to average throughput over\n\n\ndrop_first_batch\nbool\nTrue\nDrop the first batch from profiling"
  },
  {
    "objectID": "callback.profiler.html#output",
    "href": "callback.profiler.html#output",
    "title": "Profiler Callbacks",
    "section": "Output",
    "text": "Output\nThe Simple Profiler report contains the following items divided in three Phases (Fit, Train, & Valid)\nFit:\n\nfit: total time fitting the model takes.\nepoch: duration of both training and validation epochs. Often epoch total time is the same amount of elapsed time as fit.\ntrain: duration of each training epoch.\nvalid: duration of each validation epoch.\n\nTrain:\n\nstep: total duration of all batch steps including drawing the batch. Measured from before_draw to after_batch.\ndraw: time spent waiting for a batch to be drawn. Measured from before_draw to before_batch. Ideally this value should be as close to zero as possible.\nbatch: total duration of all batch steps except drawing the batch. Measured from before_batch to after_batch.\nforward: duration of the forward pass and any additional batch modifications. Measured from before_batch to after_pred.\nloss: duration of calculating loss. Measured from after_pred to after_loss.\nbackward: duration of the backward pass. Measured from before_backward to before_step.\nopt_step: duration of the optimizer step. Measured from before_step to after_step.\nzero_grad: duration of the zero_grad step. Measured from after_step to after_batch.\n\nValid:\n\nstep: total duration of all batch steps including drawing the batch. Measured from before_draw to after_batch.\ndraw: time spent waiting for a batch to be drawn. Measured from before_draw to before_batch. Ideally this value should be as close to zero as possible.\nbatch: total duration of all batch steps except drawing the batch. Measured from before_batch to after_batch.\npredict: duration of the prediction pass and any additional batch modifications. Measured from before_batch to after_pred.\nloss: duration of calculating loss. Measured from after_pred to after_loss.\n\nThe Throughput profiler only contains step, draw, and batch."
  },
  {
    "objectID": "callback.profiler.html#examples",
    "href": "callback.profiler.html#examples",
    "title": "Profiler Callbacks",
    "section": "Examples",
    "text": "Examples\nThese examples are trained on Imagenette with an image size of 224 and batch size of 64 on a 3080 Ti.\n\nlearn = Learner(dls, xresnext50(n_out=dls.c), opt_func=adam(foreach=True),\n                metrics=Accuracy()).to_channelslast().profile()\nlearn.fit_one_cycle(2, 3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.501953\n1.734705\n0.472357\n00:18\n\n\n1\n1.040516\n0.913281\n0.712866\n00:16\n\n\n\n\n\n\n\n\nProfiling Results\n\n\nPhase\nAction\nMean Duration\nDuration Std Dev\nNumber of Calls\nSamples/Second\nTotal Time\nPercent of Total\n\n\n\n\nfit\n\n-\n-\n1\n-\n35.63 s\n100%\n\n\n\nepoch\n17.81 s\n838.2ms\n2\n-\n35.63 s\n100%\n\n\n\ntrain\n14.24 s\n797.1ms\n2\n678\n28.49 s\n80%\n\n\n\nvalid\n3.565 s\n39.48ms\n2\n1,311\n7.130 s\n20%\n\n\ntrain\nstep\n86.62ms\n41.67ms\n293\n739\n25.38 s\n71%\n\n\n\ndraw\n4.269ms\n37.39ms\n293\n-38\n1.251 s\n4%\n\n\n\nbatch\n82.35ms\n4.472ms\n293\n777\n24.13 s\n68%\n\n\nvalid\nstep\n43.05ms\n63.38ms\n123\n1,470\n5.295 s\n15%\n\n\n\ndraw\n14.46ms\n60.89ms\n123\n-744\n1.779 s\n5%\n\n\n\nbatch\n28.59ms\n11.42ms\n123\n2,214\n3.516 s\n10%\n\n\n\n\n\nBatch dropped. train and valid phases show 1 less batch than fit.\n\n\n\nlearn = Learner(dls, xresnext50(n_out=dls.c), opt_func=adam(foreach=True),\n                metrics=Accuracy()).to_channelslast().profile(ProfileMode.Simple)\nlearn.fit_one_cycle(2, 3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.497550\n2.453694\n0.428535\n00:17\n\n\n1\n0.997146\n0.888791\n0.723057\n00:17\n\n\n\n\n\n\n\n\nProfiling Results\n\n\nPhase\nAction\nMean Duration\nDuration Std Dev\nNumber of Calls\nSamples/Second\nTotal Time\nPercent of Total\n\n\n\n\nfit\n\n-\n-\n1\n-\n34.55 s\n100%\n\n\n\nepoch\n17.27 s\n44.73ms\n2\n-\n34.54 s\n100%\n\n\n\ntrain\n13.64 s\n4.756ms\n2\n709\n27.28 s\n79%\n\n\n\nvalid\n3.629 s\n48.68ms\n2\n1,291\n7.259 s\n21%\n\n\ntrain\nstep\n87.64ms\n44.58ms\n293\n730\n25.68 s\n74%\n\n\n\ndraw\n4.428ms\n39.70ms\n293\n-39\n1.297 s\n4%\n\n\n\nbatch\n83.22ms\n6.353ms\n293\n769\n24.38 s\n71%\n\n\n\nforward\n16.65ms\n5.732ms\n293\n3,843\n4.880 s\n14%\n\n\n\nloss\n771.3µs\n196.1µs\n293\n82,977\n226.0ms\n1%\n\n\n\nbackward\n19.10ms\n5.501ms\n293\n3,351\n5.597 s\n16%\n\n\n\nopt_step\n45.46ms\n5.934ms\n293\n1,408\n13.32 s\n39%\n\n\n\nzero_grad\n1.106ms\n298.9µs\n293\n-\n324.1ms\n1%\n\n\nvalid\nstep\n43.94ms\n67.12ms\n123\n1,441\n5.404 s\n16%\n\n\n\ndraw\n15.77ms\n63.35ms\n123\n-807\n1.940 s\n6%\n\n\n\nbatch\n28.16ms\n11.90ms\n123\n2,248\n3.464 s\n10%\n\n\n\npredict\n26.60ms\n11.17ms\n123\n2,379\n3.272 s\n9%\n\n\n\nloss\n1.353ms\n1.795ms\n123\n46,800\n166.4ms\n0%\n\n\n\n\n\nBatch dropped. train and valid phases show 1 less batch than fit."
  },
  {
    "objectID": "callback.profiler.html#new-training-loop",
    "href": "callback.profiler.html#new-training-loop",
    "title": "Profiler Callbacks",
    "section": "New Training Loop",
    "text": "New Training Loop\nThe show_training_loop output below shows where the new before_draw event fits into the training loop.\n\nlearn = synth_learner()\nlearn.show_training_loop()\n\nStart Fit\n   - before_fit     : [TrainEvalCallback, Recorder, ProgressCallback]\n  Start Epoch Loop\n     - before_epoch   : [Recorder, ProgressCallback]\n    Start Train\n       - before_train   : [TrainEvalCallback, Recorder, ProgressCallback]\n      Start Batch Loop\n         - before_draw    : []\n         - before_batch   : [CastToTensor]\n         - after_pred     : []\n         - after_loss     : []\n         - before_backward: []\n         - before_step    : []\n         - after_step     : []\n         - after_cancel_batch: []\n         - after_batch    : [TrainEvalCallback, Recorder, ProgressCallback]\n      End Batch Loop\n    End Train\n     - after_cancel_train: [Recorder]\n     - after_train    : [Recorder, ProgressCallback]\n    Start Valid\n       - before_validate: [TrainEvalCallback, Recorder, ProgressCallback]\n      Start Batch Loop\n         - **CBs same as train batch**: []\n      End Batch Loop\n    End Valid\n     - after_cancel_validate: [Recorder]\n     - after_validate : [Recorder, ProgressCallback]\n  End Epoch Loop\n   - after_cancel_epoch: []\n   - after_epoch    : [Recorder]\nEnd Fit\n - after_cancel_fit: []\n - after_fit      : [ProgressCallback]"
  },
  {
    "objectID": "callback.profiler.html#logging",
    "href": "callback.profiler.html#logging",
    "title": "Profiler Callbacks",
    "section": "Logging",
    "text": "Logging\nProfiler callbacks support logging to Weights & Biases and TensorBoard via the LogDispatch callback. If either the fastai.callback.wandb.WandbCallback or fastai.callback.tensorboard.TensorBoardCallback are added to Learner, will automatically logs samples/second for draw, batch, forward, loss, backward, and opt_step.\nIf Weights & Biases is installed Simple Profiler also logs two tables to active wandb run:\n\nprofile_report: formatted report from Simple Profiler\nprofile_results: raw results from Simple Profiler"
  },
  {
    "objectID": "callback.ema.html",
    "href": "callback.ema.html",
    "title": "Exponential Moving Average",
    "section": "",
    "text": "EMACallback and EMASchedule have a fast fused implementation using PyTorch ForEach methods from PyTorch’s _multi_tensor optimizers. The fused EMA step is 1 to 8.4 times faster1 than a standard Python EMA step via a for loop.\nsource"
  },
  {
    "objectID": "callback.ema.html#footnotes",
    "href": "callback.ema.html#footnotes",
    "title": "Exponential Moving Average",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEMACallback performance was benchmarked on a GeForce 3080 Ti using PyTorch 1.13.1, Cuda 11.7, Mixed Precision, and Channels Last (except DeBERTa and ViT). Results may differ on other models, hardware, and across benchmarking runs. Speedup is calculated from the total time spent on the EMA step.↩︎"
  },
  {
    "objectID": "callback.cutmixup.html",
    "href": "callback.cutmixup.html",
    "title": "CutMixUp & Friends",
    "section": "",
    "text": "fastxtend replaces fastai.callback.mixup.MixUp and fastai.callback.mixup.CutMix with backwards compatible versions that support optional training with MultiLoss via MixHandlerX.\nCutMixUp and CutMixUpAugment allow applying MixUp, CutMix, and Augmentations using one callback. Optionally element-wise on the same batch.\nsource"
  },
  {
    "objectID": "callback.cutmixup.html#cutmixup-examples",
    "href": "callback.cutmixup.html#cutmixup-examples",
    "title": "CutMixUp & Friends",
    "section": "CutMixUp Examples",
    "text": "CutMixUp Examples\nCutMix with uniform cuts\n\nwith less_random():\n    cutmix = CutMixUp(cutmix_ratio=1, mixup_ratio=0, element=False)\n    test_cutmixup(cutmix)\n\n\n    \n        \n      \n      \n    \n    \n\n\n\n\n\nCutMix with random cuts\n\nwith less_random():\n    cutmix = CutMixUp(cutmix_ratio=1, mixup_ratio=0, cutmix_uniform=False, element=False)\n    test_cutmixup(cutmix)\n\n\n    \n        \n      \n      \n    \n    \n\n\n\n\n\nMixUp\n\nwith less_random():\n    mixup = CutMixUp(mix_alpha=1., cutmix_ratio=0, mixup_ratio=1, element=False)\n    test_cutmixup(mixup)\n\n\n    \n        \n      \n      \n    \n    \n\n\n\n\n\nCutMix and MixUp on the same batch\n\nwith less_random():\n    mixup = CutMixUp(mix_alpha=1., cutmix_ratio=1, mixup_ratio=1)\n    test_cutmixup(mixup)"
  },
  {
    "objectID": "callback.cutmixup.html#cutmixaugment-examples",
    "href": "callback.cutmixup.html#cutmixaugment-examples",
    "title": "CutMixUp & Friends",
    "section": "CutMixAugment Examples",
    "text": "CutMixAugment Examples\nCutMix with weak augmentations\n\nwith less_random():\n    cutmix = CutMixUpAugment(cutmix_ratio=1, mixup_ratio=0, augment_ratio=0, cutmix_uniform=False,\n                             cutmixup_augs=aug_transforms(size=112, max_warp=0.1, max_lighting=0.1),\n                             element=False)\n    test_cutmixup(cutmix, True, size=112)\n\n\n    \n        \n      \n      \n    \n    \n\n\n\n\n\nMixUp with weak augmentations\n\nwith less_random():\n    mixup = CutMixUpAugment(mix_alpha=1., cutmix_ratio=0, mixup_ratio=1, augment_ratio=0,\n                            cutmixup_augs=aug_transforms(max_rotate=20),\n                            element=False)\n    test_cutmixup(mixup, True)\n\n\n    \n        \n      \n      \n    \n    \n\n\n\n\n\nJust the strong Augmentations\n\nwith less_random():\n    augment = CutMixUpAugment(cutmix_ratio=0, mixup_ratio=0, augment_ratio=1, element=False)\n    test_cutmixup(augment, True)\n\n\n    \n        \n      \n      \n    \n    \n\n\n\n\n\nMixup, CutMix, and strong Augmentations on the same batch\n\nwith less_random():\n    batch = CutMixUpAugment(mix_alpha=1., cutmix_ratio=1, mixup_ratio=1, augment_ratio=1, cutmix_uniform=False)\n    test_cutmixup(batch, True)\n\n\n    \n        \n      \n      \n    \n    \n\n\n\n\n\nMixup and CutMix with weak augmentations and strong Augmentations on the same batch\n\nwith less_random():\n    batch = CutMixUpAugment(mix_alpha=1., cutmix_ratio=1, mixup_ratio=1, augment_ratio=1,\n                            cutmix_uniform=False, cutmixup_augs=aug_transforms(max_rotate=20))\n    test_cutmixup(batch, True)"
  },
  {
    "objectID": "audio.05_mixup.html",
    "href": "audio.05_mixup.html",
    "title": "Audio MixUp & Friends",
    "section": "",
    "text": "source\n\nAudioMixHandler\n\n AudioMixHandler (alpha:float=0.5, interp_label:bool|None=None)\n\nMixup base for TensorAudio\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalpha\nfloat\n0.5\nAlpha & beta parametrization for Beta distribution\n\n\ninterp_label\nbool | None\nNone\nBlend or stack labels. Defaults to loss_func.y_int if None\n\n\n\n\nsource\n\n\nAudioMixUp\n\n AudioMixUp (alpha=0.5, stack_y=True)\n\nImplementation of https://arxiv.org/abs/1710.09412 for TensorAudio\n\nsource\n\n\nAudioCutMix\n\n AudioCutMix (alpha:float=1.0, uniform:bool=True, p:float=1.0,\n              interp_label:bool|None=None)\n\nImplementation of https://arxiv.org/abs/1710.09412 for TensorAudio\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalpha\nfloat\n1.0\nAlpha & beta parametrization for Beta distribution\n\n\nuniform\nbool\nTrue\nUniform patches across batch. True matches fastai CutMix\n\n\np\nfloat\n1.0\nPer Image probablily of applying CutMix if uniform is False\n\n\ninterp_label\nbool | None\nNone\nBlend or stack labels. Defaults to loss’ y_int if None\n\n\n\n\nsource\n\n\nAudioCutMixUp\n\n AudioCutMixUp (mix_alpha=0.4, cut_alpha=1.0, stack_y=True, cut_ratio=1,\n                mix_ratio=1)\n\nImplementation of Mixup or CutMix for TensorAudio\n\nsource\n\n\nAudioCutMixUpAugment\n\n AudioCutMixUpAugment (mix_alpha=0.4, cut_alpha=1.0, stack_y=True,\n                       aug_ratio=1, cut_ratio=1, mix_ratio=1,\n                       augs_only=None, wave_augs=False)\n\nImplementation of Mixup, CutMix, or Augment for TensorAudio"
  },
  {
    "objectID": "ffcv.operations.html",
    "href": "ffcv.operations.html",
    "title": "FFCV Operations",
    "section": "",
    "text": "fastxtend provides multiple FFCV operations, including existing FFCV operations as a reference, a fastai compatible ToDevice, and fastai compatible Tensor conversions.\nBy default, these operations are imported under ft if using from fastxtend.ffcv.all import *."
  },
  {
    "objectID": "ffcv.operations.html#ffcv-operations-reference",
    "href": "ffcv.operations.html#ffcv-operations-reference",
    "title": "FFCV Operations",
    "section": "FFCV Operations Reference",
    "text": "FFCV Operations Reference\nThese operations are from FFCV. You can find the original documentation at the FFCV API Reference.\n\n\nConvert\n\n Convert (target_dtype)\n\nConvert to target data type.\n\n\n\nView\n\n View (target_dtype)\n\nView array using np.view or torch.view."
  },
  {
    "objectID": "ffcv.operations.html#todevice",
    "href": "ffcv.operations.html#todevice",
    "title": "FFCV Operations",
    "section": "ToDevice",
    "text": "ToDevice\nWhile FFCV has a ToDevice operation, it is recommended to use the fastxtend ToDevice operation for compatability with fastai features.\n\nsource\n\nToDevice\n\n ToDevice (device:int|str|torch.device|None=None, non_blocking:bool=True)\n\nCopy tensor to Cuda device and retains metadata\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndevice\nint | str | torch.device | None\nNone\nCopy tensor to this Cuda device. Defaults to fastai’s default_device()\n\n\nnon_blocking\nbool\nTrue\nAsynchronous if copying from CPU to GPU"
  },
  {
    "objectID": "ffcv.operations.html#convert-to-fastai-tensors",
    "href": "ffcv.operations.html#convert-to-fastai-tensors",
    "title": "FFCV Operations",
    "section": "Convert to fastai Tensors",
    "text": "Convert to fastai Tensors\nWhile FFCV has ToTensor and ToTorchImage operations for converting NumPy arrays to PyTorch Tensors, it is recommended to use these fastxtend operations for compatability with fastai features.\n\nsource\n\nToTensorBase\n\n ToTensorBase (tensor_cls:TensorBase=&lt;class\n               'fastai.torch_core.TensorBase'&gt;)\n\nConvert from Numpy array to fastai TensorBase or tensor_cls.\n\nsource\n\n\nToTensorImage\n\n ToTensorImage (tensor_cls:TensorImageBase=&lt;class\n                'fastai.torch_core.TensorImage'&gt;)\n\nConvenience operation to convert from Numpy array to fastai TensorImage or tensor_cls.\n\nsource\n\n\nToTensorImageBW\n\n ToTensorImageBW ()\n\nConvenience operation to convert from Numpy array to fastai TensorImageBW.\n\nsource\n\n\nToTensorMask\n\n ToTensorMask ()\n\nConvenience operation to convert from Numpy array to fastai TensorMask.\n\nsource\n\n\nToTensorCategory\n\n ToTensorCategory ()\n\nConvenience operation to convert from Numpy array to fastxtend TensorCategory.\n\nsource\n\n\nToTensorMultiCategory\n\n ToTensorMultiCategory ()\n\nConvenience operation convert from Numpy array to fastxtend TensorMultiCategory.\n\nsource\n\n\nToTensorTitledTensorScalar\n\n ToTensorTitledTensorScalar ()\n\nConvenience operation convert from Numpy array to fastai TitledTensorScalar."
  },
  {
    "objectID": "vision.augment.itemtensor.html",
    "href": "vision.augment.itemtensor.html",
    "title": "Tensor Item Transforms",
    "section": "",
    "text": "Augmentations patched: RandomCrop, CropPad, Resize, RandomResizedCrop, & RatioResize.\n\n\n\n\n\n\nNote\n\n\n\nThese are all patched extensions of the fastai.vision.augment item transforms.\n\n\nFor details on how they work, see the fastai documentation."
  },
  {
    "objectID": "vision.augment.itemtensor.html#implementation-patches-and-methods",
    "href": "vision.augment.itemtensor.html#implementation-patches-and-methods",
    "title": "Tensor Item Transforms",
    "section": "",
    "text": "Augmentations patched: RandomCrop, CropPad, Resize, RandomResizedCrop, & RatioResize.\n\n\n\n\n\n\nNote\n\n\n\nThese are all patched extensions of the fastai.vision.augment item transforms.\n\n\nFor details on how they work, see the fastai documentation."
  },
  {
    "objectID": "vision.augment.itemtensor.html#randomcrop",
    "href": "vision.augment.itemtensor.html#randomcrop",
    "title": "Tensor Item Transforms",
    "section": "RandomCrop",
    "text": "RandomCrop\n\nsource\n\nencodes\n\n encodes\n          (x:Union[fastai.torch_core.TensorImage,fastai.torch_core.TensorM\n          ask])\n\nExtends RandomCrop to TensorImage & TensorMask\n\n_,axs = plt.subplots(1,3,figsize=(12,4))\nf = RandomCrop(200)\nfor ax in axs: f(img).show(ctx=ax)\n\n\n\n\nOn the validation set, a center crop is always taken.\n\n_,axs = plt.subplots(1,3,figsize=(12,4))\nfor ax in axs: f(img, split_idx=1).show(ctx=ax)"
  },
  {
    "objectID": "vision.augment.itemtensor.html#croppad",
    "href": "vision.augment.itemtensor.html#croppad",
    "title": "Tensor Item Transforms",
    "section": "CropPad",
    "text": "CropPad\n\nsource\n\nencodes\n\n encodes\n          (x:Union[fastai.torch_core.TensorImage,fastai.torch_core.TensorM\n          ask])\n\nExtends CropPad to TensorImage & TensorMask\n\n_,axs = plt.subplots(1,3,figsize=(12,4))\nfor ax,mode in zip(axs.flatten(), [PadMode.Zeros, PadMode.Border, PadMode.Reflection]):\n    img.crop_pad((600,700), pad_mode=mode).show(ctx=ax, title=mode)"
  },
  {
    "objectID": "vision.augment.itemtensor.html#resize",
    "href": "vision.augment.itemtensor.html#resize",
    "title": "Tensor Item Transforms",
    "section": "Resize",
    "text": "Resize\n\nsource\n\nencodes\n\n encodes\n          (x:Union[fastai.torch_core.TensorImage,fastai.torch_core.TensorM\n          ask])\n\nExtends Resize to TensorImage & TensorMask\n\n_,axs = plt.subplots(1,3,figsize=(12,4))\nfor ax,method in zip(axs.flatten(), [ResizeMethod.Squish, ResizeMethod.Pad, ResizeMethod.Crop]):\n    rsz = Resize(256, method=method)\n    rsz(img, split_idx=0).show(ctx=ax, title=method)\n\n\n\n\nOn the validation set, the crop is always a center crop (on the dimension that’s cropped).\n\n_,axs = plt.subplots(1,3,figsize=(12,4))\nfor ax,method in zip(axs.flatten(), [ResizeMethod.Squish, ResizeMethod.Pad, ResizeMethod.Crop]):\n    rsz(img, split_idx=1).show(ctx=ax, title=method)"
  },
  {
    "objectID": "vision.augment.itemtensor.html#randomresizedcrop",
    "href": "vision.augment.itemtensor.html#randomresizedcrop",
    "title": "Tensor Item Transforms",
    "section": "RandomResizedCrop",
    "text": "RandomResizedCrop\n\nsource\n\nencodes\n\n encodes\n          (x:Union[fastai.torch_core.TensorImage,fastai.torch_core.TensorM\n          ask])\n\nExtends RandomResizedCrop to TensorImage & TensorMask\n\ncrop = RandomResizedCrop(256)\n_,axs = plt.subplots(3,3,figsize=(9,9))\nfor ax in axs.flatten():\n    crop(img).show(ctx=ax)\n\n\n\n\nSquish is used on the validation set, removing val_xtra proportion of each side first.\n\n_,axs = plt.subplots(1,3,figsize=(12,4))\nfor ax in axs.flatten():\n    cropped = crop(img, split_idx=1).show(ctx=ax)"
  },
  {
    "objectID": "vision.augment.itemtensor.html#ratioresize",
    "href": "vision.augment.itemtensor.html#ratioresize",
    "title": "Tensor Item Transforms",
    "section": "RatioResize",
    "text": "RatioResize\n\nsource\n\nencodes\n\n encodes\n          (x:Union[fastai.torch_core.TensorImage,fastai.torch_core.TensorM\n          ask])\n\nExtends RatioResize to TensorImage & TensorMask\n\nRatioResize(256)(img).show()\n\n&lt;AxesSubplot:&gt;"
  },
  {
    "objectID": "torch_core.html",
    "href": "torch_core.html",
    "title": "Torch Core",
    "section": "",
    "text": "source\n\nTensorCategory\n\n TensorCategory (x, **kwargs)\n\nfastai’s TensorCategory with a show method\n\nsource\n\n\nTensorMultiCategory\n\n TensorMultiCategory (x, **kwargs)\n\nfastai’s TensorMultiCategory with a show method"
  },
  {
    "objectID": "callback.amp.html",
    "href": "callback.amp.html",
    "title": "Automatic Mixed Precision",
    "section": "",
    "text": "With supported hardware, fastxtend supports training in both float16 and bfloat16 automatic mixed precision.\nFor details on float16 mixed precision training, please see the fastai mixed precision documentation."
  },
  {
    "objectID": "callback.amp.html#mixedprecision-callback",
    "href": "callback.amp.html#mixedprecision-callback",
    "title": "Automatic Mixed Precision",
    "section": "MixedPrecision Callback",
    "text": "MixedPrecision Callback\nfastxtend’s MixedPrecision is a drop in replacement for fastai.callback.fp16.MixedPrecision for float16 mixed precision. Set amp_mode to train in bfloat16 mixed precision or use Learner.to_bf16.\n\nsource\n\nAMPMode\n\n AMPMode (value, names=None, module=None, qualname=None, type=None,\n          start=1)\n\nAutomatic mixed precision modes for ease of completion\n\nsource\n\n\nMixedPrecision\n\n MixedPrecision (amp_mode:str|AMPMode=&lt;AMPMode.FP16: 'fp16'&gt;,\n                 init_scale=65536.0, growth_factor=2.0,\n                 backoff_factor=0.5, growth_interval=2000, enabled=True)\n\nMixed precision training using Pytorch’s Automatic Mixed Precision (AMP)\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\namp_mode\nstr | AMPMode\nAMPMode.FP16\nMixed Precision training mode. Supports fp16 and bf16.\n\n\ninit_scale\nfloat\n65536.0\n\n\n\ngrowth_factor\nfloat\n2.0\n\n\n\nbackoff_factor\nfloat\n0.5\n\n\n\ngrowth_interval\nint\n2000\n\n\n\nenabled\nbool\nTrue\n\n\n\n\namp_mode accepts both AMPMode enums and ‘fp16’ or ‘bf16’ strings.\nPassing GradScaler arguments to MixedPrecision when training in bfloat16 has no effect, as bfloat16 mixed precision does not used a gradient scaler."
  },
  {
    "objectID": "callback.amp.html#convenience-methods",
    "href": "callback.amp.html#convenience-methods",
    "title": "Automatic Mixed Precision",
    "section": "Convenience Methods",
    "text": "Convenience Methods\nIn addition to the fastai convenience methods, fastxtend adds Learner.to_bf16 for training in bfloat16 mixed precision.\n\nsource\n\nLearner.to_fp16\n\n Learner.to_fp16 (init_scale=65536.0, growth_factor=2.0,\n                  backoff_factor=0.5, growth_interval=2000, enabled=True)\n\nSet Learner to float16 mixed precision using PyTorch AMP\n\nsource\n\n\nLearner.to_bf16\n\n Learner.to_bf16 ()\n\nSet Learner to bfloat16 mixed precision using PyTorch AMP\n\nsource\n\n\nLearner.to_fp32\n\n Learner.to_fp32 ()\n\nSet Learner to float32 precision"
  }
]