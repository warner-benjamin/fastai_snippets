{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp data.loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "# Contains code from:\n",
    "# fastai - Apache License 2.0 - Copyright (c) 2023 fast.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader\n",
    "> fastai DataLoader Mixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastxtend's `DataLoaderMixin` allows adding fastai functionality to non-fastai DataLoaders. <code>DataLoaderMixin</code> supports batch transforms, `one_batch`, `show_batch`, and `show_results`, although inputs will need to be converted to fastai typed tensors for show methods to work.\n",
    "\n",
    "For an example of using <code>DataLoaderMixin</code>, look at the source code for `Loader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "from fastcore.basics import GetAttr, detuplify, Inf\n",
    "from fastcore.dispatch import retain_types, explode_types\n",
    "from fastcore.meta import funcs_kwargs\n",
    "from fastcore.transform import Pipeline\n",
    "\n",
    "from fastai.data.core import show_batch, show_results\n",
    "\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@funcs_kwargs\n",
    "class BaseLoader(GetAttr):\n",
    "    \"Provides callbacks for DataLoaders which inherit from `BaseLoader`\"\n",
    "    _methods = 'before_iter after_batch after_iter'.split()\n",
    "    def __init__(self, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def before_iter(self, x=None, *args, **kwargs):\n",
    "        \"Called before `BaseLoader` starts to read/iterate over the dataset.\"\n",
    "        return x\n",
    "\n",
    "    def after_batch(self, x=None, *args, **kwargs):\n",
    "        \"After collating mini-batch of items, the mini-batch is passed through this function.\"\n",
    "        return x\n",
    "\n",
    "    def after_iter(self, x=None, *args, **kwargs):\n",
    "        \"Called after `BaseLoader` has fully read/iterated over the dataset.\"\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class DataLoaderMixin(BaseLoader):\n",
    "    \"A Mixin class for adding fastai DataLoader functionality to other DataLoader\"\n",
    "    _methods = 'before_iter after_batch after_iter'.split()\n",
    "    def __init__(self,\n",
    "        device:str|int|torch.device|None=None, # Device to place batch. Defaults to fastai's `default_device`\n",
    "        n_inp:int|None=None, # Number of inputs to the model. Defaults to pipelines length minus 1\n",
    "        split_idx:int|None=None, # Apply batch transform(s) to training (0) or validation (1) set. Defaults to valid if order is `SEQEUNTIAL`\n",
    "        do_setup:bool=True, # Run `setup()` for batch transform(s)\n",
    "        **kwargs\n",
    "    ):\n",
    "        if 'batch_tfms' in kwargs:\n",
    "            if 'after_batch' not in kwargs:\n",
    "                kwargs['after_batch'] = kwargs.pop('batch_tfms')\n",
    "            else:\n",
    "                raise ValueError(f'Cannot pass both `after_batch` and `batch_tfms` to `{self.__name__}`')\n",
    "\n",
    "        self._split_idx = split_idx\n",
    "\n",
    "        kwargs['after_batch'] = Pipeline(kwargs.get('after_batch', None), split_idx=self._split_idx)\n",
    "        if do_setup:\n",
    "            kwargs['after_batch'].setup(self)\n",
    "\n",
    "        if device is None:\n",
    "            self.device = default_device()\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        self._n_inp = n_inp\n",
    "\n",
    "        self._callback_warning(kwargs)\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def decode(self, b:Tuple[Tensor,...]):\n",
    "        \"Decode batch `b`\"\n",
    "        return to_cpu(self.after_batch.decode(self._retain_dl(b)))\n",
    "\n",
    "    def decode_batch(self, b:Tuple[Tensor,...], max_n:int=9):\n",
    "        \"Decode up to `max_n` input(s) from batch `b`\"\n",
    "        return self._decode_batch(self.decode(b), max_n)\n",
    "\n",
    "    def one_batch(self):\n",
    "        \"Return one processed batch of input(s) and target(s)\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def show_batch(self,\n",
    "        b:Tuple[Tensor,...]|None=None, # Batch to show. If None calls `one_batch`\n",
    "        max_n:int=9, # Maximum number of items to show\n",
    "        ctxs=None, # List of `ctx` objects to show data. Could be matplotlib axis, DataFrame etc\n",
    "        show:bool=True, # If False, return decoded batch instead of showing\n",
    "        unique:bool=False, # Whether to show only one\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"Show `max_n` input(s) and target(s) from the batch.\"\n",
    "        if unique:\n",
    "            old_get_idxs = self.get_idxs\n",
    "            self.get_idxs = lambda: Inf.zeros\n",
    "        if b is None:\n",
    "            b = self.one_batch()\n",
    "        if not show:\n",
    "            return self._pre_show_batch(b, max_n=max_n)\n",
    "        # Uses Type Dispatch to call the correct `show_batch` for b\n",
    "        show_batch(*self._pre_show_batch(b, max_n=max_n), ctxs=ctxs, max_n=max_n, **kwargs)\n",
    "        if unique:\n",
    "            self.get_idxs = old_get_idxs\n",
    "\n",
    "    def show_results(self,\n",
    "        b, # Batch to show results for\n",
    "        out, # Predicted output from model for the batch\n",
    "        max_n:int=9, # Maximum number of items to show\n",
    "        ctxs=None, # List of `ctx` objects to show data. Could be matplotlib axis, DataFrame etc\n",
    "        show:bool=True, # If False, return decoded batch instead of showing\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"Show `max_n` results with input(s), target(s) and prediction(s).\"\n",
    "        x,y,its = self.show_batch(b, max_n=max_n, show=False)\n",
    "        b_out = type(b)(b[:self.n_inp] + (tuple(out) if is_listy(out) else (out,)))\n",
    "        x1,_,outs = self.show_batch(b_out, max_n=max_n, show=False)\n",
    "        if its is None:\n",
    "            res = (x, x1, None, None)\n",
    "        else:\n",
    "            res = (x, y, its, outs.itemgot(slice(self.n_inp,None)))\n",
    "        if not show:\n",
    "            return res\n",
    "        # Uses Type Dispatch to call the correct `show_results` for b\n",
    "        show_results(*res, ctxs=ctxs, max_n=max_n, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def n_inp(self) -> int:\n",
    "        \"Number of elements in a batch for model input\"\n",
    "        return self._n_inp\n",
    "\n",
    "    @property\n",
    "    def bs(self) -> int:\n",
    "        \"Number of items a batch\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return self._device\n",
    "\n",
    "    @device.setter\n",
    "    def device(self, device:int|str|torch.device):\n",
    "        # parse device\n",
    "        device, *_ = torch._C._nn._parse_to(device=device)\n",
    "        self._device = device\n",
    "        # Device setter for DataLoaderMixin.batch_tfms\n",
    "        if hasattr(self.after_batch, 'fs'):\n",
    "            self._pipeline_device(self.after_batch.fs)\n",
    "\n",
    "    def to(self, device:int|str|torch.device):\n",
    "        \"Sets `self.device=device`.\"\n",
    "        self.device = device\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def split_idx(self) -> int|None:\n",
    "        return self._split_idx\n",
    "\n",
    "    @split_idx.setter\n",
    "    def split_idx(self, split_idx:int):\n",
    "        \"Sets fastai batch transforms to train (split_idx=0) or valid (split_idx=1)\"\n",
    "        self._split_idx = split_idx\n",
    "        if isinstance(self.after_batch, Pipeline):\n",
    "            self.after_batch.split_idx = split_idx\n",
    "\n",
    "    def __iter__(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _one_pass(self, b=None):\n",
    "        if b is None:\n",
    "            b = self.one_batch()\n",
    "        self._types = explode_types(b)\n",
    "\n",
    "    def _retain_dl(self, b):\n",
    "        if not getattr(self, '_types', None):\n",
    "            self._one_pass(b)\n",
    "        return retain_types(b, typs=self._types)\n",
    "\n",
    "    def _decode_batch(self, b, max_n=9):\n",
    "        return L(batch_to_samples(b, max_n=max_n))\n",
    "\n",
    "    def _pre_show_batch(self, b, max_n=9):\n",
    "        \"Decode `b` to be ready for `show_batch`\"\n",
    "        b = self.decode(b)\n",
    "        if hasattr(b, 'show'):\n",
    "            return b,None,None\n",
    "        its = self._decode_batch(b, max_n)\n",
    "        if not is_listy(b):\n",
    "            b,its = [b],L((o,) for o in its)\n",
    "        return detuplify(b[:self.n_inp]),detuplify(b[self.n_inp:]),its\n",
    "\n",
    "    def _pipeline_device(self, pipe):\n",
    "        \"Device setter for fastai pipeline\"\n",
    "        for tfm in pipe:\n",
    "            if hasattr(tfm, 'to') and callable(tfm.to):\n",
    "                tfm.to(self.device, non_blocking=True)\n",
    "            else:\n",
    "                for a in L(getattr(tfm, 'parameters', None)):\n",
    "                    setattr(tfm, a, getattr(tfm, a).to(self.device, non_blocking=True))\n",
    "\n",
    "    def _callback_warning(self, kwargs):\n",
    "        for name in ['item_tfms', 'after_item', 'before_batch']:\n",
    "            if name in kwargs:\n",
    "                msg = f\"fastxtend's `BaseDataLoader` will not call any {name} \" \\\n",
    "                      f\"methods. {name} is for use with a fastai DataLoader.\\n\"\n",
    "                warn(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DataLoaderMixin.one_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DataLoaderMixin.show_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DataLoaderMixin.show_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DataLoaderMixin.to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DataLoaderMixin.n_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DataLoaderMixin.split_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DataLoaderMixin.decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DataLoaderMixin.decode_batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
