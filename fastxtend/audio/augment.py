# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/audio.03_augment.ipynb (unless otherwise specified).


from __future__ import annotations


__all__ = ['Flip', 'Roll', 'AudioPadMode', 'RandomCropPad', 'VolumeMode', 'Volume', 'TimeStretch', 'TimeMasking',
           'FrequencyMasking', 'AmplitudeToDBMode', 'AmplitudeToDB']

# Cell
#nbdev_comment from __future__ import annotations

import random
import math

import torchaudio.transforms as tatfms
import torchaudio.functional as TAF
from torch import _VF

from fastcore.transform import DisplayedTransform

from fastai.data.transforms import Normalize
from fastai.vision.augment import RandTransform

from ..transform import BatchRandTransform
from .core import TensorAudio, TensorSpec, TensorMelSpec
from ..imports import *

# Internal Cell
def _get_audio_attr(x:TensorAudio|TensorSpec|TensorMelSpec|tuple[TensorAudio|TensorSpec|TensorMelSpec,...], attr:str) -> Any:
    if isinstance(x, tuple): return getattr(x[0], attr)
    else:                    return getattr(x, attr)

# Cell
class Flip(RandTransform):
    order, split_idx = 5, 0
    def __init__(self, p:float=0.5):
        super().__init__(p=p)

    def encodes(self, x:TensorAudio) -> Tensor:
        return x.fliplr()

# Cell
class Roll(RandTransform):
    order, split_idx = 6, 0
    def __init__(self,
        p:float=0.5,
        max_roll:float=0.5,
    ):
        store_attr('max_roll')
        super().__init__(p=p)

    def before_call(self,
        b:TensorAudio|tuple[TensorAudio,...],
        split_idx:int # Index of the train/valid dataset
    ):
        super().before_call(b, split_idx)
        self.shift = int(random.uniform(-1,1) * self.max_roll * _get_audio_attr(b, 'samples'))

    def encodes(self, x:TensorAudio) -> Tensor:
        return x.roll(shifts=self.shift, dims=-1)

# Cell
class AudioPadMode(Enum):
    "All methods of padding audio as attributes to get tab-completion and typo-proofing",
    Constant = 1
    ConstantPre = 2
    ConstantPost = 3
    Repeat = 4
    Reflect = 5

# Internal Cell
@patch
def crop_pad(x:TensorAudio,
    end_len:int,
    crop_start:int|None=None,
    pad_len:int=0,
    padmode:AudioPadMode=AudioPadMode.Constant,
    constant:float=0
):
    if crop_start:
        x = x[:,crop_start:crop_start+end_len]
    elif pad_len:
        if padmode==AudioPadMode.ConstantPre:
            x = _VF.constant_pad_nd(x, (pad_len, 0), constant)
        elif padmode==AudioPadMode.ConstantPost:
            x = _VF.constant_pad_nd(x, (0, pad_len), constant)
        elif padmode==AudioPadMode.Repeat:
            if pad_len >= end_len:
                x = x.repeat(1, 1+max(pad_len//end_len, 1))
            else:
                x = torch.cat([x, x[:,0:pad_len]], dim=1)
        else:
            pad = pad_len//2
            if pad==0: pad += 1
            if padmode==AudioPadMode.Constant:
                x = _VF.constant_pad_nd(x, (pad, pad), constant)
            elif padmode==AudioPadMode.Reflect:
                x = torch._C._nn.reflection_pad1d(x, (pad, pad))

    if x.samples > end_len: x = x[:,0:end_len]
    return x

# Cell
class RandomCropPad(RandTransform):
    split_idx, order = None, 25
    def __init__(self,
        duration:Number|None=None, # Crop length in seconds
        samples:int|None=None, # Crop length in samples
        padmode:AudioPadMode=AudioPadMode.Constant,
        constant:Number=0
    ):
        if duration is None and samples is None:
            raise ValueError('One of `duration` or `samples` must be set')
        if constant >= 1 or constant <= -1:
            raise ValueError('`constant` must be between [-1,1]')
        store_attr()
        super().__init__()

    def before_call(self,
        b:TensorAudio|tuple[TensorAudio,...],
        split_idx:int # Index of the train/valid dataset
    ):
        self.samples = self.samples if self.samples is not None else int(self.duration*_get_audio_attr(b, 'sr'))
        self.orig_samples = _get_audio_attr(b, 'samples')
        if split_idx:
            self.crop_start = (self.orig_samples-self.samples)//2 if self.samples < self.orig_samples else None
        else:
            self.crop_start = torch.randint(0,self.orig_samples-self.samples, (1,)) if self.samples < self.orig_samples else None

        self.pad_len = (self.samples-self.orig_samples)//2 if self.samples > self.orig_samples else 0

    def encodes(self, x:TensorAudio) -> Tensor:
        return x.crop_pad(self.samples, self.crop_start, self.pad_len, self.padmode, self.constant)

# Cell
class VolumeMode(Enum):
    "All AmplitudeToDB mode's as attributes to get tab-completion and typo-proofing",
    DB = 1
    Power = 2
    Amplitude = 3

# Cell
class Volume(RandTransform):
    order, split_idx = 30, 0
    def __init__(self,
        p:float=0.5,
        gain:Number|None=None, # If none, randomly select from `gain_range`
        gain_range:tuple[Number,Number] = (-18, 6),
        vol_mode:VolumeMode=VolumeMode.DB # One of "db", "amplitude", or "power"
    ):
        super().__init__(p=p)
        if vol_mode != VolumeMode.DB:
            gain_range = (max(gain_range[0],0), gain_range[1])
        self.random_gain = gain is None
        store_attr(but='p')

    def before_call(self,
        b:TensorAudio|tuple[TensorAudio,...],
        split_idx:int # Index of the train/valid dataset
    ):
        super().before_call(b, split_idx)
        if self.random_gain: self.gain = random.uniform(*self.gain_range)

    def encodes(self, x:TensorAudio) -> Tensor:
        if self.vol_mode == VolumeMode.DB:
            x = TAF.gain(x, self.gain)
        elif self.vol_mode == VolumeMode.Amplitude:
            x = x * self.gain
        elif self.vol_mode == VolumeMode.Power:
            x = TAF.gain(x, 10 * math.log10(self.gain))

        return torch.clamp(x, -1, 1)

# Cell
class TimeStretch(BatchRandTransform):
    split_idx, order = 0, 80
    def __init__(self,
        p:float=0.25, # Per-item probability
        fixed_rate:float|None=None, # Rate to speed up or slow down by. If none, rate is randomly chosen from `rate_bounds`
        n_freq:int|None=None, # Number of filter banks from stft, defaults to batch's `n_fft // 2 + 1`
        hop_length:float|None=None, #  Length of hop between STFT windows, defaults to batch value
        rate_bounds:tuple[float,float] = (0.75, 1.25), # Min and max rate to speed up or slow down by
        **kwargs
    ):
        store_attr(but='p')
        if n_freq is None and hop_length is None:
            self.phase_advance = None
        else:
            self.phase_advance = torch.linspace(0, math.pi * hop_length, n_freq)
        super().__init__(p=p)

    def before_call(self,
        b:TensorSpec|tuple[TensorSpec,...],
        split_idx:int # Index of the train/valid dataset
    ):
        super().before_call(b, split_idx)
        if self.phase_advance is None:
            hop_length, n_freq, device =_get_audio_attr(b, 'hop_length'), _get_audio_attr(b, 'n_fft')//2+1, _get_audio_attr(b, 'device')
            self.phase_advance = torch.linspace(0, math.pi * hop_length, n_freq, device=device)
        self.rate = random.uniform(*self.rate_bounds) if self.fixed_rate is None else self.fixed_rate

    def encodes(self, x:TensorSpec) -> Tensor:
        return TAF.phase_vocoder(x, self.rate, self.phase_advance)

    def to(self, *args, **kwargs):
        device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(*args, **kwargs)
        if self.phase_advance is not None:
            self.phase_advance.to(device)

# Cell
class TimeMasking(BatchRandTransform):
    split_idx, order = 0, 81
    def __init__(self,
        p:float=0.25, # Per-item probability
        max_mask:float=0.2, # Maximum possible length of the mask  [0, max_mask)
        iid_masks:bool=True, # Apply different masks to each example/channel in the batch
    ):
        store_attr(but='p,iid_masks')
        self.tm = tatfms.TimeMasking(1,iid_masks)
        super().__init__(p=p)

    def before_call(self,
        b:TensorSpec|tuple[TensorSpec,...],
        split_idx:int # Index of the train/valid dataset
    ):
        super().before_call(b, split_idx)
        self.tm.time_mask_param = int(self.max_mask * _get_audio_attr(b, 'shape')[-1])

    def encodes(self, x:TensorSpec|TensorMelSpec) -> Tensor:
        return self.tm(x)

# Cell
class FrequencyMasking(BatchRandTransform):
    split_idx, order = 0, 82
    def __init__(self,
        p:float=0.25, # Per-item probability
        max_mask:float=0.2, # Maximum possible length of the mask [0, max_mask)
        iid_masks:bool=True, # Apply different masks to each example/channel in the batch
    ):
        store_attr(but='p,iid_masks')
        self.fm = tatfms.FrequencyMasking(1,iid_masks)
        super().__init__(p=p)

    def before_call(self,
        b:TensorSpec|tuple[TensorSpec,...],
        split_idx:int # Index of the train/valid dataset
    ):
        super().before_call(b, split_idx)
        self.fm.freq_mask_param = int(self.max_mask * _get_audio_attr(b, 'shape')[-2])

    def encodes(self, x:TensorSpec|TensorMelSpec) -> Tensor:
        return self.fm(x)

# Cell
class AmplitudeToDBMode(Enum):
    "All AmplitudeToDB mode's as attributes to get tab-completion and typo-proofing",
    Power = 'power'
    Magnitude = 'magnitude'

# Cell
class AmplitudeToDB(DisplayedTransform):
    order = 99
    def __init__(self,
        mode:AmplitudeToDBMode=AmplitudeToDBMode.Power,
        top_db:float|None=None
    ):
        self.amdb = tatfms.AmplitudeToDB(mode.value, top_db)

    def encodes(self, x:TensorSpec|TensorMelSpec) -> Tensor:
        return self.amdb(x)

    def to(self, *args, **kwargs):
        device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(*args, **kwargs)
        self.amdb.to(device)