{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp callback.lr_finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR Finder\n",
    "> A duplicate of fast.ai's `lr_find`, except it can restore the dataloader and random state after running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.xtras import is_listy\n",
    "from fastcore.foundation import patch, docs\n",
    "from fastcore.basics import tuplify\n",
    "from fastai.callback.schedule import ParamScheduler, SchedExp, SuggestionMethod\n",
    "from fastai.callback.core import *\n",
    "from fastai.torch_core import tensor, get_random_states, set_random_states\n",
    "from fastai.learner import Learner\n",
    "from functools import partial\n",
    "from copy import deepcopy\n",
    "import os, torch, collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@docs\n",
    "class LRFinder(ParamScheduler):\n",
    "    \"Training with exponentially growing learning rate\"\n",
    "    def __init__(self, start_lr=1e-7, end_lr=10, num_it=100, stop_div=True, restore_state=True):\n",
    "        if num_it < 6: num_it = 6\n",
    "        self.scheds = {'lr': [SchedExp(s, e) for (s,e) in zip(start_lr,end_lr)\n",
    "                             ] if is_listy(start_lr) else SchedExp(start_lr, end_lr)}\n",
    "        self.num_it,self.stop_div,self.restore_state = num_it,stop_div,restore_state\n",
    "\n",
    "    def before_fit(self):\n",
    "        super().before_fit()\n",
    "        if self.restore_state:\n",
    "            self.old_dls = deepcopy(self.learn.dls)\n",
    "            self.states = get_random_states()\n",
    "        self.learn.save('_tmp')\n",
    "        self.best_loss = float('inf')\n",
    "\n",
    "    def before_batch(self): self._update_val(self.train_iter/self.num_it)\n",
    "\n",
    "    def after_batch(self):\n",
    "        super().after_batch()\n",
    "        if self.smooth_loss < self.best_loss: self.best_loss = self.smooth_loss\n",
    "        if self.smooth_loss > 4*self.best_loss and self.stop_div: raise CancelFitException()\n",
    "        if self.train_iter >= self.num_it: raise CancelFitException()\n",
    "\n",
    "    def before_validate(self): raise CancelValidException()\n",
    "\n",
    "    def after_fit(self):\n",
    "        self.learn.opt.zero_grad() # Needed before detaching the optimizer for future fits\n",
    "        tmp_f = self.path/self.model_dir/'_tmp.pth'\n",
    "        if tmp_f.exists():\n",
    "            self.learn.load('_tmp', with_opt=True)\n",
    "            os.remove(tmp_f)\n",
    "        if self.restore_state:\n",
    "            self.learn.dls = self.old_dls\n",
    "            set_random_states(**self.states)\n",
    "\n",
    "    _docs = {\"before_fit\": \"Initialize container for hyper-parameters and save the model & optimizer, optionally saving dataloader & random state\",\n",
    "             \"before_batch\": \"Set the proper hyper-parameters in the optimizer\",\n",
    "             \"after_batch\": \"Record hyper-parameters of this batch and potentially stop training\",\n",
    "             \"after_fit\": \"Save the hyper-parameters in the recorder if there is one and load the original model & optimizer, optionally restoring dataloader & random state\",\n",
    "             \"before_validate\": \"Skip the validation part of training\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"LRFinder.before_fit\" class=\"doc_header\"><code>LRFinder.before_fit</code><a href=\"__main__.py#L11\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>LRFinder.before_fit</code>()\n",
       "\n",
       "Initialize container for hyper-parameters and save the model & optimizer, optionally saving dataloader & random state"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(LRFinder.before_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"LRFinder.before_batch\" class=\"doc_header\"><code>LRFinder.before_batch</code><a href=\"__main__.py#L19\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>LRFinder.before_batch</code>()\n",
       "\n",
       "Set the proper hyper-parameters in the optimizer"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(LRFinder.before_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"LRFinder.after_batch\" class=\"doc_header\"><code>LRFinder.after_batch</code><a href=\"__main__.py#L21\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>LRFinder.after_batch</code>()\n",
       "\n",
       "Record hyper-parameters of this batch and potentially stop training"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(LRFinder.after_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"LRFinder.before_validate\" class=\"doc_header\"><code>LRFinder.before_validate</code><a href=\"__main__.py#L27\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>LRFinder.before_validate</code>()\n",
       "\n",
       "Skip the validation part of training"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(LRFinder.before_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"LRFinder.after_fit\" class=\"doc_header\"><code>LRFinder.after_fit</code><a href=\"__main__.py#L29\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>LRFinder.after_fit</code>()\n",
       "\n",
       "Save the hyper-parameters in the recorder if there is one and load the original model & optimizer, optionally restoring dataloader & random state"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(LRFinder.after_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lr_find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def lr_find(self:Learner, start_lr=1e-7, end_lr=10, num_it=100, stop_div=True, show_plot=True, suggest_funcs=(SuggestionMethod.Valley), restore_state=True):\n",
    "    \"Launch a mock training to find a good learning rate and return suggestions based on `suggest_funcs` as a named tuple. Use `restore_state` to reset dataloaders and random state after running.\"\n",
    "    n_epoch = num_it//len(self.dls.train) + 1\n",
    "    cb=LRFinder(start_lr=start_lr, end_lr=end_lr, num_it=num_it, stop_div=stop_div,restore_state=restore_state)\n",
    "    with self.no_logging(): self.fit(n_epoch, cbs=cb)\n",
    "    if suggest_funcs is not None:\n",
    "        lrs, losses = tensor(self.recorder.lrs[num_it//10:-5]), tensor(self.recorder.losses[num_it//10:-5])\n",
    "        nan_idxs = torch.nonzero(torch.isnan(losses.view(-1)))\n",
    "        if len(nan_idxs) > 0:\n",
    "            drop_idx = min(nan_idxs)\n",
    "            lrs = lrs[:drop_idx]\n",
    "            losses = losses[:drop_idx]\n",
    "        _suggestions, nms = [], []\n",
    "        for func in tuplify(suggest_funcs):\n",
    "            nms.append(func.__name__ if not isinstance(func, partial) else func.func.__name__) # deal with partials\n",
    "            _suggestions.append(func(lrs, losses, num_it))\n",
    "\n",
    "        SuggestedLRs = collections.namedtuple('SuggestedLRs', nms)\n",
    "        lrs, pnts = [], []\n",
    "        for lr, pnt in _suggestions:\n",
    "            lrs.append(lr)\n",
    "            pnts.append(pnt)\n",
    "        if show_plot: self.recorder.plot_lr_find(suggestions=pnts, nms=nms)\n",
    "        return SuggestedLRs(*lrs)\n",
    "\n",
    "    elif show_plot: self.recorder.plot_lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without `restore_state` running `lr_find` advances both the random state and dataloaders and behaves the same way as [fast.ai's](https://docs.fast.ai/callback.schedule.html#Learner.lr_find) `lr_find`. Which means the following two code blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slow\n",
    "with no_random():\n",
    "    dls = get_dls()\n",
    "    learn = Learner(dls, xresnet18(n_out=dls.c))\n",
    "\n",
    "with no_random():\n",
    "    learn.lr_find(restore_state=False)\n",
    "    learn.fit_one_cycle(2, 3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slow\n",
    "with no_random():\n",
    "    dls = get_dls()\n",
    "    learn = Learner(dls, xresnet18(n_out=dls.c))\n",
    "\n",
    "with no_random():\n",
    "    learn.fit_one_cycle(2, 3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "will result with different training output.\n",
    "\n",
    "While the default of `restore_state=True` prevents this from occuring, it has the potential downside of showing less variance in learning rate results. As every call to `lr_find` will be over the same first `n_iter` items using the same random state. Without `no_random` set, most of the variation appears to be from cuda not being set in deterministic mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted augment.tensor_item_tfm.ipynb.\n",
      "Converted callback.cutmixup.ipynb.\n",
      "Converted callback.lr_finder.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted schedulers.fit_flat_varied.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#slow\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
