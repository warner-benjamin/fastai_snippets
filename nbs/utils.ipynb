{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp utils.__init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility\n",
    "> A collection of utility methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import torch, random, gc\n",
    "import numpy as np\n",
    "from fastai.learner import Learner\n",
    "from fastai.data.core import DataLoaders\n",
    "from fastcore.foundation import contextmanager\n",
    "from fastai.callback.core import set_random_states, get_random_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def free_gpu_memory(learn:Learner, dls:DataLoaders=None):\n",
    "    \"Frees GPU memory using `gc.collect` and `torch.cuda.empty_cache`\"\n",
    "    learn.dls, learn, dls = None, None, None\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@contextmanager\n",
    "def less_random(seed=42, deterministic=True):\n",
    "    \"Stores and retrieves state of random number generators. Sets random seed for `random`, `torch`, and `numpy`. Does not set `torch.backends.cudnn.benchmark = False`\"\n",
    "    states = get_random_states()\n",
    "    \n",
    "    try: torch.manual_seed(seed)\n",
    "    except NameError: pass\n",
    "    try: torch.cuda.manual_seed_all(seed)\n",
    "    except NameError: pass\n",
    "    try: np.random.seed(seed%(2**32-1))\n",
    "    except NameError: pass\n",
    "    random.seed(seed)\n",
    "    if deterministic:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    try:\n",
    "        yield #we are managing global variables\n",
    "    finally:\n",
    "        set_random_states(**states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random state manager which provides some reproducibility without sacrificing potential training speed.\n",
    "\n",
    "Unlike fast.ai's [`no_random`](https://docs.fast.ai/torch_core.html#no_random), `less_random` does not set `torch.backends.cudnn.benchmark = False` so it's possible to train faster. Training runs on the same GPU, PyTorch, & CUDA setup should be reproducible, but different hardware/software setup will probably have less reproducibility then using `no_random`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "# modified from https://github.com/thomasbrandon/mish-cuda/blob/master/test/perftest.py\n",
    "def scale_time(val:float, spec:str=\"#0.4G\"):\n",
    "    \"Scale fractional second `time` values and return formatted to `spec`\"\n",
    "    if val == 0: return '-'\n",
    "    PREFIXES = np.array([c for c in u\"yzafpnÂµm kMGTPEZY\"])\n",
    "    exp = np.int8(np.log10(np.abs(val)) // 3 * 3 * np.sign(val))\n",
    "    val /= 10.**exp\n",
    "    prefix = PREFIXES[exp//3 + len(PREFIXES)//2]\n",
    "    return f\"{val:{spec}}{prefix}s\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
