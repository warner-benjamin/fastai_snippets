{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "    - optimizer.fused.html\n",
    "    - optimizer.foreach.html\n",
    "    - optimizer.torchscript.html\n",
    "    - optimizer.adan.html\n",
    "    - optimizer.lion.html\n",
    "    - optimizer.stableadam.html\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp optimizer.optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "# Contains code from:\n",
    "# fastai - Apache License 2.0 - Copyright (c) 2023 fast.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "> Fast fastai optimizers with optimi low precision and bitsandbytes 8-bit implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "documentation placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "\n",
    "from fastcore.basics import partialler\n",
    "\n",
    "from fastai.optimizer import (Optimizer, weight_decay, l2_reg, average_grad, average_sqr_grad,\n",
    "                               step_stat, qhadam_step, larc_layer_lr, larc_step, lamb_step, rms_prop_step)\n",
    "\n",
    "try:\n",
    "    from fastxtend.optimizer.optimi import (AdamOptimiOptimizer, AdanOptimiOptimizer, LionOptimiOptimizer,\n",
    "                                            RAdamOptimiOptimizer, RangerOptimiOptimizer, SGDOptimiOptimizer,\n",
    "                                            StableAdamWOptimiOptimizer)\n",
    "    OPTIMI = True\n",
    "except ImportError:\n",
    "    OPTIMI = False\n",
    "\n",
    "try:\n",
    "    from packaging.version import parse\n",
    "    import bitsandbytes\n",
    "    from fastxtend.optimizer.eightbit import (SGD8bitOptimizer, RMSProp8bitOptimizer, AdamW8bitOptimizer,\n",
    "                                              LARS8bitOptimizer, LAMB8bitOptimizer, Lion8bitOptimizer)\n",
    "    EIGHTBIT = True\n",
    "except ImportError:\n",
    "    EIGHTBIT = False\n",
    "\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimi and Eight-bit Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These optimizers support both optimi's low precision and bitsandbytes' eight-bit implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def Adam(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    mom:float=0.9, # Gradient moving average (β1) coefficient\n",
    "    sqr_mom:float=0.99, # Gradient squared moving average (β2) coefficient\n",
    "    eps:float=1e-5, # Added for numerical stability\n",
    "    wd:float=0.01, # Optional weight decay\n",
    "    decouple_wd:bool=True, # Apply decoupled weight decay (AdamW) instead of L2 penalty (Adam)\n",
    "    decouple_lr:bool=False, # Apply fully decoupled weight decay (AdamW) instead of L2 penalty (Adam). Unsupported for `eightbit=True`.\n",
    "    kahan_sum:bool|None=None, # More accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters. Unsupported for `eightbit=True`. (float16 or bfloat16). If unspecified, automatically applies for low precision parameters. Unsupported for `eightbit=True`.\n",
    "    foreach:bool|None=None, # Use ForEach implementation. If unspecified, tries to use foreach over for-loop implementation.\n",
    "    eightbit:bool=False, # Use bitsandbytes' eight-bit implementation instead of optimi's implementation.\n",
    "    **eightbitargs # Additional eight-bit arguments. See `AdamW8bitOptimizer` for details.\n",
    ") -> AdamOptimiOptimizer|AdamW8bitOptimizer:\n",
    "    \"A fastai Adam/AdamW optimizer with low precision, foreach, and eight-bit implementations\"\n",
    "\n",
    "    if not eightbit:\n",
    "        if OPTIMI:\n",
    "            return AdamOptimiOptimizer(params, lr=lr, mom=mom, sqr_mom=sqr_mom, wd=wd,\n",
    "                                       eps=eps, decouple_wd=decouple_wd, decouple_lr=decouple_lr,\n",
    "                                       kahan_sum=kahan_sum, foreach=foreach)\n",
    "        else:\n",
    "            raise ImportError('optimi package not found. Run `pip install torch-optimi`.')\n",
    "    else:\n",
    "        if EIGHTBIT:\n",
    "            if (not decouple_wd and wd > 0) or (decouple_lr and wd > 0):\n",
    "                raise NotImplementedError(f'Eight-bit Adam only supports decoupled weight decay: {decouple_wd=}')\n",
    "            return AdamW8bitOptimizer(params, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd, **eightbitargs)\n",
    "        else:\n",
    "            raise ImportError(f'{eightbit=}. bitsandbytes package not found. Run `pip install bitsandbytes`.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def adam(\n",
    "    mom:float=0.9, # Gradient moving average (β1) coefficient\n",
    "    sqr_mom:float=0.99, # Gradient squared moving average (β2) coefficient\n",
    "    eps:float=1e-5, # Added for numerical stability\n",
    "    wd:float=0.01, # Optional weight decay\n",
    "    decouple_wd:bool=True, # Apply decoupled weight decay (AdamW) instead of L2 penalty (Adam)\n",
    "    decouple_lr:bool=False, # Apply fully decoupled weight decay (AdamW) instead of L2 penalty (Adam). Unsupported for `eightbit=True`.\n",
    "    kahan_sum:bool|None=None, # More accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters. Unsupported for `eightbit=True`. (float16 or bfloat16). If unspecified, automatically applies for low precision parameters. Unsupported for `eightbit=True`.\n",
    "    foreach:bool|None=None, # Use faster ForEach implementation. If unspecified, tries to use foreach over for-loop implementation.\n",
    "    eightbit:bool=False, # Use bitsandbytes' eight-bit implementation instead of optimi's implementation.\n",
    "    **eightbitargs # Additional eight-bit arguments. See `AdamW8bitOptimizer` for details.\n",
    ") -> AdamOptimiOptimizer|AdamW8bitOptimizer:\n",
    "    \"A fastai-compatible Adam/AdamW optimizer with low precision, foreach, and eight-bit implementations\"\n",
    "    return partialler(Adam, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd, decouple_wd=decouple_wd,\n",
    "                      decouple_lr=decouple_lr, kahan_sum=kahan_sum, foreach=foreach,\n",
    "                      eightbit=eightbit, **eightbitargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lion -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def Lion(\n",
    "    params: Listified[Tensor],  # Model parameters or parameter groups\n",
    "    lr: float,  # Default learning rate\n",
    "    beta1: float = 0.9,  # Update gradient moving average (β1) coefficient\n",
    "    beta2: float = 0.99,  # Gradient moving average (β2) coefficient\n",
    "    wd: float = 0.1,  # Decoupled weight decay\n",
    "    decouple_lr: bool = False,  # Apply fully decoupled weight decay\n",
    "    kahan_sum: bool | None = None,  # More accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters. Unsupported for `eightbit=True`.\n",
    "    foreach: bool | None = None,  # Use faster ForEach implementation. If unspecified, tries to use foreach over for-loop implementation.\n",
    "    eightbit: bool = False,  # Use bitsandbytes' eight-bit implementation\n",
    "    **eightbitargs  # Additional eight-bit arguments\n",
    ") -> LionOptimiOptimizer | Lion8bitOptimizer:\n",
    "    \"A fastai-compatible Lion optimizer with low precision, foreach, and eight-bit implementations\"\n",
    "\n",
    "    if not eightbit:\n",
    "        if OPTIMI:\n",
    "            return LionOptimiOptimizer(params, lr=lr, beta1=beta1, beta2=beta2, wd=wd,\n",
    "                                       decouple_lr=decouple_lr, kahan_sum=kahan_sum, foreach=foreach)\n",
    "        else:\n",
    "            raise ImportError('optimi package not found. Run `pip install torch-optimi`.')\n",
    "    else:\n",
    "        if EIGHTBIT:\n",
    "            if decouple_lr and wd > 0:\n",
    "                raise NotImplementedError('Eight-bit Lion only supports decoupled weight decay.')\n",
    "            return Lion8bitOptimizer(params, lr=lr, beta1=beta1, beta2=beta2, wd=wd, **eightbitargs)\n",
    "        else:\n",
    "            raise ImportError(f'{eightbit=}. bitsandbytes package not found. Run `pip install bitsandbytes`.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def lion(\n",
    "    beta1: float = 0.9,  # Update gradient moving average (β1) coefficient\n",
    "    beta2: float = 0.99,  # Gradient moving average (β2) coefficient\n",
    "    wd: float = 0.1,  # Decoupled weight decay\n",
    "    decouple_lr: bool = False,  # Apply fully decoupled weight decay\n",
    "    kahan_sum: bool | None = None,  # More accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters. Unsupported for `eightbit=True`.\n",
    "    foreach: bool | None = None,  # Use faster ForEach implementation. If unspecified, tries to use foreach over for-loop implementation.\n",
    "    eightbit: bool = False,  # Use bitsandbytes' eight-bit implementation\n",
    "    **eightbitargs  # Additional eight-bit arguments\n",
    ") -> LionOptimiOptimizer | Lion8bitOptimizer:\n",
    "    \"A partial function for the Lion optimizer with low precision, foreach, and eight-bit implementations\"\n",
    "    return partialler(Lion, beta1=beta1, beta2=beta2, wd=wd, decouple_lr=decouple_lr,\n",
    "                      kahan_sum=kahan_sum, foreach=foreach, eightbit=eightbit, **eightbitargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def SGD(\n",
    "    params: Listified[Tensor],  # Model parameters or parameter groups\n",
    "    lr: float,  # Default learning rate\n",
    "    mom: float = 0.,  # Gradient moving average (β1) coefficient\n",
    "    wd: float = 0.,  # Optional weight decay (decoupled or L2)\n",
    "    decouple_wd: bool = True,  # Apply decoupled weight decay (SGDW) or L2 regularization (SGD)\n",
    "    decouple_lr: bool = False,  # Apply fully decoupled weight decay\n",
    "    kahan_sum: bool | None = None,  # More accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters. Unsupported for `eightbit=True`.\n",
    "    foreach: bool | None = None,  # Use faster ForEach implementation. If unspecified, tries to use foreach over for-loop implementation.\n",
    "    eightbit: bool = False,  # Use bitsandbytes' eight-bit implementation\n",
    "    **eightbitargs  # Additional eight-bit arguments\n",
    ") -> SGDOptimiOptimizer | SGD8bitOptimizer:\n",
    "    \"A fastai-compatible SGD optimizer with low precision, foreach, and eight-bit implementations\"\n",
    "\n",
    "    if not eightbit:\n",
    "        if OPTIMI:\n",
    "            return SGDOptimiOptimizer(params, lr=lr, mom=mom, wd=wd, decouple_wd=decouple_wd,\n",
    "                                      decouple_lr=decouple_lr, kahan_sum=kahan_sum, foreach=foreach)\n",
    "        else:\n",
    "            raise ImportError('optimi package not found. Run `pip install torch-optimi`.')\n",
    "    else:\n",
    "        if EIGHTBIT:\n",
    "            if decouple_wd and wd > 0:\n",
    "                raise NotImplementedError('Eight-bit SGD only supports L2 weight decay.')\n",
    "            return SGD8bitOptimizer(params, lr=lr, mom=mom, wd=wd, **eightbitargs)\n",
    "        else:\n",
    "            raise ImportError(f'{eightbit=}. bitsandbytes package not found. Run `pip install bitsandbytes`.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def sgd(\n",
    "    mom: float = 0.,  # Gradient moving average (β1) coefficient\n",
    "    wd: float = 0.,  # Optional weight decay (decoupled or L2)\n",
    "    decouple_wd: bool = True,  # Apply decoupled weight decay (SGDW) or L2 regularization (SGD)\n",
    "    decouple_lr: bool = False,  # Apply fully decoupled weight decay\n",
    "    kahan_sum: bool | None = None,  # More accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters. Unsupported for `eightbit=True`.\n",
    "    foreach: bool | None = None,  # Use faster ForEach implementation. If unspecified, tries to use foreach over for-loop implementation.\n",
    "    eightbit: bool = False,  # Use bitsandbytes' eight-bit implementation\n",
    "    **eightbitargs  # Additional eight-bit arguments\n",
    ") -> SGDOptimiOptimizer | SGD8bitOptimizer:\n",
    "    \"Partial function for the SGD optimizer with low precision, foreach, and eight-bit implementations\"\n",
    "    return partialler(SGD, mom=mom, wd=wd, decouple_wd=decouple_wd, decouple_lr=decouple_lr,\n",
    "                      kahan_sum=kahan_sum, foreach=foreach, eightbit=eightbit, **eightbitargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimi-only Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These optimizers only use the optimi implementation and do not have an eight-bit version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adan -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def Adan(\n",
    "    params: Listified[Tensor],  # Model parameters or parameter groups\n",
    "    lr: float,  # Default learning rate\n",
    "    beta1: float = 0.98,  # Gradient moving average (β1) coefficient\n",
    "    beta2: float = 0.92,  # Gradient difference moving average (β2) coefficient\n",
    "    beta3: float = 0.99,  # Gradient squared moving average (β3) coefficient\n",
    "    eps: float = 1e-8,  # Added for numerical stability\n",
    "    wd: float = 0.02,  # Decoupled weight decay\n",
    "    decouple_lr: bool = False,  # Apply fully decoupled weight decay\n",
    "    adam_wd: bool = False,  # Apply weight decay before parameter update (Adam-style), instead of after the update per Adan algorithm\n",
    "    kahan_sum: bool | None = None,  # More accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters.\n",
    "    foreach: bool | None = None,  # Use faster ForEach implementation. If unspecified, tries to use foreach over for-loop implementation.\n",
    ") -> AdanOptimiOptimizer:\n",
    "    \"A fastai-compatible Adan optimizer with low precision and foreach implementations\"\n",
    "    if OPTIMI:\n",
    "        return AdanOptimiOptimizer(params, lr=lr, beta1=beta1, beta2=beta2, beta3=beta3, wd=wd,\n",
    "                                   eps=eps, decouple_lr=decouple_lr, adam_wd=adam_wd,\n",
    "                                   kahan_sum=kahan_sum, foreach=foreach)\n",
    "    else:\n",
    "        raise ImportError('optimi package not found. Run `pip install torch-optimi`.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def adan(\n",
    "    beta1: float = 0.98,  # Gradient moving average (β1) coefficient\n",
    "    beta2: float = 0.92,  # Gradient difference moving average (β2) coefficient\n",
    "    beta3: float = 0.99,  # Gradient squared moving average (β3) coefficient\n",
    "    eps: float = 1e-8,  # Added for numerical stability\n",
    "    wd: float = 0.02,  # Decoupled weight decay\n",
    "    decouple_lr: bool = False,  # Apply fully decoupled weight decay\n",
    "    adam_wd: bool = False,  # Apply weight decay before parameter update (Adam-style), instead of after the update per Adan algorithm\n",
    "    kahan_sum: bool | None = None,  # More accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters.\n",
    "    foreach: bool | None = None,  # Use faster ForEach implementation. If unspecified, tries to use foreach over for-loop implementation.\n",
    ") -> AdanOptimiOptimizer:\n",
    "    \"A partial function for the Adan optimizer with low precision and foreach implementations\"\n",
    "    return partialler(Adan, beta1=beta1, beta2=beta2, beta3=beta3, eps=eps, wd=wd,\n",
    "                      decouple_lr=decouple_lr, adam_wd=adam_wd, kahan_sum=kahan_sum,\n",
    "                      foreach=foreach)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAdam -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def RAdam(\n",
    "    params: Listified[Tensor],  # Model parameters or parameter groups\n",
    "    lr: float,  # Default learning rate\n",
    "    mom: float = 0.9,  # Gradient moving average (β1) coefficient\n",
    "    sqr_mom: float = 0.99,  # Gradient squared moving average (β2) coefficient\n",
    "    eps: float = 1e-5,  # Added for numerical stability\n",
    "    wd: float = 0.,  # Optional weight decay (decoupled or L2)\n",
    "    decouple_wd: bool = True,  # Apply decoupled weight decay (RAdamW) or L2 regularization (RAdam)\n",
    "    decouple_lr: bool = False,  # Apply fully decoupled weight decay\n",
    "    kahan_sum: bool | None = None,  # More accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters.\n",
    "    foreach: bool | None = None,  # Use faster ForEach implementation. If unspecified, tries to use foreach over for-loop implementation.\n",
    ") -> RAdamOptimiOptimizer:\n",
    "    \"A fastai-compatible RAdam optimizer with low precision and foreach implementations\"\n",
    "\n",
    "    if OPTIMI:\n",
    "        return RAdamOptimiOptimizer(params, lr=lr, mom=mom, sqr_mom=sqr_mom, wd=wd, eps=eps,\n",
    "                                    decouple_wd=decouple_wd, decouple_lr=decouple_lr,\n",
    "                                    kahan_sum=kahan_sum, foreach=foreach)\n",
    "    else:\n",
    "        raise ImportError('optimi package not found. Run `pip install torch-optimi`.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def radam(\n",
    "    mom: float = 0.9,  # Gradient moving average (β1) coefficient\n",
    "    sqr_mom: float = 0.99,  # Gradient squared moving average (β2) coefficient\n",
    "    eps: float = 1e-5,  # Added for numerical stability\n",
    "    wd: float = 0.,  # Optional weight decay (decoupled or L2)\n",
    "    decouple_wd: bool = True,  # Apply decoupled weight decay (RAdamW) or L2 regularization (RAdam)\n",
    "    decouple_lr: bool = False,  # Apply fully decoupled weight decay\n",
    "    kahan_sum: bool | None = None,  # More accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters.\n",
    "    foreach: bool | None = None,  # Use faster ForEach implementation. If unspecified, tries to use foreach over for-loop implementation.\n",
    ") -> RAdamOptimiOptimizer:\n",
    "    \"Partial function for the RAdam optimizer with low precision and foreach implementations\"\n",
    "    return partialler(RAdam, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd, decouple_wd=decouple_wd,\n",
    "                      decouple_lr=decouple_lr, kahan_sum=kahan_sum, foreach=foreach)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranger -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def Ranger(\n",
    "    params: Listified[Tensor],  # Model parameters or parameter groups\n",
    "    lr: float,  # Default learning rate\n",
    "    mom: float = 0.95,  # Gradient moving average (β1) coefficient\n",
    "    sqr_mom: float = 0.99,  # Gradient squared moving average (β2) coefficient\n",
    "    eps: float = 1e-6,  # Added for numerical stability\n",
    "    wd: float = 0.01,  # Optional weight decay (decoupled or L2)\n",
    "    k: int = 6,  # How often to conduct Lookahead step\n",
    "    alpha: float = 0.5,  # Slow weight moving average coefficient\n",
    "    decouple_wd: bool = True,  # Apply decoupled weight decay (RangerW) or L2 regularization (Ranger)\n",
    "    decouple_lr: bool = False,  # Apply fully decoupled weight decay\n",
    "    kahan_sum: bool | None = None,  # More accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters. Unsupported for `eightbit=True`.\n",
    "    foreach: bool | None = None  # Use faster ForEach implementation. If unspecified, tries to use foreach over for-loop implementation.\n",
    ") -> RangerOptimiOptimizer:\n",
    "    \"Convenience method for `Lookahead` with `RAdam` with low precision and foreach implementations\"\n",
    "\n",
    "    if OPTIMI:\n",
    "        return RangerOptimiOptimizer(params, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd,\n",
    "                                     k=k, alpha=alpha, decouple_wd=decouple_wd, decouple_lr=decouple_lr,\n",
    "                                     kahan_sum=kahan_sum, foreach=foreach)\n",
    "    else:\n",
    "        raise ImportError('optimi package not found. Run `pip install torch-optimi`.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def ranger(\n",
    "    mom: float = 0.95,  # Gradient moving average (β1) coefficient\n",
    "    sqr_mom: float = 0.99,  # Gradient squared moving average (β2) coefficient\n",
    "    eps: float = 1e-6,  # Added for numerical stability\n",
    "    wd: float = 0.01,  # Optional weight decay (decoupled or L2)\n",
    "    k: int = 6,  # How often to conduct Lookahead step\n",
    "    alpha: float = 0.5,  # Slow weight moving average coefficient\n",
    "    decouple_wd: bool = True,  # Apply decoupled weight decay (RangerW) or L2 regularization (Ranger)\n",
    "    decouple_lr: bool = False,  # Apply fully decoupled weight decay\n",
    "    kahan_sum: bool | None = None,  # More accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters. Unsupported for `eightbit=True`.\n",
    "    foreach: bool | None = None  # Use faster ForEach implementation. If unspecified, tries to use foreach over for-loop implementation.\n",
    ") -> RangerOptimiOptimizer:\n",
    "    \"Partial function for the Ranger optimizer using RAdam with low precision and foreach implementations\"\n",
    "    return partialler(Ranger, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd, k=k,\n",
    "                      alpha=alpha, decouple_wd=decouple_wd, decouple_lr=decouple_lr,\n",
    "                      kahan_sum=kahan_sum, foreach=foreach)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StableAdamW -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def StableAdamW(\n",
    "    params: Listified[Tensor],  # Model parameters or parameter groups\n",
    "    lr: float,  # Default learning rate\n",
    "    mom: float = 0.9,  # Gradient moving average (β1) coefficient\n",
    "    sqr_mom: float = 0.99,  # Gradient squared moving average (β2) coefficient\n",
    "    eps: float = 1e-5,  # Added for numerical stability\n",
    "    wd: float = 0.01,  # Optional weight decay\n",
    "    decouple_lr: bool = False,  # Apply fully decoupled weight decay\n",
    "    kahan_sum: bool | None = None,  # More accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters. Unsupported for `eightbit=True`.\n",
    "    foreach: bool | None = None,  # Use faster ForEach implementation. If unspecified, tries to use foreach over for-loop implementation.\n",
    ") -> StableAdamWOptimiOptimizer:\n",
    "    \"A fastai-compatible StableAdamW optimizer with low precision and foreach implementations\"\n",
    "\n",
    "    if OPTIMI:\n",
    "        return StableAdamWOptimiOptimizer(params, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd,\n",
    "                                          decouple_lr=decouple_lr, kahan_sum=kahan_sum, foreach=foreach)\n",
    "    else:\n",
    "        raise ImportError('optimi package not found. Run `pip install torch-optimi`.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def stableadamw(\n",
    "    mom: float = 0.9,  # Gradient moving average (β1) coefficient\n",
    "    sqr_mom: float = 0.99,  # Gradient squared moving average (β2) coefficient\n",
    "    eps: float = 1e-5,  # Added for numerical stability\n",
    "    wd: float = 0.01,  # Optional weight decay (decoupled or L2)\n",
    "    decouple_lr: bool = False,  # Apply fully decoupled weight decay\n",
    "    kahan_sum: bool | None = None,  # More accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters. Unsupported for `eightbit=True`.\n",
    "    foreach: bool | None = None,  # Use faster ForEach implementation. If unspecified, tries to use foreach over for-loop implementation.\n",
    ") -> StableAdamWOptimiOptimizer:\n",
    "    \"Partial function for the StableAdamW optimizer with low precision and foreach implementations\"\n",
    "    return partialler(StableAdamW, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd,\n",
    "                      decouple_lr=decouple_lr, kahan_sum=kahan_sum, foreach=foreach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastai with Eight-bit Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These optimizers either use the fastai or bitsandbytes eight-bit implemenations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Larc -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def Larc(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    mom:float=0.9, # Gradient moving average (β1) coefficient\n",
    "    clip:bool=True, # LARC if clip=True, LARS if clip=False\n",
    "    trust_coeff:float=0.02, # Trust coeffiecnet for calculating layerwise LR\n",
    "    eps:float=1e-8, # Added for numerical stability\n",
    "    wd:float=0., # Optional weight decay (decoupled or L2)\n",
    "    decouple_wd:bool=True, # Apply decoupled weight decay or L2 regularization. Ignored if `eightbit=True`\n",
    "    eightbit:bool=False, # Use fused 8-bit implementation. Only supports LARS: `clip=False`\n",
    "    hide_warning:bool=False, # Hide warning\n",
    "    **eightbitargs\n",
    ") -> Optimizer|LARS8bitOptimizer:\n",
    "    \"A fastai LARC/LARS optimizer with eight-bit implementations\"\n",
    "    if eightbit:\n",
    "        if EIGHTBIT:\n",
    "            if clip:\n",
    "                raise NotImplementedError(f'{eightbit=} only supports the LARS optimizer. Set `clip=False`.')\n",
    "            if decouple_wd and wd > 0:\n",
    "                raise NotImplementedError(f'8-bit LARS only supports L2 weight decay: {decouple_wd=}')\n",
    "            return LARS8bitOptimizer(params, lr=lr, mom=mom, wd=wd, trust_coeff=trust_coeff, **eightbitargs)\n",
    "        else:\n",
    "            raise ImportError(f'{eightbit=}. bitsandbytes package not found. Run `pip install bitsandbytes`.')\n",
    "    else:\n",
    "        if not hide_warning:\n",
    "            warn(\"fastxtend doesn't have a non-eight-bit Lamb implementation, using the\"\n",
    "                 \" fastai implementation. Pass `hide_warning=True` to hide this message.\")\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        if mom!=0.: cbs.append(average_grad)\n",
    "        cbs += [partial(larc_layer_lr, clip=clip), larc_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, trust_coeff=trust_coeff, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def larc(\n",
    "    mom:float=0.9, # Gradient moving average (β1) coefficient\n",
    "    clip:bool=True, # LARC if clip=True, LARS if clip=False\n",
    "    trust_coeff:float=0.02, # Trust coeffiecnet for calculating layerwise LR\n",
    "    eps:float=1e-8, # Added for numerical stability\n",
    "    wd:float=0., # Optional weight decay (decoupled or L2)\n",
    "    decouple_wd:bool=True, # Apply decoupled weight decay or L2 regularization\n",
    "    eightbit:bool=False, # Use fused 8-bit implementation. Only supports LARS\n",
    "    hide_warning:bool=False, # Hide warning\n",
    "    **eightbitargs\n",
    ") -> Optimizer|LARS8bitOptimizer:\n",
    "    \"Partial function for the LARC/LARS optimizer with fused TorchScript & 8-bit implementations\"\n",
    "    return partialler(Larc, mom=mom, clip=clip, eps=eps, trust_coeff=trust_coeff,\n",
    "                      wd=wd, decouple_wd=decouple_wd, eightbit=eightbit, hide_warning=hide_warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lamb -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def Lamb(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    mom:float=0.9, # Gradient moving average (β1) coefficient\n",
    "    sqr_mom:float=0.99, # Gradient squared moving average (β2) coefficient\n",
    "    eps:float=1e-5, # Added for numerical stability\n",
    "    wd:float=0., # Optional weight decay (decoupled or L2)\n",
    "    decouple_wd:bool=True, # Apply decoupled weight decay or L2 regularization. Ignored if `eightbit=True`\n",
    "    eightbit:bool=False, # Use fused 8-bit implementation. Only supports Decoupled weight decay\n",
    "    hide_warning:bool=False, # Hide warning\n",
    "    **eightbitargs\n",
    ") -> Optimizer|LAMB8bitOptimizer:\n",
    "    \"A fastai LAMB optimizer with fused ForEach, TorchScript, & 8-bit implementations\"\n",
    "    if eightbit:\n",
    "        if EIGHTBIT:\n",
    "            if parse(bitsandbytes.__version__) <= parse('0.43.1') and not hide_warning:\n",
    "                raise ValueError(\"8-bit LAMB in bitsandbytes will error out weights too small to quantize. \"\n",
    "                                 \"Pass `hide_warning=True` to ignore and use anyway.\")\n",
    "            if not decouple_wd and wd > 0:\n",
    "                raise NotImplementedError(f'8-bit LAMB only supports Decoupled weight decay: {decouple_wd=}')\n",
    "            return LAMB8bitOptimizer(params, lr=lr, mom=mom, sqr_mom=sqr_mom,\n",
    "                                     eps=eps, wd=wd, **eightbitargs)\n",
    "        else:\n",
    "            raise ImportError(f'{eightbit=}. bitsandbytes package not found. Run `pip install bitsandbytes`.')\n",
    "    else:\n",
    "        if not hide_warning:\n",
    "            warn(\"fastxtend doesn't have a non-eight-bit Lamb implementation, using the\"\n",
    "                 \" fastai implementation. Pass `hide_warning=True` to hide this message.\")\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, lamb_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def lamb(\n",
    "    mom:float=0.9, # Gradient moving average (β1) coefficient\n",
    "    sqr_mom:float=0.99, # Gradient squared moving average (β2) coefficient\n",
    "    eps:float=1e-5, # Added for numerical stability\n",
    "    wd:float=0., # Optional weight decay (decoupled or L2)\n",
    "    decouple_wd:bool=True, # Apply decoupled weight decay or L2 regularization\n",
    "    eightbit:bool=False, # Use fused 8-bit implementation. Only supports Decoupled weight decay\n",
    "    hide_warning:bool=False, # Hide warning\n",
    "    **eightbitargs\n",
    ") -> Optimizer|LAMB8bitOptimizer:\n",
    "    \"Partial function for the LAMB optimizer with fused ForEach, TorchScript, & 8-bit implementations\"\n",
    "    return partialler(Lamb, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd, decouple_wd=decouple_wd,\n",
    "                      eightbit=eightbit, hide_warning=hide_warning, **eightbitargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def RMSProp(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    mom:float=0., # Gradient moving average (β1) coefficient\n",
    "    sqr_mom:float=0.99, # Gradient squared moving average (β2) coefficient\n",
    "    eps:float=1e-8, # Added for numerical stability\n",
    "    wd:float=0., # Optional weight decay (true or L2)\n",
    "    decouple_wd:bool=True, # Apply true weight decay or L2 regularization. Ignored if `eightbit=True`\n",
    "    eightbit:bool=False, # Use fused 8-bit implementation. Only supports Decoupled weight decay\n",
    "    hide_warning:bool=False, # Hide warning\n",
    "    **eightbitargs\n",
    ") -> Optimizer|RMSProp8bitOptimizer:\n",
    "    \"A fastai RMSProp/RMSPropW optimizer with fused TorchScript and 8-bit implementations\"\n",
    "    if eightbit:\n",
    "        if EIGHTBIT:\n",
    "            if decouple_wd and wd > 0:\n",
    "                raise NotImplementedError(f'8-bit RMSProp only supports L2 weight decay: {decouple_wd=}')\n",
    "            if mom > 0:\n",
    "                raise NotImplementedError(f'8-bit RMSProp does not use momentum: {mom=}')\n",
    "            return RMSProp8bitOptimizer(params, lr=lr, sqr_mom=sqr_mom, eps=eps, wd=wd, **eightbitargs)\n",
    "        else:\n",
    "            raise ImportError(f'{eightbit=}. bitsandbytes package not found. Run `pip install bitsandbytes`.')\n",
    "    else:\n",
    "        if not hide_warning:\n",
    "            warn(\"fastxtend doesn't have a non-eight-bit RMSProp implementation, using the\"\n",
    "                 \" fastai implementation. Pass `hide_warning=True` to hide this message.\")\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        cbs += ([average_sqr_grad] if mom==0. else [average_grad, average_sqr_grad])\n",
    "        cbs.append(rms_prop_step)\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, wd=wd, eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def rmsprop(\n",
    "    mom:float=0., # Gradient moving average (β1) coefficient\n",
    "    sqr_mom:float=0.99, # Gradient squared moving average (β2) coefficient\n",
    "    eps:float=1e-8, # Added for numerical stability\n",
    "    wd:float=0., # Optional weight decay (decoupled or L2)\n",
    "    decouple_wd:bool=True, # Apply decoupled weight decay or L2 regularization\n",
    "    eightbit:bool=False, # Use fused 8-bit implementation\n",
    "    hide_warning:bool=False, # Hide warning\n",
    "    **eightbitargs\n",
    ") -> Optimizer|RMSProp8bitOptimizer:\n",
    "    \"Partial function for the RMSProp/RMSPropW optimizer with fused TorchScript and 8-bit implementations\"\n",
    "    return partialler(RMSProp, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd,\n",
    "                      decouple_wd=decouple_wd, eightbit=eightbit,\n",
    "                      hide_warning=hide_warning, **eightbitargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastai-only Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This optimizer only has a fastai implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def QHAdam(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    mom:float=0.999, # Gradient moving average (β1) coefficient\n",
    "    sqr_mom:float=0.999, # Gradient squared moving average (β2) coefficient\n",
    "    nu_1:float=0.7, # QH immediate discount factor\n",
    "    nu_2:float=1.0, # QH momentum discount factor\n",
    "    eps:float=1e-8, # Added for numerical stability\n",
    "    wd:float=0., # Optional weight decay (decoupled or L2)\n",
    "    decouple_wd:bool=True, # Apply decoupled weight decay (QHAdamW) or L2 regularization (QHAdam)\n",
    "    hide_warning:bool=False, # Hide warning\n",
    ") -> Optimizer:\n",
    "    \"The fastai QHAdam/QHAdamW optimizer\"\n",
    "    if not hide_warning:\n",
    "        warn(\"fastxtend doesn't have a QHAdam implementation, using the fastai\"\n",
    "             \" implementation. Pass `hide_warning=True` to hide this message.\")\n",
    "    cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "    cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, qhadam_step]\n",
    "    return Optimizer(params, cbs, lr=lr, nu_1=nu_1, nu_2=nu_2, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def qhadam(\n",
    "    mom:float=0.999, # Gradient moving average (β1) coefficient\n",
    "    sqr_mom:float=0.999, # Gradient squared moving average (β2) coefficient\n",
    "    nu_1:float=0.7, # QH immediate discount factor\n",
    "    nu_2:float=1.0, # QH momentum discount factor\n",
    "    eps:float=1e-8, # Added for numerical stability\n",
    "    wd:float=0., # Optional weight decay (decoupled or L2)\n",
    "    decouple_wd:bool=True, # Apply decoupled weight decay (QHAdamW) or L2 regularization (QHAdam)\n",
    "    hide_warning:bool=False, # Hide warning\n",
    ") -> Optimizer:\n",
    "    \"Partial function for the fastai QHAdam/QHAdamW optimizer\"\n",
    "    return partialler(QHAdam, mom=mom, sqr_mom=sqr_mom, nu_1=nu_1, nu_2=nu_2, eps=eps,\n",
    "                      wd=wd, decouple_wd=decouple_wd, hide_warning=hide_warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "import inspect\n",
    "from itertools import product\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from fastai.basics import default_device\n",
    "from fastai.data.core import TfmdDL, DataLoaders\n",
    "from fastai.learner import Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, device, dtype):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size, bias=True, device=device, dtype=dtype)\n",
    "        self.act = torch.nn.Mish()\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, 1, bias=False, device=device, dtype=dtype)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.norm(self.act(self.fc1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "def synth_dbunch(a=2, b=3, bs=16, n_train=10, n_valid=2, cuda=False):\n",
    "    def get_data(n):\n",
    "        x = torch.randn(bs*n, 256)\n",
    "        return TensorDataset(x, a*x + b + 0.1*torch.randn(bs*n, 1))\n",
    "    train_ds = get_data(n_train)\n",
    "    valid_ds = get_data(n_valid)\n",
    "    device = default_device() if cuda else None\n",
    "    train_dl = TfmdDL(train_ds, bs=bs, shuffle=True, num_workers=0)\n",
    "    valid_dl = TfmdDL(valid_ds, bs=bs, num_workers=0)\n",
    "    return DataLoaders(train_dl, valid_dl, device=device)\n",
    "\n",
    "\n",
    "def synth_learner(n_trn=10, n_val=2, optimizer=sgd, cuda=False, lr=1e-3, **kwargs):\n",
    "    data=synth_dbunch(n_train=n_trn, n_valid=n_val, cuda=cuda)\n",
    "    model=MLP(256, 512, device=default_device() if cuda else None, dtype=torch.float32)\n",
    "    return Learner(data, model, lr=lr, opt_func=optimizer, loss_func=nn.MSELoss(), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "def dict_product(params):\n",
    "    keys = params.keys()\n",
    "    for combination in product(*params.values()):\n",
    "        yield dict(zip(keys, combination))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "def filter_arguments(optimizer, args):\n",
    "    optimizer_args = inspect.signature(optimizer).parameters\n",
    "    return {k: v for k, v in args.items() if k in optimizer_args}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "\n",
    "# Since all of these optimziers have consistency tests in their respective libaries\n",
    "# and most in their notebooks, this test makes sure the integration works without errors.\n",
    "def test_optimizers(optimizers, eightbit, optimi):\n",
    "    params = {\n",
    "        'wd': [0, 1e-2],\n",
    "        'decouple_wd': [True, False],\n",
    "        'decouple_lr': [True, False],\n",
    "        'foreach': [True, False],\n",
    "        'eightbit': [True, False]\n",
    "    }\n",
    "    if not eightbit:\n",
    "        params.pop('eightbit')\n",
    "    if not optimi:\n",
    "        params.pop('foreach')\n",
    "    for optimizer in optimizers:\n",
    "        for args in dict_product(params):\n",
    "            if args.get('eightbit', False) and (args['decouple_lr'] or args.get('foreach', False)):\n",
    "                pass\n",
    "            opt = optimizer(**filter_arguments(optimizer, args))\n",
    "            learn = synth_learner(optimizer=opt, cuda=torch.cuda.is_available())\n",
    "            try:\n",
    "                with learn.no_logging():\n",
    "                    learn.fit(5)\n",
    "                    if args.get('eightbit', False):\n",
    "                        assert learn.opt.state[next(learn.model.parameters())]['state1'].dtype == torch.uint8\n",
    "            except NotImplementedError:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "test_optimizers((adam, lion, sgd), eightbit=True, optimi=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "test_optimizers((adan, radam, ranger, stableadamw), eightbit=False, optimi=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "\n",
    "# lamb doesn't work with bnb 0.43.1\n",
    "if parse(bitsandbytes.__version__) > parse('0.43.1'):\n",
    "    optimizers = (larc, lamb, rmsprop)\n",
    "else:\n",
    "    optimizers = (larc, rmsprop)\n",
    "test_optimizers(optimizers, eightbit=True, optimi=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "test_optimizers((qhadam,), eightbit=False, optimi=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
