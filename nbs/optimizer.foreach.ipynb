{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp optimizer.foreach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ForEach Optimizers\n",
    "> fastai optimizers using PyTorch ForEach methods for improved performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "from typing import Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from fastcore.basics import range_of, merge\n",
    "\n",
    "from fastai.optimizer import Optimizer\n",
    "\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Utils -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastai.optimizer import (_update, weight_decay, l2_reg, average_grad, sgd_step, momentum_step, \n",
    "                              average_sqr_grad, rms_prop_step, step_stat, adam_step, radam_step, \n",
    "                              larc_layer_lr, larc_step, lamb_step, Lookahead)\n",
    "\n",
    "from fastxtend.test_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def tst_param(val, grad=None):\n",
    "    \"Create a tensor with `val` and a gradient of `grad` for testing\"\n",
    "    res = tensor([val]).float()\n",
    "    res.grad = tensor([val/10 if grad is None else grad]).float()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def tst_params():\n",
    "    r = L.range(4)\n",
    "    return r.map(tst_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ForEach Optimizers are adapted from the PyTorch [_multi_tensor optim implementations](https://github.com/pytorch/pytorch/tree/master/torch/optim)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def sgd_foreach_step(p:list[Tensor], g:list[Tensor], no_wd_p:list[Tensor], no_wd_g:list[Tensor], grad_avg:list[Tensor], \n",
    "                     no_wd_grad_avg:list[Tensor], lr:float, wd:float, mom:float, decouple_wd:bool, dampening:bool=False, **kwargs):\n",
    "    if len(p) > 0 and wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            torch._foreach_mul_(p, 1 - lr * wd)\n",
    "        else:\n",
    "            # l2_reg\n",
    "            torch._foreach_add_(g, p, alpha=wd)\n",
    "        \n",
    "    # combine wd and non-wd lists\n",
    "    if len(no_wd_p) > 0:\n",
    "        p += no_wd_p\n",
    "        g += no_wd_g\n",
    "        if mom != 0:\n",
    "            grad_avg += no_wd_grad_avg\n",
    "\n",
    "    if mom != 0:\n",
    "        # average_grad\n",
    "        damp = 1-mom if dampening else 1.\n",
    "        torch._foreach_mul_(grad_avg, mom)\n",
    "        torch._foreach_add_(grad_avg, g, alpha=damp)\n",
    "\n",
    "        # momentum_step\n",
    "        torch._foreach_add_(p, grad_avg, alpha=-lr)\n",
    "    else:\n",
    "        # sgd_step\n",
    "        torch._foreach_add_(p, g, alpha=-lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class SGDForEachOptimizer(Optimizer):\n",
    "    \"An `Optimizer` with a modified step for SGD ForEach\"\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        if closure is not None: raise NotImplementedError(\"fastai optimizers currently do not support closure\")\n",
    "        for pg, hyper in zip(self.param_lists, self.hypers):\n",
    "            do_wd_p, do_wd_g, do_wd_grad_avg, no_wd_p, no_wd_g, no_wd_grad_avg = [], [], [], [], [], []\n",
    "\n",
    "            for p in pg:\n",
    "                if hasattr(p, 'grad') and p.grad is not None:\n",
    "                    state = self.state[p]\n",
    "\n",
    "                    if 'grad_avg' not in state:\n",
    "                        state['grad_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format) if hyper['mom'] != 0 else None\n",
    "\n",
    "                    if hyper['wd'] != 0 and state.get('do_wd', True):\n",
    "                        do_wd_p.append(p)\n",
    "                        do_wd_g.append(p.grad)\n",
    "                        do_wd_grad_avg.append(state['grad_avg'])\n",
    "                    else:\n",
    "                        no_wd_p.append(p)\n",
    "                        no_wd_g.append(p.grad)\n",
    "                        no_wd_grad_avg.append(state['grad_avg'])\n",
    "\n",
    "            self.cbs[0](do_wd_p, do_wd_g, no_wd_p, no_wd_g, do_wd_grad_avg, no_wd_grad_avg, **hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def SGD(params, lr, mom=0., wd=0., decouple_wd=True, foreach=False):\n",
    "    \"A `Optimizer` or `SGDForEachOptimizer` for SGD with `lr` and `mom` and `params`\"\n",
    "    if foreach:\n",
    "        cb = partial(sgd_foreach_step, decouple_wd=decouple_wd)\n",
    "        return SGDForEachOptimizer(params, cb, lr=lr, mom=mom, wd=wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        if mom != 0: cbs.append(average_grad)\n",
    "        cbs.append(sgd_step if mom==0 else momentum_step)\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "\n",
    "# Test Vanilla SGD\n",
    "params_org = tst_params()\n",
    "opt_org = SGD(params_org, lr=0.1, foreach=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_for = tst_params()\n",
    "opt_for = SGD(params_for, lr=0.1, foreach=True)\n",
    "opt_for.step()\n",
    "\n",
    "test_close([p.item() for p in params_org], [i*0.99 for i in range(4)])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_for])\n",
    "\n",
    "opt_org.step()\n",
    "opt_for.step()\n",
    "test_close([p.item() for p in params_org], [i*0.98 for i in range(4)])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_for])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def adam_foreach_step(p:list[Tensor], g:list[Tensor], no_wd_p:list[Tensor], no_wd_g:list[Tensor], grad_avg:list[Tensor], \n",
    "                      no_wd_grad_avg:list[Tensor], sqr_avg:list[Tensor], no_wd_sqr_avg:list[Tensor], steps:np.ndarray[Any, float],\n",
    "                      lr:float, wd:float, mom:float, sqr_mom:float, eps:float, decouple_wd:bool, **kwargs):\n",
    "\n",
    "    if len(p) > 0 and wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            torch._foreach_mul_(p, 1 - lr * wd)\n",
    "        else:\n",
    "            # l2_reg\n",
    "            torch._foreach_add_(g, p, alpha=wd)\n",
    "        \n",
    "    # combine wd and non-wd lists\n",
    "    if len(no_wd_p) > 0:\n",
    "        p += no_wd_p\n",
    "        g += no_wd_g\n",
    "        grad_avg += no_wd_grad_avg\n",
    "        sqr_avg += no_wd_sqr_avg\n",
    "\n",
    "    # average_grad, dampening=True\n",
    "    torch._foreach_mul_(grad_avg, mom)\n",
    "    torch._foreach_add_(grad_avg, g, alpha=1-mom)\n",
    "\n",
    "    # average_sqr_grad\n",
    "    torch._foreach_mul_(sqr_avg, sqr_mom)\n",
    "    torch._foreach_addcmul_(sqr_avg, g, g, value=1-sqr_mom)\n",
    "\n",
    "    # adam_step\n",
    "    debias1 = -lr / (1 - mom**steps)\n",
    "    debias2 = np.sqrt(1 - sqr_mom**steps)\n",
    "\n",
    "    sqr_avg_debias2 = torch._foreach_sqrt(sqr_avg)\n",
    "    torch._foreach_div_(sqr_avg_debias2, debias2.tolist())\n",
    "    torch._foreach_add_(sqr_avg_debias2, eps)\n",
    "\n",
    "    torch._foreach_addcdiv_(p, grad_avg, sqr_avg_debias2, debias1.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class AdamForEachOptimizer(Optimizer):\n",
    "    \"An `Optimizer` with a modified step for Adam ForEach\"\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        if closure is not None: raise NotImplementedError(\"fastai optimizers currently do not support closure\")\n",
    "        for pg, hyper in zip(self.param_lists, self.hypers):\n",
    "            do_wd_p, do_wd_g, do_wd_grad_avg, do_wd_sqr_avg, do_wd_steps = [], [], [], [], []\n",
    "            no_wd_p, no_wd_g, no_wd_grad_avg, no_wd_sqr_avg, no_wd_steps = [], [], [], [], []\n",
    "\n",
    "            for p in pg:\n",
    "                if hasattr(p, 'grad') and p.grad is not None:\n",
    "                    state = self.state[p]\n",
    "\n",
    "                    if 'grad_avg' not in state:\n",
    "                        state['grad_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        state['sqr_avg']  = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        state['step'] = 0\n",
    "\n",
    "                    state['step'] += 1\n",
    "                    if hyper['wd'] != 0 and state.get('do_wd', True):\n",
    "                        do_wd_p.append(p)\n",
    "                        do_wd_g.append(p.grad)\n",
    "                        do_wd_grad_avg.append(state['grad_avg'])\n",
    "                        do_wd_sqr_avg.append(state['sqr_avg'])\n",
    "                        do_wd_steps.append(state['step'])\n",
    "                    else:\n",
    "                        no_wd_p.append(p)\n",
    "                        no_wd_g.append(p.grad)\n",
    "                        no_wd_grad_avg.append(state['grad_avg'])\n",
    "                        no_wd_sqr_avg.append(state['sqr_avg'])\n",
    "                        no_wd_steps.append(state['step'])\n",
    "\n",
    "            steps = np.array([*do_wd_steps, *no_wd_steps], dtype=np.float32)\n",
    "            self.cbs[0](do_wd_p, do_wd_g, no_wd_p, no_wd_g, do_wd_grad_avg, no_wd_grad_avg, do_wd_sqr_avg, no_wd_sqr_avg, steps, **hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def Adam(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0.01, decouple_wd=True, foreach=False):\n",
    "    \"A `Optimizer` or `JitOptimizer` for Adam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n",
    "    if foreach:\n",
    "        cb = partial(adam_foreach_step, decouple_wd=decouple_wd)\n",
    "        return AdamForEachOptimizer(params, cb, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, adam_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = Adam(params_org, lr=0.1, wd=0, foreach=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_for = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_for = Adam(params_for, lr=0.1, wd=0, foreach=True)\n",
    "opt_for.step()\n",
    "\n",
    "step = -0.1 * 0.1 / (math.sqrt(0.1**2) + 1e-8)\n",
    "test_close(params_org[0], tensor([1+step, 2+step, 3+step]))\n",
    "test_close([p[0].item() for p in params_org], [p[0].item() for p in params_for])\n",
    "\n",
    "opt_org.step()\n",
    "opt_for.step()\n",
    "test_close(params_org[0], tensor([1+2*step, 2+2*step, 3+2*step]), eps=1e-3)\n",
    "test_close([p[0].item() for p in params_org], [p[0].item() for p in params_for])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAdam -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def radam_foreach_step(p:list[Tensor], g:list[Tensor], no_wd_p:list[Tensor], no_wd_g:list[Tensor], grad_avg:list[Tensor], \n",
    "                       no_wd_grad_avg:list[Tensor], sqr_avg:list[Tensor], no_wd_sqr_avg:list[Tensor], ones:list[Tensor], \n",
    "                       steps:np.ndarray[float], lr:float, wd:float, mom:float, sqr_mom:float, eps:float, decouple_wd:bool, **kwargs):\n",
    "\n",
    "    if len(p) > 0 and wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            torch._foreach_mul_(p, 1 - lr * wd)\n",
    "        else:\n",
    "            # l2_reg\n",
    "            torch._foreach_add_(g, p, alpha=wd)\n",
    "        \n",
    "    # combine wd and non-wd lists\n",
    "    if len(no_wd_p) > 0:\n",
    "        p += no_wd_p\n",
    "        g += no_wd_g\n",
    "        grad_avg += no_wd_grad_avg\n",
    "        sqr_avg += no_wd_sqr_avg\n",
    "\n",
    "    # average_grad, dampening=True\n",
    "    torch._foreach_mul_(grad_avg, mom)\n",
    "    torch._foreach_add_(grad_avg, g, alpha=1-mom)\n",
    "\n",
    "    # average_sqr_grad\n",
    "    torch._foreach_mul_(sqr_avg, sqr_mom)\n",
    "    torch._foreach_addcmul_(sqr_avg, g, g, value=1-sqr_mom)\n",
    "\n",
    "    # radam_step\n",
    "    debias1 = -lr / (1 - mom**steps)\n",
    "    debias2 = np.sqrt(1 - sqr_mom**steps).tolist()\n",
    "    \n",
    "    r_inf = 2/(1-sqr_mom) - 1\n",
    "    r = r_inf - 2*steps*sqr_mom**steps/(1-sqr_mom**steps)\n",
    "\n",
    "    rect   = np.where(r > 5, debias1*np.emath.sqrt(((r-4) * (r-2) * r_inf)/((r_inf-4)*(r_inf-2)*r)), 0).tolist()\n",
    "    unrect = np.where(r <= 5, debias1, 0).tolist()\n",
    "\n",
    "    # rectified step\n",
    "    sqrt_avg_debias2 = torch._foreach_sqrt(sqr_avg)\n",
    "    torch._foreach_div_(sqrt_avg_debias2, debias2)\n",
    "    torch._foreach_add_(sqrt_avg_debias2, eps)\n",
    "    torch._foreach_addcdiv_(p, grad_avg, sqrt_avg_debias2, scalars=rect)\n",
    "\n",
    "    # unrectified step. cannot scale with foreach_add, so divide by one with foreach_addcdiv\n",
    "    torch._foreach_addcdiv_(p, grad_avg, ones, scalars=unrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class RAdamForEachOptimizer(Optimizer):\n",
    "    \"An `Optimizer` with a modified step for RAdam ForEach\"\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        if closure is not None: raise NotImplementedError(\"fastai optimizers currently do not support closure\")\n",
    "        for pg, hyper in zip(self.param_lists, self.hypers):\n",
    "            do_wd_p, do_wd_g, do_wd_grad_avg, do_wd_sqr_avg, do_wd_steps, do_wd_ones = [], [], [], [], [], []\n",
    "            no_wd_p, no_wd_g, no_wd_grad_avg, no_wd_sqr_avg, no_wd_steps, no_wd_ones = [], [], [], [], [], []\n",
    "\n",
    "            for p in pg:\n",
    "                if hasattr(p, 'grad') and p.grad is not None:\n",
    "                    state = self.state[p]\n",
    "\n",
    "                    if 'grad_avg' not in state:\n",
    "                        state['grad_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        state['sqr_avg']  = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        state['ones']     = torch.ones_like(p, memory_format=torch.preserve_format)\n",
    "                        state['step'] = 0\n",
    "\n",
    "                    state['step'] += 1\n",
    "                    if hyper['wd'] != 0 and state.get('do_wd', True):\n",
    "                        do_wd_p.append(p)\n",
    "                        do_wd_g.append(p.grad)\n",
    "                        do_wd_grad_avg.append(state['grad_avg'])\n",
    "                        do_wd_sqr_avg.append(state['sqr_avg'])\n",
    "                        do_wd_ones.append(state['ones'])\n",
    "                        do_wd_steps.append(state['step'])\n",
    "                    else:\n",
    "                        no_wd_p.append(p)\n",
    "                        no_wd_g.append(p.grad)\n",
    "                        no_wd_grad_avg.append(state['grad_avg'])\n",
    "                        no_wd_sqr_avg.append(state['sqr_avg'])\n",
    "                        no_wd_ones.append(state['ones'])\n",
    "                        no_wd_steps.append(state['step'])\n",
    "\n",
    "            steps = np.array([*do_wd_steps, *no_wd_steps], dtype=np.float32)\n",
    "            ones = do_wd_ones + no_wd_ones\n",
    "            self.cbs[0](do_wd_p, do_wd_g, no_wd_p, no_wd_g, do_wd_grad_avg, no_wd_grad_avg, do_wd_sqr_avg, no_wd_sqr_avg, ones, steps, **hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def RAdam(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0., beta=0., decouple_wd=True, foreach=False):\n",
    "    \"A `Optimizer` or `ForEachOptimizer` for RAdam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n",
    "    if foreach:\n",
    "        cb = partial(radam_foreach_step, decouple_wd=decouple_wd)\n",
    "        if beta != 0: warn('RAdam foreach does not use beta, set foreach=False if beta!=0')\n",
    "        return RAdamForEachOptimizer(params, cb, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, radam_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd, beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = RAdam(params_org, lr=0.1, foreach=False)\n",
    "\n",
    "params_for = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_for = RAdam(params_for, lr=0.1, foreach=True)\n",
    "\n",
    "#The r factor is lower than 5 during the first 5 steps so updates use the average of gradients (all the same)\n",
    "r_inf = 2/(1-0.99) - 1\n",
    "for i in range(5): \n",
    "    r = r_inf - 2*(i+1)*0.99**(i+1)/(1-0.99**(i+1))\n",
    "    assert r <= 5\n",
    "    opt_org.step()\n",
    "    opt_for.step()\n",
    "p = tensor([0.95, 1.9, 2.85])\n",
    "test_close(params_org[0], p)\n",
    "test_close(params_org[0], params_for[0])\n",
    "\n",
    "#The r factor is greater than 5 for the sixth step so we update with RAdam\n",
    "r = r_inf - 2*6*0.99**6/(1-0.99**6)\n",
    "assert r > 5\n",
    "opt_org.step()\n",
    "opt_for.step()\n",
    "v = math.sqrt(((r-4) * (r-2) * r_inf)/((r_inf-4)*(r_inf-2)*r))\n",
    "step = -0.1*0.1*v/(math.sqrt(0.1**2) + 1e-8)\n",
    "test_close(params_org[0], p+step)\n",
    "test_close(params_org[0], params_for[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAMB -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def lamb_foreach_step(p:list[Tensor], g:list[Tensor], no_wd_p:list[Tensor], no_wd_g:list[Tensor], grad_avg:list[Tensor],\n",
    "                      no_wd_grad_avg:list[Tensor], sqr_avg:list[Tensor], no_wd_sqr_avg:list[Tensor], ones:list[Tensor],\n",
    "                      steps:np.ndarray[float], lr:float, wd:float, mom:float, sqr_mom:float, eps:float, decouple_wd:bool, **kwargs):\n",
    "\n",
    "    if len(p) > 0 and wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            torch._foreach_mul_(p, 1 - lr * wd)\n",
    "        else:\n",
    "            # l2_reg\n",
    "            torch._foreach_add_(g, p, alpha=wd)\n",
    "        \n",
    "    # combine wd and non-wd lists\n",
    "    if len(no_wd_p) > 0:\n",
    "        p += no_wd_p\n",
    "        g += no_wd_g\n",
    "        grad_avg += no_wd_grad_avg\n",
    "        sqr_avg += no_wd_sqr_avg\n",
    "\n",
    "    # average_grad, dampening=True\n",
    "    torch._foreach_mul_(grad_avg, mom)\n",
    "    torch._foreach_add_(grad_avg, g, alpha=1-mom)\n",
    "\n",
    "    # average_sqr_grad\n",
    "    torch._foreach_mul_(sqr_avg, sqr_mom)\n",
    "    torch._foreach_addcmul_(sqr_avg, g, g, value=1-sqr_mom)\n",
    "\n",
    "    # lamb_step\n",
    "    debias1 = 1 - mom**steps\n",
    "    debias2 = np.sqrt(1 - sqr_mom**steps)\n",
    "\n",
    "    debias2 = torch._foreach_div(torch._foreach_sqrt(sqr_avg), debias2.tolist())\n",
    "    torch._foreach_add_(debias2, eps)\n",
    "    lstep = torch._foreach_div(grad_avg, debias1.tolist())\n",
    "    torch._foreach_div_(lstep, debias2)\n",
    "\n",
    "    # there currently is no foreach_mean or foreach_where/if methods\n",
    "    q = []\n",
    "    for i in range(len(p)):\n",
    "        r1 = p[i].pow(2).mean().sqrt().item()\n",
    "        r2 = lstep[i].pow(2).mean().sqrt().item()\n",
    "        if r1 == 0 or r2 == 0:\n",
    "            q.append(-lr)\n",
    "        else:\n",
    "            q.append(-lr*min(r1/r2, 10.))\n",
    "\n",
    "    # cannot scale with foreach_add, so divide by one with foreach_addcdiv\n",
    "    torch._foreach_addcdiv_(p, lstep, ones, scalars=q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class LambForEachOptimizer(RAdamForEachOptimizer):\n",
    "    \"An `Optimizer` with a modified step for Lamb ForEach\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def Lamb(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0., decouple_wd=True, foreach=False):\n",
    "    \"A `Optimizer` or `ForEachOptimizer` for Lamb with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n",
    "    if foreach:\n",
    "        cb = partial(lamb_foreach_step, decouple_wd=decouple_wd)\n",
    "        return LambForEachOptimizer(params, cb, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, lamb_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = Lamb(params_org, lr=0.1, foreach=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_for = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_for = Lamb(params_for, lr=0.1, foreach=True)\n",
    "opt_for.step()\n",
    "\n",
    "test_close(params_org[0], tensor([0.7840,1.7840,2.7840]), eps=1e-3)\n",
    "test_close(params_org[0], params_for[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranger -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def ranger_foreach_step(p:list[Tensor], g:list[Tensor], no_wd_p:list[Tensor], no_wd_g:list[Tensor], grad_avg:list[Tensor],\n",
    "                        no_wd_grad_avg:list[Tensor], sqr_avg:list[Tensor], no_wd_sqr_avg:list[Tensor], ones:list[Tensor], \n",
    "                        slow_p:list[Tensor], steps:np.ndarray[float], lr:float, wd:float, mom:float, sqr_mom:float, eps:float, \n",
    "                        decouple_wd:bool, count:int, k:int, alpha:float, **kwargs):\n",
    "\n",
    "    if len(p) > 0 and wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            torch._foreach_mul_(p, 1 - lr * wd)\n",
    "        else:\n",
    "            # l2_reg\n",
    "            torch._foreach_add_(g, p, alpha=wd)\n",
    "        \n",
    "    # combine wd and non-wd lists\n",
    "    if len(no_wd_p) > 0:\n",
    "        p += no_wd_p\n",
    "        g += no_wd_g\n",
    "        grad_avg += no_wd_grad_avg\n",
    "        sqr_avg += no_wd_sqr_avg\n",
    "\n",
    "    # average_grad, dampening=True\n",
    "    torch._foreach_mul_(grad_avg, mom)\n",
    "    torch._foreach_add_(grad_avg, g, alpha=1-mom)\n",
    "\n",
    "    # average_sqr_grad\n",
    "    torch._foreach_mul_(sqr_avg, sqr_mom)\n",
    "    torch._foreach_addcmul_(sqr_avg, g, g, value=1-sqr_mom)\n",
    "\n",
    "    # radam_step\n",
    "    debias1 = -lr / (1 - mom**steps)\n",
    "    debias2 = np.sqrt(1 - sqr_mom**steps).tolist()\n",
    "    \n",
    "    r_inf = 2/(1-sqr_mom) - 1\n",
    "    r = r_inf - 2*steps*sqr_mom**steps/(1-sqr_mom**steps)\n",
    "\n",
    "    rect   = np.where(r > 5, debias1*np.emath.sqrt(((r-4) * (r-2) * r_inf)/((r_inf-4)*(r_inf-2)*r)), 0).tolist()\n",
    "    unrect = np.where(r <= 5, debias1, 0).tolist()\n",
    "\n",
    "    # rectified step\n",
    "    sqrt_avg_debias2 = torch._foreach_sqrt(sqr_avg)\n",
    "    torch._foreach_div_(sqrt_avg_debias2, debias2)\n",
    "    torch._foreach_add_(sqrt_avg_debias2, eps)\n",
    "    torch._foreach_addcdiv_(p, grad_avg, sqrt_avg_debias2, scalars=rect)\n",
    "\n",
    "    # unrectified step. cannot scale with foreach_add, so divide by one with foreach_addcdiv\n",
    "    torch._foreach_addcdiv_(p, grad_avg, ones, scalars=unrect)\n",
    "\n",
    "    if count % k == 0:\n",
    "        torch._foreach_add_(slow_p, torch._foreach_sub(p, slow_p), alpha=alpha)\n",
    "        # there currently is no foreach_set method\n",
    "        p = [pi.set_(slow_pi) for pi, slow_pi in zip(p, slow_p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class RangerOptimizer(Optimizer):\n",
    "    \"An `Optimizer` with a modified step for Lookahead TorchScript optimizers\"\n",
    "    def __init__(self, params:Tensor, cbs:list, train_bn:bool=True, **defaults):\n",
    "        super().__init__(params, cbs, train_bn, **defaults)\n",
    "        self._init_state()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        self.count += 1\n",
    "        if closure is not None: raise NotImplementedError(\"fastai optimizers currently do not support closure\")\n",
    "        for pg, hyper in zip(self.param_lists, self.hypers):\n",
    "            do_wd_p, do_wd_g, do_wd_grad_avg, do_wd_sqr_avg, do_wd_steps, do_wd_ones, do_wd_slow = [], [], [], [], [], [], []\n",
    "            no_wd_p, no_wd_g, no_wd_grad_avg, no_wd_sqr_avg, no_wd_steps, no_wd_ones, no_wd_slow = [], [], [], [], [], [], []\n",
    "\n",
    "            for p in pg:\n",
    "                if hasattr(p, 'grad') and p.grad is not None:\n",
    "                    state = self.state[p]\n",
    "\n",
    "                    if 'grad_avg' not in state:\n",
    "                        state['grad_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        state['sqr_avg']  = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        state['ones']     = torch.ones_like(p, memory_format=torch.preserve_format)\n",
    "                        state['slow_p']   = p.clone().detach()\n",
    "                        state['step'] = 0\n",
    "\n",
    "                    state['step'] += 1\n",
    "                    if hyper['wd'] != 0 and state.get('do_wd', True):\n",
    "                        do_wd_p.append(p)\n",
    "                        do_wd_g.append(p.grad)\n",
    "                        do_wd_grad_avg.append(state['grad_avg'])\n",
    "                        do_wd_sqr_avg.append(state['sqr_avg'])\n",
    "                        do_wd_ones.append(state['ones'])\n",
    "                        do_wd_slow.append(state['slow_p'])\n",
    "                        do_wd_steps.append(state['step'])\n",
    "                    else:\n",
    "                        no_wd_p.append(p)\n",
    "                        no_wd_g.append(p.grad)\n",
    "                        no_wd_grad_avg.append(state['grad_avg'])\n",
    "                        no_wd_sqr_avg.append(state['sqr_avg'])\n",
    "                        no_wd_ones.append(state['ones'])\n",
    "                        no_wd_slow.append(state['slow_p'])\n",
    "                        no_wd_steps.append(state['step'])\n",
    "\n",
    "            steps = np.array([*do_wd_steps, *no_wd_steps], dtype=np.float32)\n",
    "            ones = do_wd_ones + no_wd_ones\n",
    "            slow_p = do_wd_slow + no_wd_slow\n",
    "            self.cbs[0](do_wd_p, do_wd_g, no_wd_p, no_wd_g, do_wd_grad_avg, no_wd_grad_avg, \n",
    "                        do_wd_sqr_avg, no_wd_sqr_avg, ones, slow_p, steps, count=self.count, **hyper)\n",
    "\n",
    "    def clear_state(self):\n",
    "        super().clear_state()\n",
    "        self._init_state()\n",
    "\n",
    "    def state_dict(self):\n",
    "        state = super().state_dict()\n",
    "        state.update({'count': self.count})\n",
    "        return state\n",
    "\n",
    "    def load_state_dict(self, sd):\n",
    "        self.count = sd.pop('count')\n",
    "        super().load_state_dict(sd)\n",
    "\n",
    "    def _init_state(self): \n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def ranger(params, lr, mom=0.95, sqr_mom=0.99, eps=1e-6, wd=0.01, beta=0., k=6, alpha=0.5, decouple_wd=True, foreach=False):\n",
    "    \"Convenience method for `Lookahead` with `RAdam`\"\n",
    "    if foreach:\n",
    "        cb = partial(ranger_foreach_step, decouple_wd=decouple_wd)\n",
    "        return RangerOptimizer(params, cb, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd, beta=beta, k=k, alpha=alpha)\n",
    "    else:\n",
    "        return Lookahead(RAdam(params, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd, beta=beta), k=k, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "po = tensor([1,2,3])\n",
    "\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = ranger(params_org, lr=0.1, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0., beta=0., foreach=False)\n",
    "\n",
    "params_for = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_for = ranger(params_for, lr=0.1, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0., beta=0., foreach=True)\n",
    "\n",
    "#The first 5 steps are normal RAdam steps\n",
    "#The r factor is lower than 5 during the first 5 steps so updates use the average of gradients (all the same)\n",
    "r_inf = 2/(1-0.99) - 1\n",
    "for i in range(5): \n",
    "    r = r_inf - 2*(i+1)*0.99**(i+1)/(1-0.99**(i+1))\n",
    "    assert r <= 5\n",
    "    opt_org.step()\n",
    "    opt_for.step()\n",
    "p = tensor([0.95, 1.9, 2.85])\n",
    "test_close(params_org[0], p)\n",
    "test_close(params_org[0], params_for[0])\n",
    "\n",
    "#The r factor is greater than 5 for the sixth step so we update with RAdam\n",
    "r = r_inf - 2*6*0.99**6/(1-0.99**6)\n",
    "assert r > 5\n",
    "opt_org.step()\n",
    "opt_for.step()\n",
    "v = math.sqrt(((r-4) * (r-2) * r_inf)/((r_inf-4)*(r_inf-2)*r))\n",
    "step = -0.1*0.1*v/(math.sqrt(0.1**2) + 1e-8)\n",
    "\n",
    "#Since k=6, sixth step is a moving average of the 6 RAdam steps with the initial weight\n",
    "test_close(params_org[0], po+((p+step)-po)*0.5)\n",
    "test_close(params_org[0], params_for[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('fastai')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
