{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp optimizer.utils\n",
    "#|default_cls_lvl 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer Utils\n",
    "> bUtilities for fastxtend optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "\n",
    "from fastcore.basics import even_mults\n",
    "\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def _convert_params(o:list, **defaults) -> list:\n",
    "    \"Convert fastai param_lists to PyTorch param_groups, adding defaults if group doesn't have it\"\n",
    "    splitter = []\n",
    "    for group in o:\n",
    "        if isinstance(group, dict):\n",
    "            splitter.append({**defaults, **group})\n",
    "        else:\n",
    "            splitter.append({'params':group, **defaults})\n",
    "    return splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class FastaiOptimizerAdapter:\n",
    "    \"Base for adding fastai optimizer functionality to PyTorch optimizers\"\n",
    "    _keep_on_clear = ['force_train', 'do_wd']\n",
    "\n",
    "    def get_params(self,\n",
    "        n:slice|int=slice(None), # Extended slicing over the optimizer `param_lists`\n",
    "        with_grad:bool=False # Get all param tuples. If `True` select only those with a gradient\n",
    "    ):\n",
    "        \"Slice of parameters and parameter states\"\n",
    "        return L((p, self.state[p]) for pg in self.param_groups[n] for p in pg['params']\n",
    "                    if (hasattr(p, 'grad') and p.grad is not None) or with_grad==False)\n",
    "\n",
    "    def clear_state(self):\n",
    "        \"Reset the state of the optimizer\"\n",
    "        for p,state in self.get_params():\n",
    "            self.state[p] = {k: state[k] for k in self._keep_on_clear if k in state}\n",
    "\n",
    "    def _set_require_grad(self,\n",
    "        rg:bool, # Requires grad: if `True` sets gradient for parameters, else uses state `state[\"force_train\"]`\n",
    "        p:Tensor, # Parameter to set gradient\n",
    "        state:dict, # Parameter's state dict\n",
    "    ):\n",
    "        p.requires_grad_(rg or state.get('force_train', False))\n",
    "\n",
    "    def freeze_to(self, n:int):\n",
    "        \"Freeze parameter groups up to `n`\"\n",
    "        self.frozen_idx = n if n >= 0 else len(self.param_groups) + n\n",
    "        if self.frozen_idx >= len(self.param_groups):\n",
    "            warn(f\"Freezing {self.frozen_idx} groups; model has {len(self.param_groups)}; whole model is frozen.\")\n",
    "        for o in self.get_params(slice(n, None)):\n",
    "            self._set_require_grad(True, *o)\n",
    "        for o in self.get_params(slice(None, n)):\n",
    "            self._set_require_grad(False, *o)\n",
    "\n",
    "    def freeze(self):\n",
    "        \"Freeze up to last parameter group\"\n",
    "        assert(len(self.param_groups) > 1)\n",
    "        self.freeze_to(-1)\n",
    "\n",
    "    def unfreeze(self):\n",
    "        \"Unfreeze the entire model\"\n",
    "        self.freeze_to(0)\n",
    "\n",
    "    @property\n",
    "    def hypers(self):\n",
    "        return [{k:v for k,v in pg.items() if k != 'params'} for pg in self.param_groups]\n",
    "\n",
    "    def set_hypers(self, **kwargs):\n",
    "        \"`set_hyper` for all `kwargs`\"\n",
    "        L(kwargs.items()).starmap(self.set_hyper)\n",
    "\n",
    "    def _set_hyper(self, k, v):\n",
    "        \"Set the value(s) in `v` for hyper-parameter `k`\"\n",
    "        for v_,h in zip(v, self.param_groups):\n",
    "            h[k] = v_\n",
    "\n",
    "    def set_hyper(self, k, v):\n",
    "        \"Set the value(s) in `v` for hyper-parameter `k`\"\n",
    "        if isinstance(v, slice):\n",
    "            if v.start:\n",
    "                v = even_mults(v.start, v.stop, len(self.param_groups))\n",
    "            else:\n",
    "                v = [v.stop/10]*(len(self.param_groups)-1) + [v.stop]\n",
    "        v = L(v, use_list=None)\n",
    "        if len(v)==1:\n",
    "            v = v*len(self.param_groups)\n",
    "        assert len(v) == len(self.param_groups), f\"Trying to set {len(v)} values for {k} but there are {len(self.param_groups)} parameter groups.\"\n",
    "        self._set_hyper(k, v)\n",
    "\n",
    "    @property\n",
    "    def param_lists(self):\n",
    "        return [pg['params'] for pg in self.param_groups]\n",
    "\n",
    "    @param_lists.setter\n",
    "    def param_lists(self, v):\n",
    "        for pg,v_ in zip(self.param_groups,v):\n",
    "            pg['params'] = v_\n",
    "\n",
    "    def convert_params(self, params, **defaults):\n",
    "        \"Convert fastai param_lists to PyTorch param_groups, adding defaults if group doesn't have it\"\n",
    "        params = L(params)\n",
    "        return _convert_params(params, **defaults) if isinstance(params[0], (L,list)) else params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(FastaiOptimizerAdapter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
