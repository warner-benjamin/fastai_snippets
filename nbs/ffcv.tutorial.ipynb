{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp ffcv.tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# Contains code from:\n",
    "# fastai - Apache License 2.0 - Copyright (c) 2023 fast.ai\n",
    "# FFCV - Apache License 2.0 - Copyright (c) 2022 FFCV Team"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with fastxtend & FFCV\n",
    "> Use fastxtend's FFCV integration to accelerate fastai training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastxtend integrates [FFCV](https://ffcv.io) with fastai. You can now use the speed of the FFCV dataloader natively with fastai batch transforms, callbacks, and other dataloader features.\n",
    "\n",
    "*insert benchmark charts here*\n",
    "\n",
    "[FFCV is](https://ffcv.io) \"a drop-in data loading system that dramatically increases data throughput in model training.\" It accelerates[^mosiac] model training by combining Numba compiled item transforms with a custom data format and data loading code.\n",
    "\n",
    "FFCV has a [getting started tutorial](https://docs.ffcv.io/quickstart.html) which pairs well with this guide, providing additional context and depth.\n",
    "\n",
    "[^mosiac]: MosiacML [found](https://www.mosaicml.com/blog/composer-ffcv-faster-together) that using FFCV led to a ~1.85x increase in thourouput, from ~17,800 images/sec to ~30,000 images/sec on a 2x 32-core CPU and 8x A100 system."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing fastai, FFCV, & fastxtend\n",
    "\n",
    "The easiest way to install fastai, fastxtend, and FFCV is to use Conda on Linux (or WSL):\n",
    "\n",
    "```bash\n",
    "conda create -n fastxtend python=3.10 cupy pkg-config compilers libjpeg-turbo opencv \\\n",
    "tqdm terminaltables psutil numpy=1.23.5 numba librosa=0.9.2 pytorch torchvision \\\n",
    "torchaudio pytorch-cuda=11.7 fastai nbdev timm kornia wandb jupyter jupyterlab \\\n",
    "ipykernel -c pytorch -c nvidia/label/cuda-11.7.1 -c fastai -c conda-forge\n",
    "\n",
    "conda activate fastxtend\n",
    "```\n",
    "\n",
    "replacing `pytorch-cuda=11.7` and `-c nvidia/label/cuda-11.7.1`[^cuda] with your prefered PyTorch supported version of Cuda.\n",
    "\n",
    "And then install fastxtend via pip:\n",
    "\n",
    "```bash\n",
    "pip install fastxtend[ffcv]\n",
    "```\n",
    "\n",
    "If you are using Windows[^windows], please follow the [FFCV Windows installation guide](https://github.com/libffcv/ffcv#windows), then install fastxtend via pip.\n",
    "\n",
    "[^cuda]: Without the label, `pytorch-cuda=11.7` doesn't restrict Cuda libraries to 11.7 or 11.7.1.\n",
    "\n",
    "[^windows]: fastxtend Windows support is currently untested. It is recommended to use WSL on Windows."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing FFCV via fastxtend\n",
    "\n",
    "fastxtend's FFCV integration has been designed to use `__all__` to safely import everything needed to use FFCV with fastai and fastxtend.\n",
    "\n",
    "Run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|slow\n",
    "from fastai.vision.all import *\n",
    "from fastxtend.vision.all import *\n",
    "from fastxtend.ffcv.all import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and you are ready to go.\n",
    "\n",
    "You can also import the FFCV integration individually from `fastxtend.ffcv`.\n",
    "\n",
    ":::{.callout-note collapse=\"false\"}\n",
    "#### Note: Importing Transforms & Operations\n",
    "fastxtend's FFCV transforms and operations are imported under the `ft` prefix, since they sometimes overlap with fastai batch transforms.\n",
    "\n",
    "For example, the FFCV augmenation `RandomErasing` is an Numba FFCV version of the batch transform `fastai.vision.augment.RandomErasing`.\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a FFCV Dataset\n",
    "\n",
    "Before we can start training with FFCV, our dataset needs to be converted into FFCV's custom beaton format. This can be done using the FFCV [`DatasetWriter`](https://docs.ffcv.io/writing_datasets.html).\n",
    "\n",
    "fastxtend provides the `rgb_dataset_to_ffcv` convenience method for easy FFCV image dataset creation. `rgb_dataset_to_ffcv` expects a PyTorch compatible `Dataset` or any Python iterator.\n",
    "\n",
    "First, create an Imagenette dataset using the [fastai datablock API](https://docs.fast.ai/tutorial.datablock.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|slow\n",
    "path = URLs.IMAGENETTE_320\n",
    "source = untar_data(path)\n",
    "dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "                   splitter=GrandparentSplitter(valid_name='val'),\n",
    "                   get_items=get_image_files, get_y=parent_label)\n",
    "dset = dblock.datasets(source)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use `rgb_dataset_to_ffcv` to create two FFCV files: one for the training dataset and one for the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|slow\n",
    "rgb_dataset_to_ffcv(dset.train, 'imagenette_320_train.ffcv')\n",
    "\n",
    "rgb_dataset_to_ffcv(dset.valid, 'imagenette_320_valid.ffcv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-tip collapse=\"true\"}\n",
    "#### Tip: Change Chunk Size\n",
    "If you have more (or less) memory, you can increase (or decrease) `DatasetWriter`'s `chunk_size` from the default of 100.\n",
    ":::\n",
    "\n",
    "If Imagenette was not already the resized, we could pass `max_resolution` or `min_resolution` to resize the images. To recreate Imagenette 320 from the full size dataset, pass `min_resolution=320`:\n",
    "\n",
    "```python\n",
    "rgb_dataset_to_ffcv(\n",
    "    dset.train, 'imagenette_320_valid.ffcv', min_resolution=320)\n",
    "```\n",
    "\n",
    "this will use Pillow to resize the image, using the `LANCZOS` resample method, by default.\n",
    "\n",
    ":::{.callout-tip collapse=\"true\"}\n",
    "#### Tip: Pillow-SIMD\n",
    "To accelerate image resizing, you'll probably want [Pillow-SIMD](https://github.com/uploadcare/pillow-simd) installed.\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a fastxtend Loader\n",
    "\n",
    "fastxtend adds fastai features to [FFCV's Loader](https://docs.ffcv.io/making_dataloaders.html), including `one_batch`, `show_batch`, `show_results`, and support for batch transforms, to name a few.\n",
    "\n",
    "Currently `fastai.data.block.DataBlock` is unsupported for creating a fastxtend `Loader`, so we'll have to create it from scratch. For reference, here is the fastai DataBlock we'll be recreating[^recreating] using the fastxtend `Loader`.\n",
    "\n",
    "```python\n",
    "dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "                   splitter=GrandparentSplitter(valid_name='val'),\n",
    "                   get_items=get_image_files, get_y=parent_label,\n",
    "                   item_tfms=[RandomResizedCrop(224), FlipItem(0.5)],\n",
    "                   batch_tfms=[*aug_transforms(), Normalize(*imagenet_stats)])\n",
    "dls = dblock.dataloaders(source, bs=64, num_workers=num_cpus())\n",
    "```\n",
    "\n",
    ":::{.callout-important collapse=\"true\"}\n",
    "#### Important: Loader Transform Type & Order Matters\n",
    "Unlike fastai, fastxtend's FFCV integration (and FFCV itself) does not distiguish between training and validation transforms.\n",
    "\n",
    "Neither does it automatically create a validation pipeline or automatically reorder transforms and operations.\n",
    "\n",
    "The user is responsible for adding the correct decoders, transforms, and operations to the correct pipelines in the correct order.\n",
    ":::\n",
    "\n",
    "[^recreating]: This will not be a one-to-one recreation, as `fastai.vision.augment.RandomResizedCrop` adds padding and squishes the validation images, while the fastxend `Loader` will use a standard center crop for validation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Pipelines\n",
    "\n",
    "FFCV uses [pipelines](https://docs.ffcv.io/making_dataloaders.html#pipelines) to declare what input fields to read, how to decode them, and which operations and transforms to apply on them.\n",
    "\n",
    "We'll need to create three pipelines: one for the training images, validation images, and labels.\n",
    "\n",
    "We also need to make sure that the decoders, transforms, and operations are all in the correct order, as they will be executed sequentially."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Pipeline\n",
    "\n",
    "Reading a FFCV dataset requires a FFCV decoder. FFCV has [multiple decoders](https://docs.ffcv.io/api/decoders.html), but we'll use [`RandomResizedCropRGBImageDecoder`](https://docs.ffcv.io/api/decoders.html#ffcv.fields.decoders.RandomResizedCropRGBImageDecoder) which integrates a random resizing crop into image loading.\n",
    "\n",
    "And we'll add `RandomHorizontalFlip` to flip the image.\n",
    "\n",
    "```python\n",
    "train_pipe = [\n",
    "    RandomResizedCropRGBImageDecoder(output_size=(224,224)),\n",
    "    ft.RandomHorizontalFlip(0.5)\n",
    "]\n",
    "```\n",
    ":::{.callout-tip collapse=\"true\"}\n",
    "#### Tip: fastxtend has Image Transforms\n",
    "fastxtend provides multiple FFCV [transforms](ffcv.transforms.html), including existing FFCV transforms with harmonized arguments pending FFCV transforms, and fastai transforms implemented as FFCV transforms.\n",
    ":::\n",
    "\n",
    "After passing through FFCV's Numba compiled transforms, the Imagenette images are still in CPU memory as NumPy arrays. Before we can pass them to our model, they need to be converted to `fastai.torch_core.TensorImage` and moved to the GPU.\n",
    "\n",
    "We'll extend our training pipeline by adding the `ToTensorImage` and `ToDevice` operations.\n",
    "\n",
    "```python\n",
    "train_pipe.extend([ft.ToTensorImage(), ft.ToDevice('cuda')])\n",
    "```\n",
    "\n",
    "`Loader` will now asynchronously transfer each training image batch to the GPU.\n",
    "\n",
    ":::{.callout-warning collapse=\"true\"}\n",
    "#### Warning: Don't use FFCV's Tensor Operations\n",
    "FFCV has [`ToTensor`](https://docs.ffcv.io/api/transforms.html#ffcv.transforms.ToTensor), [`ToTorchImage`](https://docs.ffcv.io/api/transforms.html#ffcv.transforms.ToTorchImage), and [<code>ToDevice</code>](https://docs.ffcv.io/api/transforms.html#ffcv.transforms.ToDevice) operations for converting NumPy arrays to PyTorch Tensors and moving to the GPU. These are compatible with PyTorch dataloaders but they are not compatible with fastai as they will strip the required types for fastai features, such as batch transforms, callbacks, plotting, etc.\n",
    "\n",
    "Use fastxtend's `ToTensorImage` and `ToDevice` for compatibility with fastai features.\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Pipeline\n",
    "\n",
    "With the training pipeline finalized, it's time to create the validation pipeline.\n",
    "\n",
    "FFCV and fastxtend currently have one validation image decoder: [`CenterCropRGBImageDecoder`](https://docs.ffcv.io/api/decoders.html#ffcv.fields.decoders.CenterCropRGBImageDecoder), which resizes and center crops the validation image. This is identical to `fastai.vision.augment.Resize` valdiation behavior.\n",
    "\n",
    "```python\n",
    "valid_pipe = [\n",
    "    CenterCropRGBImageDecoder(output_size=(224,224), ratio=1),\n",
    "    ft.ToTensorImage(),\n",
    "    ft.ToDevice('cuda')\n",
    "]\n",
    "```\n",
    "Like `train_pipe`, we use `ToTensorImage` to convert to the correct tensor type and `ToDevice` asynchronously transfer each batch to the GPU."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Pipeline\n",
    "\n",
    "Now we have our image pipeline for our training and validaton datasets but we need to create our label pipeline.\n",
    "\n",
    "Since this is a single label dataset, we have integers as labels, so we'll use an [`IntDecoder`](https://docs.ffcv.io/api/decoders.html#ffcv.fields.decoders.IntDecoder) and convert to `TensorCategory`, followed by squeezing the extra dimension[^dim] with `Squeeze` and using `ToDevice` to transfer to the GPU.\n",
    "\n",
    "```python\n",
    "label_pipe = [\n",
    "    IntDecoder(), ft.ToTensorCategory(),\n",
    "    ft.Squeeze(), ft.ToDevice('cuda')\n",
    "]\n",
    "```\n",
    "\n",
    "[^dim]: Since this is a single label problem, we need to remove the added dimension after the batch is created. If this were a multi-labeled dataset, we'd skip the squeezing step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Required & Optional Batch Transforms\n",
    "\n",
    "After `Loader` finishes processing the pipelines, the images are batched on the GPU, but will be in `uint8` format and unnormalized.\n",
    "\n",
    "FFCV has operations to handle both, but using them will convert the images from `TensorImage` to `Tensor`, limiting compatibility with other fastai features, such as callbacks, plotting, etc.\n",
    "\n",
    "Since fastxtend's `Loader` supports fastai batch transforms, we'll use them instead."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required Batch Transform\n",
    "\n",
    "To convert the `uint8` tensors to `float` and normalize the images, we'll use `fastai.data.transforms.IntToFloatTensor` and `fastai.data.transforms.Normalize` to preserve tensor types and metadata.\n",
    "\n",
    "Unlike the FFCV transforms we've used so far, the fastai transforms in `Loader` will automatically reorder themselves into and use Type Dispatch to apply to the correct tensor types.\n",
    "\n",
    "```python\n",
    "batch_tfms = [\n",
    "    IntToFloatTensor,\n",
    "    Normalize.from_stats(*imagenet_stats)\n",
    "]\n",
    "```\n",
    "\n",
    "It is recommended, but not required, to normalize an image batch.\n",
    "\n",
    ":::{.callout-important collapse=\"true\"}\n",
    "#### Important: IntToFloatTensor is a Required Batch Transform\n",
    "`fastai.data.transforms.IntToFloatTensor` is a required batch transform `batch_tfm` when training on image data for fastai features compatibility.\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Batch Transforms\n",
    "\n",
    "It's also possible to add any fastai batch transform to `Loader`'s `batch_tfms`, such as `fastai.vision.augment.aug_transforms`:\n",
    "\n",
    "```python\n",
    "batch_tfms = [\n",
    "    IntToFloatTensor,\n",
    "    *aug_transforms(),\n",
    "    Normalize(*imagenet_stats)\n",
    "]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Loader\n",
    "\n",
    "With the image pipelines, label pipeline, and batch transforms set up, we can now create our dataloaders, one `Loader` for train and one for valid.\n",
    "\n",
    "#### Training Loader\n",
    "\n",
    "Starting with the training `Loader`, we can manually set all the training specific arguments:\n",
    "\n",
    "```python\n",
    "Loader('imagenette_320_train.ffcv',\n",
    "    batch_size=64,\n",
    "    num_workers=num_cpus(),\n",
    "    os_cache=True,\n",
    "    order=OrderOption.RANDOM,\n",
    "    drop_last=True,\n",
    "    pipelines={'image': train_pipeline, 'label': label_pipeline},\n",
    "    batch_tfms=batch_tfms,\n",
    "    batches_ahead=2,\n",
    "    device='cuda',\n",
    "    split_idx=0,\n",
    "    n_inp=1\n",
    ")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loader Arguments\n",
    "\n",
    "There are a handful of important `Loader` arguments which need further explanation:\n",
    "\n",
    "- `order`: Controls how much memory is used for dataset caching and whether the dataset is randomly shuffled. Can be one of `RANDOM`, `QUASI_RANDOM`, or `SEQUENTIAL`. See the note below for more details. Defaults to `SEQUENTIAL`, which is unrandomized.\n",
    "\n",
    "- `os_cache`: By default, FFCV will attempt to cache the entire dataset into RAM using the operating system's caching. This can be changed by setting `os_cache=False` or setting the enviroment variable 'FFCV_DEFAULT_CACHE_PROCESS' to \"True\" or \"1\". If `os_cache=False` then `order` must be set to `QUASI_RANDOM` for the training `Loader`.\n",
    "\n",
    "- `num_workers`: If not set, will use all CPU cores up to 16 by default.\n",
    "\n",
    "- `batches_ahead`: Controls the number of batches ahead the `Loader` works. Increasing uses more RAM, both CPU and GPU. Defaults to 3.\n",
    "\n",
    "- `n_inp`: Controls which inputs to pass to the model. By default, set to number of pipelines minus 1.\n",
    "\n",
    "- `drop_last`: Whether to drop the last partial batch. By default, will set to True if `order` is `RANDOM` or `QUASI_RANDOM`, False if `SEQUENTIAL`.\n",
    "\n",
    "- `device`: The device to place the processed batches of data on. Defaults to `fastai.torch_core.default_device` if not set.\n",
    "\n",
    "- `split_idx`: This tells the fastai batch transforms what dataset they are operating on. By default will use 0 (train) if `order` is `RANDOM` or `QUASI_RANDOM`, 1 (valid) if `SEQUENTIAL`.\n",
    "\n",
    ":::{.callout-note collapse=\"false\"}\n",
    "#### Note: Order Memory Usage\n",
    "Each `order` option requires differing amounts of system memory.\n",
    "\n",
    "- `RANDOM` caches the entire dataset in memory for fast random sampling. `RANDOM` uses the most memory.\n",
    "\n",
    "- `QUASI_RANDOM` caches a subset of the dataset at a time in memory and randomly samples from the subset. Use when the entire dataset cannot fit into memory.\n",
    "\n",
    "- `SEQUENTIAL` requires least memory. It loads a few samples ahead of time. As the name suggests, it is not random, and primarly is for validation.\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplified Training Loader\n",
    "\n",
    "Since Imagenette 320 is small enough to load both the training and validation images into RAM[^assuming], the only arguments for the training `Loader` that must be set are:\n",
    "\n",
    "[^assuming]: Assuming your machine has more than 8GB of RAM.\n",
    "\n",
    "```python\n",
    "Loader('imagenette_320_train.ffcv',\n",
    "    batch_size=64,\n",
    "    order=OrderOption.RANDOM,\n",
    "    pipelines={'image': train_pipeline, 'label': label_pipeline},\n",
    "    batch_tfms=batch_tfms,\n",
    "    batches_ahead=2\n",
    ")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Loader\n",
    "\n",
    "Next is the validation `Loader`.\n",
    "\n",
    "```python\n",
    "Loader('imagenette_320_valid.ffcv',\n",
    "    batch_size=64,\n",
    "    pipelines={'image': valid_pipeline, 'label': label_pipeline},\n",
    "    batch_tfms=batch_tfms,\n",
    "    batches_ahead=2\n",
    ")\n",
    "```\n",
    "\n",
    "This example only sets the required arguments, relying on the `Loader` defaults for the rest."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all Together\n",
    "\n",
    "The last step is to wrap both the training and validation `Loader` in a `fastai.data.core.DataLoaders`.\n",
    "\n",
    "The example below shows all the steps covered so far in a single codeblock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|slow\n",
    "loaders = {}\n",
    "for i, name in enumerate(['train', 'valid']):\n",
    "    file = f'imagenette_320_{name}'\n",
    "\n",
    "    label_pipe = [\n",
    "        IntDecoder(), ft.ToTensorCategory(),\n",
    "        ft.Squeeze(), ft.ToDevice('cuda')\n",
    "    ]\n",
    "\n",
    "    if name is 'train':\n",
    "        image_pipe = [\n",
    "            RandomResizedCropRGBImageDecoder(output_size=(224,224), scale=(0.35, 1)),\n",
    "            ft.RandomHorizontalFlip(), ft.ToTensorImage(), ft.ToDevice('cuda')\n",
    "        ]\n",
    "        order = OrderOption.RANDOM\n",
    "    else:\n",
    "        image_pipe = [\n",
    "            CenterCropRGBImageDecoder(output_size=(224,224), ratio=1),\n",
    "            ft.ToTensorImage(), ft.ToDevice('cuda')\n",
    "        ]\n",
    "        order = OrderOption.SEQUENTIAL\n",
    "\n",
    "    batch_tfms = [IntToFloatTensor, *aug_transforms(), Normalize(*imagenet_stats)]\n",
    "\n",
    "    loaders[name] = Loader(f'{file}.ffcv',\n",
    "                        batch_size=64,\n",
    "                        order=order,\n",
    "                        pipelines={'image': image_pipe, 'label': label_pipe},\n",
    "                        batch_tfms=batch_tfms,\n",
    "                        batches_ahead=2\n",
    "                    )\n",
    "\n",
    "dls = DataLoaders(loaders['train'], loaders['valid'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with a fastxtend Loader\n",
    "\n",
    "With the `DataLoaders` created, the only thing left to do is create a `fastai.learner.Learner` with our model, optimizer, loss function, metrics, and callbacks of choice.\n",
    "\n",
    "Here we recreate the setup which should train Imagenette to ~92.9% in 315 seconds on a 3080 Ti, depending on randomness and hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|slow\n",
    "learn = Learner(dls, xresnet50(n_out=10), opt_func=ranger(foreach=True),\n",
    "                loss_func=nn.CrossEntropyLoss(label_smoothing=0.1), metrics=Accuracy(),\n",
    "                cbs=ProgressiveResize(increase_mode=IncreaseMode.Epoch)).to_channelslast()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first batch will be slower, as Numba needs to compile and FFCV needs to allocate memory for each transform and operation.\n",
    "\n",
    "Once the compilation is over, we will benefit from FFCV's accelerated data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|slow\n",
    "#|cuda\n",
    "learn.fit_flat_cos(20, 8e-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with FFCV and fastai\n",
    "\n",
    "While the fastxtend `Loader` and custom FFCV file format are great for accelerated training, they are not as useful for inference. This tutorial we will use the fastai DataBlock for inference.\n",
    "\n",
    "Since FFCV uses OpenCV for resizing and fastai uses Pillow, we cannot use the default fastai pipeline.\n",
    "\n",
    ":::{.callout-important collapse=\"true\"}\n",
    "#### Important: Unlike fastai, FFCV uses OpenCV\n",
    "FFCV is hardcoded to use OpenCV's `INTER_AREA` when resizing images, while fastai uses Pillow. This means we cannot use `fastai.vision.data.ImageBlock` for inference.\n",
    ":::\n",
    "\n",
    "fastxtend's [FFCV Inference module](ffcv.inference.html) provides a `FFAIImageBlock` and `FFAICenterCrop` item transform which use OpenCV for resizing images.\n",
    "\n",
    ":::{.callout-tip collapse=\"true\"}\n",
    "#### Tip: FFAI is for Inference\n",
    "Any class or method with the prefix FFAI is intended for inference after training with the fastxtend `Loader`.\n",
    ":::\n",
    "\n",
    "We can create an inference dataloader using the fastai DataBlock API which will create images identically to the `Loader` pipeline we created earlier in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|slow\n",
    "inference_dblock = DataBlock(blocks=(FFAIImageBlock, CategoryBlock),\n",
    "                             splitter=GrandparentSplitter(valid_name='val'),\n",
    "                             get_items=get_image_files, get_y=parent_label,\n",
    "                             item_tfms=[FFAICenterCrop(224)],\n",
    "                             batch_tfms=[*aug_transforms(), Normalize(*imagenet_stats)])\n",
    "inference_dls = dblock.dataloaders(source, bs=64, num_workers=num_cpus())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then assuming we had a folder with test images in `'/test'`, we'd set the `learn` dataloader to the new `inference_dls` and perform inference like normal.\n",
    "\n",
    "```python\n",
    "learn.dls = inference_dls\n",
    "test_dl = dls.test_dl('/test')\n",
    "preds, _ = learn.get_preds(dl=test_dl)\n",
    "```\n",
    "\n",
    "For more details on inference, check out my [Inference with fastai](https://benjaminwarner.dev/2021/10/01/inference-with-fastai) tutorial."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility and Other Limitations\n",
    "\n",
    "One downside of FFCV is it provides less reproducibility than most dataloader solutions.\n",
    "\n",
    "While `Loader` has a `seed` argument, it currently only affects the order data is loaded. With a couple exceptions, the Numba transforms are neither seeded nor reproducible. These transforms are also independent across pipelines.\n",
    "\n",
    "This means many image-to-image training tasks, such as image segmentation, cannot easily use `Loader` as the inputs and outputs will not be identically resized, flipped, etc.\n",
    "\n",
    "However, MetaAI recently announced [FFCV-SSL](https://github.com/facebookresearch/FFCV-SSL), a fork of FFCV which, among other things, has reproducible transforms. This FFCV limitation might be resolved in the not-too-distant future."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
