# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/optimizer.eightbit.ipynb.

# %% ../../nbs/optimizer.eightbit.ipynb 1
# Contains code from:
# bitsandbytes - MIT License - Copyright (c) Facebook, Inc. and its affiliates.
# fastai - Apache License 2.0 - Copyright (c) 2023 fast.ai

# %% ../../nbs/optimizer.eightbit.ipynb 4
from __future__ import annotations

from bitsandbytes.optim.optimizer import Optimizer1State, Optimizer2State, MockArgs
import bitsandbytes.functional as BF

from fastcore.basics import even_mults

from .utils import FastaiOptimizerAdapter, _convert_params
from ..imports import *

# %% auto 0
__all__ = ['SGD8bitOptimizer', 'RMSProp8bitOptimizer', 'AdamW8bitOptimizer', 'LARS8bitOptimizer', 'LAMB8bitOptimizer',
           'Lion8bitOptimizer']

# %% ../../nbs/optimizer.eightbit.ipynb 7
class EightBitFastaiAdapter(FastaiOptimizerAdapter):
    "Base for adding fastai optimizer functionality to eight-bit optimizers"

    def get_config(self, gindex, pindex, group):
        config = {}
        config["mom"] = group["mom"]
        config["sqr_mom"] = group["sqr_mom"]
        config["wd"] = group["wd"]
        config["eps"] = group["eps"]
        config["lr"] = group["lr"]
        config["optim_bits"] = self.args.optim_bits
        config["min_8bit_size"] = self.args.min_8bit_size
        config["percentile_clipping"] = self.args.percentile_clipping
        config["block_wise"] = self.args.block_wise
        config["max_unorm"] = self.args.max_unorm
        config["skip_zeros"] = self.args.skip_zeros

        if (gindex, pindex) in self.mng.index2config:
            config.update(self.mng.index2config[(gindex, pindex)])
        return config

# %% ../../nbs/optimizer.eightbit.ipynb 9
class EightBitCommon:
    "Common changes to EightBit Optimizers"
    @torch.no_grad()
    def step(self, closure=None):
        """Performs a single optimization step.

        Arguments:
            closure (callable, optional): A closure that reevaluates the model
                and returns the loss.
        """
        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        if not self.initialized:
            self.check_overrides()
            self.to_gpu()  # needed for fairseq pure fp16 training
            self.initialized = True

        #if self.is_paged: self.page_mng.prefetch_all()
        for gindex, group in enumerate(self.param_groups):
            for pindex, p in enumerate(group["params"]):
                if p.grad is None:
                    continue
                state = self.state[p]
                if 'step' not in state:
                    self.init_state(group, p, gindex, pindex)

                self.prefetch_state(p)
                self.update_step(group, p, gindex, pindex)
                if self.sync_each_step:
                    torch.cuda.synchronize()
        if self.is_paged or not self.sync_each_step:
            # all paged operation are asynchronous, we need
            # to sync to make sure all tensors are in the right state
            torch.cuda.synchronize()

        return loss

# %% ../../nbs/optimizer.eightbit.ipynb 11
class EightBit1StateOptimizer(EightBitCommon, EightBitFastaiAdapter, Optimizer1State):
    "Adds fastai optimizer functionality & compatability to `Optimizer1State`"
    def __init__(
        self,
        optimizer_name,
        params,
        lr=1e-3,
        mom=0.9,
        sqr_mom=0.0,
        eps=1e-8,
        wd=0.0,
        optim_bits=8,
        args=None,
        min_8bit_size=4096,
        percentile_clipping=100,
        block_wise=True,
        max_unorm=0.0,
        skip_zeros=False,
        is_paged=False,
        sync_each_step=False
    ):
        if not 0.0 <= lr:
            raise ValueError(f"Invalid learning rate: {lr}")
        if not 0.0 <= eps:
            raise ValueError(f"Invalid epsilon value: {eps}")
        if not 0.0 <= mom < 1.0:
            raise ValueError(f"Invalid mom value: {mom}")
        if not 0.0 <= sqr_mom < 1.0:
            raise ValueError(f"Invalid sqr_mom value: {sqr_mom}")
        if not 0.0 <= wd:
            raise ValueError(f"Invalid weight_decay value: {wd}")
        defaults = dict(lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)
        params = super().convert_params(params)
        super(Optimizer1State, self).__init__(params=params, defaults=defaults, optim_bits=optim_bits, is_paged=is_paged)

        if args is None:
            args = {}
            args["optim_bits"] = optim_bits
            args["percentile_clipping"] = 100
            args["min_8bit_size"] = min_8bit_size
            args["percentile_clipping"] = percentile_clipping
            args["block_wise"] = block_wise
            args["max_unorm"] = max_unorm
            args["skip_zeros"] = skip_zeros

            self.args = MockArgs(args)
        else:
            self.args = args

        self.optimizer_name = optimizer_name
        self.sync_each_step = sync_each_step

    @torch.no_grad()
    def update_step(self, group, p, gindex, pindex):
        state = self.state[p]
        grad = p.grad

        config = self.get_config(gindex, pindex, group)

        state["step"] += 1
        step = state["step"]

        if config["percentile_clipping"] < 100:
            current_gnorm, clip_value, gnorm_scale = BF.percentile_clipping(
                grad, state["gnorm_vec"], step, config["percentile_clipping"]
            )
        else:
            gnorm_scale = 1.0

        if state["state1"].dtype == torch.float:
            BF.optimizer_update_32bit(
                optimizer_name=self.optimizer_name,
                g=grad,
                p=p,
                state1=state["state1"],
                beta1=config["mom"],
                eps=config["eps"],
                step=step,
                lr=config["lr"],
                state2=None,
                beta2=config["sqr_mom"],
                weight_decay=state.get('wd', config['wd']) if state.get('do_wd', True) else 0.0,
                gnorm_scale=gnorm_scale,
                unorm_vec=state["unorm_vec"] if config["max_unorm"] > 0.0 else None,
                max_unorm=config["max_unorm"],
                skip_zeros=config["skip_zeros"],
            )

        elif state["state1"].dtype == torch.uint8 and not config["block_wise"]:
            BF.optimizer_update_8bit(
                optimizer_name=self.optimizer_name,
                g=grad,
                p=p,
                state1=state["state1"],
                state2=None,
                beta1=config["mom"],
                beta2=config['sqr_mom'],
                eps=config["eps"],
                step=step,
                lr=config["lr"],
                qmap1=state["qmap1"],
                qmap2=None,
                max1=state["max1"],
                max2=None,
                new_max1=state["new_max1"],
                new_max2=None,
                weight_decay=state.get('wd', config['wd']) if state.get('do_wd', True) else 0.0,
                gnorm_scale=gnorm_scale,
                unorm_vec=state["unorm_vec"] if config["max_unorm"] > 0.0 else None,
                max_unorm=config["max_unorm"],
            )

            state["max1"], state["new_max1"] = state["new_max1"], state["max1"]
        elif state["state1"].dtype == torch.uint8 and config["block_wise"]:
            BF.optimizer_update_8bit_blockwise(
                optimizer_name=self.optimizer_name,
                g=grad,
                p=p,
                state1=state["state1"],
                state2=None,
                beta1=config["mom"],
                beta2=config['sqr_mom'],
                eps=config["eps"],
                step=step,
                lr=config["lr"],
                qmap1=state["qmap1"],
                qmap2=None,
                absmax1=state["absmax1"],
                absmax2=None,
                weight_decay=state.get('wd', config['wd']) if state.get('do_wd', True) else 0.0,
                gnorm_scale=gnorm_scale,
                skip_zeros=config["skip_zeros"],
            )

# %% ../../nbs/optimizer.eightbit.ipynb 13
class EightBit2StateOptimizer(EightBitCommon, EightBitFastaiAdapter, Optimizer2State):
    "Adds fastai optimizer functionality & compatability to `Optimizer2State`"
    def __init__(
        self,
        optimizer_name,
        params,
        lr=1e-3,
        mom=0.9,
        sqr_mom=0.999,
        eps=1e-8,
        wd=0.0,
        optim_bits=8,
        args=None,
        min_8bit_size=4096,
        percentile_clipping=100,
        block_wise=True,
        max_unorm=0.0,
        skip_zeros=False,
        is_paged=False,
        sync_each_step=False
    ):
        if not 0.0 <= lr:
            raise ValueError(f"Invalid learning rate: {lr}")
        if not 0.0 <= eps:
            raise ValueError(f"Invalid epsilon value: {eps}")
        if not 0.0 <= mom < 1.0:
            raise ValueError(f"Invalid mom value: {mom}")
        if not 0.0 <= sqr_mom < 1.0:
            raise ValueError(f"Invalid sqr_mom value: {sqr_mom}")
        if not 0.0 <= wd:
            raise ValueError(f"Invalid weight_decay value: {wd}")
        defaults = dict(lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)
        params = super().convert_params(params)
        super(Optimizer2State, self).__init__(params, defaults, optim_bits, is_paged)

        if args is None:
            args = {}
            args["optim_bits"] = optim_bits
            args["percentile_clipping"] = 100
            args["min_8bit_size"] = min_8bit_size
            args["percentile_clipping"] = percentile_clipping
            args["block_wise"] = block_wise
            args["max_unorm"] = max_unorm
            args["skip_zeros"] = skip_zeros

            self.args = MockArgs(args)
        else:
            self.args = args

        self.optimizer_name = optimizer_name
        self.sync_each_step = sync_each_step

    @torch.no_grad()
    def update_step(self, group, p, gindex, pindex):
        state = self.state[p]
        grad = p.grad

        config = self.get_config(gindex, pindex, group)

        state["step"] += 1
        step = state["step"]

        if config["percentile_clipping"] < 100:
            current_gnorm, clip_value, gnorm_scale = BF.percentile_clipping(
                grad, state["gnorm_vec"], step, config["percentile_clipping"]
            )
        else:
            gnorm_scale = 1.0

        if state["state1"].dtype == torch.float:
            BF.optimizer_update_32bit(
                optimizer_name=self.optimizer_name,
                g=grad,
                p=p,
                state1=state["state1"],
                beta1=config["mom"],
                eps=config["eps"],
                step=step,
                lr=config["lr"],
                state2=state["state2"],
                beta2=config["sqr_mom"],
                weight_decay=state.get('wd', config['wd']) if state.get('do_wd', True) else 0.0,
                gnorm_scale=gnorm_scale,
                unorm_vec=state["unorm_vec"] if config["max_unorm"] > 0.0 else None,
                max_unorm=config["max_unorm"],
                skip_zeros=config["skip_zeros"],
            )

        elif state["state1"].dtype == torch.uint8 and not config["block_wise"]:
            BF.optimizer_update_8bit(
                optimizer_name=self.optimizer_name,
                g=grad,
                p=p,
                state1=state["state1"],
                state2=state["state2"],
                beta1=config["mom"],
                beta2=config['sqr_mom'],
                eps=config["eps"],
                step=step,
                lr=config["lr"],
                qmap1=state["qmap1"],
                qmap2=state["qmap2"],
                max1=state["max1"],
                max2=state["max2"],
                new_max1=state["new_max1"],
                new_max2=state["new_max2"],
                weight_decay=state.get('wd', config['wd']) if state.get('do_wd', True) else 0.0,
                gnorm_scale=gnorm_scale,
                unorm_vec=state["unorm_vec"] if config["max_unorm"] > 0.0 else None,
                max_unorm=config["max_unorm"],
            )

            # swap maxes
            state["max1"], state["new_max1"] = state["new_max1"], state["max1"]
            state["max2"], state["new_max2"] = state["new_max2"], state["max2"]
        elif state["state1"].dtype == torch.uint8 and config["block_wise"]:
            BF.optimizer_update_8bit_blockwise(
                optimizer_name=self.optimizer_name,
                g=grad,
                p=p,
                state1=state["state1"],
                state2=state["state2"],
                beta1=config["mom"],
                beta2=config['sqr_mom'],
                eps=config["eps"],
                step=step,
                lr=config["lr"],
                qmap1=state["qmap1"],
                qmap2=state["qmap2"],
                absmax1=state["absmax1"],
                absmax2=state["absmax2"],
                weight_decay=state.get('wd', config['wd']) if state.get('do_wd', True) else 0.0,
                gnorm_scale=gnorm_scale,
                skip_zeros=config["skip_zeros"],
            )

# %% ../../nbs/optimizer.eightbit.ipynb 16
class SGD8bitOptimizer(EightBit1StateOptimizer):
    "A fastai-compatible bitsandbytes 8-bit SGD optimizer"
    def __init__(
        self,
        params,
        lr,
        mom,
        wd=0,
        args=None,
        min_8bit_size=4096,
        percentile_clipping=100,
        block_wise=True,
        sync_each_step=False
    ):
        if mom == 0:
            raise NotImplementedError(f"8-bit SGD without momentum {mom=} is not supported")
        super().__init__(
            optimizer_name="momentum",
            params=params,
            lr=lr,
            mom=mom,
            sqr_mom=0.0,
            eps=0.0,
            wd=wd,
            optim_bits=8,
            args=args,
            min_8bit_size=min_8bit_size,
            percentile_clipping=percentile_clipping,
            block_wise=block_wise,
            sync_each_step=sync_each_step
        )

# %% ../../nbs/optimizer.eightbit.ipynb 17
class RMSProp8bitOptimizer(EightBit1StateOptimizer):
    "A fastai-compatible bitsandbytes 8-bit RMSProb optimizer"
    def __init__(
        self,
        params,
        lr=1e-2,
        sqr_mom=0.99,
        eps=1e-8,
        wd=0,
        args=None,
        min_8bit_size=4096,
        percentile_clipping=100,
        block_wise=True,
        sync_each_step=False
    ):
        if sqr_mom == 0:
            raise NotImplementedError(f"8-bit RMSProp with {sqr_mom=} is not supported")
        super().__init__(
            optimizer_name="rmsprop",
            params=params,
            lr=lr,
            mom=sqr_mom,
            sqr_mom=0.0,
            eps=eps,
            wd=wd,
            optim_bits=8,
            args=args,
            min_8bit_size=min_8bit_size,
            percentile_clipping=percentile_clipping,
            block_wise=block_wise,
            sync_each_step=sync_each_step
        )

# %% ../../nbs/optimizer.eightbit.ipynb 18
class AdamW8bitOptimizer(EightBit2StateOptimizer):
    "A fastai-compatible bitsandbytes 8-bit AdamW optimizer"
    def __init__(
        self,
        params,
        lr=1e-3,
        mom=0.9,
        sqr_mom=0.99,
        eps=1e-8,
        wd=1e-2,
        args=None,
        min_8bit_size=4096,
        percentile_clipping=100,
        block_wise=True,
        is_paged=False,
        sync_each_step=False
    ):
        super().__init__(
            optimizer_name="adam",
            params=params,
            lr=lr,
            mom=mom,
            sqr_mom=sqr_mom,
            eps=eps,
            wd=wd,
            optim_bits=8,
            args=args,
            min_8bit_size=min_8bit_size,
            percentile_clipping=percentile_clipping,
            block_wise=block_wise,
            is_paged=is_paged,
            sync_each_step=sync_each_step
        )

# %% ../../nbs/optimizer.eightbit.ipynb 19
class LARS8bitOptimizer(EightBit1StateOptimizer):
    "A fastai-compatible bitsandbytes 8-bit LARS optimizer"
    def __init__(
        self,
        params,
        lr,
        mom=0,
        wd=0,
        args=None,
        min_8bit_size=4096,
        percentile_clipping=100,
        trust_coeff=0.02,
        sync_each_step=False
    ):
        if mom == 0:
            raise NotImplementedError(f"8-bit LARS without momentum {mom=} is not supported")
        super().__init__(
            optimizer_name="lars",
            params=params,
            lr=lr,
            mom=mom,
            sqr_mom=0.0,
            eps=0.0,
            wd=wd,
            optim_bits=8,
            args=args,
            min_8bit_size=min_8bit_size,
            percentile_clipping=percentile_clipping,
            max_unorm=trust_coeff,
            block_wise=False,
            sync_each_step=sync_each_step,
        )

# %% ../../nbs/optimizer.eightbit.ipynb 20
class LAMB8bitOptimizer(EightBit2StateOptimizer):
    "A fastai-compatible bitsandbytes 8-bit LAMB optimizer"
    def __init__(
        self,
        params,
        lr=1e-3,
        mom=0.9,
        sqr_mom=0.999,
        eps=1e-8,
        wd=0,
        args=None,
        min_8bit_size=4096,
        percentile_clipping=100,
        block_wise=False,
        sync_each_step=False
    ):
        super().__init__(
            optimizer_name="lamb",
            params=params,
            lr=lr,
            mom=mom,
            sqr_mom=sqr_mom,
            eps=eps,
            wd=wd,
            optim_bits=8,
            args=args,
            min_8bit_size=min_8bit_size,
            percentile_clipping=percentile_clipping,
            block_wise=block_wise,
            max_unorm=1.0,
            sync_each_step=sync_each_step,
        )

# %% ../../nbs/optimizer.eightbit.ipynb 21
class Lion8bitOptimizer(EightBit1StateOptimizer):
    "A fastai-compatible bitsandbytes 8-bit Lion optimizer"
    def __init__(self,
        params,
        lr=1e-4,
        beta1=0.9,
        beta2=0.99,
        wd=0,
        args=None,
        min_8bit_size=4096,
        percentile_clipping=100,
        block_wise=True,
        is_paged=False,
        sync_each_step=False
    ):
        super().__init__(
            optimizer_name="lion",
            params=params,
            lr=lr,
            mom=beta1,
            sqr_mom=beta2,
            eps=0.0,
            wd=wd,
            optim_bits=8,
            args=args,
            min_8bit_size=min_8bit_size,
            percentile_clipping=percentile_clipping,
            block_wise=block_wise,
            is_paged=is_paged,
            sync_each_step=sync_each_step,
        )
