{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp vision.models.attention_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import math, torch\n",
    "import torch.nn as nn\n",
    "from fastai.torch_core import Module\n",
    "from fastai.layers import ConvLayer\n",
    "from torch.nn.parameter import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Attention Modules\n",
    "> Three attention modules in addition to Squeeze and Excitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Channel Attention -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efficient Channel Attention modified from https://github.com/BangguWu/ECANet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ECA(Module):\n",
    "    \"Efficient Channel Attention, from https://arxiv.org/abs/1910.03151.\"\n",
    "    def __init__(self, \n",
    "        nf, # number of input features\n",
    "        ks:int=None, # if set, Cov1D uses a fixed kernel size instead of adaptive kernel size\n",
    "        gamma:int=2, # used for adaptive kernel size, see paper for more details\n",
    "        beta:int=1 # used for adaptive kernel size, see paper for more details\n",
    "    ):\n",
    "        if ks is None:\n",
    "            ks = int(abs(math.log(nf, 2)+ beta) / gamma)\n",
    "            ks = max(3, ks if ks % 2 == 1 else ks + 1)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv = nn.Conv1d(1, 1, kernel_size=ks, padding=(ks-1)//2, bias=False) \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.avg_pool(x)\n",
    "        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)\n",
    "        y = self.sigmoid(y)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "# MIT License\n",
    "\n",
    "# Original Efficient Channel Attention implementation, Copyright (c) 2019 BangguWu, Qilong Wang \n",
    "\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Attention -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ShuffleAttention(Module):\n",
    "    \"Implementation of Shuffle Attention, from https://arxiv.org/abs/2102.00240\"\n",
    "    def __init__(self, \n",
    "        nf, # number of input features\n",
    "        groups=64 # number of subfeature groups, usually 32 or 64\n",
    "    ):\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.groups = groups\n",
    "        self.cweight = Parameter(torch.zeros(1, nf // (2 * groups), 1, 1))\n",
    "        self.cbias = Parameter(torch.ones(1, nf // (2 * groups), 1, 1))\n",
    "        self.sweight = Parameter(torch.zeros(1, nf // (2 * groups), 1, 1))\n",
    "        self.sbias = Parameter(torch.ones(1, nf // (2 * groups), 1, 1))\n",
    "        self.norm = nn.GroupNorm(nf // (2 * groups), nf // (2 * groups))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        x = x.reshape(b*self.groups, -1, h, w) # group into subfeatures\n",
    "        xc, xs = x.chunk(2, dim=1) # split channels\n",
    "\n",
    "        # channel attention\n",
    "        yc = self.pool(xc)\n",
    "        yc = self.cweight * yc + self.cbias\n",
    "        xc = xc * self.sigmoid(yc)\n",
    "\n",
    "        # spatial attention\n",
    "        ys = self.norm(xs)\n",
    "        ys = self.sweight * ys + self.sbias\n",
    "        xs = xs * self.sigmoid(ys)\n",
    "\n",
    "        # concat and shuffle\n",
    "        x = torch.cat([xc, xs], dim=1)\n",
    "        x = x.reshape(b, 2, -1, h, w)\n",
    "        x = x.permute(0, 2, 1, 3, 4)\n",
    "        return x.reshape(b, c, h, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet Attention -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Triplet Attention lightly modified from https://github.com/landskape-ai/triplet-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class ZPool(Module):\n",
    "    def forward(self, x):\n",
    "        return torch.cat((torch.max(x, 1)[0].unsqueeze(1), torch.mean(x, 1).unsqueeze(1)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class AttentionGate(Module):\n",
    "    def __init__(self, \n",
    "        ks:int=7 # kernel size for Conv2D\n",
    "    ):\n",
    "        self.compress = ZPool()\n",
    "        self.conv = ConvLayer(2, 1, ks, padding=(ks-1)//2, act_cls=None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_compress = self.compress(x)\n",
    "        x_out = self.conv(x_compress)\n",
    "        scale = torch.sigmoid_(x_out)\n",
    "        return x * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TripletAttention(Module):\n",
    "    \"Lightly modified implementation of Triplet Attention, from http://arxiv.org/abs/2010.03045\"\n",
    "    def __init__(self, \n",
    "        nf, # unused input features, for compatibility\n",
    "        ks:int=7, # kernel size for AttentionGate\n",
    "        no_spatial=False # exclude Spatial attention as third attention\n",
    "    ):\n",
    "        self.cw = AttentionGate(ks=ks)\n",
    "        self.hc = AttentionGate(ks=ks)\n",
    "        self.no_spatial = no_spatial\n",
    "        if not no_spatial:\n",
    "            self.hw = AttentionGate(ks=ks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_perm1 = x.permute(0, 2, 1, 3).contiguous()\n",
    "        x_out1 = self.cw(x_perm1)\n",
    "        x_out11 = x_out1.permute(0, 2, 1, 3).contiguous()\n",
    "        x_perm2 = x.permute(0, 3, 2, 1).contiguous()\n",
    "        x_out2 = self.hc(x_perm2)\n",
    "        x_out21 = x_out2.permute(0, 3, 2, 1).contiguous()\n",
    "        if not self.no_spatial:\n",
    "            x_out = self.hw(x)\n",
    "            x_out = 1 / 3 * (x_out + x_out11 + x_out21)\n",
    "        else:\n",
    "            x_out = 1 / 2 * (x_out11 + x_out21)\n",
    "        return x_out\n",
    "\n",
    "# MIT License\n",
    "\n",
    "# Original Triplet Attention implementation, Copyright (c) 2020 LandskapeAI\n",
    "\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
