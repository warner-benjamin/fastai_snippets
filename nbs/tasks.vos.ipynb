{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp tasks.vos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Beta] Virtual Outlier Synthesis\n",
    "> Means to train a fastai model using [Virtual Outlier Synthesis](https://arxiv.org/abs/2202.01197)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VOS requires the metrics refactor from this draft PR to work. You need to pip install from git.\n",
    "\n",
    "```\n",
    "pip install git+https://github.com/warner-benjamin/fastai.git@metrics_rewrite\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "\n",
    "# Original VOS implementation from https://github.com/deeplearning-wisc/vos\n",
    "\n",
    "# Original code, Copyright 2022 by Xuefeng Du, Zhaoning Wang, Mu Cai & Yixuan Li\n",
    "# Licensed under the Apache License, Version 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'RecorderCallback' from 'fastai.learner' (/home/benja/DataScience/fastai/fastai/learner.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7903/772721541.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdelegates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearner\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRecorderCallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplacing_yield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_core\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'RecorderCallback' from 'fastai.learner' (/home/benja/DataScience/fastai/fastai/learner.py)"
     ]
    }
   ],
   "source": [
    "#export\n",
    "from __future__ import annotations\n",
    "from enum import Enum, auto\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "from fastcore.basics import store_attr, ifnone\n",
    "from fastcore.foundation import L, mk_class, patch\n",
    "from fastcore.meta import delegates, contextmanager\n",
    "\n",
    "from fastai.learner import Learner, Recorder, replacing_yield\n",
    "from fastai.torch_core import Module, defaults, default_device\n",
    "from fastai.callback.core import Callback\n",
    "from fastai.callback.fp16 import MixedPrecision\n",
    "from fastai.data.transforms import get_c\n",
    "from fastai.vision.learner import create_body, num_features_model, apply_init, model_meta, _default_meta\n",
    "from fastai.losses import CrossEntropyLossFlat, LabelSmoothingCrossEntropy, LabelSmoothingCrossEntropyFlat, BaseLoss\n",
    "from fastai.data.block import TransformBlock\n",
    "from fastai.data.transforms import Categorize, CategoryMap, Category\n",
    "from fastai.torch_core import TensorCategory\n",
    "from fastai.layers import *\n",
    "\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporary Learner Stuff -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "from fastai.basics import Path\n",
    "from fastai.optimizer import Adam\n",
    "from fastai.learner import trainable_params, CancelFitException\n",
    "from fastcore.xtras import ContextManagers\n",
    "\n",
    "# This section adds LearnerState to `Learner` via patches, which currently is a draft pr in the fastai repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "class LearnerState(Enum):\n",
    "    \"All built in states for `Learner\"\n",
    "    Fit = 1\n",
    "    Eval = 2\n",
    "    LRFind = 3\n",
    "    ContFit = 4\n",
    "    Idle = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "@patch\n",
    "def __init__(self:Learner, dls, model, loss_func=None, opt_func=Adam, lr=defaults.lr, splitter=trainable_params, cbs=None,\n",
    "                metrics=None, path=None, model_dir='models', wd=None, wd_bn_bias=False, train_bn=True,\n",
    "                moms=(0.95,0.85,0.95)):\n",
    "    path = Path(path) if path is not None else getattr(dls, 'path', Path('.'))\n",
    "    if loss_func is None:\n",
    "        loss_func = getattr(dls.train_ds, 'loss_func', None)\n",
    "        assert loss_func is not None, \"Could not infer loss function from the data, please pass a loss function.\"\n",
    "    self.dls,self.model = dls,model\n",
    "    store_attr(but='dls,model,cbs')\n",
    "    self.training,self.create_mbar,self.logger,self.opt,self.cbs = False,True,print,None,L()\n",
    "    self.add_cbs(L(defaults.callbacks)+L(cbs))\n",
    "    self(\"after_create\")\n",
    "    self.state=LearnerState.Idle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "@patch\n",
    "@contextmanager\n",
    "def set_state(self:Learner, state): return replacing_yield(self, 'state', state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "@patch\n",
    "def fit(self:Learner, n_epoch, lr=None, wd=None, cbs=None, reset_opt=False):\n",
    "    with self.added_cbs(cbs), self.set_state(LearnerState.Fit):\n",
    "        if reset_opt or not self.opt: self.create_opt()\n",
    "        if wd is None: wd = self.wd\n",
    "        if wd is not None: self.opt.set_hypers(wd=wd)\n",
    "        self.opt.set_hypers(lr=self.lr if lr is None else lr)\n",
    "        self.n_epoch = n_epoch\n",
    "        self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "@patch\n",
    "def validation_context(self:Learner, cbs=None, inner=False, state=LearnerState.Eval):\n",
    "    cms = [self.no_logging(),self.no_mbar(),self.set_state(state)]\n",
    "    if cbs: cms.append(self.added_cbs(cbs))\n",
    "    if not inner: cms.append(self)\n",
    "    return ContextManagers(cms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOS Model Heads -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VOSHead(Module):\n",
    "    def __init__(self, \n",
    "        nf:int,\n",
    "        n_out:int,\n",
    "        lin_ftrs:list[int]|None=None,\n",
    "        p:float|None=None,\n",
    "        concat_pool:bool=True,\n",
    "        first_bn:bool=True,\n",
    "        bn_final:bool=False,\n",
    "        lin_first:bool=False,\n",
    "        y_range:float|None=None,\n",
    "        act_cls:Callable[...,nn.Module]=defaults.activation,\n",
    "        fastai_head:bool=False,\n",
    "        head_pool:Callable[[int,int],nn.Module]=AdaptiveAvgPool\n",
    "    ):\n",
    "        if fastai_head:\n",
    "            if concat_pool: nf *= 2\n",
    "            lin_ftrs = [nf, 512, n_out] if lin_ftrs is None else [nf] + lin_ftrs + [n_out]\n",
    "            bns = [first_bn] + [True]*len(lin_ftrs[1:])\n",
    "            ps = L(p if p is not None else 0.5)\n",
    "            if len(ps) == 1: ps = [ps[0]/2] * (len(lin_ftrs)-2) + ps\n",
    "            actns = [act_cls(inplace=True)] * (len(lin_ftrs)-2) + [None]\n",
    "            pool = AdaptiveConcatPool2d() if concat_pool else nn.AdaptiveAvgPool2d(1)\n",
    "            pool_layers = [pool, Flatten()]\n",
    "            if lin_first: pool_layers.append(nn.Dropout(ps.pop(0)))\n",
    "            self.pool = nn.Sequential(*pool_layers)\n",
    "            layers = []\n",
    "            for ni,no,bn,p,actn in zip(lin_ftrs[:-1], lin_ftrs[1:], bns, ps, actns):\n",
    "                layers += LinBnDrop(ni, no, bn=bn, p=p, act=actn, lin_first=lin_first)\n",
    "            if lin_first: layers.append(nn.Linear(lin_ftrs[-2], n_out))\n",
    "            if bn_final: layers.append(nn.BatchNorm1d(lin_ftrs[-1], momentum=0.01))\n",
    "            if y_range is not None: layers.append(SigmoidRange(*y_range))\n",
    "            self.head = nn.Sequential(*layers)\n",
    "        else:\n",
    "            p = p if p is not None else 0.0\n",
    "            self.pool = nn.Sequential(*[head_pool(sz=1, ndim=2), Flatten()])\n",
    "            self.head = nn.Sequential(*[nn.Dropout(p), nn.Linear(nf, n_out)])\n",
    "        self.weight_energy = torch.nn.Linear(n_out, 1)\n",
    "        self.log_reg = torch.nn.Linear(1, 2)\n",
    "        self.nf = nf\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        return self.head(x), x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(VOSHead.__init__)\n",
    "def create_vos_model(\n",
    "    arch:nn.Module, \n",
    "    n_out:int, \n",
    "    pretrained:bool=True, \n",
    "    cut:int|None=None, \n",
    "    n_in:int=3, \n",
    "    init:Callable=nn.init.kaiming_normal_, \n",
    "    custom_head:nn.Module|None=None,\n",
    "    concat_pool:bool=True, \n",
    "    **kwargs\n",
    "):\n",
    "    \"Create custom convnet architecture\"\n",
    "    meta = model_meta.get(arch, _default_meta)\n",
    "    body = create_body(arch, n_in, pretrained, ifnone(cut, meta['cut']))\n",
    "    if custom_head is None:\n",
    "        nf = num_features_model(nn.Sequential(*body.children()))\n",
    "        head = VOSHead(nf, n_out, concat_pool=concat_pool, **kwargs)\n",
    "    else: head = custom_head\n",
    "    model = nn.Sequential(body, head)\n",
    "    if init is not None: apply_init(model[1], init)\n",
    "    torch.nn.init.uniform_(model[1].weight_energy.weight)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOS Blocks -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TensorVOSCategory(TensorCategory):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VOSCategorize(Categorize):\n",
    "    \"Reversible transform of category string to `vocab` id\"\n",
    "    loss_func,order=CrossEntropyLossFlat(),1\n",
    "    def __init__(self, vocab=None, sort=True, add_na=False, ood_threshold=0.5):\n",
    "        if vocab is not None: vocab = CategoryMap(vocab, sort=sort, add_na=add_na)\n",
    "        store_attr()\n",
    "\n",
    "    def setups(self, dsets):\n",
    "        if self.vocab is None and dsets is not None:\n",
    "            vals = set()\n",
    "            for b in dsets: vals.add(b[0])\n",
    "            self.vocab = CategoryMap(list(vals), sort=self.sort, add_na=self.add_na)\n",
    "        self.c = len(self.vocab)\n",
    "\n",
    "    def encodes(self, o):\n",
    "        try:\n",
    "            return TensorVOSCategory([self.vocab.o2i[o[0]], o[1]])\n",
    "        except KeyError as e:\n",
    "            raise KeyError(f\"Label '{o}' was not included in the training dataset\") from e\n",
    "\n",
    "    def decodes(self, o):\n",
    "        if o[1] < self.ood_threshold:\n",
    "            return Category(self.vocab[o[0]])\n",
    "        else:\n",
    "            return(Category(\"Unknown: OOD\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def VOSCategoryBlock(vocab=None, sort=True, add_na=False, ood_threshold=0.5):\n",
    "    \"`TransformBlock` for single-label categorical targets\"\n",
    "    return TransformBlock(type_tfms=VOSCategorize(vocab=vocab, sort=sort,\n",
    "                                                  add_na=add_na, ood_threshold=ood_threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOS Loss Wrapper -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VOSLossWrapper():\n",
    "    def __init__(self, loss):\n",
    "        self.loss = loss\n",
    "        functools.update_wrapper(self, self.loss)\n",
    "\n",
    "    def __repr__(self): return f\"VOS {self.loss}\"\n",
    "    @property\n",
    "    def reduction(self): return self.loss.reduction\n",
    "    @reduction.setter\n",
    "    def reduction(self, v): self.loss.reduction = v\n",
    "\n",
    "    def __call__(self, inp, targ, **kwargs):\n",
    "        return self.loss.__call__(inp, targ.view(-1) if self.flatten else targ, **kwargs)\n",
    "\n",
    "    def to(self, device):\n",
    "        if isinstance(self.loss, (nn.Module, BaseLoss)): self.loss.to(device)\n",
    "\n",
    "    def decodes(self, x):\n",
    "        return (self.loss.decodes(x[0]), x[1].argmax(dim=-1))\n",
    "\n",
    "    def activation(self, x):\n",
    "        return (self.loss.activation(x[0]), x[1].softmax(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOS Callback -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VOSSampleStrat(Enum):\n",
    "    \"Sample features via simple for loop, partial for loop, or as a batch. Batch will affect variance of samples.\"\n",
    "    Paper = 1\n",
    "    Partial = 2\n",
    "    Batch = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def list_dist(mean_embed_id, temp_precision, n_generate, select, nf, n_out, start, step):\n",
    "    ood_samples = []\n",
    "    for idx in range(n_out):\n",
    "        new_dis = MultivariateNormal(mean_embed_id[idx], scale_tril=temp_precision)\n",
    "        negative_samples = new_dis.rsample((n_generate,))\n",
    "        prob_density = new_dis.log_prob(negative_samples)\n",
    "        \n",
    "        # keep the data in the low density area.\n",
    "        _, index_prob = torch.topk(-prob_density, select)\n",
    "        ood_samples.append(negative_samples[index_prob])\n",
    "    return torch.cat(ood_samples, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def partial_dist(mean_embed_id, temp_precision, n_generate, select, nf, n_out, start, step):\n",
    "    ood_samples = []\n",
    "    for idx in range(start, n_out, step):\n",
    "        new_dis = MultivariateNormal(mean_embed_id[idx], scale_tril=temp_precision)\n",
    "        negative_samples = new_dis.rsample((n_generate,))\n",
    "        prob_density = new_dis.log_prob(negative_samples)\n",
    "\n",
    "        # keep the data in the low density area.\n",
    "        _, index_prob = torch.topk(-prob_density, select)\n",
    "        for i in range(step):\n",
    "            ood_samples.append(negative_samples[index_prob[i]])\n",
    "    return torch.cat(ood_samples, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def batched_dist(mean_embed_id, temp_precision, n_generate, select, nf, n_out, iter):\n",
    "    new_dis = MultivariateNormal(mean_embed_id, scale_tril=temp_precision)\n",
    "    negative_samples = new_dis.rsample((n_generate,))\n",
    "    prob_density = new_dis.log_prob(negative_samples)\n",
    "\n",
    "    # keep the data in the low density area\n",
    "    _, idx_prob = torch.topk(-prob_density.t(), select)\n",
    "    negative_samples = negative_samples.permute(1,0,2)\n",
    "    return negative_samples.gather(dim=1, index=idx_prob.repeat(1,1,nf)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VOSCallback(Callback):\n",
    "    order = MixedPrecision.order-1\n",
    "    \"Base class with shared initializtion for Virtual Outlier Synthesis\"\n",
    "    def __init__(self, \n",
    "        n_out:int|None=None, # Number of labels, defaults to `learn.dls.c``\n",
    "        start_epoch:int|None=None, # Epoch to start Virtual Outlier Synthesis, defaults to `start_pct`\n",
    "        start_pct:float=0.40, # Iteration percent to start Virtual Outlier Synthesis, overriden by `start_epoch`\n",
    "        n_generate:int=10000, # Number of VOS candidates to generate per class\n",
    "        n_samples:int=1, # Number of VOS candidates to select per class\n",
    "        feature_cache:int=1000, # Number of class features to cache for VOS generation\n",
    "        vos_loss_weight:float=0.1, # Weight for VOS loss\n",
    "        vos_loss_func:nn.Module|None=None, # Type of Cross Entropy loss, defaults to `Learner.loss_func` or `nn.CrossEntropyLoss`\n",
    "        sample_strategy:VOSSampleStrat=VOSSampleStrat.Paper, # Vos Sampling Strategy to use\n",
    "        every_n_batches:int|None=None # Sample all classes `every_n_batches`. Use with `sample_strategy=VOSSampleStart.Partial`. Recommend to increase `n_sample`\n",
    "    ):\n",
    "        store_attr()\n",
    "        if sample_strategy==VOSSampleStrat.Paper:\n",
    "            self._sample_dist = list_dist\n",
    "        elif sample_strategy==VOSSampleStrat.Batch:\n",
    "            self._sample_dist = batched_dist\n",
    "        elif sample_strategy==VOSSampleStrat.Partial:\n",
    "            if every_n_batches is None: self.every_n_batches=2\n",
    "            if n_samples==1: \n",
    "                warn('Using the Partial sample strategy without increasing `n_samples`')\n",
    "                self.n_samples = every_n_batches\n",
    "            self._sample_dist = partial_dist\n",
    "\n",
    "    def before_fit(self):\n",
    "        assert isinstance(self.learn.model[1], VOSHead), 'VOSCallback requires model have a be `VOSHead'\n",
    "        self.mixed = hasattr(self.learn, 'mixed_precision')\n",
    "        if self.learn.state == LearnerState.Fit:\n",
    "            self.device = device = getattr(self.learn.dls, 'device', default_device())\n",
    "            self.n_out = ifnone(self.n_out, get_c(self.learn.dls))\n",
    "            self.nf = self.learn.model[1].nf\n",
    "            assert self.n_out, \"`n_out` is not defined and could not be inferred from data, set `dls.c` or pass `n_out`\"\n",
    "            self.eye_matrix = torch.eye(self.nf, device=self.device)\n",
    "            self.features = torch.zeros(self.n_out, self.feature_cache, self.nf).to(device=self.device)\n",
    "            self.number_dict = {}\n",
    "            for i in range(self.n_out): self.number_dict[i] = 0\n",
    "            self.sample_count, self.total_samples = 0, self.n_out * self.feature_cache\n",
    "        if isinstance(self.learn.loss_func, (nn.CrossEntropyLoss,CrossEntropyLossFlat,LabelSmoothingCrossEntropy,LabelSmoothingCrossEntropyFlat)):\n",
    "            self.vos_loss_func = self.learn.loss_func\n",
    "        elif self.vos_loss_func is None:\n",
    "            self.vos_loss_func = nn.CrossEntropyLoss()\n",
    "        self.train_vos = self.learn.state != LearnerState.LRFind\n",
    "        self.eval_vos  = self.learn.state == LearnerState.Eval\n",
    "        self.targ_vos  = isinstance(self.learn.dls.valid.one_batch()[1], TensorVOSCategory)\n",
    "        if self.targ_vos: self.learn.loss_func = VOSLossWrapper(self.learn.loss_func)\n",
    "        self.sample_iter=0\n",
    "\n",
    "    def after_pred(self):\n",
    "        pred, self.batch_features = self.learn.pred\n",
    "        self.targs = self.y\n",
    "        self.batch_features = self.batch_features.detach().clone()\n",
    "        self.learn.pred = pred\n",
    "        if not self.training and self.targ_vos and len(self.yb):\n",
    "            self.targs, self.energy_targ = self.targs.split(1,-1)\n",
    "            self.targs, self.energy_targ= self.targs.squeeze(1), self.energy_targ.squeeze(1)\n",
    "            self.learn.yb = tuple([self.targs],)\n",
    "\n",
    "    def after_loss(self):\n",
    "        if self.train_vos:\n",
    "            self.orig_loss = self.learn.loss.detach().clone()\n",
    "            if self.training:  self._vos_train_step()\n",
    "            elif len(self.yb): self._vos_valid_step()\n",
    "            else:              self._vos_pred_step()\n",
    "\n",
    "    def _vos_train_step(self):\n",
    "        cpu_targs = self.targs.cpu().int().numpy()\n",
    "        if self.sample_count >= self.total_samples:\n",
    "            # Enque and deque ID features\n",
    "            for idx, targ in enumerate(cpu_targs):\n",
    "                self.features[targ] = torch.cat((self.features[targ][1:], self.batch_features[idx].view(1, -1)), dim=0)\n",
    "\n",
    "            if self.learn.pct_train >= self.start_pct:\n",
    "                # the covariance finder needs the data to be centered\n",
    "                fs = self.features.shape\n",
    "                mean_embed_id = self.features.mean(1)\n",
    "                X = (self.features - mean_embed_id[:, None, :]).view(fs[0]*fs[1], fs[2])\n",
    "                \n",
    "                # add the variance.\n",
    "                temp_precision = torch.mm(X.t(), X) / len(X)\n",
    "                temp_precision += 0.0001 * self.eye_matrix\n",
    "                temp_precision = torch.linalg.cholesky(temp_precision)\n",
    "                \n",
    "                # gausian sample ood features\n",
    "                ood_samples = self._sample_dist(mean_embed_id, temp_precision, self.n_generate, self.n_samples, \n",
    "                                                self.nf, self.n_out, self.sample_iter, self.every_n_batches)\n",
    "\n",
    "                id_energy = self._log_sum_exp(self.learn.pred, 1)\n",
    "                ood_pred = self.learn.model[1].head(ood_samples)\n",
    "                ood_energy = self._log_sum_exp(ood_pred, 1)\n",
    "\n",
    "                self.energy_pred = torch.cat((id_energy, ood_energy), -1)\n",
    "                self.energy_targ = torch.cat((torch.ones(len(id_energy), device=self.device, dtype=torch.long), \n",
    "                                              torch.zeros(len(ood_pred), device=self.device, dtype=torch.long)), -1)\n",
    "\n",
    "                self.energy_pred = self.learn.model[1].log_reg(self.energy_pred.view(-1, 1))\n",
    "                if self.mixed: self.energy_pred = self.energy_pred.float()\n",
    "                self.energy_loss = self.vos_loss_weight * self.vos_loss_func(self.energy_pred, self.energy_targ)\n",
    "                self.learn.loss_grad += self.energy_loss\n",
    "                self.learn.loss += self.energy_loss.detach()\n",
    "                if self.sample_iter >= self.every_n_batches:\n",
    "                    self.sample_iter = 0\n",
    "                else:\n",
    "                    self.sample_iter+=1\n",
    "        else:\n",
    "            for idx, targ in enumerate(cpu_targs):\n",
    "                if self.number_dict[targ] < self.feature_cache:\n",
    "                    self.features[targ][self.number_dict[targ]] = self.batch_features[idx]\n",
    "                    self.number_dict[targ] += 1\n",
    "                    self.sample_count += 1\n",
    "                else:\n",
    "                    self.features[targ] = torch.cat((self.features[targ][1:], self.batch_features[idx].view(1, -1)), dim=0)\n",
    "            self.energy_loss = torch.tensor(0., device=self.device)\n",
    "\n",
    "    def _vos_valid_step(self):\n",
    "        if self.eval_vos or (self.learn.pct_train > self.start_pct and self.sample_count >= self.total_samples):\n",
    "            energy = self._log_sum_exp(self.learn.pred, 1)\n",
    "            if not self.targ_vos:            \n",
    "                self.energy_targ = torch.ones(len(energy), device=self.device, dtype=torch.long)\n",
    "            self.energy_pred = self.learn.model[1].log_reg(energy.view(-1, 1))\n",
    "\n",
    "            if self.mixed: self.energy_pred = self.energy_pred.float()\n",
    "            self.energy_loss = self.vos_loss_weight * self.vos_loss_func(self.energy_pred, self.energy_targ)\n",
    "            self.learn.loss += self.energy_loss.detach()\n",
    "        else:\n",
    "            self.energy_loss = torch.tensor(0., device=self.device)\n",
    "            self.energy_pred = torch.tensor([[1., 0.],[0., 1.]], device=self.device)\n",
    "            self.energy_targ = torch.tensor([1, 0], device=self.device)\n",
    "\n",
    "    def _vos_pred_step(self):\n",
    "        energy = self._log_sum_exp(self.learn.pred, 1)\n",
    "        self.energy_pred = self.learn.model[1].log_reg(energy.view(-1, 1))\n",
    "        if self.mixed: self.energy_pred = self.energy_pred.float()\n",
    "\n",
    "\n",
    "    def _log_sum_exp(self, value, dim=None, keepdim=False):\n",
    "        \"Numerically stable implementation of the operation value.exp().sum(dim, keepdim).log()\"\n",
    "        # TODO: torch.max(value, dim=None) threw an error at time of writing\n",
    "        if dim is not None:\n",
    "            m, _ = torch.max(value, dim=dim, keepdim=True)\n",
    "            value = value - m\n",
    "            if keepdim is False:\n",
    "                m = m.squeeze(dim)\n",
    "            return m + torch.log(torch.sum(F.relu(self.learn.model[1].weight_energy.weight) * torch.exp(value), dim=dim, keepdim=keepdim))\n",
    "        else:\n",
    "            m = torch.max(value)\n",
    "            sum_exp = torch.sum(torch.exp(value - m))\n",
    "            return m + torch.log(sum_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VOSEvalCallback(Callback):\n",
    "    order = Recorder.order+1\n",
    "    def before_fit(self):\n",
    "        assert hasattr(self.learn, 'vos'), 'VOSEvalCallback requires `VOSCallback'\n",
    "        self.voscb = self.learn.vos\n",
    "        self.train_vos = self.voscb.train_vos\n",
    "        self.eval_vos  = self.voscb.eval_vos\n",
    "        self.targ_vos  = self.voscb.targ_vos\n",
    "\n",
    "    def after_batch(self):\n",
    "        if not self.training:\n",
    "            if self.voscb.eval_vos or (self.learn.pct_train > self.start_pct and self.voscb.sample_count >= self.voscb.total_samples):\n",
    "                self.learn.pred = (self.learn.pred, self.voscb.energy_pred)\n",
    "                self.learn.yb = tuple([torch.stack([self.y, self.voscb.energy_targ], dim=1)],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOS Metrics -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "from fastai.learner import Metric, AvgMetric, AccumMetric, AvgSmoothMetric, ActivationType, MetricType, LogMetric, find_bs, AvgLoss, AvgSmoothLoss, to_detach\n",
    "from fastai.metrics import accuracy, skm_to_fastai\n",
    "import sklearn.metrics as skm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "class VOSMetric(Metric):\n",
    "    \"Blueprint for defining a metric with accumulate\"\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        self.pred, self.targ = learn.vos.energy_pred, learn.vos.energy_targ\n",
    "        if self.activation in [ActivationType.Softmax, ActivationType.BinarySoftmax]:\n",
    "            self.pred = F.softmax(self.pred, dim=self.dim_argmax)\n",
    "            if self.activation == ActivationType.BinarySoftmax: self.pred = self.pred[:, -1]\n",
    "        elif self.activation == ActivationType.Sigmoid: self.pred = torch.sigmoid(self.pred)\n",
    "        elif self.dim_argmax: self.pred = self.pred.argmax(dim=self.dim_argmax)\n",
    "        if self.thresh: self.pred = (self.pred >= self.thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(VOSMetric)\n",
    "class VOSAvgMetric(VOSMetric, AvgMetric):\n",
    "    \"Average the values of `func` taking into account potential different batch sizes\"\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        VOSMetric.accumulate(self, learn)\n",
    "        bs = find_bs(learn.yb)\n",
    "        if self.to_np: self.pred,self.targ = learn.to_detach(self.pred).numpy(),learn.to_detach(self.targ).numpy()\n",
    "        self.total += (self.func(self.targ, self.pred, **self.fkwargs) if self.invert_arg else self.func(self.pred, self.targ, **self.fkwargs))*bs\n",
    "        self.count += bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(VOSMetric)\n",
    "class VOSAccumMetric(VOSMetric, AccumMetric):\n",
    "    \"Stores predictions and targets on CPU in accumulate to perform final calculations with `func`.\"\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        \"Store targs and preds from `learn`, using activation function and argmax as appropriate\"\n",
    "        VOSMetric.accumulate(self, learn)\n",
    "        self.pred,self.targ = learn.to_detach(self.pred),learn.to_detach(self.targ)\n",
    "        self.accum_values(self.pred, self.targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(VOSMetric, but='log_metric')\n",
    "class VOSAvgSmoothMetric(VOSMetric, AvgSmoothMetric):\n",
    "    \"Smooth average the values of `func` (exponentially weighted with `beta`). Only computed on training set.\"\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        VOSMetric.accumulate(self, learn)\n",
    "        if self.to_np: self.pred,self.targ = learn.to_detach(self.pred).numpy(),learn.to_detach(self.targ).numpy()\n",
    "        val = self.func(self.targ, self.pred, **self.fkwargs) if self.invert_arg else self.func(self.pred, self.targ, **self.fkwargs)\n",
    "        if self.to_np: self.val = self.val*self.beta + val*(1-self.beta)\n",
    "        else: self.val = torch.lerp(to_detach(val, gather=False), self.val, self.beta)\n",
    "        self.count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(VOSMetric)\n",
    "def func_to_vos_metric(func, metric_type, is_class, thresh=None, axis=-1, activation=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Convert `func` metric to a VOS Metric\"\n",
    "\n",
    "    dim_argmax = axis if is_class and thresh is None else None\n",
    "    if activation is None:\n",
    "        activation = ActivationType.Sigmoid if (is_class and thresh is not None) else ActivationType.No\n",
    "\n",
    "    if metric_type==MetricType.Accum:\n",
    "        return VOSAccumMetric(func, dim_argmax=dim_argmax, activation=activation,\n",
    "                              thresh=thresh, log_metric=log_metric, **kwargs)\n",
    "    elif metric_type==MetricType.Avg:\n",
    "        return VOSAvgMetric(func, dim_argmax=dim_argmax, activation=activation,\n",
    "                            thresh=thresh, log_metric=log_metric, **kwargs)\n",
    "    elif metric_type==MetricType.Smooth:\n",
    "        if log_metric!=LogMetric.Train:\n",
    "            name = func.func.__name__ if hasattr(func, 'func') else  func.__name__\n",
    "            raise ValueError(f'Error with {name}: AvgSmoothMetric can only run on train. Set `log_metric` to LogMetric.Train.')\n",
    "        return VOSAvgSmoothMetric(func, dim_argmax=dim_argmax, activation=activation, thresh=thresh, **kwargs)\n",
    "    else:\n",
    "        name = func.func.__name__ if hasattr(func, 'func') else  func.__name__\n",
    "        raise ValueError(f\"Unsupported `metric_type` {metric_type} for metric {name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(Metric)\n",
    "def skm_to_vos_metric(func, is_class=True, thresh=None, axis=-1, activation=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Convert `func` from sklearn.metrics to a vos metric\"\n",
    "    return func_to_vos_metric(func, MetricType.Accum, is_class, thresh, axis, activation,\n",
    "                              log_metric, to_np=True, invert_arg=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def VOSAccuracy(axis=-1, metric_type=MetricType.Avg, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Compute VOS out of distribution detection accuracy\"\n",
    "    return func_to_vos_metric(accuracy, metric_type, True, axis=axis, log_metric=log_metric, name='vos_accuracy', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def VOSAUROC(axis=-1, average='macro', sample_weight=None, max_fpr=None, multi_class='raise', log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Area Under the Receiver Operating Characteristic Curve for VOS out of distribution detection\"\n",
    "    return skm_to_vos_metric(skm.roc_auc_score, axis=axis, activation=ActivationType.BinarySoftmax,\n",
    "                             average=average, sample_weight=sample_weight, max_fpr=max_fpr, multi_class=multi_class,\n",
    "                             log_metric=log_metric, name='vos_auroc', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def VOSAPScore(axis=-1, average='macro', pos_label=1, sample_weight=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Average Precision for VOS out of distribution detection\"\n",
    "    return skm_to_vos_metric(skm.average_precision_score, axis=axis, activation=ActivationType.BinarySoftmax,\n",
    "                             average=average, pos_label=pos_label, sample_weight=sample_weight, log_metric=log_metric,\n",
    "                             name='vos_ap_score', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def stable_cumsum(arr, rtol=1e-05, atol=1e-08):\n",
    "    \"\"\"Use high precision for cumsum and check that final value matches sum\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : array-like\n",
    "        To be cumulatively summed as flat\n",
    "    rtol : float\n",
    "        Relative tolerance, see ``np.allclose``\n",
    "    atol : float\n",
    "        Absolute tolerance, see ``np.allclose``\n",
    "    \"\"\"\n",
    "    out = np.cumsum(arr, dtype=np.float64)\n",
    "    expected = np.sum(arr, dtype=np.float64)\n",
    "    if not np.allclose(out[-1], expected, rtol=rtol, atol=atol):\n",
    "        raise RuntimeError('cumsum was found to be unstable: '\n",
    "                           'its last element does not correspond to sum')\n",
    "    return out\n",
    "\n",
    "\n",
    "def fpr_and_fdr_at_recall(y_true, y_score, recall_level=0.95, pos_label=None):\n",
    "    classes = np.unique(y_true)\n",
    "    if (pos_label is None and\n",
    "            not (np.array_equal(classes, [0, 1]) or\n",
    "                     np.array_equal(classes, [-1, 1]) or\n",
    "                     np.array_equal(classes, [0]) or\n",
    "                     np.array_equal(classes, [-1]) or\n",
    "                     np.array_equal(classes, [1]))):\n",
    "        raise ValueError(\"Data is not binary and pos_label is not specified\")\n",
    "    elif pos_label is None:\n",
    "        pos_label = 1.\n",
    "\n",
    "    # make y_true a boolean vector\n",
    "    y_true = (y_true == pos_label)\n",
    "\n",
    "    # sort scores and corresponding truth values\n",
    "    desc_score_indices = np.argsort(y_score, kind=\"mergesort\")[::-1]\n",
    "    y_score = y_score[desc_score_indices]\n",
    "    y_true = y_true[desc_score_indices]\n",
    "\n",
    "    # y_score typically has many tied values. Here we extract\n",
    "    # the indices associated with the distinct values. We also\n",
    "    # concatenate a value for the end of the curve.\n",
    "    distinct_value_indices = np.where(np.diff(y_score))[0]\n",
    "    threshold_idxs = np.r_[distinct_value_indices, y_true.size - 1]\n",
    "\n",
    "    # accumulate the true positives with decreasing threshold\n",
    "    tps = stable_cumsum(y_true)[threshold_idxs]\n",
    "    fps = 1 + threshold_idxs - tps      # add one because of zero-based indexing\n",
    "\n",
    "    thresholds = y_score[threshold_idxs]\n",
    "\n",
    "    recall = tps / tps[-1]\n",
    "\n",
    "    last_ind = tps.searchsorted(tps[-1])\n",
    "    sl = slice(last_ind, None, -1)      # [last_ind::-1]\n",
    "    recall, fps, tps, thresholds = np.r_[recall[sl], 1], np.r_[fps[sl], 0], np.r_[tps[sl], 0], thresholds[sl]\n",
    "\n",
    "    cutoff = np.argmin(np.abs(recall - recall_level))\n",
    "\n",
    "    return fps[cutoff] / (np.sum(np.logical_not(y_true)))   # , fps[cutoff]/(fps[cutoff] + tps[cutoff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def VOSFPR(axis=-1, recall_level=0.95, pos_label=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"False Positive Rate at Recall Level for VOS out of distribution detection\"\n",
    "    return skm_to_vos_metric(fpr_and_fdr_at_recall, axis=axis, activation=ActivationType.BinarySoftmax,\n",
    "                             recall_level=recall_level, pos_label=pos_label,  log_metric=log_metric, \n",
    "                             name=f'vos_fpr_{int(recall_level*100)}', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "class VOSAvgLoss(AvgLoss):\n",
    "    \"Average the MultiLoss losses taking into account potential different batch sizes\"\n",
    "    def __init__(self, name='vos_loss'): \n",
    "        self._name = name\n",
    "    \n",
    "    def accumulate(self, learn):\n",
    "        bs = find_bs(learn.yb)\n",
    "        self.total += learn.to_detach(learn.vos.energy_loss.mean())*bs\n",
    "        self.count += bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "class OrigAvgLoss(AvgLoss):\n",
    "    \"Average the MultiLoss losses taking into account potential different batch sizes\"\n",
    "    def __init__(self, name='orig_loss'):\n",
    "        self._name = name\n",
    "    \n",
    "    def accumulate(self, learn):\n",
    "        bs = find_bs(learn.yb)\n",
    "        self.total += learn.to_detach(learn.vos.orig_loss.mean())*bs\n",
    "        self.count += bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "class VOSAvgSmoothLoss(AvgSmoothLoss):\n",
    "    \"Smooth average of the MultiLoss losses (exponentially weighted with `beta`)\"\n",
    "    def __init__(self, name='vos_loss', beta=0.98):\n",
    "        super().__init__(beta)\n",
    "        self._name = name\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        self.count += 1\n",
    "        self.val = torch.lerp(to_detach(learn.vos.energy_loss.mean(), gather=False), self.val, self.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "class OrigAvgSmoothLoss(AvgSmoothLoss):\n",
    "    \"Smooth average of the MultiLoss losses (exponentially weighted with `beta`)\"\n",
    "    def __init__(self, name='orig_loss', beta=0.98): \n",
    "        super().__init__(beta)\n",
    "        self._name = name\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        self.count += 1\n",
    "        self.val = torch.lerp(to_detach(learn.vos.orig_loss.mean(), gather=False), self.val, self.beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOS Learner -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(create_vos_model)\n",
    "def vos_learner(dls, arch, n_out=None, pretrained=True, fastai_head=False, \n",
    "                vos_metrics=[VOSAccuracy(), VOSAPScore(), VOSFPR()],\n",
    "                # vos callback args\n",
    "                start_epoch:int|None=None, start_pct:float=0.40, n_generate:int=10000, n_samples:int=1, \n",
    "                feature_cache:int=1000, vos_loss_weight:float=0.1, vos_loss_func:nn.Module|None=None,\n",
    "                sample_strategy:VOSSampleStrat=VOSSampleStrat.Paper,\n",
    "                # learner args\n",
    "                loss_func=None, opt_func=Adam, lr=defaults.lr, splitter=None, cbs=None, metrics=None, path=None,\n",
    "                model_dir='models', wd=None, wd_bn_bias=False, train_bn=True, moms=(0.95,0.85,0.95),\n",
    "                # other model args\n",
    "                **kwargs):\n",
    "    \"Build a convnet style learner from `dls` and `arch`\"\n",
    "    meta = model_meta.get(arch, _default_meta)\n",
    "\n",
    "    if n_out is None: n_out = get_c(dls)\n",
    "    assert n_out, \"`n_out` is not defined and could not be inferred from data, set `dls.c` or pass `n_out`\"\n",
    "    model = create_vos_model(arch, n_out, pretrained=pretrained, fastai_head=fastai_head, **kwargs)\n",
    "\n",
    "    if metrics is None: metrics = L()\n",
    "    else: metrics=L(metrics)\n",
    "    metrics = L([OrigAvgSmoothLoss(), VOSAvgSmoothLoss(), OrigAvgLoss(), VOSAvgLoss()]) + metrics\n",
    "    for m in vos_metrics:\n",
    "        if m not in metrics:\n",
    "            metrics.append(m)\n",
    "    \n",
    "    vos_cb = VOSCallback(n_out, start_epoch, start_pct, n_generate, n_samples, \n",
    "                         feature_cache, vos_loss_weight, vos_loss_func, sample_strategy)\n",
    "    if cbs is None: cbs = vos_cb\n",
    "    else: cbs = L(cbs) + L(vos_cb)\n",
    "\n",
    "    if isinstance(dls.valid.one_batch()[1], TensorVOSCategory):\n",
    "        cbs += L(VOSEvalCallback())\n",
    "\n",
    "    splitter=ifnone(splitter, meta['split'])\n",
    "    learn = Learner(dls=dls, model=model, loss_func=loss_func, opt_func=opt_func, lr=lr, splitter=splitter, cbs=cbs,\n",
    "                   metrics=metrics, path=path, model_dir=model_dir, wd=wd, wd_bn_bias=wd_bn_bias, train_bn=train_bn,\n",
    "                   moms=moms)\n",
    "    if pretrained: learn.freeze()\n",
    "    # keep track of args for loggers\n",
    "    store_attr('arch,pretrained', self=learn, **kwargs)\n",
    "    return learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
