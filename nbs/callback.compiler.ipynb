{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp callback.compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "# Contains code from:\n",
    "# fastai - Apache License 2.0 - Copyright (c) 2023 fast.ai\n",
    "# PyTorch - PyTorch BSD-style license - Copyright (c) 2013-present PyTorch contributors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Compile\n",
    "> Callbacks and patches to integrate `torch.compile` into fastai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `CompilerCallback` and `DynamoExplainCallback` provide an easy to use `torch.compile` integration for fastai.\n",
    "\n",
    "For more information on `torch.compile` please read *[PyTorch's getting started](https://pytorch.org/docs/master/compile/get-started.html)* guide. For troubleshooting `torch.compile` refer to this [PyTorch Nightly guide](https://pytorch.org/docs/master/compile/index.html#troubleshooting-and-gotchas).\n",
    "\n",
    "This module is not imported via any fastxtend all imports. You must import it separately after importing fastai and fastxtend as it modifies model saving, loading, and training:\n",
    "\n",
    "```python\n",
    "from fastxtend.callback import compiler\n",
    "# or\n",
    "from fastxtend.callback.compiler import *\n",
    "```\n",
    "\n",
    "To use, create a `fastai.learner.Learner` with a `torch.compile` compatible model and call [`compile`](#learner.compile)  on the `Learner` or pass `CompilerCallback` to the `Learner` of fit method callbacks.\n",
    "```python\n",
    "Learner(...).compile()\n",
    "# or\n",
    "Learner(..., cbs=CompilerCallback())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "from typing import Dict\n",
    "\n",
    "from copy import deepcopy\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "from packaging.version import parse\n",
    "\n",
    "import torch._dynamo as dynamo\n",
    "from torch.serialization import FILE_LIKE\n",
    "\n",
    "from fastai.learner import Learner, save_model, join_path_file, _cast_tensor\n",
    "from fastai.callback import schedule\n",
    "from fastai.callback.core import Callback, TrainEvalCallback, CancelFitException\n",
    "\n",
    "import fastai\n",
    "if parse(fastai.__version__) < parse('2.7.13'):\n",
    "    from fastxtend.callback.amp import MixedPrecision\n",
    "else:\n",
    "    from fastai.callback.fp16 import MixedPrecision\n",
    "\n",
    "try:\n",
    "    from fastxtend.ffcv.loader import Loader\n",
    "    FFCV = True\n",
    "except ImportError:\n",
    "    FFCV = False\n",
    "\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastai.vision.learner import vision_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "_min_torch_20 = ismin_torch('2.0')\n",
    "_only_torch_20 = ismin_torch('2.0') and notmax_torch('2.1.0')\n",
    "_min_torch_21 = ismin_torch('2.1.0')\n",
    "\n",
    "if not _min_torch_20:\n",
    "    warn('Imported `fastxtend.callback.compiler`, which requires a minimum of PyTorch 2.0 to work.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def is_compiled(model:nn.Module):\n",
    "    \"Check whether a `nn.Module` model has been compiled\"\n",
    "    return (hasattr(model, '_compiled_call_impl') and getattr(model, '_compiled_call_impl') is not None) \\\n",
    "            or isinstance(model, dynamo.OptimizedModule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class CompileMode(str, Enum):\n",
    "    \"All valid `torch.compile` modes for tab-completion and typo-proofing\"\n",
    "    default        = 'default'\n",
    "    reduceoverhead = 'reduce-overhead'\n",
    "    maxautotune    = 'max-autotune'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the 'reduce-overhead' mode doesn't appear to work with all models, and 'max-autotune' shouldn't be used per *[Compile troubleshooting and gotchas](https://pytorch.org/docs/master/compile/index.html#troubleshooting-and-gotchas)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class MatMulPrecision(str, Enum):\n",
    "    \"All valid `matmul_precision` modes for tab-completion and typo-proofing\"\n",
    "    highest = 'highest'\n",
    "    high    = 'high'\n",
    "    medium  = 'medium'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CompilerCallback -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class CompilerCallback(Callback):\n",
    "    \"A callback for using `torch.compile` (beta) with fastai\"\n",
    "    order = TrainEvalCallback.order + 1 # Compiling needs to occur on the GPU, but before distributed training starts\n",
    "\n",
    "    def __init__(self,\n",
    "        fullgraph:bool=False, # Prevent breaking model into subgraphs\n",
    "        dynamic:bool=False, # Use dynamic shape tracing\n",
    "        backend:str|Callable='inductor', # `torch.compile` backend to use\n",
    "        mode:str|CompileMode|None=None, # `torch.compile` mode to use\n",
    "        options:Dict[str, Union[str,int,bool]]|None=None, # Extra options to pass to compile backend\n",
    "        matmul_precision:str|MatMulPrecision='high', # Set Ampere and newer matmul precision\n",
    "        recompile:bool=False, # Force a compiled model to recompile. Use when freezing/unfreezing a compiled model.\n",
    "        verbose:bool=False, # Verbose output\n",
    "    ):\n",
    "        mode = CompileMode(mode).value if mode is not None else mode\n",
    "        matmul_precision = MatMulPrecision(matmul_precision).value\n",
    "        if mode is not None and options is not None:\n",
    "            raise ValueError(f\"Both {mode=} or {options=} cannot be set at the same time.\")\n",
    "        store_attr(but='recompile')\n",
    "        self._recompile = recompile\n",
    "\n",
    "    def before_fit(self):\n",
    "        if not _min_torch_20:\n",
    "            self.run = False\n",
    "            warn(\"Attempting to use `CompilerCallback` without PyTorch 2.0 or greater. Disabling.\")\n",
    "            return\n",
    "\n",
    "        if torch.cuda.get_device_capability() >= (8, 0) and torch.get_float32_matmul_precision() != self.matmul_precision:\n",
    "            if self.verbose and self.matmul_precision != 'highest':\n",
    "                print(f\"Your GPU has modern tensor cores, automatically enabling by setting `torch.set_float32_matmul_precision('{self.matmul_precision}')`\")\n",
    "            torch.set_float32_matmul_precision(self.matmul_precision)\n",
    "\n",
    "        if hasattr(self.learn, 'progressive_resize') and _only_torch_20:\n",
    "            warn(\"Using `ProgressiveResize` and `torch.compile` at the same time with PyTorch 2.0 will result in a new compile every size change.\")\n",
    "        msg = \"\"\n",
    "        if self.dynamic and _only_torch_20:\n",
    "            msg += \"Using PyTorch 2.0 `compile` with dynamic shapes is under active development and might fail. Upgrade to PyTorch 2.1.\\n\"\n",
    "        if self.mode == 'max-autotune':\n",
    "            msg += \"Using `torch.compile` with `mode='max-autotune'` is under active development and might fail\\n\"\n",
    "        if msg != \"\":\n",
    "            msg += \"See https://pytorch.org/docs/master/compile/index.html#troubleshooting-and-gotchas for more details\"\n",
    "            warn(msg)\n",
    "        if self.mode == 'reduce-overhead':\n",
    "            warn(\"Using `torch.compile` & fastai with `mode='reduce-overhead'` currently doesn't appear to train.\")\n",
    "\n",
    "        if self._recompile and is_compiled(self.learn.model):\n",
    "            if self.verbose:\n",
    "                print(\"Recompiling model\")\n",
    "            self._reset_compiled()\n",
    "        self._recompile = False\n",
    "\n",
    "        if not is_compiled(self.learn.model):\n",
    "            if not isinstance(self.learn.model, nn.Module):\n",
    "                warn(\"Model is not \")\n",
    "            if hasattr(self.learn.model, 'compile'):\n",
    "                self.learn.model.compile(fullgraph=self.fullgraph, dynamic=self.dynamic,\n",
    "                                         backend=self.backend, mode=self.mode, options=self.options)\n",
    "            else:\n",
    "                compiled_model = torch.compile(self.learn.model, fullgraph=self.fullgraph,\n",
    "                                               dynamic=self.dynamic, backend=self.backend,\n",
    "                                               mode=self.mode, options=self.options)\n",
    "                self.learn.model = compiled_model._orig_mod\n",
    "                self.learn.model._orig_forward = self.learn.model.forward\n",
    "                self.learn.model.forward = compiled_model.dynamo_ctx(self.learn.model.forward)\n",
    "                self.learn.model._compiled_call_impl = True\n",
    "        else:\n",
    "            warn(\"Model is already compiled. To recompile pass `recomple=True` to CompilerCallback.\")\n",
    "\n",
    "    def _reset_compiled(self):\n",
    "        if is_compiled(self.learn.model):\n",
    "            dynamo.reset()\n",
    "            self.learn.model._compiled_call_impl = None\n",
    "            if hasattr(self.learn.model, '_orig_forward'):\n",
    "                self.learn.model.forward = self.learn.model._orig_forward\n",
    "            if isinstance(self.learn.model, dynamo.OptimizedModule):\n",
    "                self.learn.model = self.learn.model._orig_mod"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `torch.compile` with `mode='max-autotune'` is under active development and might fail. See *[Compile troubleshooting and gotchas](https://pytorch.org/docs/master/compile/index.html#troubleshooting-and-gotchas)* for more details.\n",
    "\n",
    "By default, <code>CompilerCallback</code> will set matmul ops to use TensorFloat32 for supported GPUs, which is the recommended setting for `torch.compile`. Set `matmul_precision='highest'` to turn off or `matmul_precision='medium'` to enable `bfloat16` mode.\n",
    "\n",
    "fastxtend provides the [`compile`](#learner.compile) convenience method for easily enabling `torch.compile`. Or you can pass <code>CompilerCallback</code> to the `cbs` argument of the `fastai.learner.Learner` or a fit method.\n",
    "\n",
    "```python\n",
    "learn = Learner(..., cbs=CompilerCallback())\n",
    "learn.fine_tune(1)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DynamoExplainCallback -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class DynamoExplainCallback(Callback):\n",
    "    \"A callback to automate finding graph breaks with PyTorch Compile's Dynamo Explain\"\n",
    "    order = MixedPrecision.order+1 # DynamoExplain occurs on the GPU before any training starts\n",
    "\n",
    "    def __init__(self,\n",
    "        print_results:bool=True, # Print enabled `torch._dynamo.explain` output(s)\n",
    "        out_guards:bool=False, # Print the `out_guards` output\n",
    "        ops_per_graph:bool=False, # Print the `ops_per_graph` output\n",
    "        break_reasons:bool=False, # Print the `break_reasons` output\n",
    "    ):\n",
    "        self.print_results = print_results\n",
    "        self.print_out_guards = out_guards\n",
    "        self.print_ops_per_graph = ops_per_graph\n",
    "        self.print_break_reasons = break_reasons\n",
    "\n",
    "    def before_fit(self):\n",
    "        if not _min_torch_20:\n",
    "            self.run = False\n",
    "            warn(\"Attempting to use `DynamoExplainCallback` without PyTorch 2.0 or greater. Canceling training.\")\n",
    "            raise CancelFitException()\n",
    "\n",
    "        self.explanation, self.out_guards, self.graphs, self.ops_per_graph, self.break_reasons, self.explanation_verbose = '','','','','',''\n",
    "        states = get_random_states()\n",
    "        try:\n",
    "            if FFCV and isinstance(self.dls.train, Loader) and self.dls.train.async_tfms:\n",
    "                # With `async_tfms`, `Loader` needs to initialize all `Loader.batches_ahead` Cuda streams\n",
    "                # for the training dataloader. Since FFCV doesn't support seeded transforms and the reset\n",
    "                # random state only seeds the dataset order, this shouldn't effect training outcome.\n",
    "                b = self.dls.train.one_batch(batches_ahead=True)\n",
    "            elif hasattr(self.dls.valid, 'one_batch'):\n",
    "                b = self.dls.valid.one_batch()\n",
    "            else:\n",
    "                b = next(iter(self.dls.valid))\n",
    "                model_device = next(self.model.parameters()).device\n",
    "                b = to_device(b, model_device)\n",
    "            i = getattr(self.dls, 'n_inp', 1 if len(b)==1 else len(b)-1)\n",
    "            self.learn.xb, self.learn.yb = b[:i], b[i:]\n",
    "\n",
    "            if hasattr(self.learn, 'mixed_precision'):\n",
    "                self.learn.mixed_precision.autocast.__enter__()\n",
    "\n",
    "            if _only_torch_20:\n",
    "                self.explanation, self.out_guards, self.graphs, self.ops_per_graph, self.break_reasons, self.explanation_verbose \\\n",
    "                    = dynamo.explain(self.learn.model, *_cast_tensor(self.learn.xb))\n",
    "            else:\n",
    "                self.explain_output = dynamo.explain(self.learn.model)(*_cast_tensor(self.learn.xb))\n",
    "\n",
    "            if hasattr(self.learn, 'mixed_precision'):\n",
    "                self.learn.mixed_precision.autocast.__exit__(None, None, None)\n",
    "\n",
    "            self.learn.opt.zero_grad()\n",
    "        finally:\n",
    "            set_random_states(**states)\n",
    "\n",
    "        if self.print_results:\n",
    "            print('\\nDynamo Explain Report\\n')\n",
    "            if _min_torch_21:\n",
    "                print_copy = deepcopy(self.explain_output)\n",
    "                if not self.print_ops_per_graph:\n",
    "                    print_copy.ops_per_graph = None\n",
    "                if not self.print_out_guards:\n",
    "                    print_copy.out_guards = None\n",
    "                print(print_copy)\n",
    "                print_copy = None\n",
    "            else:\n",
    "                output = \"Explanation:\\n\"\n",
    "                output += f\"  {self.explanation}\\n\"\n",
    "                output += \"Break Reasons:\\n\"\n",
    "                for idx, break_reason in enumerate(self.break_reasons):\n",
    "                    output += f\"  Break Reason {idx+1}:\\n\"\n",
    "                    output += f\"    Reason: {break_reason.reason}\\n\"\n",
    "                    output += \"    User Stack:\\n\"\n",
    "                    for frame_summary in break_reason.user_stack:\n",
    "                        output += f\"      {frame_summary}\\n\"\n",
    "\n",
    "                if self.ops_per_graph is not None and self.print_ops_per_graph:\n",
    "                    output += \"Ops per Graph:\\n\"\n",
    "                    for idx, ops in enumerate(self.ops_per_graph):\n",
    "                        output += f\"  Ops {idx+1}:\\n\"\n",
    "                        for op in ops:\n",
    "                            output += f\"    {op}\\n\"\n",
    "\n",
    "                if self.out_guards is not None and self.print_out_guards:\n",
    "                    output += \"Out Guards:\\n\"\n",
    "                    for i, guard in enumerate(self.out_guards[0]):\n",
    "                        output += f\"  Guard {i+1}:\\n\"\n",
    "                        output += f\"    {str(guard)}\"\n",
    "\n",
    "                print(output)\n",
    "\n",
    "            print('\\n')\n",
    "\n",
    "        raise CancelFitException()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DynamoExplainCallback` automates finding graph breaks using `torch._dynamo.explain` per the *[Identifying the cause of a graph break](https://pytorch.org/docs/stable/torch.compiler_faq.html#identifying-the-cause-of-a-graph-break)* section in the *[PyTorch Compile FAQ](https://pytorch.org/docs/stable/torch.compiler_faq.html)*. <code>DynamoExplainCallback</code> uses one batch from the validation dataloader[^Loader] to generate the `_dynamo.explain` report(s) and then cancels training.\n",
    "\n",
    "To use, pass <code>DynamoExplainCallback</code> to the `cbs` argument of the `fastai.learner.Learner` or fit method.\n",
    "\n",
    "```python\n",
    "learn = Learner(..., cbs=DynamoExplainCallback())\n",
    "learn.fit(1)\n",
    "```\n",
    "\n",
    "By default, <code>DynamoExplainCallback</code> prints the basic explanation output from `_dynamo.explain`, with arguments to enable printing `out_guards` and/or `ops_per_graph`.\n",
    "\n",
    "All `_dynamo.explain` outputs are stored as attributes in the callback for later reference. For example, to view the out_guards after running `Learner` with <code>DynamoExplainCallback</code>:\n",
    "\n",
    "```python\n",
    "# PyTorch 2.0\n",
    "print(learn.dynamo_explain.out_guards)\n",
    "\n",
    "# PyTorch 2.1\n",
    "print(learn.dynamo_explain.explain_output.out_guards)\n",
    "```\n",
    "\n",
    "[^Loader]: Unless using the FFCV `Loader`, then it uses the training dataloader. This doesn't effect seeded training as FFCV dataloaders do not seed transforms, only dataset order."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convenience Method\n",
    "\n",
    "fastxtend adds a convenience method to `fastai.learner.Learner` to easily enable `torch.compile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def compile(self:Learner,\n",
    "    fullgraph:bool=False, # Prevent breaking model into subgraphs\n",
    "    dynamic:bool=False, # Use dynamic shape tracing. Sets to `False` if PyTorch < 2.1\n",
    "    backend:str|Callable='inductor', # `torch.compile` backend to use\n",
    "    mode:str|CompileMode|None=None, # `torch.compile` mode to use\n",
    "    options:Dict[str, Union[str,int,bool]]|None=None, # Extra options to pass to compile backend\n",
    "    matmul_precision:str|MatMulPrecision='high', # Set Ampere and newer matmul precision\n",
    "    recompile:bool=False, # Force a compiled model to recompile. Use when freezing/unfreezing a compiled model.\n",
    "    verbose:bool=False, # Verbose output\n",
    "):\n",
    "    \"Set `Learner` to compile model using `torch.compile` via `CompilerCallback`\"\n",
    "    return self.add_cb(CompilerCallback(fullgraph=fullgraph, dynamic=dynamic if _min_torch_21 else False,\n",
    "                                        backend=backend, mode=mode, options=options,\n",
    "                                        matmul_precision=matmul_precision,\n",
    "                                        recompile=recompile, verbose=verbose))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`compile` only sets `dynamic` if using PyTorch 2.1 or later, for PyTorch 2.0 it's hardcoded to `False`. You can override this by directly setting via `CompilerCallback`.\n",
    "\n",
    "To use, call the `compile` method after initalizing a `fastai.learner.Learner`.\n",
    "\n",
    "```python\n",
    "learn = Learner(...).compile()\n",
    "learn.fine_tune(1)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compatability Patches\n",
    "\n",
    "These patches integrate `torch.compile` with fastai exporting, loading, freezing, unfreezing, and fine tuning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def export(self:Learner,\n",
    "    fname:FILE_LIKE='export.pkl', # Learner export file name, path, bytes, or IO\n",
    "    pickle_module:Any=pickle, # Module used for pickling metadata and objects\n",
    "    pickle_protocol:int=2 # Pickle protocol used\n",
    "):\n",
    "    \"Export the content of `self` without the items and the optimizer state for inference\"\n",
    "    if rank_distrib(): return # don't export if child proc\n",
    "    self._end_cleanup()\n",
    "    old_dbunch = self.dls\n",
    "    self.dls = self.dls.new_empty()\n",
    "    state = self.opt.state_dict() if self.opt is not None else None\n",
    "    self.opt = None\n",
    "    compiled_forward = None\n",
    "    # torch.compiled models currently cannot be pickled.\n",
    "    if _only_torch_20 and is_compiled(self.model):\n",
    "        compiled_forward = self.model.forward\n",
    "        self.model.forward = self.model._orig_forward\n",
    "        delattr(self.model, '_orig_forward')\n",
    "    with warnings.catch_warnings():\n",
    "        # To avoid the warning that come from PyTorch about model not being checked\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        torch.save(self, self.path/fname, pickle_module=pickle_module, pickle_protocol=pickle_protocol)\n",
    "    if _min_torch_20 and compiled_forward is not None:\n",
    "        self.model._orig_forward = self.model.forward\n",
    "        self.model.forward = compiled_forward\n",
    "    self.create_opt()\n",
    "    if state is not None: self.opt.load_state_dict(state)\n",
    "    self.dls = old_dbunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def load_learner(\n",
    "    fname:FILE_LIKE, # File name, path, bytes, or IO\n",
    "    cpu:bool=True, # Load model to CPU\n",
    "    pickle_module=pickle # Module used for unpickling metadata and objects\n",
    "):\n",
    "    \"Load a `Learner` object in `fname`, by default putting it on the `cpu`\"\n",
    "    distrib_barrier()\n",
    "    map_loc = 'cpu' if cpu else default_device()\n",
    "    try: res = torch.load(fname, map_location=map_loc, pickle_module=pickle_module)\n",
    "    except AttributeError as e:\n",
    "        e.args = [f\"Custom classes or functions exported with your `Learner` not available in namespace.\\Re-declare/import before loading:\\n\\t{e.args[0]}\"]\n",
    "        raise\n",
    "    if cpu:\n",
    "        res.dls.cpu()\n",
    "        if hasattr(res, 'channels_last'):\n",
    "            res = res.to_contiguous(to_fp32=True)\n",
    "        elif hasattr(res, 'mixed_precision'):\n",
    "            res = res.to_fp32()\n",
    "        elif hasattr(res, 'non_native_mixed_precision'):\n",
    "            res = res.to_non_native_fp32()\n",
    "        if hasattr(res, 'compiler'):\n",
    "            res = res.remove_cb(CompilerCallback)\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `load_learner` will remove the `CompilerCallback`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing and Unfreezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def freeze_to(self:Learner, n:int):\n",
    "    \"Freeze parameter groups up to `n`\"\n",
    "    if self.opt is None:\n",
    "        self.create_opt()\n",
    "    self.opt.freeze_to(n)\n",
    "    self.opt.clear_state()\n",
    "    if _min_torch_20 and is_compiled(self.model):\n",
    "        if hasattr(self, 'compiler'):\n",
    "            self.compiler._recompile = True\n",
    "        else:\n",
    "            warn(\"Freezing or unfreezing a compiled model isn't supported.\"\\\n",
    "                 \"\\nThe model must be recompiled to take effect.\"\\\n",
    "                 \"\\nPass `CompilerCallback(..., recompile=True)` to `Learner.cbs`\"\\\n",
    "                 \"\\nor call `torch._dynamo.reset() and recompile model.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freezing and unfreezing models works, but they need to be recompiled after. `freeze_to` will set `CompilerCallback` to recompile the model or warn users they need to manually recompile."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "@delegates(Learner.fit_one_cycle)\n",
    "def fine_tune(self:Learner,\n",
    "    epochs:int, # Number of unfrozen epochs to train\n",
    "    base_lr:float=2e-3, # Base learning rate, model head unfrozen learning rate\n",
    "    freeze_epochs:int=1, # Number of frozen epochs to train\n",
    "    lr_mult:Numeric=100, # Model stem unfrozen learning rate: `base_lr/lr_mult`\n",
    "    pct_start:float=0.3, # Start unfrozen learning rate cosine annealing\n",
    "    div:Numeric=5.0, # Initial unfrozen learning rate: `base_lr/div`\n",
    "    compile_frozen:bool=False, # Compile model during frozen finetuning if `CompilerCallback` is used\n",
    "    **kwargs\n",
    "):\n",
    "    \"Fine tune with `Learner.freeze` for `freeze_epochs`, then with `Learner.unfreeze` for `epochs`, using discriminative LR.\"\n",
    "    self.freeze()\n",
    "    if _min_torch_20 and hasattr(self, 'compiler') and not compile_frozen:\n",
    "        self.compiler.run = is_compiled(self.model)\n",
    "    self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs)\n",
    "    base_lr /= 2\n",
    "    self.unfreeze()\n",
    "    self.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `fine_tune` will not compile the `freeze_epochs`, but this can be overridden by passing `freeze_compile=True`. If the model is already compiled, this will have no effect."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "from packaging.version import parse\n",
    "import tempfile\n",
    "\n",
    "import fastai\n",
    "\n",
    "from fastcore.basics import num_cpus\n",
    "\n",
    "if parse(fastai.__version__) < parse('2.7.11'):\n",
    "    from fastxtend.callback.channelslast import *\n",
    "else:\n",
    "    from fastai.callback.channelslast import *\n",
    "from fastai.data.external import URLs, untar_data\n",
    "from fastai.data.block import DataBlock, CategoryBlock\n",
    "from fastai.data.transforms import GrandparentSplitter, get_image_files, parent_label, Normalize\n",
    "from fastai.vision.augment import Resize, aug_transforms\n",
    "from fastai.vision.core import imagenet_stats\n",
    "from fastai.vision.data import ImageBlock\n",
    "from fastai.vision.models import resnet34\n",
    "\n",
    "from fastxtend.metrics import Accuracy\n",
    "from fastxtend.optimizer.fused import adam\n",
    "from fastxtend.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.858504</td>\n",
       "      <td>2.472007</td>\n",
       "      <td>0.326624</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.443800</td>\n",
       "      <td>1.235949</td>\n",
       "      <td>0.599490</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.099457</td>\n",
       "      <td>0.968236</td>\n",
       "      <td>0.686879</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "imagenette = untar_data(URLs.IMAGENETTE_160)\n",
    "\n",
    "with less_random():\n",
    "    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "                       splitter=GrandparentSplitter(valid_name='val'),\n",
    "                       get_items=get_image_files, get_y=parent_label,\n",
    "                       item_tfms=Resize(128),\n",
    "                       batch_tfms=[*aug_transforms(), Normalize.from_stats(*imagenet_stats)])\n",
    "    dls = dblock.dataloaders(imagenette, bs=128, num_workers=num_cpus(), pin_memory=True)\n",
    "\n",
    "    learn = Learner(dls, resnet34(num_classes=dls.c), opt_func=adam(foreach=True),\n",
    "                    metrics=Accuracy()).to_channelslast().compile(fullgraph=True)\n",
    "    learn.fit_one_cycle(3, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "free_gpu_memory(learn, dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.755060</td>\n",
       "      <td>0.175099</td>\n",
       "      <td>0.945223</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.248197</td>\n",
       "      <td>0.140395</td>\n",
       "      <td>0.957452</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.176600</td>\n",
       "      <td>0.125655</td>\n",
       "      <td>0.960764</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.111107</td>\n",
       "      <td>0.104562</td>\n",
       "      <td>0.967389</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "imagenette = untar_data(URLs.IMAGENETTE_160)\n",
    "\n",
    "with less_random():\n",
    "    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "                       splitter=GrandparentSplitter(valid_name='val'),\n",
    "                       get_items=get_image_files, get_y=parent_label,\n",
    "                       item_tfms=Resize(128),\n",
    "                       batch_tfms=[*aug_transforms(), Normalize.from_stats(*imagenet_stats)])\n",
    "    dls = dblock.dataloaders(imagenette, bs=128, num_workers=num_cpus(), pin_memory=True)\n",
    "\n",
    "    learn = vision_learner(dls, resnet34, opt_func=adam(foreach=True),\n",
    "                           metrics=Accuracy()).to_channelslast()\n",
    "    learn.fine_tune(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "free_gpu_memory(learn, dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.759313</td>\n",
       "      <td>0.172038</td>\n",
       "      <td>0.945987</td>\n",
       "      <td>00:40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.249120</td>\n",
       "      <td>0.127831</td>\n",
       "      <td>0.959236</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.171835</td>\n",
       "      <td>0.124405</td>\n",
       "      <td>0.962038</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.108882</td>\n",
       "      <td>0.104857</td>\n",
       "      <td>0.967389</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "imagenette = untar_data(URLs.IMAGENETTE_160)\n",
    "\n",
    "with less_random():\n",
    "    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "                       splitter=GrandparentSplitter(valid_name='val'),\n",
    "                       get_items=get_image_files, get_y=parent_label,\n",
    "                       item_tfms=Resize(128),\n",
    "                       batch_tfms=[*aug_transforms(), Normalize.from_stats(*imagenet_stats)])\n",
    "    dls = dblock.dataloaders(imagenette, bs=128, num_workers=num_cpus(), pin_memory=True)\n",
    "\n",
    "    learn = vision_learner(dls, resnet34, opt_func=adam(foreach=True),\n",
    "                           metrics=Accuracy()).to_channelslast().compile()\n",
    "    learn.fine_tune(3, compile_frozen=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(#2) [0.10485660284757614,0.9673885107040405]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "tmp_de = tempfile.TemporaryDirectory(dir=learn.path)\n",
    "tmp_pe = Path(Path(tmp_de.name).stem)\n",
    "tmp_ds = tempfile.TemporaryDirectory(dir=learn.path/learn.model_dir)\n",
    "tmp_ps = Path(Path(tmp_ds.name).stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('models/tmpbpa6ct7y/test.pth')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "learn.save(tmp_ps/'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "learn.export(tmp_pe/'export.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(#2) [0.10484044998884201,0.9673885107040405]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "saved = vision_learner(dls, resnet34, opt_func=adam(foreach=True),\n",
    "                       metrics=Accuracy()).to_channelslast()\n",
    "saved.load(tmp_ps/'test')\n",
    "saved.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "exported = load_learner(tmp_pe/'export.pkl')\n",
    "exported.dls = learn.dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "tmp_ds.cleanup()\n",
    "tmp_de.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/31 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(#2) [0.10487388074398041,0.9673885107040405]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "exported.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "free_gpu_memory(learn, dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dynamo Explain Report\n",
      "\n",
      "Graph Count: 1\n",
      "Graph Break Count: 0\n",
      "Op Count: 17\n",
      "Break Reasons:\n",
      "Compile Times: TorchDynamo compilation metrics:\n",
      "Function                          Runtimes (s)\n",
      "------------------------------  --------------\n",
      "_compile                                0.8507\n",
      "OutputGraph.call_user_compiler          0.0013\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "imagenette = untar_data(URLs.IMAGENETTE_160)\n",
    "\n",
    "with less_random():\n",
    "    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "                        splitter=GrandparentSplitter(valid_name='val'),\n",
    "                        get_items=get_image_files, get_y=parent_label,\n",
    "                        item_tfms=Resize(128),\n",
    "                        batch_tfms=[Normalize.from_stats(*imagenet_stats)])\n",
    "    dls = dblock.dataloaders(imagenette, bs=64, num_workers=num_cpus(), pin_memory=True)\n",
    "\n",
    "    learn = vision_learner(dls, resnet34, opt_func=adam(foreach=True), metrics=Accuracy(),\n",
    "                           cbs=DynamoExplainCallback()).to_channelslast()\n",
    "    learn.fit(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
