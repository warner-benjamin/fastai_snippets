{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp ffcv.writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "# Contains code from:\n",
    "# FFCV - Apache License 2.0 - Copyright (c) 2022 FFCV Team"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFCV Writer\n",
    "> FFCV's `DatasetWriter` Modified to Support fastxtend's `RGBImageField`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastxtend's `RGBImageField` only differs in encoding from [FFCV's RGBImageField](https://docs.ffcv.io/api/fields.html#ffcv.fields.RGBImageField). Decoding is the same for both.\n",
    "\n",
    "This module modifies FFCV's DatasetWriter to write fastxtend's <code>RGBImageField</code> as FFCV's RGBImageField during dataset creation so both [FFCV's Loader](https://docs.ffcv.io/making_dataloaders.html) and fastxtend's `Loader` will read <code>RGBImageField</code> without requiring a custom field.\n",
    "\n",
    "For dataset interoperability, use fastxtend's `DatasetWriter` when creating FFCV datasets using fastxtendâ€™s `RGBImageField`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from typing import Mapping\n",
    "import numpy as np\n",
    "from multiprocessing import shared_memory, cpu_count\n",
    "\n",
    "from ffcv.utils import is_power_of_2\n",
    "from ffcv.types import get_metadata_type, HeaderType, FieldDescType, CURRENT_VERSION\n",
    "from ffcv.fields import Field, BytesField, IntField, FloatField, NDArrayField, JSONField, TorchTensorField\n",
    "from ffcv.fields import RGBImageField as _RGBImageField\n",
    "from ffcv.writer import DatasetWriter as _DatasetWriter\n",
    "from ffcv.writer import MIN_PAGE_SIZE, MAX_PAGE_SIZE\n",
    "\n",
    "from fastxtend.ffcv.fields import RGBImageField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "HANDLER_TYPE_ID = {\n",
    "    None: 255, \n",
    "    FloatField: 0, \n",
    "    IntField: 1, \n",
    "    _RGBImageField: 2, \n",
    "    RGBImageField: 2,\n",
    "    BytesField: 3, \n",
    "    NDArrayField: 4, \n",
    "    JSONField: 5, \n",
    "    TorchTensorField: 6\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DatasetWriter -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class DatasetWriter(_DatasetWriter):\n",
    "    \"\"\"Writes given dataset into FFCV format (.beton).\n",
    "    Supports indexable objects (e.g., PyTorch Datasets) and webdataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fname: str\n",
    "        File name to store dataset in FFCV format (.beton)\n",
    "    fields : Mapping[str, Field]\n",
    "        Map from keys to Field's (order matters!)\n",
    "    page_size : int\n",
    "        Page size used internally\n",
    "    num_workers : int\n",
    "        Number of processes to use\n",
    "    \"\"\"\n",
    "    def __init__(self, fname: str, fields: Mapping[str, Field],\n",
    "                 page_size: int = 4 * MIN_PAGE_SIZE, num_workers: int = -1):\n",
    "        self.fields = fields\n",
    "        self.fname = fname\n",
    "        self.metadata_type = get_metadata_type(list(self.fields.values()))\n",
    "\n",
    "        self.num_workers = num_workers\n",
    "        # We use all cores by default\n",
    "        if self.num_workers < 1:\n",
    "            self.num_workers = cpu_count()\n",
    "\n",
    "        if not is_power_of_2(page_size):\n",
    "            raise ValueError(f'page_size isnt a power of 2')\n",
    "        if page_size < MIN_PAGE_SIZE:\n",
    "            raise ValueError(f\"page_size can't be lower than{MIN_PAGE_SIZE}\")\n",
    "        if page_size >= MAX_PAGE_SIZE:\n",
    "            raise ValueError(f\"page_size can't be bigger(or =) than{MAX_PAGE_SIZE}\")\n",
    "\n",
    "        self.page_size = page_size\n",
    "\n",
    "\n",
    "    def prepare(self):\n",
    "\n",
    "        with open(self.fname, 'wb') as fp:\n",
    "            # Prepare the header data\n",
    "            header = np.zeros(1, dtype=HeaderType)[0]\n",
    "            header['version'] = CURRENT_VERSION\n",
    "            header['num_samples'] = self.num_samples\n",
    "            header['num_fields'] = len(self.fields)\n",
    "            header['page_size'] = self.page_size\n",
    "            self.header = header\n",
    "\n",
    "            # We will write the header at the end because we need to know where\n",
    "            # The memory allocation table is in the file\n",
    "            # We still write it here to make space for it later\n",
    "            fp.write(self.header.tobytes())\n",
    "\n",
    "\n",
    "            # Writes the information about the fields\n",
    "            fields_descriptor = np.zeros(len(self.fields), dtype=FieldDescType)\n",
    "\n",
    "            fieldname_max_len = fields_descriptor[0]['name'].shape[0]\n",
    "\n",
    "            for i, (name, field) in enumerate(self.fields.items()):\n",
    "                type_id = HANDLER_TYPE_ID.get(type(field), 255)\n",
    "                encoded_name = name.encode('ascii')\n",
    "                encoded_name = np.frombuffer(encoded_name, dtype='<u1')\n",
    "                actual_length = min(fieldname_max_len, len(encoded_name))\n",
    "                fields_descriptor[i]['type_id'] = type_id\n",
    "                fields_descriptor[i]['name'][:actual_length] = (\n",
    "                    encoded_name[:actual_length])\n",
    "                fields_descriptor[i]['arguments'][:] = field.to_binary()[0]\n",
    "\n",
    "            fp.write(fields_descriptor.tobytes())\n",
    "\n",
    "        total_metadata_size = self.num_samples * self.metadata_type.itemsize\n",
    "\n",
    "        # Shared memory for all the writers to fill the information\n",
    "        self.metadata_sm = 3\n",
    "        self.metadata_start = (HeaderType.itemsize + fields_descriptor.nbytes)\n",
    "\n",
    "        self.metadata_sm = shared_memory.SharedMemory(create=True,\n",
    "                                                      size=total_metadata_size)\n",
    "\n",
    "        self.data_region_start = self.metadata_start + total_metadata_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DatasetWriter.from_indexed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DatasetWriter.from_webdataset)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
