{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp text.huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "# Contains code from:\n",
    "# blurr - Apache License 2.0 - Copyright (c) Wayde Gilliam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "\n",
    "import inspect, warnings\n",
    "from typing import Dict, Iterable, Sequence\n",
    "\n",
    "import torch._dynamo as dynamo\n",
    "from torch.utils.data import Sampler, Dataset\n",
    "from torch.utils.data import DataLoader as _DataLoader\n",
    "from torch.utils.data.dataloader import _worker_init_fn_t, _collate_fn_t\n",
    "\n",
    "from transformers import PreTrainedModel\n",
    "from transformers import logging as hf_logging\n",
    "\n",
    "from fastai.callback.core import Callback\n",
    "from fastai.losses import BaseLoss\n",
    "\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face\n",
    "> Basic compatability between fastai and Hugging Face Transformers models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastxtend provides basic compatibility for training Hugging Face [Transformers](https://huggingface.co/docs/transformers/index) models using the `fastai.learner.Learner`.\n",
    "\n",
    "For a fully developed Hugging Face integration, you should check out [blurr](https://ohmeow.github.io/blurr).\n",
    "\n",
    "To use fastxend's compatibility, setup the Hugging Face dataset, dataloader, and model per the [Transformers documentation](https://huggingface.co/docs/transformers/index), exchaning the PyTorch `Dataloader` for the `HuggingFaceLoader`. Then wrap the dataloaders in `fastai.data.core.DataLoaders` and create a `Learner` with the Hugging Face model, `HuggingFaceLoss`, and `HuggingFaceCallback`. This will automatically setup the compatibility and use the Hugging Face model's built in loss.\n",
    "\n",
    "```python\n",
    "train_dataset = dataset['train'].with_format('torch')\n",
    "train_dataloader = HuggingFaceLoader(\n",
    "    train_dataset, batch_size=batch_size,\n",
    "    collate_fn=data_collator, shuffle=True,\n",
    "    drop_last=True, num_workers=num_cpus()\n",
    ")\n",
    "\n",
    "valid_dataset = dataset['validation'].with_format('torch')\n",
    "valid_dataloader = HuggingFaceLoader(\n",
    "    valid_dataset, batch_size=batch_size,\n",
    "    collate_fn=data_collator, shuffle=False,\n",
    "    drop_last=False, num_workers=num_cpus()\n",
    ")\n",
    "\n",
    "dls = DataLoaders(train_dataloader, valid_dataloader)\n",
    "\n",
    "hf_model = GPTForCausalLM(...)\n",
    "learn = Learner(dls, hf_model, loss_func=HuggingFaceLoss(), ...,\n",
    "                cbs=HuggingFaceCallback()).to_bf16()\n",
    "\n",
    "```\n",
    "\n",
    "To train with a different loss, pass in a loss to `Learner` as normal, and `HuggingFaceCallback` will ignore the Hugging Face model's built in loss calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "warnings.simplefilter(\"ignore\")\n",
    "hf_logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class HuggingFaceLoss(BaseLoss):\n",
    "    \"To use the Hugging Face model's built in loss function, pass this loss to `Learner`\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.func = None\n",
    "\n",
    "    def __call__(self, inp:Tensor, targ:Tensor|None=None, **kwargs):\n",
    "        return tensor(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class HuggingFaceWrapper(nn.Module):\n",
    "    \"A minimal compatibility wrapper between a Hugging Face model and `Learner`\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: PreTrainedModel, # Hugging Face compatible model\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self._forward_args = inspect.getfullargspec(self.model.forward).args\n",
    "\n",
    "    def forward(self, x:Dict):\n",
    "        return self.model(**{k:v for k,v in x.items() if k in self._forward_args})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, you won't need to use the <code>HuggingFaceWrapper</code> as `HuggingFaceCallback` will automatically add it for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class HuggingFaceCallback(Callback):\n",
    "    run_valid = True\n",
    "    \"Applies `HuggingFaceWrapper` and handles using model's built in loss or fastai `Learner` loss\"\n",
    "    def __init__(self,\n",
    "        labels:str='labels', # Input batch labels key\n",
    "        loss:str='loss', # Model output loss key\n",
    "        logits:str='logits', # Model output logits key\n",
    "    ):\n",
    "        self._label_key, self._loss_key, self._logit_key = labels, loss, logits\n",
    "\n",
    "    def after_create(self):\n",
    "        self._model_loss = isinstance(self.learn.loss_func, HuggingFaceLoss)\n",
    "        if not isinstance(self.model, HuggingFaceWrapper) and not isinstance(self.model, dynamo.OptimizedModule):\n",
    "            self.learn.model = HuggingFaceWrapper(self.learn.model)\n",
    "\n",
    "    def before_batch(self):\n",
    "        self._loss = None\n",
    "        if self._model_loss:\n",
    "            # Learner skips backward pass if yb isn't set\n",
    "            self.learn.yb = (1,)\n",
    "        else:\n",
    "            self.learn.yb = (self.xb[0][self._label_key],)\n",
    "\n",
    "    def after_pred(self):\n",
    "        outputs = self.learn.pred\n",
    "        if self._model_loss:\n",
    "            self._loss = to_float(outputs[self._loss_key])\n",
    "        self.learn.pred = outputs.get(self._logit_key, None)\n",
    "\n",
    "    def after_loss(self):\n",
    "        if self._model_loss:\n",
    "            self.learn.loss_grad = self._loss\n",
    "            self.learn.loss = self.learn.loss_grad.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `HuggingFaceLoss` is passed to `fastai.learner.Learner`, then <code>HuggingFaceCallback</code> will use the Hugging Face model's built in loss.\n",
    "\n",
    "If any other loss function is passed to `Learner`, <code>HuggingFaceCallback</code> will prevent the built in loss from being calculated and will use the `Learner` loss function instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class HuggingFaceLoader(_DataLoader):\n",
    "    \"A minimal compatibility wrapper between a Hugging Face Dataloader and `Learner`\"\n",
    "    def __init__(self,\n",
    "        dataset:Dataset, # dataset from which to load the data\n",
    "        batch_size:int, # Batch size\n",
    "        shuffle:bool|None = None, # Randomize the order of data at each epoch (default: False)\n",
    "        sampler:Sampler|Iterable|None = None, # Determines how to draw samples from the dataset. Cannot be used with shuffle.\n",
    "        batch_sampler:Sampler[Sequence]|Iterable[Sequence]|None = None, # Rreturns a batch of indices at a time. Cannot be used with batch_size, shuffle, sampler, or drop_last.\n",
    "        num_workers:int=0, # Number of processes to use for data loading. 0 means using the main process (default: 0).\n",
    "        collate_fn:_collate_fn_t|None = None, # Function that merges a list of samples into a mini-batch of Tensors. Used for map-style datasets.\n",
    "        pin_memory:bool=False, # Copy Tensors into device/CUDA pinned memory before returning them\n",
    "        drop_last:bool=False, # Drop the last incomplete batch if the dataset size is not divisible by the batch size\n",
    "        timeout:float=0, # Timeout value for collecting a batch from workers\n",
    "        worker_init_fn:_worker_init_fn_t|None = None, # called on each worker subprocess with the worker id as input\n",
    "        multiprocessing_context=None,\n",
    "        generator=None,\n",
    "        prefetch_factor:int|None=None, # number of batches loaded in advance by each worker\n",
    "        persistent_workers:bool=False, # if True, the data loader will not shutdown the worker processes after a dataset has been consumed once\n",
    "        pin_memory_device:str= \"\", # the data loader will copy Tensors into device pinned memory before returning them if pin_memory is set to true\n",
    "    ):\n",
    "        super().__init__(\n",
    "            dataset=dataset, batch_size=batch_size, shuffle=shuffle, sampler=sampler,\n",
    "            batch_sampler=batch_sampler, num_workers=num_workers, collate_fn=collate_fn,\n",
    "            pin_memory=pin_memory, drop_last=drop_last, timeout=timeout,\n",
    "            worker_init_fn=worker_init_fn, multiprocessing_context=multiprocessing_context,\n",
    "            generator=generator, prefetch_factor=prefetch_factor, persistent_workers=persistent_workers,\n",
    "            pin_memory_device=pin_memory_device\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def bs(self) -> int:\n",
    "        \"Number of items a batch\"\n",
    "        return self.batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        for b in super().__iter__():\n",
    "            yield (b,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face datasets, and thus dataloaders, return dictionaries while fastai expects tuples. <code>HuggingFaceLoader</code> is a PyTorch Dataloader which wraps the Hugging Face dictionary in a tuple."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
