{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import inspect, time, math\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.metrics as skm\n",
    "import scipy.stats as scs\n",
    "\n",
    "from fastcore.basics import class2attr\n",
    "from fastcore.dispatch import cast\n",
    "\n",
    "from fastprogress.core import format_time\n",
    "\n",
    "from fastai.callback.core import Callback\n",
    "from fastai.learner import AvgMetric, Learner, Metric, Recorder, _maybe_item\n",
    "from fastai.metrics import AccumMetric\n",
    "from fastai.torch_core import flatten_check\n",
    "\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import random\n",
    "from nbdev.showdoc import *\n",
    "from fastxtend.test_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Extended\n",
    "> A backwards compatible reimplementation of fastai metrics to increase usability and flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All fastxtend metrics are classes which inherit from fastai's `Metric` and run on `Learner` via a modified `Recorder` callback. \n",
    "\n",
    "There are three main metric types: `AvgMetricX`, `AccumMetricX`, and `AvgSmoothMetricX`. These correspond one-to-one with fastai's `AvgMetric`, `AccumMetric`, and `AvgSmoothMetric`.\n",
    "\n",
    "To jump to the fastxtend metrics reference, click [here](#Metrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Metric\n",
    "\n",
    "To use the accuracy metric, or any fastxtend metrics detailed below, create a `Learner` like normal (or task specific learner such as `vision_learner`, `text_classifier_learner`, etc) and add the metric(s) to the `metrics` argument:\n",
    "\n",
    "```python\n",
    "from fastai.vision.all import *\n",
    "from fastxtend.vision.all import *\n",
    "\n",
    "Learner(..., metrics=Accuracy())\n",
    "```\n",
    "\n",
    "Fastxtend metrics can be mixed with fastai metrics:\n",
    "\n",
    "```python\n",
    "Learner(..., metrics=[accuracy, Accuracy()])\n",
    "```\n",
    "\n",
    "Fastxtend metrics can be logged during training, validation, or both by setting the `log_metric` argument to `LogMetric.Train`, `LogMetric.Valid`, or `LogMetric.Both`. The sole exception is `AvgSmoothMetricX` which only logs during training.\n",
    "\n",
    "> Note: By default, a fastxtend metric will log during validation. Fastai metrics can only log during validation. \n",
    "\n",
    "To log a fastxtend metric during training pass `LogMetric.Train` to `log_metric`:\n",
    "\n",
    "```python\n",
    "Learner(..., metrics=Accuracy(log_metric=LogMetric.Train))\n",
    "```\n",
    "\n",
    "Non-scikit-learn metrics can have the log type set via the `metric_type` argument to one of `MetricType.Avg`, `MetricType.Accum`, `MetricType.Smooth`, corresponding to `AvgMetricX`, `AccumMetricX`, and `AvgSmoothMetricX`, respectively.\n",
    "\n",
    "To log a smooth metric on the training set and normal metric on the valid set:\n",
    "\n",
    "```python\n",
    "Learner(..., \n",
    "        metrics=[Accuracy(log_metric=LogMetric.Train, metric_type=MetricType.Smooth), \n",
    "                 Accuracy()])\n",
    "```\n",
    "\n",
    "Fastxtend metrics also support custom names via the `name` argument:\n",
    "\n",
    "```python\n",
    "Learner(..., metrics=Accuracy(name='metric_name'))\n",
    "```\n",
    "\n",
    "which will result in Accuracy logging under \"metric_name\" instead of the default \"accuracy\".\n",
    "\n",
    "If a fastxtend metric is logged with multiple `MetricType`s, the fastxtend `Recorder` will automatically deduplication the metric names. Unless the metric's `name` argument is set. Then fastxtend will not deduplicate any metric names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Metric\n",
    "\n",
    "`AvgMetricX`, `AccumMetricX`, and `AvgSmoothMetricX` all require `func`, which is a funcational implementation of the metric. The signature of `func` should be `inp,targ` (where `inp` are the predictions of the model and `targ` the corresponding labels).\n",
    "\n",
    "Fastxtend metrics can be logged during training, validation, or both by setting the `log_metric` argument to `LogMetric.Train`, `LogMetric.Valid`, or `LogMetric.Both`. The sole exception is `AvgSmoothMetricX` which only computes during training.\n",
    "\n",
    "`AvgMetricX`, `AccumMetricX`, and `AvgSmoothMetricX` will automatically recognize and pass any `func`'s unique arguments to `func`.\n",
    "\n",
    "> Important: Some metrics, like Root Mean Squared Error, will have incorrect results if passed to `AvgMetricX` via `MetricType.Avg`, as the mean of multiple batches of RMSE isn't equal to the RMSE of the whole dataset. For these metrics use `AccumMetricX` via `MetricType.Accum`.\n",
    "\n",
    "An example of creating a fastxtend metric from a functional implementation:\n",
    "\n",
    "```python\n",
    "def example_accuracy(inp, targ):\n",
    "    return (inp == targ).float().mean()\n",
    "\n",
    "def ExampleAccuracy(dim_argmax=-1, log_metric=LogMetric.Valid, **kwargs):\n",
    "    return AvgMetricX(example_accuracy, dim_argmax=dim_argmax, log_metric=log_metric, **kwargs)\n",
    "```\n",
    "\n",
    "Alternatively, use the `func_to_metric` convenience method to create the metric:\n",
    "\n",
    "```python\n",
    "def ExampleAccuracy(axis=-1, log_metric=LogMetric.Valid, **kwargs):\n",
    "    return func_to_metric(example_accuracy, MetricType.Avg, True, axis=axis, log_metric=log_metric, **kwargs)\n",
    "```\n",
    "\n",
    "It is also possible to inherit directly from `MetricX` to create a fastxtend metric.\n",
    "\n",
    "```python\n",
    "class ExampleAccuracy(MetricX):\n",
    "    def __init__(self, dim_argmax=-1, log_metric=LogMetric.Valid, **kwargs):\n",
    "    super().__init__(dim_argmax=dim_argmax, log_metric=log_metric, **kwargs)\n",
    "\n",
    "    def reset(self): self.preds,self.targs = [],[]\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        super().accumulate(learn)\n",
    "        self.preds.append(learn.to_detach(self.pred))\n",
    "        self.targs.append(learn.to_detach(self.targ))\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if len(self.preds) == 0: return\n",
    "        preds,targs = torch.cat(self.preds),torch.cat(self.targs)\n",
    "        return (preds == targs).float().mean()\n",
    "```\n",
    "> Important: If your custom <code>MetricX</code> has state depending on tensors, don't forget to store it on the CPU to avoid any potential memory leaks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Metrics Functionality\n",
    "\n",
    "`MetricX`, and classes which inherit from `MetricX` such as `AvgMetricX`, `AccumMetricX`, and `AvgSmoothMetricX`, have optional helper functionality in `MetricX.accumulate` to assist in developing metrics.\n",
    "\n",
    "For classification problems with single label, predictions need to be transformed with a softmax then an argmax before being compared to the targets. Since a softmax doesn't change the order of the numbers, apply the argmax. Pass along `dim_argmax` to have this done by `MetricX` (usually -1 will work pretty well). If the metric implementation requires probabilities and not predictions, use `softmax=True`.\n",
    "\n",
    "For classification problems with multiple labels, or if targets are one-hot encoded, predictions may need to pass through a sigmoid (if it wasn't included in in the model) then be compared to a given threshold (to decide between 0 and 1), this is done by `MetricX` by passing `sigmoid=True` and/or a value for `thresh`.\n",
    "\n",
    "`AvgMetricX`, `AccumMetricX`, and `AvgSmoothMetricX` have two additional arguments to assist in creating metrics: `to_np` and `invert_arg`.\n",
    "\n",
    "For example, if using a functional metric from sklearn.metrics, predictions and labels will need to be converted to numpy arrays with `to_np=True`. Also, scikit-learn metrics adopt the convention `y_true`, `y_preds` which is the opposite from fastai, so pass `invert_arg=True` to make `AvgMetricX`, `AccumMetricX`, and `AvgSmoothMetricX` do the inversion. Alternatively, use the `skm_to_fastxtend` convenience method to handle sklearn.metrics automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended Metric -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LogMetric(Enum):\n",
    "    \"All possible logging types for `MetricX`\",\n",
    "    Train = 1\n",
    "    Valid = 2\n",
    "    Both = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class MetricType(Enum):\n",
    "    \"All possible types of `MetricX`\",\n",
    "    Avg = 1\n",
    "    Accum = 2\n",
    "    Smooth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ActivationType(Enum):\n",
    "    \"All possible activation classes for `MetricX\",\n",
    "    No = 1\n",
    "    Sigmoid = 2\n",
    "    Softmax = 3\n",
    "    BinarySoftmax = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class MetricX(Metric):\n",
    "    \"Blueprint for defining an extended metric with accumulate\"\n",
    "    log_metric=LogMetric.Valid\n",
    "    def __init__(self, dim_argmax=None, activation=ActivationType.No, thresh=None, log_metric=None, name=None):\n",
    "        store_attr(but='log_metric, name')\n",
    "        self.log_metric = ifnone(log_metric, self.log_metric)\n",
    "        self._name = ifnone(name, class2attr(self, 'MetricX'))\n",
    "        \n",
    "    def reset(self):\n",
    "        \"Reset inner state to prepare for new computation\"\n",
    "        pass\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        \"Store targs and preds from `learn`, using activation function and argmax as appropriate\"\n",
    "        self.pred, self.targ = learn.pred, *learn.yb\n",
    "        if self.activation in [ActivationType.Softmax, ActivationType.BinarySoftmax]:\n",
    "            self.pred = F.softmax(self.pred, dim=self.dim_argmax)\n",
    "            if self.activation == ActivationType.BinarySoftmax: self.pred = self.pred[:, -1]\n",
    "        elif self.activation == ActivationType.Sigmoid: self.pred = torch.sigmoid(self.pred)\n",
    "        elif self.dim_argmax: self.pred = self.pred.argmax(dim=self.dim_argmax)\n",
    "        if self.thresh: self.pred = (self.pred >= self.thresh)\n",
    "        \n",
    "    @property\n",
    "    def value(self):\n",
    "        \"The value of the metric\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def name(self): \n",
    "        \"Name of the `Metric`, camel-cased and with Metric removed. Or custom name if provided\"\n",
    "        return self._name\n",
    "\n",
    "    @name.setter\n",
    "    def name(self, value): self._name = value\n",
    "\n",
    "    def _split_kwargs(self, method, **kwargs):\n",
    "        args = [k for k,v in inspect.signature(method).parameters.items() if v.default != inspect.Parameter.empty]\n",
    "        return {k: kwargs[k] for k in kwargs.keys() if k in args}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: By default, a `MetricX` will only log during validation. Metrics can individually set to run during training, validation, or both by passing `LogMetric.Train`, `LogMetric.Valid`, or `LogMetric.Both` to `log_metric`, respectively.\n",
    "\n",
    "For classification problems with single label, predictions need to be transformed with a softmax then an argmax before being compared to the targets. Since a softmax doesn't change the order of the numbers, apply the argmax. Pass along `dim_argmax` to have this done by `MetricX` (usually -1 will work pretty well). If the metric implementation requires probabilities and not predictions, use `softmax=True`.\n",
    "\n",
    "For classification problems with multiple labels, or if targets are one-hot encoded, predictions may need to pass through a sigmoid (if it wasn't included in in the model) then be compared to a given threshold (to decide between 0 and 1), this is done by `MetricX` by passing `sigmoid=True` and/or a value for `thresh`.\n",
    "\n",
    "Metrics can be simple averages (like accuracy) but sometimes their computation is a little bit more complex and can't be averaged over batches (like precision or recall), which is why we need a special `AccumMetricX` class for them. For simple functions that can be computed as averages over batches, we can use the class `AvgMetricX`, otherwise you'll need to implement the following methods.\n",
    "\n",
    "> Note: If your custom <code>MetricX</code> has state depending on tensors, don't forget to store it on the CPU to avoid any potential memory leaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"MetricX.reset\" class=\"doc_header\"><code>MetricX.reset</code><a href=\"__main__.py#L10\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>MetricX.reset</code>()\n",
       "\n",
       "Reset inner state to prepare for new computation"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(MetricX.reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"MetricX.accumulate\" class=\"doc_header\"><code>MetricX.accumulate</code><a href=\"__main__.py#L14\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>MetricX.accumulate</code>(**`learn`**)\n",
       "\n",
       "Store targs and preds from `learn`, using activation function and argmax as appropriate"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(MetricX.accumulate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"MetricX.value\" class=\"doc_header\"><code>MetricX.value</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "The value of the metric"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(MetricX.value, name='MetricX.value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"MetricX.name\" class=\"doc_header\"><code>MetricX.name</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "Name of the `Metric`, camel-cased and with Metric removed. Or custom name if provided"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(MetricX.name, name='MetricX.name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Testing -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#For testing: a fake learner and a metric that isn't an average\n",
    "@delegates()\n",
    "class TstLearner(Learner):\n",
    "    def __init__(self,dls=None,model=None,**kwargs): self.pred,self.xb,self.yb = None,None,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def _l2_mean(x,y): return torch.sqrt((x.float()-y.float()).pow(2).mean())\n",
    "def _mse(x,y): return F.mse_loss(*flatten_check(x.float(),y.float()))\n",
    "\n",
    "#Go through a fake cycle with various batch sizes and computes the value of met\n",
    "def compute_val(met, x1, x2):\n",
    "    met.reset()\n",
    "    vals = [0,6,15,20]\n",
    "    learn = TstLearner()\n",
    "    for i in range(3):\n",
    "        learn.pred,learn.yb = x1[vals[i]:vals[i+1]],(x2[vals[i]:vals[i+1]],)\n",
    "        met.accumulate(learn)\n",
    "    return met.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def _test_metric(metrictype, metric):\n",
    "    x1,x2 = torch.randn(20,5),torch.randn(20,5)\n",
    "    tst = metrictype(metric)\n",
    "    test_close(compute_val(tst, x1, x2), metric(x1, x2))\n",
    "    if hasattrs(tst, ['preds', 'targs']):\n",
    "        test_eq(torch.cat(tst.preds), x1.view(-1))\n",
    "        test_eq(torch.cat(tst.targs), x2.view(-1))\n",
    "\n",
    "    #test argmax\n",
    "    x1,x2 = torch.randn(20,5),torch.randint(0, 5, (20,))\n",
    "    tst = metrictype(metric, dim_argmax=-1)\n",
    "    test_close(compute_val(tst, x1, x2), metric(x1.argmax(dim=-1), x2))\n",
    "\n",
    "    #test thresh\n",
    "    x1,x2 = torch.randn(20,5),torch.randint(0, 2, (20,5)).bool()\n",
    "    tst = metrictype(metric, thresh=0.5)\n",
    "    test_close(compute_val(tst, x1, x2), metric((x1 >= 0.5), x2))\n",
    "\n",
    "    #test sigmoid\n",
    "    x1,x2 = torch.randn(20,5),torch.randn(20,5)\n",
    "    tst = metrictype(metric, activation=ActivationType.Sigmoid)\n",
    "    test_close(compute_val(tst, x1, x2), metric(torch.sigmoid(x1), x2))\n",
    "\n",
    "    #test to_np\n",
    "    x1,x2 = torch.randn(20,5),torch.randn(20,5)\n",
    "    tst = metrictype(lambda x,y: isinstance(x, np.ndarray) and isinstance(y, np.ndarray), to_np=True)\n",
    "    assert compute_val(tst, x1, x2)\n",
    "\n",
    "    #test invert_arg\n",
    "    if hasattrs(tst, ['preds', 'targs']):\n",
    "        x1,x2 = torch.randn(20,5),torch.randn(20,5)\n",
    "        tst = metrictype(lambda x,y: torch.sqrt(x.pow(2).mean()))\n",
    "        test_close(compute_val(tst, x1, x2), torch.sqrt(x1.pow(2).mean()))\n",
    "        tst = metrictype(lambda x,y: torch.sqrt(x.pow(2).mean()), invert_arg=True)\n",
    "        test_close(compute_val(tst, x1, x2), torch.sqrt(x2.pow(2).mean()))\n",
    "    else:\n",
    "        x1,x2 = torch.randn(20,5),torch.randn(20,5)\n",
    "        tst = metrictype(lambda x,y: (x-y).mean())\n",
    "        test_close(compute_val(tst, x1, x2), (x1-x2).mean())\n",
    "        tst = metrictype(lambda x,y: (x-y).mean(), invert_arg=True)\n",
    "        test_close(compute_val(tst, x1, x2), (x2-x1).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AvgMetric -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@delegates(MetricX)\n",
    "class AvgMetricX(MetricX):\n",
    "    \"Average the values of `func` taking into account potential different batch sizes\"\n",
    "    def __init__(self, func, to_np=False, invert_arg=False, **kwargs):\n",
    "        super().__init__(**self._split_kwargs(MetricX.__init__, **kwargs))\n",
    "        self.func, self.fkwargs = func, self._split_kwargs(func, **kwargs)\n",
    "        self.to_np, self.invert_arg = to_np, invert_arg\n",
    "        self._name = ifnone(kwargs.get('name', None), self.func.func.__name__ if hasattr(self.func, 'func') else self.func.__name__)\n",
    "\n",
    "    def reset(self): self.total,self.count = 0.,0\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        super().accumulate(learn)\n",
    "        bs = find_bs(learn.yb)\n",
    "        if self.to_np: self.pred,self.targ = learn.to_detach(self.pred).numpy(),learn.to_detach(self.targ).numpy()\n",
    "        self.total += (self.func(self.targ, self.pred, **self.fkwargs) if self.invert_arg else self.func(self.pred, self.targ, **self.fkwargs))*bs\n",
    "        self.count += bs\n",
    "\n",
    "    @property\n",
    "    def value(self): return self.total/self.count if self.count != 0 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`func` is applied to each batch of predictions/targets and then averaged when `value` attribute is asked for.The signature of `func` should be `inp,targ` (where `inp` are the predictions of the model and `targ` the corresponding labels).\n",
    "\n",
    "> Important: Some metrics, like Root Mean Squared Error, will have incorrect results if passed to `AvgMetricX`, as the mean of multiple batches of RMSE isn't equal to the RMSE of the whole dataset. For these metrics use `AccumMetricX`.\n",
    "\n",
    "If using a functional metric from sklearn.metrics, predictions and labels will need to be converted to numpy arrays with `to_np=True`. Also, scikit-learn metrics adopt the convention `y_true`, `y_preds` which is the opposite from fastai, so pass `invert_arg=True` to make `AvgMetricX`, `AccumMetricX`, and `AvgSmoothMetricX` do the inversion. Alternatively, use the `skm_to_fastxtend` convenience method to handle sklearn.metrics automatically.\n",
    "\n",
    "By default, fastxtend's scikit-learn metrics use `AccumMetricX`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "learn = synth_learner()\n",
    "tst = AvgMetricX(lambda x,y: (x-y).abs().mean())\n",
    "t,u = torch.randn(100),torch.randn(100)\n",
    "tst.reset()\n",
    "for i in range(0,100,25): \n",
    "    learn.pred,learn.yb = t[i:i+25],(u[i:i+25],)\n",
    "    tst.accumulate(learn)\n",
    "test_close(tst.value, (t-u).abs().mean())\n",
    "\n",
    "_test_metric(AvgMetricX, _mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AccumMetric -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@delegates(MetricX)\n",
    "class AccumMetricX(MetricX):\n",
    "    \"Stores predictions and targets on CPU in accumulate to perform final calculations with `func`.\"\n",
    "    def __init__(self, func, to_np=False, invert_arg=False, flatten=True, **kwargs):\n",
    "        super().__init__(**self._split_kwargs(MetricX.__init__, **kwargs))\n",
    "        self.flatten, self.func, self.fkwargs = flatten, func, self._split_kwargs(func, **kwargs)\n",
    "        self.to_np, self.invert_arg = to_np, invert_arg\n",
    "        self._name = ifnone(kwargs.get('name', None), self.func.func.__name__ if hasattr(self.func, 'func') else self.func.__name__)\n",
    "\n",
    "    def reset(self):\n",
    "        \"Clear all targs and preds\"\n",
    "        self.targs,self.preds = [],[]\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        \"Store targs and preds from `learn`, using activation function and argmax as appropriate\"\n",
    "        super().accumulate(learn)\n",
    "        self.pred,self.targ = learn.to_detach(self.pred),learn.to_detach(self.targ)\n",
    "        self.accum_values(self.pred, self.targ)\n",
    "\n",
    "    def accum_values(self, preds, targs):\n",
    "        \"Store targs and preds\"\n",
    "        if self.flatten: preds,targs = flatten_check(preds,targs)\n",
    "        self.preds.append(preds)\n",
    "        self.targs.append(targs)\n",
    "\n",
    "    def __call__(self, preds, targs):\n",
    "        \"Calculate metric on one batch of data\"\n",
    "        self.reset()\n",
    "        self.accum_values(preds,targs)\n",
    "        return self.value\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        \"Value of the metric using accumulated preds and targs\"\n",
    "        if len(self.preds) == 0: return\n",
    "        preds,targs = torch.cat(self.preds),torch.cat(self.targs)\n",
    "        if self.to_np: preds,targs = preds.numpy(),targs.numpy()\n",
    "        return self.func(targs, preds, **self.fkwargs) if self.invert_arg else self.func(preds, targs, **self.fkwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`func` is only applied to the accumulated predictions/targets when the `value` attribute is asked for (so at the end of a validation/training phase, in use with `Learner` and its `Recorder`).The signature of `func` should be `inp,targ` (where `inp` are the predictions of the model and `targ` the corresponding labels).\n",
    "\n",
    "If using a functional metric from sklearn.metrics, predictions and labels will need to be converted to numpy arrays with `to_np=True`. Also, scikit-learn metrics adopt the convention `y_true`, `y_preds` which is the opposite from fastai, so pass `invert_arg=True` to make `AvgMetricX`, `AccumMetricX`, and `AvgSmoothMetricX` do the inversion. Alternatively, use the `skm_to_fastxtend` convenience method to handle sklearn.metrics automatically.\n",
    "\n",
    "By default, fastai's scikit-learn metrics use `AccumMetricX`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "_test_metric(AccumMetricX, _l2_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AvgSmoothMetric -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@delegates(MetricX, but='log_metric')\n",
    "class AvgSmoothMetricX(MetricX):\n",
    "    \"Smooth average the values of `func` (exponentially weighted with `beta`). Only computed on training set.\"\n",
    "    log_metric = LogMetric.Train\n",
    "    def __init__(self, func, beta=0.98, to_np=False, invert_arg=False, **kwargs):\n",
    "        super().__init__(**self._split_kwargs(MetricX.__init__, **kwargs))\n",
    "        self.func, self.fkwargs = func, self._split_kwargs(func, **kwargs)\n",
    "        self.beta, self.to_np, self.invert_arg = beta, to_np, invert_arg\n",
    "        self._name = ifnone(kwargs.get('name', None), self.func.func.__name__ if hasattr(self.func, 'func') else self.func.__name__)\n",
    "\n",
    "    def reset(self): self.count,self.val = 0,tensor(0.)\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        super().accumulate(learn)\n",
    "        if self.to_np: self.pred,self.targ = learn.to_detach(self.pred).numpy(),learn.to_detach(self.targ).numpy()\n",
    "        val = self.func(self.targ, self.pred, **self.fkwargs) if self.invert_arg else self.func(self.pred, self.targ, **self.fkwargs)\n",
    "        if self.to_np: self.val = val*self.beta + val*(1-self.beta)\n",
    "        else: self.val = torch.lerp(to_detach(val, gather=False), self.val, self.beta)\n",
    "        self.count += 1\n",
    "\n",
    "    @property\n",
    "    def value(self): return self.val/(1-self.beta**self.count) if self.count != 0 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`func` is only applied to the accumulated predictions/targets when the `value` attribute is asked for (so at the end of a validation/training phase, in use with `Learner` and its `Recorder`).The signature of `func` should be `inp,targ` (where `inp` are the predictions of the model and `targ` the corresponding labels).\n",
    "\n",
    "If using a functional metric from sklearn.metrics, predictions and labels will need to be converted to numpy arrays with `to_np=True`. Also, scikit-learn metrics adopt the convention `y_true`, `y_preds` which is the opposite from fastai, so pass `invert_arg=True` to make `AvgMetricX`, `AccumMetricX`, and `AvgSmoothMetricX` do the inversion. Alternatively, use the `skm_to_fastxtend` convenience method to handle sklearn.metrics automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "learn = synth_learner()\n",
    "tst = AvgSmoothMetricX(lambda x,y: (x-y).abs().mean())\n",
    "t,u = torch.randn(100),torch.randn(100)\n",
    "tst.reset()\n",
    "val = tensor(0.)\n",
    "for i, j in enumerate(range(0,100,25)):\n",
    "    learn.pred,learn.yb = t[j:j+25],(u[j:j+25],)\n",
    "    tst.accumulate(learn)\n",
    "    val = val*0.98 + ((t[j:j+25]-u[j:j+25]).abs().mean())*(1-0.98)\n",
    "    test_close(val/(1-0.98**(i+1)), tst.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AvgLoss -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AvgLossX(MetricX):\n",
    "    \"Average the losses taking into account potential different batch sizes\"\n",
    "    def reset(self): self.total,self.count = 0.,0\n",
    "    def accumulate(self, learn):\n",
    "        bs = find_bs(learn.yb)\n",
    "        self.total += learn.to_detach(learn.loss.mean())*bs\n",
    "        self.count += bs\n",
    "    @property\n",
    "    def value(self): return self.total/self.count if self.count != 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "tst = AvgLossX()\n",
    "t = torch.randn(100)\n",
    "tst.reset()\n",
    "for i in range(0,100,25): \n",
    "    learn.yb,learn.loss = t[i:i+25],t[i:i+25].mean()\n",
    "    tst.accumulate(learn)\n",
    "test_close(tst.value, t.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#With varying batch size\n",
    "tst.reset()\n",
    "splits = [0, 30, 50, 60, 100]\n",
    "for i in range(len(splits )-1): \n",
    "    learn.yb,learn.loss = t[splits[i]:splits[i+1]],t[splits[i]:splits[i+1]].mean()\n",
    "    tst.accumulate(learn)\n",
    "test_close(tst.value, t.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AvgSmoothLoss -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AvgSmoothLossX(MetricX):\n",
    "    \"Smooth average of the losses (exponentially weighted with `beta`)\"\n",
    "    log_metric = LogMetric.Train\n",
    "    def __init__(self, beta=0.98): \n",
    "        self.beta = beta\n",
    "    def reset(self):\n",
    "        self.count,self.val = 0,tensor(0.)\n",
    "    def accumulate(self, learn):\n",
    "        self.count += 1\n",
    "        self.val = torch.lerp(to_detach(learn.loss.mean(), gather=False), self.val, self.beta)\n",
    "    @property\n",
    "    def value(self): return self.val/(1-self.beta**self.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "tst = AvgSmoothLossX()\n",
    "t = torch.randn(100)\n",
    "tst.reset()\n",
    "val = tensor(0.)\n",
    "for i in range(4): \n",
    "    learn.loss = t[i*25:(i+1)*25].mean()\n",
    "    tst.accumulate(learn)\n",
    "    val = val*0.98 + t[i*25:(i+1)*25].mean()*(1-0.98)\n",
    "    test_close(val/(1-0.98**(i+1)), tst.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ValueMetric -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ValueMetricX(MetricX):\n",
    "    \"Use to include a pre-calculated metric value (for instance calculated in a `Callback`) and returned by `func`\"\n",
    "    def __init__(self, func, name=None, log_metric=None):\n",
    "        super().__init__(log_metric=log_metric)\n",
    "        self.func = func\n",
    "        self._name = ifnone(name, self.func.func.__name__ if hasattr(self.func, 'func') else self.func.__name__)\n",
    "\n",
    "    @property\n",
    "    def value(self): return self.func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def metric_value_fn(): return 5e-3\n",
    "\n",
    "vm = ValueMetricX(metric_value_fn, 'custom_value_metric')\n",
    "test_eq(vm.value, 5e-3)\n",
    "test_eq(vm.name, 'custom_value_metric')\n",
    "\n",
    "vm = ValueMetricX(metric_value_fn)\n",
    "test_eq(vm.name, 'metric_value_fn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recorder -\n",
    "Patch `Recorder` to use fastxtend metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def _dedup_metric_names(metrics, names):\n",
    "    dup = set()\n",
    "    log = metrics.map(lambda o: (LogMetric.Valid if o.log_metric==LogMetric.Both else o.log_metric) if hasattr(o, 'log_metric') else LogMetric.Valid)\n",
    "    dups = L(set([o[1] for o in zip(log, names) if o in dup or dup.add(o)]))\n",
    "    indices = names.argwhere(lambda o: o in dups)\n",
    "    for i in indices:\n",
    "        if hasattr(metrics[i], 'func'): \n",
    "            name = metrics[i].func.func.__name__ if hasattr(metrics[i].func, 'func') else metrics[i].func.__name__\n",
    "        else:\n",
    "            if isinstance(metrics[i], MetricX): \n",
    "                name = class2attr(metrics[i], 'MetricX')\n",
    "            else:\n",
    "                name = class2attr(metrics[i], 'Metric')\n",
    "        if not hasattr(metrics[i], 'name') or metrics[i].name == name: # only deduplicate default metric names\n",
    "            if isinstance(metrics[i], (AvgMetricX, AvgMetric)): \n",
    "                names[i] = f'avg_{names[i]}'\n",
    "            elif isinstance(metrics[i], (AccumMetricX, AccumMetric)): \n",
    "                names[i] = f'accm_{names[i]}'\n",
    "            elif isinstance(metrics[i], AvgSmoothMetricX): \n",
    "                names[i] = f'smth_{names[i]}'\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@patch\n",
    "def __init__(self:Recorder, add_time=True, train_metrics=False, valid_metrics=True, beta=0.98):\n",
    "    store_attr('add_time')\n",
    "    self.loss,self.smooth_loss = AvgLossX(),AvgSmoothLossX(beta=beta)\n",
    "\n",
    "@patch\n",
    "def before_fit(self:Recorder):\n",
    "    \"Prepare state for training\"\n",
    "    self.lrs,self.iters,self.losses,self.values = [],[],[],[]\n",
    "    names = self.metrics.attrgot('name')\n",
    "    if len(names.unique()) != len(names): \n",
    "        names = _dedup_metric_names(self.metrics, names)\n",
    "    train = self.metrics.argwhere(lambda o: hasattr(o, 'log_metric') and o.log_metric != LogMetric.Valid)\n",
    "    valid = self.metrics.argwhere(lambda o: not hasattr(o, 'log_metric') or o.log_metric != LogMetric.Train)\n",
    "    self._train_metsX = self.metrics[train]\n",
    "    self._valid_metsX = self.metrics[valid]\n",
    "    train_names = names[train] \n",
    "    valid_names = names[valid]\n",
    "    if len(self._train_metsX) > 0:\n",
    "        train_names = train_names.map('train_{}')\n",
    "        valid_names = valid_names.map('valid_{}')\n",
    "    smooth = self._train_metsX.argwhere(lambda o: isinstance(o, (AvgSmoothLossX, AvgSmoothMetricX)))\n",
    "    self.smooth_mets  = self._train_metsX[smooth]\n",
    "    self.smooth_names = train_names[smooth]\n",
    "    self.train_names = L('train_loss') + train_names\n",
    "    self.valid_names = L('valid_loss') + valid_names\n",
    "    self.metric_names = L('epoch') + self.train_names + self.valid_names\n",
    "    if self.add_time: self.metric_names.append('time')\n",
    "    self.smooth_loss.reset()\n",
    "    self.loss.reset()\n",
    "    self.smooth_mets.map(Self.reset())\n",
    "\n",
    "@patch\n",
    "def after_batch(self:Recorder):\n",
    "    \"Update all metrics and records lr and smooth loss in training\"\n",
    "    if len(self.yb) == 0: return\n",
    "    mets = self.train_mets() if self.training else self.valid_mets()\n",
    "    for met in mets: \n",
    "        met.accumulate(self.learn)\n",
    "    if not self.training: return\n",
    "    self.lrs.append(self.opt.hypers[-1]['lr'])\n",
    "    self.losses.append(self.smooth_loss.value)\n",
    "    self.learn.smooth_loss = self.smooth_loss.value\n",
    "\n",
    "@patch\n",
    "def before_epoch(self:Recorder):\n",
    "    \"Set timer if `self.add_time=True`\"\n",
    "    self.cancel_train,self.cancel_valid = False,False\n",
    "    if self.add_time: self.start_epoch = time.time()\n",
    "    self.log = L(getattr(self, 'epoch', 0))\n",
    "\n",
    "@patch\n",
    "def before_train(self:Recorder): \n",
    "    self._train_metsX.filter(lambda o: not isinstance(o, (AvgSmoothLossX, AvgSmoothMetricX))).map(Self.reset())\n",
    "\n",
    "@patch\n",
    "def before_validate(self:Recorder):\n",
    "    self.valid_mets().map(Self.reset())\n",
    "\n",
    "@patch\n",
    "def after_train(self:Recorder):\n",
    "    self.log += self.train_mets().map(_maybe_item)\n",
    "\n",
    "@patch\n",
    "def after_validate(self:Recorder):\n",
    "    self.log += self.valid_mets().map(_maybe_item)\n",
    "\n",
    "@patch\n",
    "def after_epoch(self:Recorder):\n",
    "    \"Store and log the loss/metric values\"\n",
    "    self.learn.final_record = self.log[1:].copy()\n",
    "    self.values.append(self.learn.final_record)\n",
    "    if self.add_time: self.log.append(format_time(time.time() - self.start_epoch))\n",
    "    self.logger(self.log)\n",
    "    self.iters.append(self.smooth_loss.count)\n",
    "\n",
    "@patch\n",
    "def train_mets(self:Recorder):\n",
    "    if getattr(self, 'cancel_train', False): return L()\n",
    "    return L(self.smooth_loss) + self._train_metsX\n",
    "\n",
    "@patch\n",
    "def valid_mets(self:Recorder):\n",
    "    if getattr(self, 'cancel_valid', False): return L()\n",
    "    return L(self.loss) + self._valid_metsX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def tst_metric(inp, targ): return F.mse_loss(inp, targ)\n",
    "\n",
    "def TestAvg(log_metric=LogMetric.Valid):\n",
    "    return AvgMetricX(tst_metric, log_metric=log_metric)\n",
    "\n",
    "def TestAccum(log_metric=LogMetric.Valid):\n",
    "    return AccumMetricX(tst_metric, log_metric=log_metric)\n",
    "\n",
    "def TestSmooth():\n",
    "    return AvgSmoothMetricX(tst_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#Test printed output\n",
    "learn = synth_learner(n_trn=5, metrics=TestAvg())\n",
    "# pat = r\"[tensor\\(\\d.\\d*\\), tensor\\(\\d.\\d*\\), tensor\\(\\d.\\d*\\), 'dd:dd']\"\n",
    "pat = r\"\\[\\d, \\d+.\\d+, \\d+.\\d+, \\d+.\\d+, '\\d\\d:\\d\\d'\\]\"\n",
    "test_stdout(lambda: learn.fit(1), pat, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "class TestRecorderCallback(Callback):\n",
    "    order=51\n",
    "    def before_fit(self): \n",
    "        self.add_time = self.recorder.add_time\n",
    "        self.beta = self.recorder.smooth_loss.beta\n",
    "        for m in self.metrics: assert isinstance(m, Metric)\n",
    "        test_eq(self.recorder.smooth_loss.val, 0.)\n",
    "        #To test what the recorder logs, we use a custom logger function.\n",
    "        self.learn.logger = self.test_log\n",
    "        self.old_smooth,self.count = tensor(0.),0\n",
    "    \n",
    "    def after_batch(self):\n",
    "        if self.training:\n",
    "            self.count += 1\n",
    "            test_eq(len(self.recorder.lrs), self.count)\n",
    "            test_eq(self.recorder.lrs[-1], self.opt.hypers[-1]['lr'])\n",
    "            test_eq(len(self.recorder.losses), self.count)\n",
    "            smooth = (1 - self.beta**(self.count-1)) * self.old_smooth * self.beta + self.loss * (1-self.beta)\n",
    "            smooth /= 1 - self.beta**self.count\n",
    "            test_close(self.recorder.losses[-1], smooth, eps=1e-4)\n",
    "            test_close(self.smooth_loss, smooth, eps=1e-4)\n",
    "            self.old_smooth = self.smooth_loss\n",
    "        self.bs += find_bs(self.yb)\n",
    "        if not self.training: test_eq(self.recorder.loss.count, self.bs)\n",
    "        for m in self.recorder.train_mets() if self.training else self.recorder.valid_mets(): \n",
    "            if isinstance(m, (AvgMetricX, AvgMetric)): test_eq(m.count, self.bs)\n",
    "            if isinstance(m, AvgSmoothMetricX): test_eq(m.count, self.count)\n",
    "        self.losses.append(self.loss.detach().cpu())\n",
    "    \n",
    "    def before_epoch(self): \n",
    "        if self.add_time: self.start_epoch = time.time()\n",
    "        self.log = [self.epoch]\n",
    "\n",
    "    def before_train(self):\n",
    "        self.bs = 0\n",
    "        self.losses = []\n",
    "        for m in self.recorder.train_mets(): \n",
    "            if isinstance(m, (AvgMetricX, AvgMetric)): test_eq(m.count, self.bs)\n",
    "            \n",
    "    def after_train(self):\n",
    "        val=0\n",
    "        if isinstance(self.recorder.train_mets()[-1], AvgSmoothMetricX):\n",
    "            for i, j in enumerate(self.losses):\n",
    "                val = val*0.98 + j*(1-0.98)\n",
    "            mean = val/(1-0.98**(i+1))\n",
    "        else:\n",
    "            mean = tensor(self.losses).mean()\n",
    "        self.log += [self.smooth_loss, mean] if len(self.recorder.train_mets()) > 1 else [self.smooth_loss]\n",
    "        test_close(self.log, self.recorder.log, 1e-4)\n",
    "        self.losses = []\n",
    "    \n",
    "    def before_validate(self):\n",
    "        self.bs = 0\n",
    "        self.losses = []\n",
    "        for m in [self.recorder.loss] + self.recorder.valid_mets(): \n",
    "            if isinstance(m, (AvgMetricX, AvgMetric, AvgLossX)): test_eq(m.count, self.bs)\n",
    "    \n",
    "    def test_log(self, log, eps=1e-4):\n",
    "        res = tensor(self.losses).mean()\n",
    "        self.log += [res, res]\n",
    "        if self.add_time: self.log.append(format_time(time.time() - self.start_epoch))\n",
    "        test_close(log[:-1], self.log[:-1], eps)\n",
    "        test_eq(log[-1], self.log[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "learn = synth_learner(n_trn=5, metrics=TestAvg(), cbs=TestRecorderCallback)\n",
    "learn.fit(1)\n",
    "test_eq(learn.recorder.metric_names, ['epoch', 'train_loss', 'valid_loss', 'tst_metric', 'time'])\n",
    "\n",
    "learn = synth_learner(n_trn=5, metrics=TestAvg(log_metric=LogMetric.Both), cbs=TestRecorderCallback)\n",
    "learn.fit(1)\n",
    "test_eq(learn.recorder.metric_names, \n",
    "        ['epoch', 'train_loss', 'train_tst_metric', 'valid_loss', 'valid_tst_metric', 'time'])\n",
    "\n",
    "learn = synth_learner(n_trn=5, metrics=[TestAvg(), TestAccum()], cbs=TestRecorderCallback)\n",
    "learn.fit(1)\n",
    "test_eq(learn.recorder.metric_names, \n",
    "        ['epoch', 'train_loss', 'valid_loss', 'avg_tst_metric', 'accm_tst_metric', 'time'])\n",
    "\n",
    "learn = synth_learner(n_trn=5, metrics=[TestAvg(log_metric=LogMetric.Both), TestAccum()], cbs=TestRecorderCallback)\n",
    "learn.fit(1)\n",
    "test_eq(learn.recorder.metric_names, \n",
    "        ['epoch', 'train_loss', 'train_avg_tst_metric', 'valid_loss', 'valid_avg_tst_metric', 'valid_accm_tst_metric', 'time'])\n",
    "\n",
    "learn = synth_learner(n_trn=5, metrics=TestAvg(), cbs=TestRecorderCallback)\n",
    "learn.recorder.add_time=False\n",
    "learn.fit(1)\n",
    "test_eq(learn.recorder.metric_names, ['epoch', 'train_loss', 'valid_loss', 'tst_metric'])\n",
    "\n",
    "learn = synth_learner(n_trn=5, metrics=[TestAvg(), TestSmooth()], cbs=TestRecorderCallback)\n",
    "learn.fit(1)\n",
    "test_eq(learn.recorder.metric_names, ['epoch','train_loss','train_tst_metric','valid_loss','valid_tst_metric','time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# test that fastai metrics still work\n",
    "from fastai.metrics import mse as fastai_mse\n",
    "learn = synth_learner(n_trn=5, metrics=fastai_mse, cbs=TestRecorderCallback)\n",
    "learn.fit(1)\n",
    "test_eq(learn.recorder.metric_names, ['epoch','train_loss','valid_loss','mse','time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#For testing: a fake learner and a metric that isn't an average\n",
    "class TstLearner(Learner):\n",
    "    def __init__(self,dls=None,model=None,**kwargs): self.pred,self.xb,self.yb = None,None,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def _l2_mean(x,y): return torch.sqrt((x.float()-y.float()).pow(2).mean())\n",
    "\n",
    "#Go through a fake cycle with various batch sizes and computes the value of met\n",
    "def compute_val(met, x1, x2):\n",
    "    met.reset()\n",
    "    vals = [0,6,15,20]\n",
    "    learn = TstLearner()\n",
    "    for i in range(3):\n",
    "        learn.pred,learn.yb = x1[vals[i]:vals[i+1]],(x2[vals[i]:vals[i+1]],)\n",
    "        met.accumulate(learn)\n",
    "    return met.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def _l2_mean(x,y): return torch.sqrt((x.argmax(dim=-1).float()-y.float()).pow(2).mean())\n",
    "x1,x2 = torch.randn(20,5),torch.randint(0, 5, (20,))\n",
    "tst = AccumMetricX(_l2_mean, dim_argmax=-1, flatten=False, activation=ActivationType.Softmax)\n",
    "test_close(compute_val(tst, x1, x2), _l2_mean(F.softmax(x1, dim=-1), x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Metric Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@delegates(MetricX)\n",
    "def func_to_metric(func, metric_type, is_class, thresh=None, axis=-1, activation=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Convert `func` metric to a fastai metric\"\n",
    "\n",
    "    dim_argmax = axis if is_class and thresh is None else None\n",
    "    if activation is None:\n",
    "        activation = ActivationType.Sigmoid if (is_class and thresh is not None) else ActivationType.No\n",
    "\n",
    "    if metric_type==MetricType.Accum:\n",
    "        return AccumMetricX(func, dim_argmax=dim_argmax, activation=activation, \n",
    "                            thresh=thresh, log_metric=log_metric, **kwargs)\n",
    "    elif metric_type==MetricType.Avg:\n",
    "        return AvgMetricX(func, dim_argmax=dim_argmax, activation=activation, \n",
    "                          thresh=thresh, log_metric=log_metric, **kwargs)\n",
    "    elif metric_type==MetricType.Smooth:\n",
    "        if log_metric!=LogMetric.Train: \n",
    "            name = func.func.__name__ if hasattr(func, 'func') else  func.__name__\n",
    "            raise ValueError(f'Error with {name}: AvgSmoothMetricX can only run on train. Set `log_metric` to LogMetric.Train.')\n",
    "        return AvgSmoothMetricX(func, dim_argmax=dim_argmax, activation=activation, thresh=thresh, **kwargs)\n",
    "    else:\n",
    "        name = func.func.__name__ if hasattr(func, 'func') else  func.__name__\n",
    "        raise ValueError(f\"Unsupported `metric_type` {metric_type} for metric {name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the quickest way to use a functional metric as a fastxtend metric.\n",
    "\n",
    "`metric_type` is one of `MetricType.Avg`, `MetricType.Accum`, or `MetricType.Smooth` which set the metric to use `AvgMetricX`, `AccumMetricX`, or `AvgSmoothMetricX`, respectively. \n",
    "\n",
    "`is_class` indicates if you are in a classification problem or not. In this case:\n",
    "- leaving `thresh` to `None` indicates it's a single-label classification problem and predictions will pass through an argmax over `axis` before being compared to the targets\n",
    "- setting a value for `thresh` indicates it's a multi-label classification problem and predictions will pass through a sigmoid (can be deactivated with `sigmoid=False`) and be compared to `thresh` before being compared to the targets\n",
    "\n",
    "If `is_class=False`, it indicates you are in a regression problem, and predictions are compared to the targets without being modified. In all cases, `kwargs` are extra keyword arguments passed to `func`.\n",
    "\n",
    "> Important: Some metrics, like Root Mean Squared Error, will have incorrect results if passed to `AvgMetricX` via `MetricType.Avg`, as the mean of multiple batches of RMSE isn't equal to the RMSE of the whole dataset. For these metrics use `AccumMetricX` by setting `metric_type` to `MetricType.Accum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@delegates(MetricX)\n",
    "def skm_to_fastxtend(func, is_class=True, thresh=None, axis=-1, activation=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Convert `func` from sklearn.metrics to a fastai metric\"\n",
    "    return func_to_metric(func, MetricType.Accum, is_class, thresh, axis, activation, \n",
    "                          log_metric, to_np=True, invert_arg=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the quickest way to use a scikit-learn metric using fastxtend metrics. It is the same as `func_to_metric` except it defaults to using `AccumMetricX`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "tst_single = skm_to_fastxtend(skm.precision_score)\n",
    "x1,x2 = torch.randn(20,2),torch.randint(0, 2, (20,))\n",
    "test_close(compute_val(tst_single, x1, x2), skm.precision_score(x2, x1.argmax(dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "tst_multi = skm_to_fastxtend(skm.precision_score, thresh=0.2)\n",
    "x1,x2 = torch.randn(20),torch.randint(0, 2, (20,))\n",
    "test_close(compute_val(tst_multi, x1, x2), skm.precision_score(x2, torch.sigmoid(x1) >= 0.2))\n",
    "\n",
    "tst_multi = skm_to_fastxtend(skm.precision_score, thresh=0.2, activation=ActivationType.No)\n",
    "x1,x2 = torch.randn(20),torch.randint(0, 2, (20,))\n",
    "test_close(compute_val(tst_multi, x1, x2), skm.precision_score(x2, x1 >= 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "tst_reg = skm_to_fastxtend(skm.r2_score, is_class=False)\n",
    "x1,x2 = torch.randn(20,5),torch.randn(20,5)\n",
    "test_close(compute_val(tst_reg, x1, x2), skm.r2_score(x2.view(-1), x1.view(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "test_close(tst_reg(x1, x2), skm.r2_score(x2.view(-1), x1.view(-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-label classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Warning: All functions defined in this section are intended for single-label classification and targets that are not one-hot encoded. For multi-label problems or one-hot encoded targets, use the version suffixed with multi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Warning: Many metrics in fastxtend are thin wrappers around sklearn functionality. However, sklearn metrics can handle python list strings, amongst other things, whereas fastxtend metrics work with PyTorch, and thus require tensors. The arguments that are passed to metrics are after all transformations, such as categories being converted to indices, have occurred. This means that when you pass a label of a metric, for instance, that you must pass indices, not strings. This can be converted with `vocab.map_obj`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def accuracy(inp, targ):\n",
    "    \"Compute accuracy with `targ` when `pred` is bs * n_classes\"\n",
    "    pred,targ = flatten_check(inp, targ)\n",
    "    return (pred == targ).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def Accuracy(axis=-1, metric_type=MetricType.Avg, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Compute accuracy with `targ` when `pred` is bs * n_classes\"\n",
    "    return func_to_metric(accuracy, metric_type, True, axis=axis, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#For testing\n",
    "def change_targ(targ, n, c):\n",
    "    idx = torch.randperm(len(targ))[:n]\n",
    "    res = targ.clone()\n",
    "    for i in idx: res[i] = (res[i]+random.randint(1,c-1))%c\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#For testing\n",
    "def compute_single(met, x1, x2):\n",
    "    met.reset()\n",
    "    learn = TstLearner()\n",
    "    learn.pred,learn.yb = x1,(x2,)\n",
    "    met.accumulate(learn)\n",
    "    return met.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "x = torch.randn(4,5)\n",
    "y = x.argmax(dim=1)\n",
    "test_eq(compute_single(Accuracy(), x, y), 1)\n",
    "y1 = change_targ(y, 2, 5)\n",
    "test_eq(compute_single(Accuracy(),x,y1), 0.5)\n",
    "test_eq(compute_single(Accuracy(),x.unsqueeze(1).expand(4,2,5), torch.stack([y,y1], dim=1)), 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def error_rate(inp, targ):\n",
    "    \"1 - `accuracy`\"\n",
    "    return 1 - accuracy(inp, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def ErrorRate(axis=-1, metric_type=MetricType.Avg, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Compute 1 - accuracy with `targ` when `pred` is bs * n_classes\"\n",
    "    return func_to_metric(error_rate, metric_type, True, axis=axis, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "x = torch.randn(4,5)\n",
    "y = x.argmax(dim=1)\n",
    "test_eq(compute_single(ErrorRate(), x,y), 0)\n",
    "y1 = change_targ(y, 2, 5)\n",
    "test_eq(compute_single(ErrorRate(), x,y1), 0.5)\n",
    "test_eq(compute_single(ErrorRate(), x.unsqueeze(1).expand(4,2,5), torch.stack([y,y1], dim=1)), 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def top_k_accuracy(inp, targ, k=5, axis=-1):\n",
    "    \"Computes the Top-k accuracy (`targ` is in the top `k` predictions of `inp`)\"\n",
    "    inp = inp.topk(k=k, dim=axis)[1]\n",
    "    targ = targ.unsqueeze(dim=axis).expand_as(inp)\n",
    "    return (inp == targ).sum(dim=-1).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def TopKAccuracy(k=5, axis=-1, metric_type=MetricType.Avg, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Computes the Top-k accuracy (`targ` is in the top `k` predictions of `inp`)\"\n",
    "    return func_to_metric(partial(top_k_accuracy, k=k, axis=axis), metric_type, False, \n",
    "                          log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "x = torch.randn(6,5)\n",
    "y = torch.arange(0,6)\n",
    "test_eq(compute_single(TopKAccuracy(), x[:5],y[:5]), 1)\n",
    "test_eq(compute_single(TopKAccuracy(), x, y), 5/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def APScoreBinary(axis=-1, average='macro', pos_label=1, sample_weight=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Average Precision for single-label binary classification problems\"\n",
    "    return skm_to_fastxtend(skm.average_precision_score, axis=axis, activation=ActivationType.BinarySoftmax,\n",
    "                         average=average, pos_label=pos_label, sample_weight=sample_weight, log_metric=log_metric, \n",
    "                         **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def BalancedAccuracy(axis=-1, sample_weight=None, adjusted=False, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Balanced Accuracy for single-label binary classification problems\"\n",
    "    return skm_to_fastxtend(skm.balanced_accuracy_score, axis=axis,\n",
    "                         sample_weight=sample_weight, adjusted=adjusted, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def BrierScore(axis=-1, sample_weight=None, pos_label=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Brier score for single-label classification problems\"\n",
    "    return skm_to_fastxtend(skm.brier_score_loss, axis=axis,\n",
    "                         sample_weight=sample_weight, pos_label=pos_label, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.brier_score_loss.html#sklearn.metrics.brier_score_loss) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def CohenKappa(axis=-1, labels=None, weights=None, sample_weight=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Cohen kappa for single-label classification problems\"\n",
    "    return skm_to_fastxtend(skm.cohen_kappa_score, axis=axis, labels=labels, weights=weights,\n",
    "                         sample_weight=sample_weight, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html#sklearn.metrics.cohen_kappa_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def F1Score(axis=-1, labels=None, pos_label=1, average='binary', sample_weight=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"F1 score for single-label classification problems\"\n",
    "    return skm_to_fastxtend(skm.f1_score, axis=axis, labels=labels, pos_label=pos_label, \n",
    "                         average=average, sample_weight=sample_weight, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def FBeta(beta, axis=-1, labels=None, pos_label=1, average='binary', sample_weight=None, \n",
    "          log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"FBeta score with `beta` for single-label classification problems\"\n",
    "    return skm_to_fastxtend(skm.fbeta_score, axis=axis, beta=beta, labels=labels, pos_label=pos_label, \n",
    "                         average=average, sample_weight=sample_weight, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def HammingLoss(axis=-1, sample_weight=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Hamming loss for single-label classification problems\"\n",
    "    return skm_to_fastxtend(skm.hamming_loss, axis=axis,\n",
    "                         sample_weight=sample_weight, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html#sklearn.metrics.hamming_loss) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def Jaccard(axis=-1, labels=None, pos_label=1, average='binary', sample_weight=None, \n",
    "            log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Jaccard score for single-label classification problems\"\n",
    "    return skm_to_fastxtend(skm.jaccard_score, axis=axis, labels=labels, pos_label=pos_label, \n",
    "                         average=average, sample_weight=sample_weight, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html#sklearn.metrics.jaccard_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def Precision(axis=-1, labels=None, pos_label=1, average='binary', sample_weight=None, \n",
    "              log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Precision for single-label classification problems\"\n",
    "    return skm_to_fastxtend(skm.precision_score, axis=axis, labels=labels, pos_label=pos_label, \n",
    "                         average=average, sample_weight=sample_weight, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def Recall(axis=-1, labels=None, pos_label=1, average='binary', sample_weight=None, \n",
    "           log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Recall for single-label classification problems\"\n",
    "    return skm_to_fastxtend(skm.recall_score, axis=axis, labels=labels, pos_label=pos_label, \n",
    "                         average=average, sample_weight=sample_weight, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def RocAuc(axis=-1, average='macro', sample_weight=None, max_fpr=None, multi_class='ovr', \n",
    "           log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Area Under the Receiver Operating Characteristic Curve for single-label multiclass classification problems\"\n",
    "    assert multi_class in ['ovr', 'ovo']\n",
    "    return skm_to_fastxtend(skm.roc_auc_score, axis=axis, activation=ActivationType.Softmax, flatten=False,\n",
    "                         average=average, sample_weight=sample_weight, max_fpr=max_fpr, multi_class=multi_class, \n",
    "                         log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def RocAucBinary(axis=-1, average='macro', sample_weight=None, max_fpr=None, multi_class='raise', \n",
    "                 log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Area Under the Receiver Operating Characteristic Curve for single-label binary classification problems\"\n",
    "    return skm_to_fastxtend(skm.roc_auc_score, axis=axis, activation=ActivationType.BinarySoftmax,\n",
    "                         average=average, sample_weight=sample_weight, max_fpr=max_fpr, multi_class=multi_class, \n",
    "                         log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def MatthewsCorrCoef(sample_weight=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Matthews correlation coefficient for single-label classification problems\"\n",
    "    return skm_to_fastxtend(skm.matthews_corrcoef, sample_weight=sample_weight, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html#sklearn.metrics.matthews_corrcoef) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def accuracy_multi(inp, targ):\n",
    "    \"Compute accuracy when `inp` and `targ` are the same size.\"\n",
    "    inp,targ = flatten_check(inp,targ)\n",
    "    return (inp==targ.bool()).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def AccuracyMulti(thresh=0.5, sigmoid=True, metric_type=MetricType.Avg, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Compute accuracy when `inp` and `targ` are the same size.\"\n",
    "    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n",
    "    return func_to_metric(accuracy_multi, metric_type, False, thresh=thresh, activation=activation, \n",
    "                          log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#For testing\n",
    "def change_1h_targ(targ, n):\n",
    "    idx = torch.randperm(targ.numel())[:n]\n",
    "    res = targ.clone().view(-1)\n",
    "    for i in idx: res[i] = 1-res[i]\n",
    "    return res.view(targ.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "x = torch.randn(4,5)\n",
    "y = (torch.sigmoid(x) >= 0.5).byte()\n",
    "test_eq(compute_single(AccuracyMulti(),x,y), 1)\n",
    "test_eq(compute_single(AccuracyMulti(),x,1-y), 0)\n",
    "y1 = change_1h_targ(y, 5)\n",
    "test_eq(compute_single(AccuracyMulti(),x,y1), 0.75)\n",
    "\n",
    "#Different thresh\n",
    "y = (torch.sigmoid(x) >= 0.2).byte()\n",
    "test_eq(compute_single(AccuracyMulti(thresh=0.2),x,y), 1)\n",
    "test_eq(compute_single(AccuracyMulti(thresh=0.2),x,1-y), 0)\n",
    "y1 = change_1h_targ(y, 5)\n",
    "test_eq(compute_single(AccuracyMulti(thresh=0.2),x,y1), 0.75)\n",
    "\n",
    "#No sigmoid\n",
    "y = (x >= 0.5).byte()\n",
    "test_eq(compute_single(AccuracyMulti(sigmoid=False),x,y), 1)\n",
    "test_eq(compute_single(AccuracyMulti(sigmoid=False),x,1-y), 0)\n",
    "y1 = change_1h_targ(y, 5)\n",
    "test_eq(compute_single(AccuracyMulti(sigmoid=False),x,y1), 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def APScoreMulti(sigmoid=True, average='macro', pos_label=1, sample_weight=None, \n",
    "                 log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Average Precision for multi-label classification problems\"\n",
    "    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n",
    "    return skm_to_fastxtend(skm.average_precision_score, activation=activation, flatten=False,\n",
    "                         average=average, pos_label=pos_label, sample_weight=sample_weight, \n",
    "                         log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def BrierScoreMulti(thresh=0.5, sigmoid=True, sample_weight=None, pos_label=None, \n",
    "                    log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Brier score for multi-label classification problems\"\n",
    "    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n",
    "    return skm_to_fastxtend(skm.brier_score_loss, thresh=thresh, activation=activation, flatten=False,\n",
    "                         sample_weight=sample_weight, pos_label=pos_label, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.brier_score_loss.html#sklearn.metrics.brier_score_loss) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def F1ScoreMulti(thresh=0.5, sigmoid=True, labels=None, pos_label=1, average='macro', sample_weight=None, \n",
    "                 log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"F1 score for multi-label classification problems\"\n",
    "    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n",
    "    return skm_to_fastxtend(skm.f1_score, thresh=thresh, activation=activation, flatten=False,\n",
    "                         labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight, \n",
    "                         log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def FBetaMulti(beta, thresh=0.5, sigmoid=True, labels=None, pos_label=1, average='macro', sample_weight=None, \n",
    "               log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"FBeta score with `beta` for multi-label classification problems\"\n",
    "    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n",
    "    return skm_to_fastxtend(skm.fbeta_score, thresh=thresh, activation=activation, flatten=False,\n",
    "                beta=beta, labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight, \n",
    "                log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def HammingLossMulti(thresh=0.5, sigmoid=True, labels=None, sample_weight=None, \n",
    "                     log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Hamming loss for multi-label classification problems\"\n",
    "    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n",
    "    return skm_to_fastxtend(skm.hamming_loss, thresh=thresh, activation=activation, flatten=False,\n",
    "                         sample_weight=sample_weight, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html#sklearn.metrics.hamming_loss) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def JaccardMulti(thresh=0.5, sigmoid=True, labels=None, pos_label=1, average='macro', sample_weight=None, \n",
    "                 log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Jaccard score for multi-label classification problems\"\n",
    "    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n",
    "    return skm_to_fastxtend(skm.jaccard_score, thresh=thresh, activation=activation, flatten=False,\n",
    "                         labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight, \n",
    "                         log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html#sklearn.metrics.jaccard_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def MatthewsCorrCoefMulti(thresh=0.5, sigmoid=True, sample_weight=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Matthews correlation coefficient for multi-label classification problems\"\n",
    "    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n",
    "    return skm_to_fastxtend(skm.matthews_corrcoef, thresh=thresh, activation=activation, flatten=False, \n",
    "                         sample_weight=sample_weight, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html#sklearn.metrics.matthews_corrcoef) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def PrecisionMulti(thresh=0.5, sigmoid=True, labels=None, pos_label=1, average='macro', sample_weight=None, \n",
    "                   log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Precision for multi-label classification problems\"\n",
    "    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n",
    "    return skm_to_fastxtend(skm.precision_score, thresh=thresh, activation=activation, flatten=False,\n",
    "                         labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight, \n",
    "                         log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def RecallMulti(thresh=0.5, sigmoid=True, labels=None, pos_label=1, average='macro', sample_weight=None, \n",
    "                log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Recall for multi-label classification problems\"\n",
    "    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n",
    "    return skm_to_fastxtend(skm.recall_score, thresh=thresh, activation=activation, flatten=False,\n",
    "                         labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight, \n",
    "                         log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def RocAucMulti(sigmoid=True, average='macro', sample_weight=None, max_fpr=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Area Under the Receiver Operating Characteristic Curve for multi-label binary classification problems\"\n",
    "    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n",
    "    return skm_to_fastxtend(skm.roc_auc_score, activation=activation, flatten=False,\n",
    "                         average=average, sample_weight=sample_weight, max_fpr=max_fpr, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1037/3315559218.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646756402876/work/torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  x,y = torch.tensor([np.arange(start=0, stop=0.2, step=0.04)]*20), torch.tensor([0, 0, 1, 1]).repeat(5)\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "roc_auc_metric = RocAucMulti(sigmoid=False)\n",
    "x,y = torch.tensor([np.arange(start=0, stop=0.2, step=0.04)]*20), torch.tensor([0, 0, 1, 1]).repeat(5)\n",
    "assert compute_val(roc_auc_metric, x, y) == 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def mse(inp,targ):\n",
    "    \"Mean squared error between `inp` and `targ`.\"\n",
    "    return F.mse_loss(*flatten_check(inp,targ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def MSE(metric_type=MetricType.Avg, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Mean squared error between `inp` and `targ`.\"\n",
    "    return func_to_metric(mse, metric_type, False, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "x1,x2 = torch.randn(4,5),torch.randn(4,5)\n",
    "test_close(compute_single(MSE(),x1,x2), (x1-x2).pow(2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def rmse(inp, targ): \n",
    "    return torch.sqrt(F.mse_loss(inp, targ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def RMSE(log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Mean squared error between `inp` and `targ`.\"\n",
    "    return func_to_metric(rmse, MetricType.Accum, False, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "x1,x2 = torch.randn(20,5),torch.randn(20,5)\n",
    "test_eq(compute_val(RMSE(), x1, x2), torch.sqrt(F.mse_loss(x1,x2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def mae(inp,targ):\n",
    "    \"Mean absolute error between `inp` and `targ`.\"\n",
    "    inp,targ = flatten_check(inp,targ)\n",
    "    return torch.abs(inp - targ).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def MAE(metric_type=MetricType.Avg, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Mean absolute error between `inp` and `targ`.\"\n",
    "    return func_to_metric(mae, metric_type, False, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "x1,x2 = torch.randn(4,5),torch.randn(4,5)\n",
    "test_eq(compute_single(MAE(),x1,x2), torch.abs(x1-x2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def msle(inp, targ):\n",
    "    \"Mean squared logarithmic error between `inp` and `targ`.\"\n",
    "    inp,targ = flatten_check(inp,targ)\n",
    "    return F.mse_loss(torch.log(1 + inp), torch.log(1 + targ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def MSLE(metric_type=MetricType.Avg, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Mean squared logarithmic error between `inp` and `targ`.\"\n",
    "    return func_to_metric(msle, metric_type, False, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "x1,x2 = torch.randn(4,5),torch.randn(4,5)\n",
    "x1,x2 = torch.relu(x1),torch.relu(x2)\n",
    "test_close(compute_single(MSLE(), x1,x2), (torch.log(x1+1)-torch.log(x2+1)).pow(2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def exp_rmspe(inp,targ):\n",
    "    inp,targ = torch.exp(inp),torch.exp(targ)\n",
    "    return torch.sqrt(((targ - inp)/targ).pow(2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def ExpRMSE(log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Root mean square percentage error of the exponential of  predictions and targets\"\n",
    "    return func_to_metric(exp_rmspe, MetricType.Accum, False, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "x1,x2 = torch.randn(20,5),torch.randn(20,5)\n",
    "test_eq(compute_val(ExpRMSE(), x1, x2), torch.sqrt((((torch.exp(x2) - torch.exp(x1))/torch.exp(x2))**2).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def ExplainedVariance(sample_weight=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Explained variance between predictions and targets\"\n",
    "    return skm_to_fastxtend(skm.explained_variance_score, is_class=False, \n",
    "                         sample_weight=sample_weight, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.explained_variance_score.html#sklearn.metrics.explained_variance_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def R2Score(sample_weight=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"R2 score between predictions and targets\"\n",
    "    return skm_to_fastxtend(skm.r2_score, is_class=False, sample_weight=sample_weight, \n",
    "                         log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def PearsonCorrCoef(dim_argmax=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Pearson correlation coefficient for regression problem\"\n",
    "    def pearsonr(x,y): return scs.pearsonr(x,y)[0]\n",
    "    return AccumMetricX(pearsonr, invert_arg=False, dim_argmax=dim_argmax, \n",
    "                        log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scipy documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html?highlight=pearson#scipy.stats.pearsonr) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "x = torch.randint(-999, 999,(20,))\n",
    "y = torch.randint(-999, 999,(20,))\n",
    "test_eq(compute_val(PearsonCorrCoef(), x, y), scs.pearsonr(x.view(-1), y.view(-1))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def SpearmanCorrCoef(dim_argmax=None, axis=0, nan_policy='propagate', log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Spearman correlation coefficient for regression problem\"\n",
    "    def spearmanr(a,b=None,**kwargs): return scs.spearmanr(a,b,**kwargs)[0]\n",
    "    return AccumMetricX(partial(spearmanr, axis=axis, nan_policy=nan_policy),\n",
    "                        invert_arg=False, dim_argmax=dim_argmax, log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scipy documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html?highlight=spearman#scipy.stats.spearmanr) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "x = torch.randint(-999, 999,(20,))\n",
    "y = torch.randint(-999, 999,(20,))\n",
    "test_eq(compute_val(SpearmanCorrCoef(), x, y), scs.spearmanr(x.view(-1), y.view(-1))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastai.torch_core.TensorImage"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "from fastai.vision.models import resnet18\n",
    "model = resnet18()\n",
    "x = cast(torch.rand(1,3,128,128), TensorImage)\n",
    "type(model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def foreground_acc(inp, targ, bkg_idx=0, axis=1):\n",
    "    \"Computes non-background accuracy for multiclass segmentation\"\n",
    "    targ = cast(targ.squeeze(1), TensorBase)\n",
    "    mask = targ != bkg_idx\n",
    "    return (inp[mask]==targ[mask]).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def ForegroundAcc(bkg_idx=0, axis=1, metric_type=MetricType.Avg, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Computes non-background accuracy for multiclass segmentation\"\n",
    "    return func_to_metric(foreground_acc, metric_type, True, bkg_idx=bkg_idx, axis=axis, \n",
    "                          log_metric=log_metric, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "x = cast(torch.randn(4,5,3,3), TensorImage)\n",
    "y = cast(x, TensorMask).argmax(dim=1)[:,None]\n",
    "test_eq(compute_single(ForegroundAcc(),x,y), 1)\n",
    "y[0] = 0 #the 0s are ignored so we get the same value\n",
    "test_eq(compute_single(ForegroundAcc(),x,y), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Dice(MetricX):\n",
    "    \"Dice coefficient metric for binary target in segmentation\"\n",
    "    def __init__(self, axis=1, log_metric=LogMetric.Valid, **kwargs):\n",
    "        super().__init__(dim_argmax=axis, log_metric=log_metric, **kwargs)\n",
    "    def reset(self): self.inter,self.union = 0,0\n",
    "    def accumulate(self, learn):\n",
    "        super().accumulate(learn)\n",
    "        self.pred,self.targ = flatten_check(self.pred, self.targ)\n",
    "        self.inter += (self.pred*self.targ).float().sum().item()\n",
    "        self.union += (self.pred+self.targ).float().sum().item()\n",
    "\n",
    "    @property\n",
    "    def value(self): return 2. * self.inter/self.union if self.union > 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "x1 = cast(torch.randn(20,2,3,3), TensorImage)\n",
    "x2 = cast(torch.randint(0, 2, (20, 3, 3)), TensorMask)\n",
    "pred = x1.argmax(1)\n",
    "inter = (pred*x2).float().sum().item()\n",
    "union = (pred+x2).float().sum().item()\n",
    "test_eq(compute_val(Dice(), x1, x2), 2*inter/union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class DiceMulti(MetricX):\n",
    "    \"Averaged Dice metric (Macro F1) for multiclass target in segmentation\"\n",
    "    def __init__(self, axis=1, log_metric=LogMetric.Valid, **kwargs):\n",
    "        super().__init__(dim_argmax=axis, log_metric=log_metric, **kwargs)\n",
    "    def reset(self): self.inter,self.union = {},{}\n",
    "    def accumulate(self, learn):\n",
    "        super().accumulate(learn)\n",
    "        self.pred,self.targ = flatten_check(self.pred, self.targ)\n",
    "        for c in range(learn.pred.shape[self.dim_argmax]):\n",
    "            p = torch.where(self.pred == c, 1, 0)\n",
    "            t = torch.where(self.targ == c, 1, 0)\n",
    "            c_inter = (p*t).float().sum().item()\n",
    "            c_union = (p+t).float().sum().item()\n",
    "            if c in self.inter:\n",
    "                self.inter[c] += c_inter\n",
    "                self.union[c] += c_union\n",
    "            else:\n",
    "                self.inter[c] = c_inter\n",
    "                self.union[c] = c_union\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        binary_dice_scores = np.array([])\n",
    "        for c in self.inter:\n",
    "            binary_dice_scores = np.append(binary_dice_scores, 2.*self.inter[c]/self.union[c] if self.union[c] > 0 else np.nan)\n",
    "        return np.nanmean(binary_dice_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DiceMulti method implements the \"Averaged F1: arithmetic mean over harmonic means\" described in this publication: https://arxiv.org/pdf/1911.03347.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "x1a = torch.ones(20,1,1,1)\n",
    "x1b = torch.clone(x1a)*0.5\n",
    "x1c = torch.clone(x1a)*0.3\n",
    "x1 = torch.cat((x1a,x1b,x1c),dim=1)   # Prediction: 20xClass0\n",
    "x2 = torch.zeros(20,1,1)              # Target: 20xClass0\n",
    "test_eq(compute_val(DiceMulti(), x1, x2), 1.)\n",
    "\n",
    "x2 = torch.ones(20,1,1)               # Target: 20xClass1\n",
    "test_eq(compute_val(DiceMulti(), x1, x2), 0.)\n",
    "\n",
    "x2a = torch.zeros(10,1,1)\n",
    "x2b = torch.ones(5,1,1)\n",
    "x2c = torch.ones(5,1,1) * 2\n",
    "x2 = torch.cat((x2a,x2b,x2c),dim=0)   # Target: 10xClass0, 5xClass1, 5xClass2\n",
    "dice1 = (2*10)/(2*10+10)              # Dice: 2*TP/(2*TP+FP+FN)\n",
    "dice2 = 0\n",
    "dice3 = 0\n",
    "test_eq(compute_val(DiceMulti(), x1, x2), (dice1+dice2+dice3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class JaccardCoeff(Dice):\n",
    "    \"Implementation of the Jaccard coefficient that is lighter in RAM\"\n",
    "    @property\n",
    "    def value(self): return self.inter/(self.union-self.inter) if self.union > 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "x1 = cast(torch.randn(20,2,3,3), TensorImage)\n",
    "x2 = cast(torch.randint(0, 2, (20, 3, 3)), TensorMask)\n",
    "pred = x1.argmax(1)\n",
    "inter = (pred*x2).float().sum().item()\n",
    "union = (pred+x2).float().sum().item()\n",
    "test_eq(compute_val(JaccardCoeff(), x1, x2), inter/(union-inter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class CorpusBLEUMetric(MetricX):\n",
    "    \"BLEU Metric calculated over the validation corpus\"\n",
    "    def __init__(self, vocab_sz=5000, axis=-1, log_metric=LogMetric.Valid, name='CorpusBLEU', **kwargs):\n",
    "        super().__init__(log_metric=log_metric, name=name, **kwargs)\n",
    "        self.axis, self.vocab_sz = axis, vocab_sz\n",
    "        self.pred_len,self.targ_len,self.samp_idx,self.corrects,self.counts, = 0,0,0,[0]*4,[0]*4\n",
    "\n",
    "    def reset(self):\n",
    "        self.pred_len,self.targ_len,self.corrects,self.counts = 0,0,[0]*4,[0]*4\n",
    "\n",
    "    class NGram():\n",
    "        def __init__(self, ngram, max_n=5000): self.ngram,self.max_n = ngram,max_n\n",
    "        def __eq__(self, other):\n",
    "            if len(self.ngram) != len(other.ngram): return False\n",
    "            return np.all(np.array(self.ngram) == np.array(other.ngram))\n",
    "        def __hash__(self): return int(sum([o * self.max_n**i for i,o in enumerate(self.ngram)]))\n",
    "\n",
    "    def get_grams(self, x, n, max_n=5000):\n",
    "        return x if n==1 else [self.NGram(x[i:i+n], max_n=max_n) for i in range(len(x)-n+1)]\n",
    "\n",
    "    def get_correct_ngrams(self, pred, targ, n, max_n=5000):\n",
    "        pred_grams,targ_grams = self.get_grams(pred, n, max_n=max_n),self.get_grams(targ, n, max_n=max_n)\n",
    "        pred_cnt,targ_cnt = Counter(pred_grams),Counter(targ_grams)\n",
    "        return sum([min(c, targ_cnt[g]) for g,c in pred_cnt.items()]),len(pred_grams)\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        if learn.training: return None\n",
    "        else:\n",
    "            last_output = learn.pred.argmax(dim=self.axis)\n",
    "            last_target = learn.y\n",
    "            for pred,targ in zip(last_output.cpu().numpy(),last_target.cpu().numpy()):\n",
    "                self.pred_len += len(pred)\n",
    "                self.targ_len += len(targ)\n",
    "                smooth_mteval = 1\n",
    "                for i in range(4):\n",
    "                    c,t = self.get_correct_ngrams(pred, targ, i+1, max_n=self.vocab_sz)\n",
    "                    if c == 0:\n",
    "                        smooth_mteval *= 2\n",
    "                        c = 1 / smooth_mteval    # exp smoothing, method 3 from http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf\n",
    "                    self.corrects[i] += c\n",
    "                    self.counts[i]   += t\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.counts == 0: return None\n",
    "        elif max(self.corrects) == 0: return 0.0\n",
    "        else:\n",
    "            precs = [c/t for c,t in zip(self.corrects,self.counts)]\n",
    "            len_penalty = math.exp(1 - self.targ_len/self.pred_len) if self.pred_len < self.targ_len else 1\n",
    "            return len_penalty * ((precs[0]*precs[1]*precs[2]*precs[3]) ** 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def create_vcb_emb(pred, targ):\n",
    "    # create vocab \"embedding\" for predictions\n",
    "    vcb_sz = max(torch.unique(torch.cat([pred, targ])))+1\n",
    "    pred_emb=torch.zeros(pred.size()[0], pred.size()[1] ,vcb_sz)\n",
    "    for i,v in enumerate(pred):\n",
    "        pred_emb[i].scatter_(1, v.view(len(v),1),1)\n",
    "    return pred_emb\n",
    "\n",
    "def compute_bleu_val(met, x1, x2):\n",
    "    met.reset()\n",
    "    learn = TstLearner()\n",
    "    learn.training=False    \n",
    "    for i in range(len(x1)): \n",
    "        learn.pred,learn.yb = x1, (x2,)\n",
    "        met.accumulate(learn)\n",
    "    return met.value\n",
    "\n",
    "targ = torch.tensor([[1,2,3,4,5,6,1,7,8]]) \n",
    "pred = torch.tensor([[1,9,3,4,5,6,1,10,8]])\n",
    "pred_emb = create_vcb_emb(pred, targ)\n",
    "test_close(compute_bleu_val(CorpusBLEUMetric(), pred_emb, targ), 0.48549)\n",
    "\n",
    "targ = torch.tensor([[1,2,3,4,5,6,1,7,8],[1,2,3,4,5,6,1,7,8]]) \n",
    "pred = torch.tensor([[1,9,3,4,5,6,1,10,8],[1,9,3,4,5,6,1,10,8]])\n",
    "pred_emb = create_vcb_emb(pred, targ)\n",
    "test_close(compute_bleu_val(CorpusBLEUMetric(), pred_emb, targ), 0.48549)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BLEU metric was introduced in [this article](https://www.aclweb.org/anthology/P02-1040) to come up with a way to evaluate the performance of translation models. It's based on the precision of n-grams in your prediction compared to your target. See the [fastai NLP course BLEU notebook](https://github.com/fastai/course-nlp/blob/master/bleu_metric.ipynb) for a more detailed description of BLEU.\n",
    "\n",
    "The smoothing used in the precision calculation is the same as in [SacreBLEU](https://github.com/mjpost/sacrebleu/blob/32c54cdd0dfd6a9fadd5805f2ea189ac0df63907/sacrebleu/sacrebleu.py#L540-L542), which in turn is \"method 3\" from the [Chen & Cherry, 2014](http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf) paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Perplexity(AvgLossX):\n",
    "    \"Perplexity (exponential of cross-entropy loss) for Language Models\"\n",
    "    @property\n",
    "    def value(self): return torch.exp(self.total/self.count) if self.count != 0 else None\n",
    "    @property\n",
    "    def name(self):  return \"perplexity\"\n",
    "\n",
    "perplexity = Perplexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "x1,x2 = torch.randn(20,5),torch.randint(0, 5, (20,))\n",
    "tst = perplexity\n",
    "tst.reset()\n",
    "vals = [0,6,15,20]\n",
    "learn = TstLearner()\n",
    "for i in range(3): \n",
    "    learn.yb = (x2[vals[i]:vals[i+1]],)\n",
    "    learn.loss = F.cross_entropy(x1[vals[i]:vals[i+1]],x2[vals[i]:vals[i+1]])\n",
    "    tst.accumulate(learn)\n",
    "test_close(tst.value, torch.exp(F.cross_entropy(x1,x2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LossMetrics -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LossMetric(AvgMetricX):\n",
    "    \"Create a metric from `loss_func.attr` named `nm`\"\n",
    "    def __init__(self, attr, nm=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "        super().__init__(noop, log_metric=log_metric, **kwargs)\n",
    "        store_attr('attr,nm')\n",
    "    def accumulate(self, learn):\n",
    "        bs = find_bs(learn.yb)\n",
    "        self.total += learn.to_detach(getattr(learn.loss_func, self.attr, 0))*bs\n",
    "        self.count += bs\n",
    "\n",
    "    @property\n",
    "    def name(self): return self.attr if self.nm is None else self.nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def LossMetrics(attrs, nms=None):\n",
    "    \"List of `LossMetric` for each of `attrs` and `nms`\"\n",
    "    if isinstance(attrs, str): attrs = attrs.split(',')\n",
    "    nms = attrs if nms is None else nms.split(',') if isinstance(nms, str) else nms\n",
    "    return [LossMetric(a, n) for a,n in zip(attrs,nms)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "class CombineL1L2(Module):\n",
    "    def forward(self, out, targ):\n",
    "        self.l1 = F.l1_loss(out, targ)\n",
    "        self.l2 = F.mse_loss(out, targ)\n",
    "        return self.l1+self.l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 9.458013534545898, 8.1944580078125, 2.3591325283050537, 5.835325241088867, '00:00']\n",
      "[1, 8.172794342041016, 5.658956050872803, 1.8951740264892578, 3.763782024383545, '00:00']\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "with less_random():\n",
    "    learn = synth_learner(metrics=LossMetrics('l1,l2'))\n",
    "    learn.loss_func = CombineL1L2()\n",
    "    learn.fit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logger Patches -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "try:\n",
    "    import wandb\n",
    "    from fastai.callback.wandb import WandbCallback\n",
    "    from fastai.callback.training import FetchPredsCallback\n",
    "\n",
    "    if not hasattr(WandbCallback,'_metrics_before_fit'): WandbCallback._metrics_before_fit = WandbCallback.before_fit\n",
    "\n",
    "    @patch\n",
    "    def before_fit(self:WandbCallback):\n",
    "        self._metrics_before_fit()\n",
    "        self.log_smooth = len(self.recorder.smooth_names) > 0\n",
    "\n",
    "    @patch\n",
    "    def after_batch(self:WandbCallback):\n",
    "        \"Log hyper-parameters and training loss\"\n",
    "        if self.training:\n",
    "            self._wandb_step += 1\n",
    "            self._wandb_epoch += 1/self.n_iter\n",
    "            hypers = {f'{k}_{i}':v for i,h in enumerate(self.opt.hypers) for k,v in h.items()}\n",
    "            if self.log_smooth: \n",
    "                for n,m in zip(self.recorder.smooth_names, self.recorder.smooth_mets): hypers[n]=m.value\n",
    "            wandb.log({'epoch': self._wandb_epoch, 'train_loss': to_detach(self.smooth_loss.clone()), 'raw_loss': to_detach(self.loss.clone()), **hypers}, step=self._wandb_step)\n",
    "\n",
    "    @patch\n",
    "    def after_epoch(self:WandbCallback):\n",
    "        \"Log validation loss and custom metrics & log prediction samples\"\n",
    "        # Correct any epoch rounding error and overwrite value\n",
    "        self._wandb_epoch = round(self._wandb_epoch)\n",
    "        wandb.log({'epoch': self._wandb_epoch}, step=self._wandb_step)\n",
    "        # Log sample predictions\n",
    "        if self.log_preds:\n",
    "            try:\n",
    "                self.log_predictions(self.learn.fetch_preds.preds)\n",
    "            except Exception as e:\n",
    "                self.log_preds = False\n",
    "                self.remove_cb(FetchPredsCallback)\n",
    "                print(f'WandbCallback was not able to get prediction samples -> {e}')\n",
    "        wandb.log({n:s for n,s in zip(self.recorder.metric_names, self.recorder.log) if n not in ['train_loss', 'epoch', 'time']+self.recorder.smooth_names}, step=self._wandb_step)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "try:\n",
    "    import tensorboard\n",
    "    from fastai.callback.tensorboard import TensorBoardCallback, tensorboard_log\n",
    "\n",
    "    if not hasattr(TensorBoardCallback,'_metrics_before_fit'): TensorBoardCallback._metrics_before_fit = TensorBoardCallback.before_fit\n",
    "\n",
    "    @patch\n",
    "    def before_fit(self:TensorBoardCallback):\n",
    "        self._metrics_before_fit()\n",
    "        self.log_smooth = len(self.recorder.smooth_names) > 0\n",
    "\n",
    "    @patch\n",
    "    def after_batch(self:TensorBoardCallback):\n",
    "        self.writer.add_scalar('train_loss', self.smooth_loss, self.train_iter)\n",
    "        for i,h in enumerate(self.opt.hypers):\n",
    "            for k,v in h.items(): self.writer.add_scalar(f'{k}_{i}', v, self.train_iter)\n",
    "        if self.log_smooth:\n",
    "            for k,v in zip(self.recorder.smooth_names, self.recorder.smooth_mets): \n",
    "                self.writer.add_scalar(f'{k}', v, self.train_iter)\n",
    "\n",
    "    @patch\n",
    "    def after_epoch(self:TensorBoardCallback):\n",
    "        for n,v in zip(self.recorder.metric_names[2:-1], self.recorder.log[2:-1]):\n",
    "            if n not in ['train_loss', 'time']+self.recorder.smooth_names:\n",
    "                self.writer.add_scalar(n, v, self.train_iter)\n",
    "        if self.log_preds:\n",
    "            b = self.dls.valid.one_batch()\n",
    "            self.learn.one_batch(0, b)\n",
    "            preds = getattr(self.loss_func, 'activation', noop)(self.pred)\n",
    "            out = getattr(self.loss_func, 'decodes', noop)(preds)\n",
    "            x,y,its,outs = self.dls.valid.show_results(b, out, show=False, max_n=self.n_preds)\n",
    "            tensorboard_log(x, y, its, outs, self.writer, self.train_iter)\n",
    "except:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
