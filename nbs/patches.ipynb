{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp patches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compatibility Patches\n",
    "> PyTorch version compatibility and fastai backport patches for fastxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from packaging.version import parse\n",
    "\n",
    "import fastai\n",
    "\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "_torch_version = parse(torch.__version__)\n",
    "_torch_20  = parse('2.0')\n",
    "_torch_113 = parse('1.13')\n",
    "_torch_112 = parse('1.12')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastai Backports -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "# This should be upstreamed in fastai 2.7.14\n",
    "if parse(fastai.__version__) < parse('2.7.14'):\n",
    "    from fastai.data.core import TfmdDL, DataLoader\n",
    "\n",
    "    def device_get(self) -> torch.device|None:\n",
    "        return self._device\n",
    "\n",
    "    def device_set(self, device:int|str|torch.device|None):\n",
    "        self._device, *_ = torch._C._nn._parse_to(device=device)\n",
    "        if hasattr(self, 'after_batch') and hasattr(self.after_batch, 'fs'):\n",
    "            for tfm in self.after_batch.fs:\n",
    "                # Check that tfm.to is callable as TabularPandas & transforms set tfm.to as an object\n",
    "                if hasattr(tfm, 'to') and callable(tfm.to):\n",
    "                    tfm.to(device)\n",
    "                else:\n",
    "                    for a in L(getattr(tfm, 'parameters', None)):\n",
    "                        if hasattr(getattr(tfm, a), 'to'):\n",
    "                            setattr(tfm, a, getattr(tfm, a).to(device))\n",
    "\n",
    "    # apply property patch to DataLoader\n",
    "    setattr(DataLoader, 'device', property(device_get, device_set))\n",
    "\n",
    "    @patch\n",
    "    def to(self:TfmdDL, device):\n",
    "        self.device = device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch 1.12 and 1.13 -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "if parse(fastai.__version__) < parse('2.7.12'):\n",
    "    @patch\n",
    "    def clone(self:TensorBase, *, memory_format=None):\n",
    "        cls = type(self)\n",
    "        return self.as_subclass(Tensor).clone(memory_format=memory_format).as_subclass(cls)\n",
    "\n",
    "    @patch\n",
    "    def new_empty(self:TensorBase, size, *, dtype=None, layout=None, device=None, pin_memory=False, requires_grad=False):\n",
    "        cls = type(self)\n",
    "        if _torch_version < _torch_113 and layout is None:\n",
    "            layout = torch.strided\n",
    "        if _torch_version < _torch_112:\n",
    "            return super(TensorBase, self).new_empty(size, dtype=dtype, layout=layout, device=device, pin_memory=pin_memory, requires_grad=requires_grad)\n",
    "        return self.as_subclass(Tensor).new_empty(size, dtype=dtype, layout=layout, device=device, pin_memory=pin_memory, requires_grad=requires_grad).as_subclass(cls)\n",
    "\n",
    "    @patch\n",
    "    def new_empty(self:TensorBase, *size, dtype=None, layout=None, device=None, pin_memory=False, requires_grad=False):\n",
    "        cls = type(self)\n",
    "        if _torch_version < _torch_113 and layout is None:\n",
    "            layout = torch.strided\n",
    "        if _torch_version < _torch_112:\n",
    "            return super(TensorBase, self).new_empty(*size, dtype=dtype, layout=layout, device=device, pin_memory=pin_memory, requires_grad=requires_grad)\n",
    "        return self.as_subclass(Tensor).new_empty(*size, dtype=dtype, layout=layout, device=device, pin_memory=pin_memory, requires_grad=requires_grad).as_subclass(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "x = TensorBase(torch.rand(4,3,16,16))\n",
    "x.test = 'test metadata'\n",
    "y = deepcopy(x)\n",
    "assert hasattr(y, 'test') and y.test == x.test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch 2.0 Nightly -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "if _torch_version >= _torch_20 and parse(fastai.__version__) < parse('2.7.12'):\n",
    "    from fastai.callback.training import ProgressCallback\n",
    "\n",
    "    @patch\n",
    "    def __reduce_ex__(self:TensorBase, proto):\n",
    "        return super(TensorBase, self).__reduce_ex__(proto)\n",
    "\n",
    "    @patch\n",
    "    def after_batch(self:ProgressCallback):\n",
    "        self.pbar.update(self.iter+1)\n",
    "        if hasattr(self, 'smooth_loss'):\n",
    "            self.pbar.comment = f'{self.smooth_loss.item():.4f}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face MutableMapping Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "# This was upstreamed in fastai 2.7.13\n",
    "if parse(fastai.__version__) < parse('2.7.13'):\n",
    "    from collections.abc import MutableMapping\n",
    "    from fastcore.dispatch import retain_type\n",
    "    from fastai.basics import defaults\n",
    "    from fastai.learner import Learner\n",
    "\n",
    "    def apply(func, x, *args, **kwargs):\n",
    "        \"Apply `func` recursively to `x`, passing on args\"\n",
    "        if is_listy(x):\n",
    "            return type(x)([apply(func, o, *args, **kwargs) for o in x])\n",
    "        if isinstance(x, (dict, MutableMapping)):\n",
    "            return {k: apply(func, v, *args, **kwargs) for k,v in x.items()}\n",
    "        res = func(x, *args, **kwargs)\n",
    "        return res if x is None else retain_type(res, x)\n",
    "\n",
    "    def to_device(b, device=None, non_blocking=False):\n",
    "        \"Recursively put `b` on `device`.\"\n",
    "        if defaults.use_cuda==False:\n",
    "            device='cpu'\n",
    "        elif device is None:\n",
    "            device=default_device()\n",
    "        def _inner(o):\n",
    "            if isinstance(o,Tensor):\n",
    "                return o.to(device, non_blocking=non_blocking)\n",
    "            return o\n",
    "        return apply(_inner, b)\n",
    "\n",
    "    @patch\n",
    "    def _set_device(self:Learner, b):\n",
    "        model_device = next(self.model.parameters()).device\n",
    "        dls_device = getattr(self.dls, 'device', default_device())\n",
    "        if model_device == dls_device:\n",
    "            return to_device(b, dls_device)\n",
    "        else:\n",
    "            return to_device(b, model_device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
