{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp callback.channelslast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Channels Last\n",
    "> Train models faster using channels last format (beta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `fastai.callback.fp16.MixedPrecision`, image models trained in channels last format on Tensor Cores can increase training throughput over contiguous format. PyTorch observed a [22% improvment](https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html#performance-gains) in ResNet50 training speed using channels last and 8-35% improvement across a selection of models tested on a V100.\n",
    "\n",
    "Channels last format is compatible with modern GPUs (Volta, Turing, or newer) and modern CPUs (Ice Lake or newer).\n",
    "\n",
    "Channels last memory format currently is implemented for NCHW Tensors. Not all PyTorch operators have been converted to support channels last. See [(Beta) Channels Last Memory Format in PyTorch](https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html) tutorial for more details.\n",
    "\n",
    ":::{.callout-note}\n",
    "The `ChannelsLast` callback has been [upstreamed](https://github.com/fastai/fastai/pull/3876) into fastai 2.7.11. This implementation will be kept for compatibility with older fastai versions for a few fastextend releases before being removed.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "from fastai.learner import Learner\n",
    "from fastai.callback.core import Callback\n",
    "from fastai.callback.fp16 import MixedPrecision\n",
    "\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channels Last -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ChannelsLast(Callback):\n",
    "    \"Channels last training using PyTorch's Channels Last Memory Format (beta)\"\n",
    "    order = -1 # Needs to run before any model modification callbacks occur (Distributed, EMA, etc)\n",
    "\n",
    "    def before_fit(self):\n",
    "        self.learn.model.to(memory_format=torch.channels_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a PyTorch model is set to channels last format, PyTorch will automatically convert any compatible NCHW input tensors to NHWC format. `ChannelsLast` sets the model to channels last format, so no changes to dataloaders or inputs are required.\n",
    "\n",
    "Using `ChannelsLast` with unsupported PyTorch operations can lead to “channel thrashing”, where channels last input is converted to contiguous format in an unsupported PyTorch operation, then back to channels last for execution on the tensor core, back to contiguous when returned to the operation, and finally to channels last for the next layer. Too many unsupported operations in a model can lead to reduced performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convenience Methods\n",
    "fastxtend adds two convenience methods to `Learner` to easily activate and disable channels last format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "@delegates(GradScaler)\n",
    "def to_channelslast(self:Learner,\n",
    "    to_fp16:bool=True, # Add `MixedPrecision` callback. Recommended for full channels last performance\n",
    "    **kwargs\n",
    "):\n",
    "    \"Set `Learner` and inputs to `channels_last` format and Mixed Precision by default\"\n",
    "    if to_fp16 and not hasattr(self, 'mixed_precision') and not hasattr(self, 'channels_last'):\n",
    "        return self.add_cbs([ChannelsLast(), MixedPrecision(**kwargs)])\n",
    "    elif not hasattr(self, 'channels_last'):\n",
    "        return self.add_cb(ChannelsLast())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def to_contiguous(self:Learner, to_fp32=False):\n",
    "    \"Set `Learner` and inputs to `contiguous_format` (default format), optionally to single precision\"\n",
    "    self.model.to(memory_format=torch.contiguous_format)\n",
    "    if to_fp32:\n",
    "        return self.remove_cbs([ChannelsLast, MixedPrecision])\n",
    "    else:\n",
    "        return self.remove_cb(ChannelsLast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# Tests contain code from:\n",
    "# fastai - Apache License 2.0 - Copyright (c) 2023 fast.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastxtend.test_utils import *\n",
    "from fastai.data.core import TfmdDL, DataLoaders\n",
    "from fastai.optimizer import SGD\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "class ChannelsLastTest(Callback):\n",
    "    \"Asserts that predictions are in channels last format\"\n",
    "    order = MixedPrecision.order-1\n",
    "    def after_pred(self):\n",
    "        assert self.pred.is_contiguous(memory_format=torch.channels_last), \"Model and/or output isn't channels last\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "def synth_dbunch(bs=16, n_train=10, n_valid=2, cuda=True):\n",
    "    def get_data(n):\n",
    "        return TensorDataset(TensorImage(torch.randn(bs*n, 3, 32, 32)))\n",
    "    train_ds = get_data(n_train)\n",
    "    valid_ds = get_data(n_valid)\n",
    "    device = default_device() if cuda else None\n",
    "    train_dl = TfmdDL(train_ds, bs=bs, shuffle=True, num_workers=0)\n",
    "    valid_dl = TfmdDL(valid_ds, bs=bs, num_workers=0)\n",
    "    return DataLoaders(train_dl, valid_dl, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>nan</td>\n",
       "      <td>None</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>None</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>nan</td>\n",
       "      <td>None</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "with no_random():\n",
    "    learn = synth_learner(cbs=[MixedPrecision,ChannelsLast,ChannelsLastTest], cuda=True, data=synth_dbunch())\n",
    "    class ConvModel(Module):\n",
    "        def __init__(self): self.conv = nn.Conv2d(3, 32, 1)\n",
    "        def forward(self,x): return self.conv(x)\n",
    "    def fakeloss(): pass\n",
    "    learn.model = ConvModel()\n",
    "    learn.opt_func = partial(SGD, mom=0.)\n",
    "    learn.loss_func=fakeloss\n",
    "    learn.fit(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
