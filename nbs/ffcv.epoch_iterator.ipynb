{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp ffcv.epoch_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "from threading import Thread, Event\n",
    "from queue import Queue, Full\n",
    "from contextlib import nullcontext\n",
    "from typing import Sequence, TYPE_CHECKING\n",
    "\n",
    "from ffcv.traversal_order.quasi_random import QuasiRandom\n",
    "from ffcv.utils import chunks\n",
    "from ffcv.pipeline.compiler import Compiler\n",
    "\n",
    "from ffcv.loader.epoch_iterator import EpochIterator, select_buffer, QUASIRANDOM_ERROR_MSG\n",
    "\n",
    "from fastcore.transform import Pipeline\n",
    "\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "if TYPE_CHECKING:\n",
    "    from fastxtend.ffcv.loader import Loader\n",
    "IS_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AsyncEpochIterator(EpochIterator):\n",
    "    def __init__(self,\n",
    "        loader: 'Loader',\n",
    "        order: Sequence[int],\n",
    "        cuda_streams: Sequence[torch.cuda.Stream],\n",
    "        after_batch: Pipeline\n",
    "    ):\n",
    "        super(EpochIterator, self).__init__(daemon=True)\n",
    "        self.loader: 'Loader' = loader\n",
    "        self.order = order\n",
    "        self.metadata = loader.reader.metadata\n",
    "        self.current_batch_slot = 0\n",
    "        batches = list(chunks(order, self.loader.batch_size))\n",
    "        self.iter_ixes = iter(batches)\n",
    "        self.closed = False\n",
    "        self.output_queue = Queue(self.loader.batches_ahead)\n",
    "        self.terminate_event = Event()\n",
    "        self.memory_context = self.loader.memory_manager.schedule_epoch(\n",
    "            batches)\n",
    "\n",
    "        if IS_CUDA:\n",
    "            self.current_stream = torch.cuda.current_stream()\n",
    "\n",
    "        try:\n",
    "            self.memory_context.__enter__()\n",
    "        except MemoryError as e:\n",
    "            if loader.traversal_order != QuasiRandom:\n",
    "                print(QUASIRANDOM_ERROR_MSG)\n",
    "                print('Full error below:')\n",
    "            raise e\n",
    "\n",
    "        self.storage_state = self.memory_context.state\n",
    "        self.cuda_streams = cuda_streams\n",
    "        self.after_batch = after_batch\n",
    "\n",
    "        self.memory_allocations = self.loader.graph.allocate_memory(\n",
    "            self.loader.batch_size,\n",
    "            self.loader.batches_ahead + 2\n",
    "        )\n",
    "        self.start()\n",
    "\n",
    "\n",
    "    def run_pipeline(self, b_ix, batch_indices, batch_slot, cuda_event):\n",
    "        self.memory_context.start_batch(b_ix)\n",
    "        args = []\n",
    "        if IS_CUDA:\n",
    "            stream = self.cuda_streams[batch_slot]\n",
    "            ctx = torch.cuda.stream(stream)\n",
    "        else:\n",
    "            ctx = nullcontext()\n",
    "\n",
    "        code, outputs = self.loader.code\n",
    "        with ctx:\n",
    "            self.loader._pipeline_device(self.after_batch)\n",
    "\n",
    "            if IS_CUDA:\n",
    "                if cuda_event:\n",
    "                    cuda_event.wait()\n",
    "\n",
    "            args = {\n",
    "                'batch_indices': batch_indices,\n",
    "                'storage_state': self.storage_state,\n",
    "                'metadata': self.metadata,\n",
    "                **{\n",
    "                    f'memory_{k}':select_buffer(v, batch_slot, len(batch_indices))\n",
    "                    for (k, v) in self.memory_allocations['operation'].items()\n",
    "                },\n",
    "                **{\n",
    "                    f'shared_memory_{k}': select_buffer(v, batch_slot, len(batch_indices))\n",
    "                    for (k, v) in self.memory_allocations['shared'].items()\n",
    "                }\n",
    "            }\n",
    "\n",
    "            for stage_code, define_outputs in code:\n",
    "                results = stage_code(**args)\n",
    "                for node_id, result in zip(define_outputs, results):\n",
    "                    args[f'result_{node_id}'] = result\n",
    "                pass\n",
    "\n",
    "            result = tuple(args[f'result_{x}'] for x in outputs)\n",
    "            return self.after_batch(result)\n",
    "\n",
    "    def close(self):\n",
    "        self.loader._pipeline_device(self.after_batch)\n",
    "        self.terminate_event.set()\n",
    "        if not self.closed:\n",
    "            self.memory_context.__exit__(None, None, None)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
