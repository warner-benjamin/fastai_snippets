{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp optimizer.lion\n",
    "#|default_cls_lvl 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "# Lion implementation based on the paper's code release\n",
    "# https://github.com/google/automl/tree/master/lion - Apache License 2.0 - Copyright 2020 Google Research"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lion: Evo**L**ved S**i**gn M**o**me**n**tum Optimizer\n",
    "> With fastai native and fused ForEach implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from fastai.optimizer import Optimizer\n",
    "\n",
    "from fastxtend.optimizer.foreach import ForEachOptimizer\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastxtend.test_utils import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lion Fastai Callback -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def lion_step(p:Tensor, lr:float, wd:float, beta1:float, beta2:float,\n",
    "              grad_avg:Tensor|None=None, do_wd:bool=True, **kwargs):\n",
    "\n",
    "    if grad_avg is None:\n",
    "        grad_avg = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "\n",
    "    # weight decay\n",
    "    if do_wd and wd != 0:\n",
    "        p.data.mul_(1-lr*wd)\n",
    "\n",
    "    # lion step\n",
    "    update = grad_avg.mul(beta1) + p.grad.data.mul(1 - beta1)\n",
    "    p.data.add_(torch.sign(update), alpha=-lr)\n",
    "\n",
    "    # Update m_k\n",
    "    grad_avg.mul_(beta2).add_(p.grad.data, alpha=1-beta2)\n",
    "\n",
    "    return {'grad_avg': grad_avg}\n",
    "\n",
    "lion_step.defaults = dict(beta1=0.9, beta2=0.99)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lion ForEach -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def lion_foreach_step(p:list[Tensor], g:list[Tensor], grad_avg:list[Tensor],\n",
    "                      do_wd:np.ndarray[Any, bool], lr:float, wd:float, beta1:float,\n",
    "                      beta2:float, **kwargs):\n",
    "\n",
    "    # weight_decay\n",
    "    if wd != 0:\n",
    "        wd = np.where(do_wd, 1-lr*wd, 1.)\n",
    "        torch._foreach_mul_(p, scalars=wd.tolist())\n",
    "\n",
    "    # lion update step\n",
    "    update = torch._foreach_mul(grad_avg, scalar=beta1)\n",
    "    torch._foreach_add_(update, g, alpha=1-beta1)\n",
    "    for u in update: u.sign_()\n",
    "    torch._foreach_add_(p, update, alpha=-lr)\n",
    "\n",
    "    # update m_k\n",
    "    torch._foreach_mul_(grad_avg, scalar=beta2)\n",
    "    torch._foreach_add_(grad_avg, g, alpha=1-beta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class LionForEachOptimizer(ForEachOptimizer):\n",
    "    \"An `Optimizer` with a modified step for Lion ForEach\"\n",
    "    def __init__(self,\n",
    "        params:Listified[Tensor], # Model parameters\n",
    "        opt_step:Callable, # `ForEachOptimizer` optimizer step\n",
    "        **defaults # Optimizer specific hyper parameters\n",
    "    ):\n",
    "        super().__init__(params, opt_step, **defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        if closure is not None: raise NotImplementedError(\"fastai optimizers currently do not support closure\")\n",
    "        for pg, hyper in zip(self.param_lists, self.hypers):\n",
    "            pl, gl, grad_avg, do_wd = [], [], [], []\n",
    "\n",
    "            for p in pg:\n",
    "                if hasattr(p, 'grad') and p.grad is not None:\n",
    "                    state = self.state[p]\n",
    "\n",
    "                    if 'grad_avg' not in state:\n",
    "                        state['grad_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "\n",
    "                    pl.append(p)\n",
    "                    gl.append(p.grad)\n",
    "                    grad_avg.append(state['grad_avg'])\n",
    "                    do_wd.append(state.get('do_wd', True))\n",
    "\n",
    "            self.opt_step(p=pl, g=gl, grad_avg=grad_avg, do_wd=np.array(do_wd, dtype=bool), **hyper)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lion was introduced by Chen et al in *[Symbolic Discovery of Optimization Algorithms](https://arxiv.org/abs/2302.06675)*.\n",
    "\n",
    "In addition to a fastai native implementation, `Lion` has a fused ForEach implementation. See the [Fused Optimizer](optimizer.fused.html) documentation for more details."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lion -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def Lion(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    beta1:float=0.9, # Update gradient moving average (β1) coefficient\n",
    "    beta2:float=0.99, # Gradient moving average (β2) coefficient\n",
    "    wd:float=0.1, # True weight decay\n",
    "    foreach:bool=False, # Use fused ForEach implementation\n",
    ") -> Optimizer|LionForEachOptimizer:\n",
    "    \"A fastai Lion optimizer with a fused ForEach implementation\"\n",
    "    if foreach:\n",
    "        return LionForEachOptimizer(params, lion_foreach_step, lr=lr,\n",
    "                                    beta1=beta1, beta2=beta2, wd=wd)\n",
    "    else:\n",
    "        return Optimizer(params, [lion_step], lr=lr, beta1=beta1, beta2=beta2, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def lion(\n",
    "    beta1:float=0.9, # Update gradient moving average (β1) coefficient\n",
    "    beta2:float=0.99, # Gradient moving average (β2) coefficient\n",
    "    wd:float=0.1, # True weight decay\n",
    "    foreach:bool=False, # Use fused ForEach implementation\n",
    ") -> Optimizer|LionForEachOptimizer:\n",
    "    \"Partial function for the Lion optimizer with a fused ForEach implementation\"\n",
    "    return partialler(Lion, beta1=beta1, beta2=beta2, wd=wd, foreach=foreach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# Test contain code from:\n",
    "# fastai - Apache License 2.0 - Copyright (c) 2023 fast.ai\n",
    "\n",
    "test_steps = 25\n",
    "\n",
    "def tst_param(val, grad=None):\n",
    "    \"Create a tensor with `val` and a gradient of `grad` for testing\"\n",
    "    res = tensor([val]).float()\n",
    "    res.grad = tensor([val/10 if grad is None else grad]).float()\n",
    "    return res\n",
    "\n",
    "def tst_params():\n",
    "    r = L.range(4)\n",
    "    return r.map(tst_param)\n",
    "\n",
    "params_org = tst_params()\n",
    "opt_org = Lion(params_org, lr=0.01)\n",
    "opt_org.step()\n",
    "\n",
    "params_for = tst_params()\n",
    "opt_for = Lion(params_for, lr=0.01, foreach=True)\n",
    "opt_for.step()\n",
    "\n",
    "# Test values from paper implementation: https://github.com/google/automl/tree/master/lion\n",
    "test_close([p.item() for p in params_org], [0., 0.9890, 1.9880, 2.9869999])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_for])\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_for.step()\n",
    "\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_for])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
