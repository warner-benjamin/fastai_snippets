{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp optimizer.adan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adan: ADAptive Nesterov Momentum Optimizer\n",
    "> Adds the Adan optimizer to fastai from [Adan: Adaptive Nesterov momentum Algorithm for Faster Optimizing Deep Models](https://arxiv.org/abs/2208.06677)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "from typing import Optional, Dict\n",
    "\n",
    "from fastai.optimizer import Optimizer, step_stat\n",
    "\n",
    "from fastxtend.optimizer.torchscript import JitOptimizer\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastxtend.test_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def debias(beta:float, step:int):\n",
    "    \"Simple debias calculation\"\n",
    "    return 1-beta**step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adan Fastai Callbacks -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def adan_setup(p, step=0, grad_avg=None, diff_avg=None, nesterov_est=None, prior_grad=None, paper_init=False, **kwargs):\n",
    "    \"Handles Adan setup and keeps track of steps\"\n",
    "    if step == 0: \n",
    "        grad_avg     = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "        diff_avg     = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "        nesterov_est = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "        if paper_init:\n",
    "            prior_grad = p.grad.clone()\n",
    "        else:\n",
    "            prior_grad = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "        step += 1\n",
    "        return {'grad_avg':grad_avg, 'diff_avg':diff_avg, 'nesterov_est':nesterov_est, 'prior_grad':prior_grad, 'step':step}\n",
    "    else:\n",
    "        step += 1\n",
    "        return {'step':step}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def adan_avgs(p, beta1, beta2, beta3, grad_avg, diff_avg, nesterov_est, prior_grad, **kwargs):\n",
    "    \"Updates Adan moving averages\"\n",
    "    # update m_k\n",
    "    grad_avg.mul_(beta1).add_(p.grad.data, alpha=1-beta1)\n",
    "\n",
    "    # update v_k\n",
    "    diff_avg.mul_(beta2).add_(p.grad.data-prior_grad, alpha=1-beta2)\n",
    "    \n",
    "    # update n_k\n",
    "    nesterov_est.mul_(beta3).add_(torch.square(torch.add(p.grad.data, torch.sub(p.grad.data, prior_grad), alpha=beta2)), alpha=1-beta3)\n",
    "\n",
    "    # set current grad as next step's prior_grad\n",
    "    prior_grad = p.grad.data.clone()\n",
    "    return {'grad_avg':grad_avg, 'diff_avg':diff_avg, 'nesterov_est':nesterov_est, 'prior_grad':prior_grad}\n",
    "\n",
    "adan_avgs.defaults = dict(beta1=0.98, beta2=0.92, beta3=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def adan_step(p, lr, eps, wd, beta1, beta2, beta3, step, grad_avg, diff_avg, nesterov_est, do_wd=True, **kwargs):\n",
    "    \"Performs the Adan step with `lr` on `p`\"\n",
    "    # calculate debias terms\n",
    "    db1, db2, db3 = debias(beta1, step), debias(beta2, step), debias(beta3, step)\n",
    "\n",
    "    # calculate applied λ \n",
    "    wd = (1+lr*wd) if wd!=0 and do_wd else 1 \n",
    "\n",
    "    # calculate η_k\n",
    "    lr = lr/torch.sqrt(nesterov_est/db3+eps) \n",
    "\n",
    "    # perform Adan step and apply to parameter `p`\n",
    "    p.data.sub_(torch.add(grad_avg/db1, diff_avg/db2, alpha=beta2).mul_(lr)).div_(wd)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adan Jit -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@torch.jit.script\n",
    "def adan_jit_step(p:Tensor, grad:Tensor, lr:float, wd:float, beta1:float, beta2:float, beta3:float, eps:float, paper_init:bool,\n",
    "                  grad_avg:Optional[Tensor]=None, diff_avg:Optional[Tensor]=None, nesterov_est:Optional[Tensor]=None, \n",
    "                  prior_grad:Optional[Tensor]=None, do_wd:bool=True, step:int=0, force_train:Optional[bool]=None, mom:Optional[float]=None):\n",
    "    dp = p\n",
    "    step += 1\n",
    "\n",
    "    if grad_avg is None: \n",
    "        grad_avg = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "    if diff_avg is None: \n",
    "        diff_avg = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "    if nesterov_est is None: \n",
    "        nesterov_est = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "    if prior_grad is None:\n",
    "        if paper_init:\n",
    "            prior_grad = grad.clone()\n",
    "        else:\n",
    "            prior_grad = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "\n",
    "    # update m_k\n",
    "    grad_avg = torch.mul(grad_avg, beta1)\n",
    "    grad_avg = torch.add(grad_avg, grad, alpha=1-beta1)\n",
    "\n",
    "    # update v_k\n",
    "    diff_avg = torch.mul(diff_avg, beta2)\n",
    "    diff_avg = torch.add(diff_avg, grad-prior_grad, alpha=1-beta2)\n",
    "    \n",
    "    # update n_k\n",
    "    nesterov_est = torch.mul(nesterov_est, beta3)\n",
    "    nesterov_est = torch.add(nesterov_est, torch.square(torch.add(grad, torch.sub(grad, prior_grad), alpha=beta2)), alpha=1-beta3)\n",
    "\n",
    "    # calculate debias terms\n",
    "    db1 = debias(beta1, step)\n",
    "    db2 = debias(beta2, step)\n",
    "    db3 = debias(beta3, step)\n",
    "\n",
    "    # calculate applied λ\n",
    "    if wd!=0 and do_wd:\n",
    "        wd = (1+lr*wd) \n",
    "    else:\n",
    "        wd = 1. \n",
    "\n",
    "    # calculate η_k\n",
    "    lr = lr/torch.sqrt(nesterov_est/db3+eps) \n",
    "\n",
    "    # perform Adan step\n",
    "    dp = torch.sub(dp, torch.mul(torch.add(grad_avg/db1, diff_avg/db2, alpha=beta2), lr))\n",
    "    dp = torch.div(dp, wd)\n",
    "\n",
    "    # set current grad as next step's prior_grad\n",
    "    prior_grad = grad.clone()\n",
    "\n",
    "    # apply results to parameter p\n",
    "    p.set_(dp)\n",
    "\n",
    "    return torch.jit.annotate(Dict[str, Union[Tensor, int]], {'grad_avg':grad_avg, 'diff_avg':diff_avg, 'nesterov_est':nesterov_est, 'prior_grad':prior_grad, 'step':step})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adan -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def Adan(params, lr, beta1=0.98, beta2=0.92, beta3=0.99, eps=1e-8, wd=0.02, paper_init=False, jit=False):\n",
    "    \"A `Optimizer` for Adan with `lr`, `beta`s, `eps` and `params`\"\n",
    "    if jit:\n",
    "        cb = partial(adan_jit_step, paper_init=paper_init)\n",
    "        return JitOptimizer(params, cb, lr=lr, beta1=beta1, beta2=beta2, beta3=beta3, eps=eps, wd=wd)\n",
    "    else:\n",
    "        cbs = [partial(adan_setup, paper_init=paper_init), adan_avgs, adan_step]\n",
    "        return Optimizer(params, cbs, lr=lr, beta1=beta1, beta2=beta2, beta3=beta3, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def tst_param(val, grad=None):\n",
    "    \"Create a tensor with `val` and a gradient of `grad` for testing\"\n",
    "    res = tensor([val]).float()\n",
    "    res.grad = tensor([val/10 if grad is None else grad]).float()\n",
    "    return res\n",
    "\n",
    "def tst_params():\n",
    "    r = L.range(4)\n",
    "    return r.map(tst_param)\n",
    "\n",
    "params_org = tst_params()\n",
    "opt_org = Adan(params_org, lr=0.1, jit=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = tst_params()\n",
    "opt_jit = Adan(params_jit, lr=0.1, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_jit])\n",
    "\n",
    "opt_org.step()\n",
    "opt_jit.step()\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_jit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def AdanLargeBatchLR(bs):\n",
    "    \"Square root rule for scaling `Adan` learning rate for large-batch training\"\n",
    "    return math.sqrt(bs/256)*6.25e-3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('fastai')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
