{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp callback.ema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "\n",
    "try:\n",
    "    import timm\n",
    "except ImportError:\n",
    "    raise ImportError(\"timm is required to use EMACallback. Install via `pip install timm`.\")\n",
    "\n",
    "from timm.utils.model_ema import ModelEmaV2\n",
    "\n",
    "from fastai.callback.core import Callback\n",
    "from fastai.callback.fp16 import MixedPrecision\n",
    "\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponential Moving Average\n",
    "> Callback to add Exponential Moving Average (EMA) of model weights during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMA implementation from [PyTorch Image Models](https://github.com/rwightman/pytorch-image-models/blob/master/timm/utils/model_ema.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class EMACallback(Callback):\n",
    "    \"Callback to implement Exponential Moving Average of model weights while training\"\n",
    "    order,run_valid = MixedPrecision.order+1,False\n",
    "    def __init__(self, \n",
    "        decay:float=0.9998, # EMA decay value\n",
    "        ema_device:torch.device|str|None=None, # Device to store EMA weights. Defaults to model device\n",
    "        validate_ema:bool=True, # Run validation metrics using EMA weights instead of model weights. If true, `ema_device` must match model device\n",
    "        replace_weights:bool=True # Replace model weights with EMA weights when finished training. If false, set `Learner.ema_model` to EMA weights\n",
    "    ):\n",
    "        store_attr()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def before_fit(self):\n",
    "        self.ema_model = ModelEmaV2(self.learn.model, self.decay, self.ema_device)\n",
    "        self.validate_ema = self.learn.model.device == self.ema_model.device if self.validate_ema else False\n",
    "\n",
    "    def after_batch(self):\n",
    "        self.ema_model.update(self.learn.model)\n",
    "\n",
    "    def before_validate(self):\n",
    "        if self.validate_ema:\n",
    "            self.temp_model = self.learn.model\n",
    "            self.learn.model = self.ema_model.module\n",
    "\n",
    "    def after_validate(self):\n",
    "        if self.validate_ema:\n",
    "            self.learn.model = self.temp_model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def after_fit(self):\n",
    "        if self.replace_weights:\n",
    "            self.learn.model = self.ema_model.module\n",
    "            self.ema_model = None\n",
    "        else:\n",
    "            self.learn.ema_model = self.ema_model.module\n",
    "        self.remove_cb(EMACallback)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ema_decay` default of 0.9998 means each update `EMACallback` will keep 99.98% of the prior EMA weights and update 0.02% towards the training model weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('fastai')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
