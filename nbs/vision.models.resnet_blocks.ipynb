{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp vision.models.resnet_blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flexible ResBlock\n",
    "> Fastai's ResBlock, but with more flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "from fastcore.meta import delegates\n",
    "from fastai.torch_core import Module\n",
    "from fastai.basics import defaults\n",
    "from fastai.layers import ConvLayer, NormType, SimpleSelfAttention, AvgPool, SEModule\n",
    "from torchvision.ops.stochastic_depth import StochasticDepth\n",
    "from fastxtend.vision.models.attention_modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ResBlock(Module):\n",
    "    \"Resnet block from `ni` to `nh` with `stride`\"\n",
    "    @delegates(ConvLayer.__init__)\n",
    "    def __init__(self, expansion, ni, nf, stride=1, groups=1, attn_mod=None, nh1=None, nh2=None, \n",
    "                 dw=False, g2=1, sa=False, sym=False, norm_type=NormType.Batch, act_cls=defaults.activation, \n",
    "                 ndim=2, ks=3, block_pool=AvgPool, pool_first=True, stoch_depth=0, **kwargs):\n",
    "        norm2 = (NormType.BatchZero if norm_type==NormType.Batch else\n",
    "                 NormType.InstanceZero if norm_type==NormType.Instance else norm_type)\n",
    "        if nh2 is None: nh2 = nf\n",
    "        if nh1 is None: nh1 = nh2\n",
    "        nf,ni = nf*expansion,ni*expansion\n",
    "        k0 = dict(norm_type=norm_type, act_cls=act_cls, ndim=ndim, **kwargs)\n",
    "        k1 = dict(norm_type=norm2, act_cls=None, ndim=ndim, **kwargs)\n",
    "        convpath  = [ConvLayer(ni,  nh2, ks, stride=stride, groups=ni if dw else groups, **k0),\n",
    "                     ConvLayer(nh2,  nf, ks, groups=g2, **k1)\n",
    "        ] if expansion == 1 else [\n",
    "                     ConvLayer(ni,  nh1, 1, **k0),\n",
    "                     ConvLayer(nh1, nh2, ks, stride=stride, groups=nh1 if dw else groups, **k0),\n",
    "                     ConvLayer(nh2,  nf, 1, groups=g2, **k1)]\n",
    "        if attn_mod: convpath.append(attn_mod(nf))\n",
    "        if sa: convpath.append(SimpleSelfAttention(nf,ks=1,sym=sym))\n",
    "        self.convpath = nn.Sequential(*convpath)\n",
    "        idpath = []\n",
    "        if ni!=nf: idpath.append(ConvLayer(ni, nf, 1, act_cls=None, ndim=ndim, **kwargs))\n",
    "        if stride!=1:\n",
    "            idpath.insert((1,0)[pool_first], block_pool(stride, ndim=ndim, ceil_mode=True))\n",
    "        self.idpath = nn.Sequential(*idpath)\n",
    "        self.act = act_cls(inplace=True) if act_cls is defaults.activation else act_cls()\n",
    "        self.depth = nn.Identity() if stoch_depth==0 else StochasticDepth(stoch_depth, 'batch')\n",
    "\n",
    "    def forward(self, x): return self.act(self.depth(self.convpath(x)) + self.idpath(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNeXt -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ResNeXtBlock(expansion, ni, nf, groups=32, stride=1, base_width=4, **kwargs):\n",
    "    w = math.floor(nf * (base_width / 64)) * groups\n",
    "    return ResBlock(expansion, ni, nf, stride=stride, groups=groups, nh2=w, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squeeze and Excitation -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def SEBlock(expansion, ni, nf, groups=1, se_reduction=16, stride=1, se_act_cls=defaults.activation, **kwargs):\n",
    "    attn_mod = partial(SEModule, reduction=se_reduction, act_cls=se_act_cls)\n",
    "    return ResBlock(expansion, ni, nf, stride=stride, groups=groups, attn_mod=attn_mod, nh1=nf*2, nh2=nf*expansion, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def SEResNeXtBlock(expansion, ni, nf, groups=32, se_reduction=16, stride=1, base_width=4, se_act_cls=defaults.activation, **kwargs):\n",
    "    w = math.floor(nf * (base_width / 64)) * groups\n",
    "    attn_mod = partial(SEModule, reduction=se_reduction, act_cls=se_act_cls)\n",
    "    return ResBlock(expansion, ni, nf, stride=stride, groups=groups, attn_mod=attn_mod, nh2=w, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Channel Attention -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ECABlock(expansion, ni, nf, groups=1, eca_ks=None, stride=1, **kwargs):\n",
    "    attn_mod = partial(ECA, ks=eca_ks)\n",
    "    return ResBlock(expansion, ni, nf, stride=stride, groups=groups, attn_mod=attn_mod, nh1=nf*2, nh2=nf*expansion, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ECAResNeXtBlock(expansion, ni, nf, groups=32, eca_ks=None, stride=1, base_width=4, **kwargs):\n",
    "    w = math.floor(nf * (base_width / 64)) * groups\n",
    "    attn_mod = partial(ECA, ks=eca_ks)\n",
    "    return ResBlock(expansion, ni, nf, stride=stride, groups=groups, attn_mod=attn_mod, nh2=w, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Attention -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def SABlock(expansion, ni, nf, groups=1, sa_grps=64, stride=1, **kwargs):\n",
    "    attn_mod = partial(ShuffleAttention, groups=sa_grps)\n",
    "    return ResBlock(expansion, ni, nf, stride=stride, groups=groups, attn_mod=attn_mod, nh1=nf*2, nh2=nf*expansion, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def SAResNeXtBlock(expansion, ni, nf, groups=32, sa_grps=64, stride=1, base_width=4, **kwargs):\n",
    "    w = math.floor(nf * (base_width / 64)) * groups\n",
    "    attn_mod = partial(ShuffleAttention, groups=sa_grps)\n",
    "    return ResBlock(expansion, ni, nf, stride=stride, groups=groups, attn_mod=attn_mod, nh2=w, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet Attention -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def TABlock(expansion, ni, nf, groups=1, ta_ks=7, stride=1, **kwargs):\n",
    "    attn_mod = partial(TripletAttention, ks=ta_ks)\n",
    "    return ResBlock(expansion, ni, nf, stride=stride, groups=groups, attn_mod=attn_mod, nh1=nf*2, nh2=nf*expansion, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def TAResNeXtBlock(expansion, ni, nf, groups=32, ta_ks=7, stride=1, base_width=4, **kwargs):\n",
    "    w = math.floor(nf * (base_width / 64)) * groups\n",
    "    attn_mod = partial(TripletAttention, ks=ta_ks)\n",
    "    return ResBlock(expansion, ni, nf, stride=stride, groups=groups, attn_mod=attn_mod, nh2=w, **kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
