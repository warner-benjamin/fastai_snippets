{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp optimizer.torchscript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchScript Optimizers\n",
    "> fastai optimizers compiled with TorchScript for improved performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "from typing import Optional, Dict\n",
    "\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from fastcore.basics import range_of, merge\n",
    "\n",
    "from fastai.optimizer import (Optimizer, _update, weight_decay, l2_reg, average_grad, sgd_step, \n",
    "                              momentum_step, average_sqr_grad, rms_prop_step, step_stat, adam_step, \n",
    "                              radam_step, qhadam_step, larc_layer_lr, larc_step, lamb_step, Lookahead)\n",
    "\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastxtend.test_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Utils -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def tst_param(val, grad=None):\n",
    "    \"Create a tensor with `val` and a gradient of `grad` for testing\"\n",
    "    res = tensor([val]).float()\n",
    "    res.grad = tensor([val/10 if grad is None else grad]).float()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def tst_params():\n",
    "    r = L.range(4)\n",
    "    return r.map(tst_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JitOptimizer -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def _update(\n",
    "    state:dict,\n",
    "    new=None # New values to update `state` dict\n",
    "):\n",
    "    if isinstance(new, dict): state.update(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class JitOptimizer(Optimizer):\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        if closure is not None: raise NotImplementedError(\"fastai optimizers currently do not support closure\")\n",
    "        for pg, hyper in zip(self.param_lists, self.hypers):\n",
    "            for p in pg:\n",
    "                if hasattr(p, 'grad') and p.grad is not None:\n",
    "                    _update(self.state[p], self.cbs[0](p, p.grad, **{**self.state[p], **hyper}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@torch.jit.script\n",
    "def sgd_jit_step(p:Tensor, grad:Tensor, lr:float, wd:float, mom:float, decouple_wd:bool, grad_avg:Optional[Tensor]=None, \n",
    "                 do_wd:bool=True, dampening:bool=False, force_train:Optional[bool]=None):\n",
    "    dp = p\n",
    "    if do_wd and wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            dp = dp.mul(1 - lr*wd)\n",
    "        else:\n",
    "            # l2_reg\n",
    "            grad = grad.add(dp, alpha=wd)\n",
    "\n",
    "    if mom != 0:\n",
    "        if grad_avg is None: \n",
    "            grad_avg = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "\n",
    "        # average_grad\n",
    "        damp = 1-mom if dampening else 1.\n",
    "        grad_avg = grad_avg.mul(mom)\n",
    "        grad_avg = grad_avg.add(grad, alpha=damp)\n",
    "\n",
    "        # momentum_step\n",
    "        dp = dp.add(grad_avg, alpha=-lr)\n",
    "        p.set_(dp)\n",
    "        return {'grad_avg': grad_avg}\n",
    "    else:\n",
    "        # sgd_step\n",
    "        dp = dp.add(grad, alpha=-lr)\n",
    "        p.set_(dp)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def SGD(params, lr, mom=0., wd=0., decouple_wd=True, jit=False):\n",
    "    \"A `Optimizer` for SGD with `lr` and `mom` and `params`\"\n",
    "    if jit:\n",
    "        cb = partial(sgd_jit_step, decouple_wd=decouple_wd)\n",
    "        return JitOptimizer(params, cb, lr=lr, mom=mom, wd=wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        if mom != 0: cbs.append(average_grad)\n",
    "        cbs.append(sgd_step if mom==0 else momentum_step)\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "\n",
    "# Test Vanilla SGD\n",
    "params_org = tst_params()\n",
    "opt_org = SGD(params_org, lr=0.1, jit=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = tst_params()\n",
    "opt_jit = SGD(params_jit, lr=0.1, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "test_close([p.item() for p in params_org], [i*0.99 for i in range(4)])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_jit])\n",
    "\n",
    "opt_org.step()\n",
    "opt_jit.step()\n",
    "test_close([p.item() for p in params_org], [i*0.98 for i in range(4)])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_jit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@torch.jit.script\n",
    "def rmsprop_jit_step(p:Tensor, grad:Tensor, lr:float, wd:float, mom:float, sqr_mom:float, eps:float, decouple_wd:bool, \n",
    "                     grad_avg:Optional[Tensor]=None, sqr_avg:Optional[Tensor]=None, do_wd:bool=True, force_train:Optional[bool]=None):\n",
    "    dp = p\n",
    "    if do_wd and wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            dp = dp.mul(1 - lr*wd)\n",
    "        else:\n",
    "            # l2_reg\n",
    "            grad = grad.add(dp, alpha=wd)\n",
    "\n",
    "    if sqr_avg is None: \n",
    "        sqr_avg = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "\n",
    "    if mom != 0:\n",
    "        if grad_avg is None: \n",
    "            grad_avg = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "\n",
    "        # average_grad, dampening=False\n",
    "        grad_avg = torch.mul(grad_avg, mom)\n",
    "        grad_avg = torch.add(grad_avg, grad)\n",
    "\n",
    "        # average_sqr_grad\n",
    "        sqr_avg = torch.mul(sqr_avg, sqr_mom)\n",
    "        sqr_avg = torch.addcmul(sqr_avg, grad, grad, value=1-sqr_mom)\n",
    "\n",
    "        # rms_prop_step\n",
    "        denom = torch.sqrt(sqr_avg)\n",
    "        denom = torch.add(denom, eps)\n",
    "        dp = torch.addcdiv(dp, grad_avg, denom, value=-lr)\n",
    "        p.set_(dp)\n",
    "        return {'grad_avg': grad_avg, 'sqr_avg': sqr_avg}\n",
    "    else:\n",
    "        # average_sqr_grad\n",
    "        sqr_avg = torch.mul(sqr_avg, sqr_mom)\n",
    "        sqr_avg = torch.addcmul(sqr_avg, grad, grad, value=1-sqr_mom)\n",
    "        \n",
    "        # rms_prop_step\n",
    "        denom = torch.sqrt(sqr_avg)\n",
    "        denom = torch.add(denom, eps)\n",
    "        dp = dp.addcdiv(grad, denom, value=-lr)\n",
    "        p.set_(dp)\n",
    "        return {'sqr_avg': sqr_avg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def RMSProp(params, lr, sqr_mom=0.99, mom=0., eps=1e-8, wd=0., decouple_wd=True, jit=False):\n",
    "    \"A `Optimizer` for RMSProp with `lr`, `sqr_mom`, `mom` and `params`\"\n",
    "    if jit:\n",
    "        cb = partial(rmsprop_jit_step, decouple_wd=decouple_wd)\n",
    "        return JitOptimizer(params, cb, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        cbs += ([average_sqr_grad] if mom==0. else [average_grad, average_sqr_grad])\n",
    "        cbs.append(rms_prop_step)\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, wd=wd, eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#Without momentum\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = RMSProp(params_org, lr=0.1, jit=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = RMSProp(params_jit, lr=0.1, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "test_close(params_org[0], tensor([0.,1.,2.]))\n",
    "test_close(params_org[0], params_jit[0])\n",
    "\n",
    "opt_org.step()\n",
    "opt_jit.step()\n",
    "step = - 0.1 * 0.1 / (math.sqrt((0.01*0.99+0.01) * 0.1**2) + 1e-8)\n",
    "test_close(params_org[0], tensor([step, 1+step, 2+step]))\n",
    "test_close(params_org[0], params_jit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#With momentum\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = RMSProp(params_org, lr=0.1, mom=0.9, jit=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = RMSProp(params_jit, lr=0.1, mom=0.9, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "test_close(params_org[0], tensor([0.,1.,2.]))\n",
    "test_close(params_org[0], params_jit[0])\n",
    "\n",
    "opt_org.step()\n",
    "opt_jit.step()\n",
    "step = - 0.1 * (0.1 + 0.9*0.1) / (math.sqrt((0.01*0.99+0.01) * 0.1**2) + 1e-8)\n",
    "test_close(params_org[0], tensor([step, 1+step, 2+step]))\n",
    "test_close(params_org[0], params_jit[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def debias(beta:float, step:int):\n",
    "    \"Simple debias calculation\"\n",
    "    return 1-beta**step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@torch.jit.script\n",
    "def adam_jit_step(p:Tensor, grad:Tensor, lr:float, wd:float, mom:float, sqr_mom:float, eps:float, \n",
    "                  decouple_wd:bool, grad_avg:Optional[Tensor]=None, sqr_avg:Optional[Tensor]=None, \n",
    "                  do_wd:bool=True, step:int=0, force_train:Optional[bool]=None):\n",
    "    dp = p\n",
    "    step += 1\n",
    "    if do_wd and wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            dp = dp.mul(1 - lr*wd)\n",
    "        else:\n",
    "            # l2_reg\n",
    "            grad = grad.add(dp, alpha=wd)\n",
    "\n",
    "    if grad_avg is None: \n",
    "        grad_avg = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "    if sqr_avg is None: \n",
    "        sqr_avg  = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "\n",
    "    # average_grad, dampening=True\n",
    "    grad_avg = torch.mul(grad_avg, mom)\n",
    "    grad_avg = torch.add(grad_avg, grad, alpha=1-mom)\n",
    "\n",
    "    # average_sqr_grad\n",
    "    sqr_avg = torch.mul(sqr_avg, sqr_mom)\n",
    "    sqr_avg = torch.addcmul(sqr_avg, grad, grad, value=1-sqr_mom)\n",
    "\n",
    "    # adam_step\n",
    "    debias1 = debias(mom, step)\n",
    "    debias2 = debias(sqr_mom, step)\n",
    "    dp = torch.addcdiv(dp, grad_avg, torch.sqrt(sqr_avg/debias2) + eps, value = -lr / debias1)\n",
    "    p.set_(dp)\n",
    "\n",
    "    return torch.jit.annotate(Dict[str, Union[Tensor, int]], {'grad_avg': grad_avg, 'sqr_avg': sqr_avg, 'step': step})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def Adam(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0.01, decouple_wd=True, jit=False):\n",
    "    \"A `Optimizer` for Adam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n",
    "    if jit:\n",
    "        cb = partial(adam_jit_step, decouple_wd=decouple_wd)\n",
    "        return JitOptimizer(params, cb, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, adam_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = Adam(params_org, lr=0.1, wd=0, jit=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = Adam(params_jit, lr=0.1, wd=0, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "step = -0.1 * 0.1 / (math.sqrt(0.1**2) + 1e-8)\n",
    "test_close(params_org[0], tensor([1+step, 2+step, 3+step]))\n",
    "test_close([p[0].item() for p in params_org], [p[0].item() for p in params_jit])\n",
    "\n",
    "opt_org.step()\n",
    "opt_jit.step()\n",
    "test_close(params_org[0], tensor([1+2*step, 2+2*step, 3+2*step]), eps=1e-3)\n",
    "test_close([p[0].item() for p in params_org], [p[0].item() for p in params_jit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAdam -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@torch.jit.script\n",
    "def radam_jit_step(p:Tensor, grad:Tensor, lr:float, wd:float, mom:float, sqr_mom:float, eps:float, beta:float,\n",
    "                  decouple_wd:bool, grad_avg:Optional[Tensor]=None, sqr_avg:Optional[Tensor]=None, \n",
    "                  do_wd:bool=True, step:int=0, force_train:Optional[bool]=None):\n",
    "    dp = p\n",
    "    step += 1\n",
    "    if do_wd and wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            dp = dp.mul(1 - lr*wd)\n",
    "        else:\n",
    "            # l2_reg\n",
    "            grad = grad.add(dp, alpha=wd)\n",
    "\n",
    "    if grad_avg is None: \n",
    "        grad_avg = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "    if sqr_avg is None: \n",
    "        sqr_avg  = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "\n",
    "    # average_grad, dampening=True\n",
    "    grad_avg = torch.mul(grad_avg, mom)\n",
    "    grad_avg = torch.add(grad_avg, grad, alpha=1-mom)\n",
    "\n",
    "    # average_sqr_grad\n",
    "    sqr_avg = torch.mul(sqr_avg, sqr_mom)\n",
    "    sqr_avg = torch.addcmul(sqr_avg, grad, grad, value=1-sqr_mom)\n",
    "\n",
    "    # radam_step\n",
    "    debias1 = debias(mom, step)\n",
    "    debias2 = debias(sqr_mom, step)\n",
    "    r_inf = 2/(1-sqr_mom) - 1\n",
    "    r = r_inf - 2*step*sqr_mom**step/(1-sqr_mom**step)\n",
    "    \n",
    "    if r > 5:\n",
    "        v = math.sqrt(((r-4) * (r-2) * r_inf)/((r_inf-4)*(r_inf-2)*r))\n",
    "        denom = torch.sqrt(sqr_avg/debias2)\n",
    "        if eps != 0: \n",
    "            denom = denom + eps\n",
    "        if beta != 0: \n",
    "            denom = F.softplus(denom, beta)\n",
    "        dp = torch.addcdiv(dp, grad_avg, denom, value = -lr*v / debias1)\n",
    "    else:\n",
    "        dp = torch.add(dp, grad_avg, alpha=-lr / debias1)\n",
    "    p.set_(dp)\n",
    "\n",
    "    return torch.jit.annotate(Dict[str, Union[Tensor, int]], {'grad_avg': grad_avg, 'sqr_avg': sqr_avg, 'step': step})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def RAdam(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0., beta=0., decouple_wd=True, jit=False):\n",
    "    \"A `Optimizer` for Adam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n",
    "    if jit:\n",
    "        cb = partial(radam_jit_step, decouple_wd=decouple_wd)\n",
    "        return JitOptimizer(params, cb, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd, beta=beta)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, radam_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd, beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = RAdam(params_org, lr=0.1, jit=False)\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = RAdam(params_jit, lr=0.1, jit=True)\n",
    "\n",
    "#The r factor is lower than 5 during the first 5 steps so updates use the average of gradients (all the same)\n",
    "r_inf = 2/(1-0.99) - 1\n",
    "for i in range(5): \n",
    "    r = r_inf - 2*(i+1)*0.99**(i+1)/(1-0.99**(i+1))\n",
    "    assert r <= 5\n",
    "    opt_org.step()\n",
    "    opt_jit.step()\n",
    "p = tensor([0.95, 1.9, 2.85])\n",
    "test_close(params_org[0], p)\n",
    "test_close(params_org[0], params_jit[0])\n",
    "\n",
    "#The r factor is greater than 5 for the sixth step so we update with RAdam\n",
    "r = r_inf - 2*6*0.99**6/(1-0.99**6)\n",
    "assert r > 5\n",
    "opt_org.step()\n",
    "opt_jit.step()\n",
    "v = math.sqrt(((r-4) * (r-2) * r_inf)/((r_inf-4)*(r_inf-2)*r))\n",
    "step = -0.1*0.1*v/(math.sqrt(0.1**2) + 1e-8)\n",
    "test_close(params_org[0], p+step)\n",
    "test_close(params_org[0], params_jit[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QHAdam -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@torch.jit.script\n",
    "def qhadam_jit_step(p:Tensor, grad:Tensor, lr:float, wd:float, mom:float, sqr_mom:float, eps:float,\n",
    "                    nu_1:float, nu_2:float, decouple_wd:bool, grad_avg:Optional[Tensor]=None, \n",
    "                    sqr_avg:Optional[Tensor]=None, do_wd:bool=True, step:int=0, force_train:Optional[bool]=None):\n",
    "    dp = p\n",
    "    step += 1\n",
    "    if do_wd and wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            dp = dp.mul(1 - lr*wd)\n",
    "        else:\n",
    "            # l2_reg\n",
    "            grad = grad.add(dp, alpha=wd)\n",
    "\n",
    "    if grad_avg is None: \n",
    "        grad_avg = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "    if sqr_avg is None: \n",
    "        sqr_avg  = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "\n",
    "    # average_grad, dampening=True\n",
    "    grad_avg = torch.mul(grad_avg, mom)\n",
    "    grad_avg = torch.add(grad_avg, grad, alpha=1-mom)\n",
    "\n",
    "    # average_sqr_grad\n",
    "    sqr_avg = torch.mul(sqr_avg, sqr_mom)\n",
    "    sqr_avg = torch.addcmul(sqr_avg, grad, grad, value=1-sqr_mom)\n",
    "\n",
    "    # qhadam_step\n",
    "    debias1 = debias(mom, step)\n",
    "    debias2 = debias(sqr_mom, step)\n",
    "    dp = torch.addcdiv(dp, ((1-nu_1) * grad) + (nu_1 * (grad_avg / debias1)),\n",
    "                       torch.sqrt(((1 - nu_2) * (grad)**2) + (nu_2 * (sqr_avg / debias2))) + eps,\n",
    "                       value = -lr)\n",
    "    p.set_(dp)\n",
    "\n",
    "    return torch.jit.annotate(Dict[str, Union[Tensor, int]], {'grad_avg': grad_avg, 'sqr_avg': sqr_avg, 'step': step})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def QHAdam(params, lr, mom=0.999, sqr_mom=0.999, nu_1=0.7, nu_2=1.0, eps=1e-8, wd=0., decouple_wd=True, jit=True):\n",
    "    \"An `Optimizer` for Adam with `lr`, `mom`, `sqr_mom`, `nus`, eps` and `params`\"\n",
    "    if jit:\n",
    "        cb = partial(qhadam_jit_step, decouple_wd=decouple_wd)\n",
    "        return JitOptimizer(params, cb, lr=lr, nu_1=nu_1, nu_2=nu_2, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, qhadam_step]\n",
    "        return Optimizer(params, cbs, lr=lr, nu_1=nu_1, nu_2=nu_2, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = QHAdam(params_org, lr=0.1, jit=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = QHAdam(params_jit, lr=0.1, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "step = -0.1 * (((1-0.7) * 0.1) + (0.7 * 0.1)) / (\n",
    "     math.sqrt(((1-1.0) * 0.1**2) + (1.0 * 0.1**2)) + 1e-8) \n",
    "\n",
    "test_close(params_org[0], tensor([1+step, 2+step, 3+step]))\n",
    "test_close(params_org[0], params_jit[0])\n",
    "\n",
    "opt_org.step()\n",
    "opt_jit.step()\n",
    "test_close(params_org[0], tensor([1+2*step, 2+2*step, 3+2*step]), eps=1e-3)\n",
    "test_close(params_org[0], params_jit[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LARS/LARC -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@torch.jit.script\n",
    "def larc_jit_step(p:Tensor, grad:Tensor, lr:float, wd:float, mom:float, eps:float, trust_coeff:float, decouple_wd:bool, \n",
    "                 clip:bool, grad_avg:Optional[Tensor]=None, do_wd:bool=True, dampening:bool=False, force_train:Optional[bool]=None):\n",
    "    dp = p\n",
    "    if do_wd and wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            dp = dp.mul(1 - lr*wd)\n",
    "        else:\n",
    "            # l2_reg\n",
    "            grad = grad.add(dp, alpha=wd)\n",
    "\n",
    "    # larc_layer_lr\n",
    "    p_norm = torch.norm(dp)\n",
    "    g_norm = torch.norm(grad)\n",
    "    local_lr = lr*trust_coeff * (p_norm) / (g_norm + p_norm * wd + eps)\n",
    "    if clip:\n",
    "        lr = min(local_lr, lr)\n",
    "    else:\n",
    "        lr = local_lr\n",
    "\n",
    "    if mom != 0:\n",
    "        if grad_avg is None: \n",
    "            grad_avg = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "\n",
    "        # average_grad, dampening=True\n",
    "        grad_avg = torch.mul(grad_avg, mom)\n",
    "        grad_avg = torch.add(grad_avg, grad)\n",
    "\n",
    "        # larc_step\n",
    "        dp = torch.add(dp, grad_avg, alpha=-lr)\n",
    "    else:\n",
    "        # larc_step\n",
    "        dp = torch.add(dp, grad, alpha=-lr)\n",
    "\n",
    "    p.set_(dp)\n",
    "    return {'grad_avg': grad_avg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def Larc(params, lr, mom=0.9, clip=True, trust_coeff=0.02, eps=1e-8, wd=0., decouple_wd=True, jit=False):\n",
    "    \"A `Optimizer` for Adam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n",
    "    if jit:\n",
    "        cb = partial(larc_jit_step, decouple_wd=decouple_wd, clip=clip)\n",
    "        return JitOptimizer(params, cb, lr=lr, mom=mom, trust_coeff=trust_coeff, eps=eps, wd=wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        if mom!=0.: cbs.append(average_grad)\n",
    "        cbs += [partial(larc_layer_lr, clip=clip), larc_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, trust_coeff=trust_coeff, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "params_org = [tst_param([1,2,3], [0.1,0.2,0.3]), tst_param([1,2,3], [0.01,0.02,0.03])]\n",
    "opt_org = Larc(params_org, lr=0.1, jit=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = [tst_param([1,2,3], [0.1,0.2,0.3]), tst_param([1,2,3], [0.01,0.02,0.03])]\n",
    "opt_jit = Larc(params_jit, lr=0.1, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "#First param local lr is 0.02 < lr so it's not clipped\n",
    "test_close(opt_org.state[params_org[0]]['local_lr'], 0.02)\n",
    "#Second param local lr is 0.2 > lr so it's clipped\n",
    "test_eq(opt_org.state[params_org[1]]['local_lr'], 0.1)\n",
    "\n",
    "test_close(params_org[0], tensor([0.998,1.996,2.994]))\n",
    "test_close(params_org[0], params_jit[0])\n",
    "test_close(params_org[1], tensor([0.999,1.998,2.997]))\n",
    "test_close(params_org[1], params_jit[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "params_org = [tst_param([1,2,3], [0.1,0.2,0.3]), tst_param([1,2,3], [0.01,0.02,0.03])]\n",
    "opt_org = Larc(params_org, lr=0.1, clip=False, jit=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = [tst_param([1,2,3], [0.1,0.2,0.3]), tst_param([1,2,3], [0.01,0.02,0.03])]\n",
    "opt_jit = Larc(params_jit, lr=0.1, clip=False, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "#No clipping\n",
    "test_close(opt_org.state[params_org[0]]['local_lr'], 0.02)\n",
    "test_close(opt_org.state[params_org[1]]['local_lr'], 0.2)\n",
    "test_close(params_org[0], tensor([0.998,1.996,2.994]))\n",
    "test_close(params_org[0], params_jit[0])\n",
    "test_close(params_org[1], tensor([0.998,1.996,2.994]))\n",
    "test_close(params_org[1], params_jit[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAMB -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@torch.jit.script\n",
    "def lamb_jit_step(p:Tensor, grad:Tensor, lr:float, wd:float, mom:float, sqr_mom:float, eps:float, \n",
    "                  decouple_wd:bool, grad_avg:Optional[Tensor]=None, sqr_avg:Optional[Tensor]=None, \n",
    "                  do_wd:bool=True, step:int=0, force_train:Optional[bool]=None):\n",
    "    dp = p\n",
    "    step += 1\n",
    "    if do_wd and wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            dp = dp.mul(1 - lr*wd)\n",
    "        else:\n",
    "            # l2_reg\n",
    "            grad = grad.add(dp, alpha=wd)\n",
    "\n",
    "    if grad_avg is None: \n",
    "        grad_avg = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "    if sqr_avg is None: \n",
    "        sqr_avg  = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "\n",
    "    # average_grad, dampening=True\n",
    "    grad_avg = torch.mul(grad_avg, mom)\n",
    "    grad_avg = torch.add(grad_avg, grad, alpha=1-mom)\n",
    "\n",
    "    # average_sqr_grad\n",
    "    sqr_avg = torch.mul(sqr_avg, sqr_mom)\n",
    "    sqr_avg = torch.addcmul(sqr_avg, grad, grad, value=1-sqr_mom)\n",
    "\n",
    "    # lamb_step\n",
    "    debias1 = debias(mom, step)\n",
    "    debias2 = debias(sqr_mom, step)\n",
    "    r1 = dp.pow(2).mean().sqrt()\n",
    "    lstep = (grad_avg/debias1) / ((sqr_avg/debias2).sqrt()+eps)\n",
    "    r2 = lstep.pow(2).mean().sqrt()\n",
    "    if r1 == 0 or r2 == 0:\n",
    "        q = 1.\n",
    "    else:\n",
    "        q = min(r1/r2, 10.)\n",
    "    dp = torch.add(dp,lstep, alpha = -lr * q)\n",
    "\n",
    "    p.set_(dp)\n",
    "\n",
    "    return torch.jit.annotate(Dict[str, Union[Tensor, int]], {'grad_avg': grad_avg, 'sqr_avg': sqr_avg, 'step': step})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def Lamb(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0., decouple_wd=True, jit=False):\n",
    "    \"A `Optimizer` for Adam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n",
    "    if jit:\n",
    "        cb = partial(lamb_jit_step, decouple_wd=decouple_wd)\n",
    "        return JitOptimizer(params, cb, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, lamb_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = Lamb(params_org, lr=0.1, jit=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = Lamb(params_jit, lr=0.1, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "test_close(params_org[0], tensor([0.7840,1.7840,2.7840]), eps=1e-3)\n",
    "test_close(params_org[0], params_jit[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ranger -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@delegates(RAdam)\n",
    "def ranger(p, lr, mom=0.95, wd=0.01, eps=1e-6, jit=False, **kwargs):\n",
    "    \"Convenience method for `Lookahead` with `RAdam`\"\n",
    "    return Lookahead(RAdam(p, lr=lr, mom=mom, wd=wd, eps=eps, jit=jit, **kwargs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('fastai')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
