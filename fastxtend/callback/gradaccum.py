# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/callback.gradaccum.ipynb.

# %% ../../nbs/callback.gradaccum.ipynb 1
# Contains code from:
# fastai - Apache License 2.0 - Copyright (c) 2023 fast.ai

# %% ../../nbs/callback.gradaccum.ipynb 4
from __future__ import annotations

import math

from fastai.callback.core import Callback, CancelBatchException
from fastai.callback.schedule import SchedCos, _Annealer

from .amp import MixedPrecision
from .utils import *
from ..imports import *

# %% auto 0
__all__ = ['GradientAccumulation', 'GradientAccumulationSchedule']

# %% ../../nbs/callback.gradaccum.ipynb 6
_grad_loggers = available_loggers.copy()
_grad_loggers['recorder'] = True

# %% ../../nbs/callback.gradaccum.ipynb 7
class GradientAccumulation(Callback):
    "Accumulate gradients before updating weights"
    order,run_valid = MixedPrecision.order-4,False
    def __init__(self,
        accum_bs:int|None, # Accumulation batch size. Defaults to `n_acc` if not set
        n_acc:int=32, # Default `accum_bs` value. Used for compatability with fastai
        log_accum_batch:bool=True, # Log each accumulated batch (True) or micro batch (False). False is default fastai behavior
    ):
        accum_bs = n_acc if accum_bs is None else accum_bs
        store_attr(but='n_acc')

    def before_fit(self):
        self.count = 0
        self._loggers = []
        self.accum_loss = None
        self.wandb_epoch = False
        if self.log_accum_batch:
            for logger, available in _grad_loggers.items():
                if available and hasattr(self.learn, logger):
                    self._loggers.append(getattr(self.learn, logger))
                    if logger=='wandb':
                        self.wandb_epoch = True

    def after_loss(self):
        bs = find_bs(self.learn.yb)
        self.count += bs
        self.accum_steps = self.accum_bs // bs
        self.learn.loss_grad /= self.accum_steps
        if self.accum_loss is None:
            self.accum_loss = torch.zeros_like(self.learn.loss)

    def before_step(self):
        "Skip weight update if we have not seen enough items"
        if self.count < self.accum_bs:
            if self.log_accum_batch:
                self.accum_loss += self.learn.loss.detach() / self.accum_steps
                self._run_loggers(False)
            # skip step/zero_grad
            raise CancelBatchException()
        else:
            self.count = 0
            if self.log_accum_batch:
                self.learn.loss = self.accum_loss + self.learn.loss.detach() / self.accum_steps
                self.accum_loss = torch.zeros_like(self.learn.loss)
                self._run_loggers(True)

    def after_epoch(self):
        "Turn on Recorder & Loggers to log final epoch metrics"
        if self.log_accum_batch:
            self._run_loggers(True)

    def _run_loggers(self, run_train):
        for l in self._loggers:
            l.run_train = run_train
        if self.wandb_epoch and not run_train:
            self.learn.wandb._wandb_epoch += 1/self.n_iter

# %% ../../nbs/callback.gradaccum.ipynb 8
class GradientAccumulationSchedule(GradientAccumulation, CallbackScheduler):
    "Gradient accumulation with a schedulable batch size"
    def __init__(self,
        start_accum_bs:int, # Initial gradient accumulation batch size
        final_accum_bs:int, # Final gradient accumulation batch size
        start:Numeric=0, # Start batch size schedule in percent of training steps (float) or epochs (int, index 0)
        finish:Numeric=0.3, # Finish batch size schedule in percent of training steps (float) or epochs (int, index 0)
        schedule:Callable[..., _Annealer]=SchedCos, # Batch size schedule type
        log_accum_batch:bool=True, # Log each accumulated batch (True) or micro batch (False). False is default fastai behavior
        micro_batch_size:int|None=None # Manually set micro-batch size if using non-fastai or non-fastxtend dataloader
    ):
        GradientAccumulation.__init__(self, accum_bs=start_accum_bs, log_accum_batch=log_accum_batch)
        CallbackScheduler.__init__(self)
        store_attr(names='start_accum_bs,final_accum_bs,start,finish,schedule')
        self.micro_batch_size=micro_batch_size

    def before_fit(self):
        self._sched_accum_bs = self.accum_bs
        if self.micro_batch_size is None:
            self.micro_batch_size=self.dls.bs

        if self.micro_batch_size > self.accum_bs:
            raise ValueError(f"{self.accum_bs=} cannot be smaller then the dataloader batch size {self.micro_batch_size=}")

        super().setup_schedule(
            n_epoch=self.n_epoch,
            dls_len=len(self.dls.train),
            start_value=self.start_accum_bs,
            final_value=self.final_accum_bs,
            start=self.start,
            finish=self.finish,
            schedule=self.schedule,
            callback_name='Gradient Accumulation Schedule')
        super().before_fit()

    def before_batch(self):
        "Update gradient accumulation batch size per schedule"
        self._sched_accum_bs = super().schedule_step(self._sched_accum_bs, self.pct_train)
        # Only update accum_bs if we are at the beginning of a new macro-batch
        if self.count == 0:
            self.accum_bs = int(math.floor(self._sched_accum_bs / self.micro_batch_size) * self.micro_batch_size)

    def _run_loggers(self, run_train):
        super()._run_loggers(run_train)
        if run_train:
            self.learn._log_values(accumulation_batch_size=self.accum_bs)
