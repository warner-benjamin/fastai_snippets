{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp optimizer.stableadam\n",
    "#|default_cls_lvl 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StableAdamW Optimizer\n",
    "> With fastai native and fused ForEach implementations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StableAdamW was introduced by Wortsman et al in *[Stable and low-precision training for large-scale vision-language models](https://arxiv.org/abs/2304.13013)*. StableAdamW is a [AdamW](https://openreview.net/forum?id=Bkg6RiCqY7)-[Adafactor](https://proceedings.mlr.press/v80/shazeer18a.html) hybrid, porting Adafactor's update clipping into [`AdamW`](optimizer.fused.html#adam-optimizer) as a per parameter learning rate modification. StableAdamW's update clipping outperforms gradient clipping on downstream tasks while avoiding model training instability.\n",
    "\n",
    ":::{.callout-note}\n",
    "This implementation of `StableAdam` also includes L2 weight decay from Adam, which is not included in the paper.\n",
    ":::\n",
    "\n",
    "In addition to a fastai native implementation, `StableAdam` has a fused ForEach implementation. See the [Fused Optimizer](optimizer.fused.html) documentation for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "from typing import Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from fastai.optimizer import Optimizer\n",
    "\n",
    "from fastxtend.optimizer.foreach import ForEachOptimizer\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastxtend.test_utils import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StableAdam Fastai Callbacks -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def debias(beta:float, step:int):\n",
    "    \"Stable Adam debias calculation\"\n",
    "    return beta*(1-beta**(step-1))/(1-beta**step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def stable_adam_step(p:Tensor, lr:float, eps:float, wd:float, mom:float, sqr_mom:float,\n",
    "                     step:int=0, grad_avg:Tensor|None=None, sqr_avg:Tensor|None=None,\n",
    "                     decouple_wd:bool=True, do_wd:bool=True, eps_t=None, **kwargs):\n",
    "    \"Updates Stable Adam moving averages and performs the Stable Adam step with `lr` on `p`\"\n",
    "    if step == 0:\n",
    "        grad_avg = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "        sqr_avg  = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "        eps_t = tensor(eps, device=p.device, dtype=p.dtype)\n",
    "\n",
    "    if wd!=0 and do_wd:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            p.data.mul_(1-lr*wd)\n",
    "        else:\n",
    "            # expiramental l2_reg. not in paper\n",
    "            p.grad.data.add_(p.data, alpha=wd)\n",
    "\n",
    "    # calculate debiased momentum (beta) terms\n",
    "    step += 1\n",
    "    db_mom = debias(mom, step)\n",
    "    db_sqr_mom = debias(sqr_mom, step)\n",
    "\n",
    "    # update moving averages (average_grad & average_sqr_grad)\n",
    "    grad_avg.mul_(db_mom).add_(p.grad.data, alpha=1-db_mom)\n",
    "    sqr_avg.mul_(db_sqr_mom).addcmul_(p.grad.data, p.grad.data, value=1-db_sqr_mom)\n",
    "\n",
    "    # compute per tensor RMS stabilization term\n",
    "    root_sqr_avg = sqr_avg.sqrt()\n",
    "    rms = torch.norm(p.grad.data.div(root_sqr_avg.maximum(eps_t)), 2)\n",
    "\n",
    "    # calculate RMS stabilized η_t\n",
    "    lr = lr / max(1, rms)\n",
    "\n",
    "    # stable adam step\n",
    "    p.data.addcdiv_(grad_avg, root_sqr_avg.add(eps), value=-lr)\n",
    "\n",
    "    return {'grad_avg':grad_avg, 'sqr_avg':sqr_avg, 'step':step, 'eps_t':eps_t}\n",
    "\n",
    "stable_adam_step.defaults = dict(mom=0.9, sqr_mom=0.99)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StableAdam ForEach -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@torch.jit.script\n",
    "def stable_adam_jit_substep(rms:Tensor, lr:float):\n",
    "    return -lr / max(1, rms.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def stable_adam_foreach_step(p:list[Tensor], g:list[Tensor], grad_avg:list[Tensor], sqr_avg:list[Tensor],\n",
    "                             ones:list[Tensor], steps:np.ndarray[Any, int], do_wd:np.ndarray[Any, bool],\n",
    "                             lr:float, wd:float, mom:float, sqr_mom:float, eps:float, decouple_wd:bool,\n",
    "                             **kwargs):\n",
    "    \"Updates Stable Adam moving averages and performs the Stable Adam step with `lr` on `p`\"\n",
    "    if wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            wd = np.where(do_wd, 1-lr*wd, 1.)\n",
    "            torch._foreach_mul_(p, scalars=wd.tolist())\n",
    "        else:\n",
    "            # expiramental l2_reg. not in paper\n",
    "            wd = np.where(do_wd, wd, 1.)\n",
    "            torch._foreach_addcdiv_(g, p, ones, scalars=wd.tolist())\n",
    "            # cannot use scalers with foreach_add & multiple tensors, so divide by one with foreach_addcdiv\n",
    "\n",
    "    # calculate debiased momentum (beta) terms\n",
    "    db_mom     = mom*(1-mom**(steps-1))/(1-mom**steps)\n",
    "    db_sqr_mom = sqr_mom*(1-sqr_mom**(steps-1))/(1-sqr_mom**steps)\n",
    "\n",
    "    # update moving average\n",
    "    torch._foreach_mul_(grad_avg, scalars=db_mom.tolist())\n",
    "    torch._foreach_addcdiv_(grad_avg, g, ones, scalars=(1-db_mom).tolist())\n",
    "\n",
    "    # update squared moving average\n",
    "    torch._foreach_mul_(sqr_avg, scalars=db_sqr_mom.tolist())\n",
    "    torch._foreach_addcmul_(sqr_avg, g, g, scalars=(1-db_sqr_mom).tolist())\n",
    "\n",
    "    # compute per tensor RMS stabilization term\n",
    "    root_sqr_avg = torch._foreach_sqrt(sqr_avg)\n",
    "    rms = torch._foreach_norm(torch._foreach_div(g, torch._foreach_maximum(root_sqr_avg, eps)), 2)\n",
    "\n",
    "    # calculate RMS stabilized η_t\n",
    "    lrs = [stable_adam_jit_substep(r, lr) for r in rms]\n",
    "\n",
    "    torch._foreach_add_(root_sqr_avg, eps)\n",
    "    torch._foreach_addcdiv_(p, grad_avg, root_sqr_avg, scalars=lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class StableAdamForEachOptimizer(ForEachOptimizer):\n",
    "    \"An `ForEachOptimizer` with a modified step for `stableadam_foreach_step`\"\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        if closure is not None:\n",
    "            raise NotImplementedError(\"fastai optimizers currently do not support closure\")\n",
    "        for pg, hyper in zip(self.param_lists, self.hypers):\n",
    "            pl, gl, grad_avg, sqr_avg, ones, steps, do_wd = [], [], [], [], [], [], []\n",
    "\n",
    "            for p in pg:\n",
    "                if p.grad is not None:\n",
    "                    state = self.state[p]\n",
    "\n",
    "                    if 'step' not in state:\n",
    "                        state['grad_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        state['sqr_avg']  = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        state['ones'] = torch.ones(1, dtype=p.dtype, device=p.device)\n",
    "                        state['step'] = 0\n",
    "\n",
    "                    state['step'] += 1\n",
    "                    pl.append(p)\n",
    "                    gl.append(p.grad)\n",
    "                    grad_avg.append(state['grad_avg'])\n",
    "                    sqr_avg.append(state['sqr_avg'])\n",
    "                    ones.append(state['ones'])\n",
    "                    steps.append(state['step'])\n",
    "                    do_wd.append(state.get('do_wd', True))\n",
    "\n",
    "            self.opt_step(p=pl, g=gl, grad_avg=grad_avg, sqr_avg=sqr_avg, ones=ones,\n",
    "                          steps=np.array(steps, dtype=np.int32), do_wd=np.array(do_wd, dtype=bool),\n",
    "                          decouple_wd=self.decouple_wd, **hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def StableAdam(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    mom:float=0.9, # Gradient moving average (β1) coefficient\n",
    "    sqr_mom:float=0.99, # Gradient squared moving average (β2) coefficient\n",
    "    eps:float=1e-5, # Added for numerical stability\n",
    "    wd:float=0.01, # Optional weight decay (true or L2)\n",
    "    decouple_wd:bool=True, # Apply true weight decay (StableAdamW) or L2 regularization (StableAdam)\n",
    "    foreach:bool=False, # Use fused ForEach implementation\n",
    ") -> Optimizer|StableAdamForEachOptimizer:\n",
    "    \"A fastai StableAdam/StableAdamW optimizer with a fused ForEach implementation\"\n",
    "    if foreach:\n",
    "        return StableAdamForEachOptimizer(params, stable_adam_foreach_step, lr=lr, mom=mom,\n",
    "                                          sqr_mom=sqr_mom, eps=eps, wd=wd, decouple_wd=decouple_wd)\n",
    "    else:\n",
    "        return Optimizer(params, [stable_adam_step], lr=lr, mom=mom,\n",
    "                         sqr_mom=sqr_mom, eps=eps, wd=wd, decouple_wd=decouple_wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def stableadam(\n",
    "    mom:float=0.9, # Gradient moving average (β1) coefficient\n",
    "    sqr_mom:float=0.99, # Gradient squared moving average (β2) coefficient\n",
    "    eps:float=1e-5, # Added for numerical stability\n",
    "    wd:float=0.01, # Optional weight decay (true or L2)\n",
    "    decouple_wd:bool=True, # Apply true weight decay (StableAdamW) or L2 regularization (StableAdam)\n",
    "    foreach:bool=False, # Use fused ForEach implementation\n",
    ") -> Optimizer|StableAdamForEachOptimizer:\n",
    "    \"Partial function for the StableAdam/StableAdamW optimizer with a fused ForEach implementation\"\n",
    "    return partialler(StableAdam, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd,\n",
    "                      decouple_wd=decouple_wd, foreach=foreach)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Hyperparameter notes from Wortsman et al:\n",
    "\n",
    "StableAdamW should be the same as AdamW, with β2, or `sqr_mom` for fastai optimizers, set to higher values such as 0.99 for best performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from functools import partial\n",
    "from fastai.optimizer import Optimizer, weight_decay, l2_reg, average_grad, average_sqr_grad, step_stat, adam_step\n",
    "from fastxtend.test_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def Adam(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0.01, decouple_wd=True):\n",
    "    cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "    cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, adam_step]\n",
    "    return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# Test contain code from:\n",
    "# fastai - Apache License 2.0 - Copyright (c) 2023 fast.ai\n",
    "\n",
    "test_steps = 25\n",
    "\n",
    "params_adam = tst_params()\n",
    "opt_adam = Adam(params_adam, lr=0.01)\n",
    "opt_adam.step()\n",
    "\n",
    "params_org = tst_params()\n",
    "opt_org = StableAdam(params_org, lr=0.01)\n",
    "opt_org.step()\n",
    "\n",
    "params_for = tst_params()\n",
    "opt_for = StableAdam(params_for, lr=0.01, foreach=True)\n",
    "opt_for.step()\n",
    "\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_adam])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_for])\n",
    "\n",
    "for i in range(test_steps):\n",
    "    if i == test_steps//2:\n",
    "        for p in params_org:\n",
    "            p.grad *= 10\n",
    "        for p in params_for:\n",
    "            p.grad *= 10\n",
    "    opt_org.step()\n",
    "    opt_for.step()\n",
    "\n",
    "# StableAdamW numerical values from StableAdamWUnfused-v1 https://gist.github.com/mitchellnw/d42e22a0b9ec02ceaf4f7b4457f51423\n",
    "test_close([p.item() for p in params_org], [0.0, 0.790968, 1.788363, 2.785763])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_for])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
