{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp callback.progresize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progressive Resizing\n",
    "> A callback to progressively resize images during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ProgressiveResize` is inspired by MosaicML's [Progressive Resizing algorithm for Composer](https://docs.mosaicml.com/en/stable/method_cards/progressive_resizing.html) which in turn was inspired by [fastai](https://github.com/fastai/fastbook/blob/780b76bef3127ce5b64f8230fce60e915a7e0735/07_sizing_and_tta.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "\n",
    "from fastcore.basics import detuplify\n",
    "from fastcore.transform import Pipeline, Transform\n",
    "\n",
    "from fastai.callback.core import Callback\n",
    "from fastai.vision.augment import AffineCoordTfm, RandomResizedCropGPU\n",
    "\n",
    "from fastxtend.callback.cutmixup import CutMixUpAugment\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "_resize_augs = (AffineCoordTfm, RandomResizedCropGPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def _to_size(t:Tensor):\n",
    "    \"Convert Tensor to size compatible values\"\n",
    "    if sum(t.shape)==2: return tuple(t.tolist())\n",
    "    else:               return tuple(t.item(),t.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def _num_steps(input_size, current_size, min_increase):\n",
    "    \"Convert Tensor to size compatible values\"\n",
    "    steps = (input_size - current_size) / min_increase\n",
    "    if sum(steps.shape)==2: \n",
    "        steps = steps[0].item()\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def _evenly_divisible(input_size, current_size, min_increase, steps):\n",
    "    min_increase = tensor(min_increase)\n",
    "    return (((input_size-current_size) % min_increase).sum() == 0) and (((input_size-current_size) - (min_increase*steps)).sum() == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ProgSizeMode(Enum):\n",
    "    \"Delete batch after resize to assist with PyTorch memory management\"\n",
    "    Auto = 'auto'\n",
    "    Strict = 'strict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ProgressiveResize(Callback):\n",
    "    order = CutMixUpAugment.order+1 # Needs to run after CutMixUpAugment\n",
    "    \"Progressively increase the size of input images during training. Final image size is the valid image size or `input_size`.\"\n",
    "    def __init__(self,\n",
    "        initial_size:float|tuple[int,int]=0.5, # Staring size to increase from. Image shape must be square\n",
    "        start:Number=0.5, # Earliest upsizing epoch in percent of training time or epoch (index 0)\n",
    "        finish:Number=0.75, # Last upsizing epoch in percent of training time or epoch (index 0)\n",
    "        min_increase:int=4, # Minimum increase per resising epoch\n",
    "        size_mode:ProgSizeMode=ProgSizeMode.Auto, # Automatically determine the resizing schedule\n",
    "        resize_mode:str='bilinear', # PyTorch interpolate mode string for progressive resizing\n",
    "        add_resize:bool=False, # Add a seperate resize step. Use if for non-fastai DataLoaders or DataLoaders without batch transforms\n",
    "        resize_valid:bool=True, # Apply progressive resizing to valid dataset\n",
    "        input_size:tuple[int,int]|None=None, # Final image size. Set if using a non-fastai DataLoaders.\n",
    "        logger_callback:str='wandb', # Log report and samples/second to `logger_callback` using `Callback.name` if avalible\n",
    "        empty_cache:bool=True, # Call `torch.cuda.empty_cache()` after each epoch to prevent memory allocation overflow\n",
    "        verbose:str=True, # Print a summary of the progressive resizing schedule\n",
    "    ):\n",
    "        store_attr()\n",
    "        self.run_valid = resize_valid\n",
    "\n",
    "    def before_fit(self):\n",
    "        if hasattr(self.learn, 'lr_finder') and not hasattr(self, \"gather_preds\"):\n",
    "            self.run = False\n",
    "            return\n",
    "\n",
    "        self.remove_resize, self.null_resize, self.remove_cutmix = True, True, False\n",
    "        self._log_after_resize = getattr(self, f'_{self.logger_callback}_log_after_resize', noop)\n",
    "        self.has_logger = hasattr(self.learn, self.logger_callback) and self._log_after_resize != noop\n",
    "        self.min_increase = tensor(self.min_increase)\n",
    "\n",
    "        # Try to automatically determine the input size\n",
    "        try:\n",
    "            n_inp = self.dls.train.n_inp\n",
    "            xb = self.dls.valid.one_batch()[:n_inp]\n",
    "            for n in range(n_inp):\n",
    "                x = detuplify(xb[n])\n",
    "                if isinstance(x, TensorImageBase):\n",
    "                    self.input_size = x.shape[-2:]\n",
    "        finally:\n",
    "            if self.input_size is None: \n",
    "                raise ValueError(f'Could not determine input size. Set `input_size`: {self.input_size}')\n",
    "            self.input_size = tensor(self.input_size)\n",
    "            if self.input_size[0] != self.input_size[1]:\n",
    "                raise ValueError(f'`ProgressiveResize` does not support non-square images: `input_size` = {self.input_size.tolist()}')\n",
    "            if self.input_size[0] % 2 != 0:\n",
    "                 raise ValueError(f\"Input shape must be even: {self.input_size}\")\n",
    "            assert self.min_increase.item() % 2 == 0, f\"Minimum increase must be even: {self.min_increase}\"\n",
    "\n",
    "        # Set the initial resize\n",
    "        if isinstance(self.initial_size, float): \n",
    "            self.current_size = (tensor(self.initial_size) * self.input_size).int()\n",
    "        elif isinstance(self.initial_size, tuple): \n",
    "            self.current_size = tensor(self.initial_size)\n",
    "\n",
    "        start_epoch  = int(self.n_epoch*self.start)  if self.start < 1  else self.start\n",
    "        finish_epoch = int(self.n_epoch*self.finish) if self.finish < 1 else self.finish\n",
    "        max_steps = finish_epoch - start_epoch \n",
    "\n",
    "        # Automatically determine the number of steps, increasing `min_increase` as needed\n",
    "        if self.size_mode == ProgSizeMode.Auto:\n",
    "            count = 10000 # prevent infinite loop\n",
    "            steps = _num_steps(self.input_size, self.current_size, self.min_increase)\n",
    "            while ((steps > max_steps) or not _evenly_divisible(self.input_size, self.current_size, self.min_increase, steps)) and count > 0:\n",
    "                self.min_increase += 2\n",
    "                steps = _num_steps(self.input_size, self.current_size, self.min_increase)\n",
    "                count -= 1\n",
    "        n_steps = _num_steps(self.input_size, self.current_size, self.min_increase)\n",
    "\n",
    "        # Double check that the number of resize steps works\n",
    "        if (n_steps > max_steps) or ((max_steps % n_steps != 0) and self.size_mode != ProgSizeMode.Auto):\n",
    "            raise ValueError(f'invalid number of steps {n_steps}')\n",
    "\n",
    "        # Double check that the step size works\n",
    "        if not _evenly_divisible(self.input_size, self.current_size, self.min_increase, n_steps):\n",
    "            raise ValueError(f'Resize amount {self.input_size-self.current_size} not evenly divisible by `min_increase` {self.min_increase}')\n",
    "\n",
    "        # Set when progressive resizing steps are applied\n",
    "        step_size = int(max_steps / n_steps)\n",
    "        start_epoch = finish_epoch - ((self.input_size-self.current_size) / self.min_increase)*step_size\n",
    "        if isinstance(start_epoch, torch.Tensor):\n",
    "            if sum(start_epoch.shape)==2: start_epoch = int(start_epoch[0].item())\n",
    "            else:                         start_epoch = int(start_epoch.item())\n",
    "        self.step_epochs = [i for i in range(start_epoch+step_size, finish_epoch+step_size, step_size)]\n",
    "\n",
    "        if self.verbose:\n",
    "            msg = f'Progressively increase the initial image size of {self.current_size.tolist()} by {self.min_increase} '\\\n",
    "                  f'pixels every {step_size} epoch{\"s\" if step_size > 1 else \"\"} for {len(self.step_epochs)} resizes.\\nStarting '\\\n",
    "                  f'at epoch {start_epoch+step_size} and finishing at epoch {finish_epoch} for a final training size of '\\\n",
    "                  f'{(self.current_size+(len(self.step_epochs))*self.min_increase).tolist()}.'\n",
    "            print(msg)\n",
    "\n",
    "        self._resize = []\n",
    "        # If `add_resize`, add a seperate resize\n",
    "        if self.add_resize:\n",
    "            self._resize_pipe = Pipeline(AffineCoordTfm(size=_to_size(self.current_size), mode=self.resize_mode))\n",
    "            self._resize.append(self._resize_pipe[0])\n",
    "            self.remove_resize = True\n",
    "        else:\n",
    "            if hasattr(self.learn, 'cutmixupaugment'):\n",
    "                # Modify the `CutMixUpAugment` augmentation pipeline \n",
    "                self._process_pipeline(self.learn.cutmixupaugment._orig_pipe, False)\n",
    "\n",
    "                # If `CutMixUpAugment` has an Affine Transform for Augmentations then use it\n",
    "                if len(self._resize) > 0:\n",
    "                    # Check for pre-mixup augment pipeline and modify it\n",
    "                    if self.learn.cutmixupaugment._docutmixaug:\n",
    "                        self._process_pipeline(self.learn.cutmixupaugment._cutmixaugs_pipe, False)\n",
    "                    else:\n",
    "                        # There isn't one, then add it a pre-mixup augment pipeline for resizing\n",
    "                        self.learn.cutmixupaugment._cutmixaugs_pipe = Pipeline(AffineCoordTfm(size=_to_size(self.current_size)))\n",
    "                        self.learn.cutmixupaugment._docutmixaug = True\n",
    "                        self._resize.append(self.learn.cutmixupaugment._cutmixaugs_pipe[0])\n",
    "                        self.remove_cutmix, self.remove_resize = True, True\n",
    "\n",
    "            else:\n",
    "                # If no `CutMixUpAugment` check the train dataloader pipeline for Affine Transforms\n",
    "                self._process_pipeline(self.dls.train.after_batch.fs, False)\n",
    "\n",
    "            # If `resize_valid` check the valid dataloader pipeline for Affine Transforms\n",
    "            if self.resize_valid:\n",
    "                self._process_pipeline(self.dls.valid.after_batch.fs, False)\n",
    "\n",
    "            # If no there are no detected resizes add a resize transform pipeline\n",
    "            if len(self._resize) == 0:\n",
    "                self.add_resize = True\n",
    "                self._resize_pipe = Pipeline(AffineCoordTfm(size=_to_size(self.current_size)))\n",
    "                self._resize.append(self._resize_pipe[0])\n",
    "                self.remove_resize = True\n",
    "\n",
    "        # Set created or detected resize to the first size and store original interpolation\n",
    "        self._orig_modes = []\n",
    "        for resize in self._resize:\n",
    "            resize.size = _to_size(self.current_size)\n",
    "            self._orig_modes.append(resize.mode)\n",
    "            resize.mode = self.resize_mode\n",
    "\n",
    "\n",
    "    def before_batch(self):\n",
    "        if self.add_resize:\n",
    "            self.learn.xb = self._resize_pipe(self.xb)\n",
    "            # self.learn.yb = self._resize_pipe(self.yb) TODO this wasn't working\n",
    "        \n",
    "    def before_train(self):\n",
    "        if len(self.step_epochs)> 0 and self.epoch >= self.step_epochs[0]:\n",
    "            _ = self.step_epochs.pop(0)\n",
    "            self.current_size += self.min_increase\n",
    "        \n",
    "            for i, resize in enumerate(self._resize):\n",
    "                if (self.current_size < self.input_size).all():\n",
    "                    resize.size = _to_size(self.current_size)\n",
    "                else:\n",
    "                    # Reset everything after progressive resizing is done\n",
    "                    if self.null_resize: \n",
    "                        resize.size = None\n",
    "                    elif self.remove_resize:\n",
    "                        if self.remove_cutmix:\n",
    "                            self.learn.cutmixupaugment._cutmixaugs_pipe = Pipeline([])\n",
    "                            self.learn.cutmixupaugment._docutmixaug = False\n",
    "                        else:\n",
    "                            self._resize_pipe = Pipeline([])\n",
    "                            self.add_resize = False\n",
    "                    else:\n",
    "                        resize.size = _to_size(self.current_size)\n",
    "                        resize.mode = self._orig_modes[i]\n",
    "        if self.has_logger: self._log_after_resize()\n",
    "\n",
    "    def after_epoch(self):\n",
    "        # This hacky fix prevents fastai/PyTorch from an exploding allocation of GPU RAM which can cause training to slowdown\n",
    "        if self.empty_cache and len(self.step_epochs) > 0 and self.epoch+1 >= self.step_epochs[0]:\n",
    "            del self.learn.xb\n",
    "            del self.learn.yb\n",
    "            del self.learn.pred\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    def _process_pipeline(self, pipe, remove_resize=False, null_resize=None):\n",
    "        for p in pipe:\n",
    "            if isinstance(p, _resize_augs):\n",
    "                self._resize.append(p)\n",
    "                if null_resize is None:\n",
    "                    self.null_resize = self.null_resize and p.size is None\n",
    "                else:\n",
    "                    self.null_resize = null_resize\n",
    "        self.remove_resize = remove_resize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exports\n",
    "try:\n",
    "    import wandb\n",
    "\n",
    "    @patch\n",
    "    def _wandb_log_after_resize(self:ProgressiveResize):\n",
    "        size = _to_size(self.current_size)\n",
    "        wandb.log({'progressive_resize_size': size[0]}, self.learn.wandb._wandb_step+1)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastxtend.test_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "class SyncthProgResizeTest(Callback):\n",
    "    order = ProgressiveResize.order+1\n",
    "    def __init__(self, input_size, start_size, increase, step_size, first_epoch, last_epoch, total_resizes):\n",
    "        store_attr()\n",
    "\n",
    "    def before_fit(self):\n",
    "        prog = self.learn.progressive_resize\n",
    "        if isinstance(self.start_size, tuple):\n",
    "            assert torch.equal(prog.current_size, tensor(self.start_size).int())\n",
    "        else:\n",
    "            assert torch.equal(prog.current_size, tensor([self.start_size,self.start_size]).int())\n",
    "        assert prog.min_increase==self.increase\n",
    "        assert prog.step_epochs[1]-prog.step_epochs[0]==self.step_size\n",
    "        assert prog.step_epochs[0]==self.first_epoch\n",
    "        assert prog.step_epochs[-1]==self.last_epoch\n",
    "        assert len(prog.step_epochs)==self.total_resizes\n",
    "        size = self.start_size\n",
    "        for i in prog.step_epochs:\n",
    "            size += self.increase\n",
    "        assert size==self.input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively increase the initial image size of [128, 128] by 32 pixels every 1 epoch for 4 resizes.\n",
      "Starting at epoch 12 and finishing at epoch 15 for a final training size of [256, 256].\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "test = SyncthProgResizeTest(input_size=256, start_size=128, increase=32, step_size=1, first_epoch=12, last_epoch=15, total_resizes=4)\n",
    "learn = synth_learner(cbs=[ProgressiveResize(input_size=[256,256]), test])\n",
    "learn('after_create')\n",
    "learn.n_epoch=20\n",
    "learn('before_fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively increase the initial image size of [128, 128] by 8 pixels every 1 epoch for 16 resizes.\n",
      "Starting at epoch 34 and finishing at epoch 49 for a final training size of [256, 256].\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "test = SyncthProgResizeTest(input_size=256, start_size=128, increase=8, step_size=1, first_epoch=34, last_epoch=49, total_resizes=16)\n",
    "learn = synth_learner(cbs=[ProgressiveResize(input_size=[256,256]), test])\n",
    "learn('after_create')\n",
    "learn.n_epoch=66\n",
    "learn('before_fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively increase the initial image size of [192, 192] by 6 pixels every 2 epochs for 32 resizes.\n",
      "Starting at epoch 163 and finishing at epoch 225 for a final training size of [384, 384].\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "test = SyncthProgResizeTest(input_size=384, start_size=192, increase=6, step_size=2, first_epoch=163, last_epoch=225, total_resizes=32)\n",
    "learn = synth_learner(cbs=[ProgressiveResize(input_size=[384,384], min_increase=6), test])\n",
    "learn('after_create')\n",
    "learn.n_epoch=300\n",
    "learn('before_fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively increase the initial image size of [192, 192] by 8 pixels every 3 epochs for 24 resizes.\n",
      "Starting at epoch 156 and finishing at epoch 225 for a final training size of [384, 384].\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "test = SyncthProgResizeTest(input_size=384, start_size=192, increase=8, step_size=3, first_epoch=156, last_epoch=225, total_resizes=24)\n",
    "learn = synth_learner(cbs=[ProgressiveResize(input_size=[384,384], min_increase=8), test])\n",
    "learn('after_create')\n",
    "learn.n_epoch=300\n",
    "learn('before_fit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|slow\n",
    "from fastcore.basics import num_cpus\n",
    "\n",
    "from fastai.data.external import URLs, untar_data\n",
    "from fastai.data.block import DataBlock, CategoryBlock\n",
    "from fastai.data.transforms import GrandparentSplitter, get_image_files, parent_label, Normalize\n",
    "from fastai.learner import Learner\n",
    "from fastai.vision.augment import Resize, aug_transforms\n",
    "from fastai.vision.core import imagenet_stats\n",
    "from fastai.vision.data import ImageBlock\n",
    "from fastai.vision.models import resnet50\n",
    "from fastxtend.callback.channelslast import *\n",
    "from fastxtend.metrics import *\n",
    "from fastxtend.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|slow\n",
    "class ProgressiveResizeTest(Callback):\n",
    "    run_valid, order = True, ProgressiveResize.order+1\n",
    "    \n",
    "    def before_fit(self):\n",
    "        self.progsize = self.learn.progressive_resize.current_size\n",
    "\n",
    "    def before_batch(self):\n",
    "        assert L(self.x.shape[-2:]) == L(self.progsize.tolist())\n",
    "            \n",
    "    def after_batch(self):\n",
    "        self.progsize = self.learn.progressive_resize.current_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|slow\n",
    "#|cuda\n",
    "imagenette = untar_data(URLs.IMAGENETTE_320)\n",
    "\n",
    "with less_random():\n",
    "    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "                        splitter=GrandparentSplitter(valid_name='val'),\n",
    "                        get_items=get_image_files, get_y=parent_label,\n",
    "                        item_tfms=Resize(256),\n",
    "                        batch_tfms=[*aug_transforms(), Normalize.from_stats(*imagenet_stats)])\n",
    "    dls =  dblock.dataloaders(imagenette, bs=128, num_workers=num_cpus(), pin_memory=True)\n",
    "\n",
    "    cbs = [ProgressiveResize, ProgressiveResizeTest]\n",
    "    learn = Learner(dls, resnet50(num_classes=dls.c), metrics=Accuracy(), cbs=cbs).to_channelslast()\n",
    "    learn.fit_one_cycle(20, 3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|slow\n",
    "#|cuda\n",
    "free_gpu_memory(learn, dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|slow\n",
    "#|cuda\n",
    "imagenette = untar_data(URLs.IMAGENETTE_320)\n",
    "with less_random():\n",
    "    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "                        splitter=GrandparentSplitter(valid_name='val'),\n",
    "                        get_items=get_image_files, get_y=parent_label,\n",
    "                        item_tfms=Resize(256),\n",
    "                        batch_tfms=[*aug_transforms(),Normalize.from_stats(*imagenet_stats)])\n",
    "    dls =  dblock.dataloaders(imagenette, bs=128, num_workers=num_cpus(), pin_memory=True)\n",
    "\n",
    "    learn = Learner(dls, resnet50(num_classes=dls.c), metrics=Accuracy()).to_channelslast()\n",
    "    learn.fit_one_cycle(20, 3e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
