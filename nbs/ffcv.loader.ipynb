{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp ffcv.loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "# Contains code from:\n",
    "# fastai - Apache License 2.0 - Copyright (c) 2023 fast.ai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFCV Loader\n",
    "> fastxtend's fastai+FFCV Integrated DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastxtend's `Loader` adds fastai features to [FFCV's Loader](https://docs.ffcv.io/making_dataloaders.html), including [<code>one_batch</code>](https://docs.fast.ai/data.core.html#dataloader.one_batch), [<code>show_batch</code>](https://docs.fast.ai/data.core.html#tfmddl.show_batch), [<code>show_results</code>](https://docs.fast.ai/data.core.html#tfmddl.show_results), and support for GPU batch transforms, to name a few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Mapping, Sequence\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from ffcv.fields.base import Field\n",
    "from ffcv.loader.loader import Loader as _Loader\n",
    "from ffcv.loader.loader import OrderOption, ORDER_TYPE, DEFAULT_OS_CACHE, ORDER_MAP\n",
    "from ffcv.pipeline.compiler import Compiler\n",
    "from ffcv.pipeline.operation import Operation\n",
    "from ffcv.transforms.ops import ToDevice as _ToDevice\n",
    "\n",
    "from fastcore.basics import GetAttr, detuplify, Inf\n",
    "from fastcore.dispatch import retain_types, explode_types\n",
    "from fastcore.meta import funcs_kwargs\n",
    "from fastcore.transform import Pipeline\n",
    "\n",
    "from fastai.data.core import show_batch, show_results\n",
    "\n",
    "from fastxtend.ffcv.epoch_iterator import EpochIterator, AsyncEpochIterator\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "_all_ = ['OrderOption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@funcs_kwargs\n",
    "class BaseDL(GetAttr):\n",
    "    \"Provides callbacks for DataLoaders which inherit from `BaseLoader`\"\n",
    "    _methods = 'before_iter after_batch after_iter'.split()\n",
    "    def __init__(self, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def before_iter(self, x=None, *args, **kwargs):\n",
    "        \"Called before `BaseLoader` starts to read/iterate over the dataset.\"\n",
    "        return x\n",
    "\n",
    "    def after_batch(self, x=None, *args, **kwargs):\n",
    "        \"After collating mini-batch of items, the mini-batch is passed through this function.\"\n",
    "        return x\n",
    "\n",
    "    def after_iter(self, x=None, *args, **kwargs):\n",
    "        \"Called after `BaseLoader` has fully read/iterated over the dataset.\"\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Loader(BaseDL, _Loader):\n",
    "    \"FFCV `Loader` with fastai Transformed DataLoader `TfmdDL` batch transforms\"\n",
    "    def __init__(self,\n",
    "        fname:str|Path, # Path to the location of the dataset (FFCV beton format)\n",
    "        batch_size:int, # Batch size\n",
    "        num_workers:int=-1, # Number of CPU cores to use in parallel (default: All available up to 16)\n",
    "        os_cache:bool=DEFAULT_OS_CACHE, # Leverage the OS for caching. Beneficial when there is enough memory to cache the dataset\n",
    "        order:ORDER_TYPE=OrderOption.SEQUENTIAL, # Dataset traversal order, one of: `SEQEUNTIAL`, `RANDOM`, `QUASI_RANDOM`\n",
    "        distributed:bool=False, # Emulates the behavior of PyTorch's DistributedSampler for distributed training\n",
    "        seed:int|None=None, # Random seed for batch ordering\n",
    "        indices:Sequence[int]|None=None, # Subset dataset by returning only these indices\n",
    "        pipelines:Mapping[str, Sequence[Operation|nn.Module]]={}, # Dictionary defining for each field the sequence of Decoders and transforms to apply\n",
    "        custom_fields:Mapping[str, Field]={}, # Dictonary informing `Loader` of the types associated to fields that are using a custom type\n",
    "        drop_last:bool|None=None, # Drop non-full batch in each epoch. Defaults to True if order is `SEQEUNTIAL`\n",
    "        batches_ahead:int=2, # Number of batches prepared in advance; balances latency and memory\n",
    "        recompile:bool=False, # Recompile at every epoch. Required if FFCV augmentations change during training\n",
    "        device:str|int|torch.device|None=None, # Device to place batch. Defaults to fastai's `default_device`\n",
    "        async_tfms:bool=False, # Asynchronously run `batch_tfms` before batch is drawn.\n",
    "        n_inp:int|None=None, # Number of inputs to the model. Defaults to pipelines length minus 1\n",
    "        split_idx:int|None=None, # Apply batch transform(s) to training (0) or validation (1) set. Defaults to valid if order is `SEQEUNTIAL`\n",
    "        do_setup:bool=True, # Run `setup()` for batch transform(s)\n",
    "        **kwargs\n",
    "    ):\n",
    "        if 'batch_tfms' in kwargs:\n",
    "            if 'after_batch' not in kwargs:\n",
    "                kwargs['after_batch'] = kwargs.pop('batch_tfms')\n",
    "            else:\n",
    "                raise ValueError('Cannot pass both `after_batch` and `batch_tfms` to `FFCVDataLoader`')\n",
    "\n",
    "        if split_idx is None:\n",
    "            self._split_idx = int(order==OrderOption.SEQUENTIAL)\n",
    "        else:\n",
    "            self._split_idx = split_idx\n",
    "\n",
    "        kwargs['after_batch'] = Pipeline(kwargs.get('after_batch', None), split_idx=self._split_idx)\n",
    "        if do_setup:\n",
    "            kwargs['after_batch'].setup(self)\n",
    "\n",
    "        self.async_tfms = async_tfms and len(kwargs['after_batch'].fs) > 0\n",
    "        self.cuda_streams = None\n",
    "\n",
    "        if drop_last is None:\n",
    "            drop_last != order==OrderOption.SEQUENTIAL\n",
    "\n",
    "        _Loader.__init__(self,\n",
    "            fname=str(Path(fname)),\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers,\n",
    "            os_cache=os_cache,\n",
    "            order=order,\n",
    "            distributed=distributed,\n",
    "            seed=seed,\n",
    "            indices=indices,\n",
    "            pipelines=pipelines,\n",
    "            custom_fields=custom_fields,\n",
    "            drop_last=drop_last,\n",
    "            batches_ahead=batches_ahead,\n",
    "            recompile=recompile\n",
    "        )\n",
    "        BaseDL.__init__(self, **kwargs)\n",
    "\n",
    "        if device is None:\n",
    "            self.device = default_device()\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        if n_inp is None:\n",
    "            self._n_inp = len(pipelines) - 1\n",
    "        else:\n",
    "            self._n_inp = n_inp\n",
    "\n",
    "        for name in ['item_tfms', 'after_item', 'before_batch']:\n",
    "            if name in kwargs:\n",
    "                if name != 'before_batch':\n",
    "                    msg = f\"fastxtend's `Loader` will not call any {name} methods. \" \\\n",
    "                          f\"{name} is for use with a fastai DataLoader.\\n\" \\\n",
    "                          f\"Instead of passing fastai Item Transforms to {name},\" \\\n",
    "                          f\"initialize the fastxtend `Loader` pipeline with FFCV transforms.\"\n",
    "                else:\n",
    "                    msg = f\"fastxtend's `Loader` will not call any {name} methods. \" \\\n",
    "                          f\"{name} are for use with a fastai DataLoader.\"\n",
    "                warn(msg)\n",
    "\n",
    "\n",
    "    def one_batch(self, batches_ahead:bool=False):\n",
    "        \"Return one processed batch of input(s) and target(s), optionally loading `batches_ahead`\"\n",
    "        for b in self._n_batches(self.batches_ahead + 2 if batches_ahead else 1):\n",
    "            # need to return the yield from _n_batches so `Loader` can reset to iterate the entire epoch\n",
    "            pass\n",
    "        return b\n",
    "\n",
    "    def show_batch(self,\n",
    "        b=None, # Batch to show\n",
    "        max_n:int=9, # Maximum number of items to show\n",
    "        ctxs=None, # List of `ctx` objects to show data. Could be matplotlib axis, DataFrame etc\n",
    "        show:bool=True, # Whether to display data\n",
    "        unique:bool=False, # Whether to show only one\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"Show `max_n` input(s) and target(s) from the batch.\"\n",
    "        if unique:\n",
    "            old_get_idxs = self.get_idxs\n",
    "            self.get_idxs = lambda: Inf.zeros\n",
    "        if b is None:\n",
    "            b = self.one_batch()\n",
    "        if not show:\n",
    "            return self._pre_show_batch(b, max_n=max_n)\n",
    "        # Uses Type Dispatch to call the correct `show_batch` for b\n",
    "        show_batch(*self._pre_show_batch(b, max_n=max_n), ctxs=ctxs, max_n=max_n, **kwargs)\n",
    "        if unique:\n",
    "            self.get_idxs = old_get_idxs\n",
    "\n",
    "    def show_results(self,\n",
    "        b, # Batch to show results for\n",
    "        out, # Predicted output from model for the batch\n",
    "        max_n:int=9, # Maximum number of items to show\n",
    "        ctxs=None, # List of `ctx` objects to show data. Could be matplotlib axis, DataFrame etc\n",
    "        show:bool=True, # Whether to display data\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"Show `max_n` results with input(s), target(s) and prediction(s).\"\n",
    "        x,y,its = self.show_batch(b, max_n=max_n, show=False)\n",
    "        b_out = type(b)(b[:self.n_inp] + (tuple(out) if is_listy(out) else (out,)))\n",
    "        x1,_,outs = self.show_batch(b_out, max_n=max_n, show=False)\n",
    "        if its is None:\n",
    "            res = (x, x1, None, None)\n",
    "        else:\n",
    "            res = (x, y, its, outs.itemgot(slice(self.n_inp,None)))\n",
    "        if not show:\n",
    "            return res\n",
    "        # Uses Type Dispatch to call the correct `show_results` for b\n",
    "        show_results(*res, ctxs=ctxs, max_n=max_n, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def n_inp(self) -> int:\n",
    "        \"Number of elements in a batch for model input\"\n",
    "        return self._n_inp\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self._device\n",
    "\n",
    "    @device.setter\n",
    "    def device(self, device:int|str|torch.device):\n",
    "        # parse device\n",
    "        device, *_ = torch._C._nn._parse_to(device=device)\n",
    "        self._device = device\n",
    "        # Device setter for FFCV Pipeline\n",
    "        for p in self.pipeline_specs.values():\n",
    "            for t in p.transforms:\n",
    "                if isinstance(t, _ToDevice):\n",
    "                    t.device = device\n",
    "        # Device setter for Loader.batch_tfms\n",
    "        if hasattr(self.after_batch, 'fs'):\n",
    "            self._pipeline_device(self.after_batch.fs)\n",
    "\n",
    "    def to(self, device:int|str|torch.device):\n",
    "        \"Sets `self.device=device`.\"\n",
    "        self.device = device\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def split_idx(self):\n",
    "        return self._split_idx\n",
    "\n",
    "    @split_idx.setter\n",
    "    def split_idx(self, split_idx:int):\n",
    "        \"Sets fastai batch transforms to train (split_idx=0) or valid (split_idx=1)\"\n",
    "        self._split_idx = split_idx\n",
    "        if isinstance(self.after_batch, Pipeline):\n",
    "            self.after_batch.split_idx = split_idx\n",
    "\n",
    "    def decode(self, b):\n",
    "        \"Decode batch `b`\"\n",
    "        return to_cpu(self.after_batch.decode(self._retain_dl(b)))\n",
    "\n",
    "    def decode_batch(self, b, max_n:int=9):\n",
    "        \"Decode up to `max_n` input(s) from batch `b`\"\n",
    "        return self._decode_batch(self.decode(b), max_n)\n",
    "\n",
    "    def _pipeline_device(self, pipe):\n",
    "        \"Device setter for fastai pipeline\"\n",
    "        for tfm in pipe:\n",
    "            if hasattr(tfm, 'to') and callable(tfm.to):\n",
    "                tfm.to(self.device, non_blocking=True)\n",
    "            else:\n",
    "                for a in L(getattr(tfm, 'parameters', None)):\n",
    "                    setattr(tfm, a, getattr(tfm, a).to(self.device, non_blocking=True))\n",
    "\n",
    "    def _iter(self):\n",
    "        Compiler.set_num_threads(self.num_workers)\n",
    "        order = self.next_traversal_order()\n",
    "        selected_order = order[:len(self) * self.batch_size]\n",
    "        self.next_epoch += 1\n",
    "\n",
    "        # Compile at the first epoch\n",
    "        if self.code is None or self.recompile:\n",
    "            self.generate_code()\n",
    "\n",
    "        # Asynchronous transforms require using the same Cuda streams for the entire run\n",
    "        if self.cuda_streams is None:\n",
    "            self.cuda_streams = [(torch.cuda.Stream() if torch.cuda.is_available() else None)\n",
    "                                  for _ in range(self.batches_ahead + 2)]\n",
    "        if self.async_tfms:\n",
    "            return AsyncEpochIterator(self, selected_order, self.after_batch)\n",
    "        else:\n",
    "            return EpochIterator(self, selected_order)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.before_iter()\n",
    "        if self.async_tfms:\n",
    "            yield from self._iter()\n",
    "        else:\n",
    "            for b in self._iter():\n",
    "                yield self.after_batch(b)\n",
    "        self.after_iter()\n",
    "        if hasattr(self, 'it'):\n",
    "            del(self.it)\n",
    "\n",
    "    def _one_pass(self, b=None):\n",
    "        if b is None:\n",
    "            b = self.one_batch()\n",
    "        self._types = explode_types(b)\n",
    "\n",
    "    def _retain_dl(self, b):\n",
    "        if not getattr(self, '_types', None):\n",
    "            self._one_pass(b)\n",
    "        return retain_types(b, typs=self._types)\n",
    "\n",
    "    def _decode_batch(self, b, max_n=9):\n",
    "        return L(batch_to_samples(b, max_n=max_n))\n",
    "\n",
    "    def _pre_show_batch(self, b, max_n=9):\n",
    "        \"Decode `b` to be ready for `show_batch`\"\n",
    "        b = self.decode(b)\n",
    "        if hasattr(b, 'show'):\n",
    "            return b,None,None\n",
    "        its = self._decode_batch(b, max_n)\n",
    "        if not is_listy(b):\n",
    "            b,its = [b],L((o,) for o in its)\n",
    "        return detuplify(b[:self.n_inp]),detuplify(b[self.n_inp:]),its\n",
    "\n",
    "    def _n_batches(self, num_batches:int=1):\n",
    "        orig_traversal_order = self.traversal_order\n",
    "        orig_indices = self.indices\n",
    "        orig_drop_last = self.drop_last\n",
    "\n",
    "        # Set Loader to only return one batch per epoch\n",
    "        if self._args['order'] == OrderOption.SEQUENTIAL:\n",
    "            self.indices = np.arange(0, self.batch_size*num_batches)\n",
    "        else:\n",
    "            self.indices = np.random.random_integers(0, self.reader.num_samples, self.batch_size*num_batches)\n",
    "        self.traversal_order = ORDER_MAP[OrderOption.SEQUENTIAL](self)\n",
    "        self.drop_last = False\n",
    "\n",
    "        # yield num_batches\n",
    "        yield from self.__iter__()\n",
    "\n",
    "        # Reset Loader state to its original status\n",
    "        self.next_epoch -= 1\n",
    "        self.indices = orig_indices\n",
    "        self.drop_last = orig_drop_last\n",
    "        self.traversal_order = orig_traversal_order"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important `Loader` arguments:\n",
    "\n",
    "- `order`: Controls how much memory is used for dataset caching and whether the dataset is randomly shuffled. Can be one of `RANDOM`, `QUASI_RANDOM`, or `SEQUENTIAL`. See the note below for more details. Defaults to `SEQUENTIAL`, which is unrandomized.\n",
    "\n",
    "- `os_cache`: By default, FFCV will attempt to cache the entire dataset into RAM using the operating system's caching. This can be changed by setting `os_cache=False` or setting the enviroment variable 'FFCV_DEFAULT_CACHE_PROCESS' to \"True\" or \"1\". If `os_cache=False` then `order` must be set to `QUASI_RANDOM` for the training `Loader`.\n",
    "\n",
    "- `num_workers`: If not set, will use all CPU cores up to 16 by default.\n",
    "\n",
    "- `batches_ahead`: Controls the number of batches ahead the `Loader` works. Increasing uses more RAM, both CPU and GPU. Defaults to 2.\n",
    "\n",
    "- `n_inp`: Controls which inputs to pass to the model. By default, set to number of pipelines minus 1.\n",
    "\n",
    "- `drop_last`: Whether to drop the last partial batch. By default, will set to True if `order` is `RANDOM` or `QUASI_RANDOM`, False if `SEQUENTIAL`.\n",
    "\n",
    "- `device`: The device to place the processed batches of data on. Defaults to `fastai.torch_core.default_device` if not set.\n",
    "\n",
    "- `async_tfms`: Asynchronously apply `batch_tfms` before the batch is drawn. Can accelerate training if GPU compute isn't fully saturated (95% or less) or if only using `IntToFloatTensor` and `Normalize`.\n",
    "\n",
    "- `split_idx`: This tells the fastai batch transforms what dataset they are operating on. By default will use 0 (train) if `order` is `RANDOM` or `QUASI_RANDOM`, 1 (valid) if `SEQUENTIAL`.\n",
    "\n",
    "- `distributed`: For distributed training on multiple GPUs. Emulates the behavior of PyTorch's [`DistributedSampler`](https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler). `QUASI_RANDOM` is unavailable with distributed training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note collapse=\"false\"}\n",
    "#### Note: Order Memory Usage\n",
    "Each `order` option requires differing amounts of system memory.\n",
    "\n",
    "- `RANDOM` caches the entire dataset in memory for fast random sampling. `RANDOM` uses the most memory.\n",
    "\n",
    "- `QUASI_RANDOM` caches a subset of the dataset at a time in memory and randomly samples from the subset. Use when the entire dataset cannot fit into memory.\n",
    "\n",
    "- `SEQUENTIAL` requires least memory. It loads a few samples ahead of time. As the name suggests, it is not random, and primarly is for validation.\n",
    ":::\n",
    "\n",
    "Asynchronous batch transforms can accelerate training by decreasing the draw time at the expense of slightly longer batch step. If the GPU isn't fully saturated, usually 95% or less compute use, this will be a net gain in training performance. `async_tfms=True` pairs well with `ProgressiveResize`, as the GPU is almost never saturated when training on smaller then full sized images. When near or fully saturated, asynchronous batch transforms usually result a wash in training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Loader.one_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Loader.show_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Loader.show_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Loader.to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Loader.n_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Loader.decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Loader.decode_batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
