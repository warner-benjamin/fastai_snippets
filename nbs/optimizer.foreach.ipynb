{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp optimizer.foreach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ForEach Optimizers\n",
    "> Fused fastai optimizers using PyTorch ForEach methods for improved performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastxtend ForEach optimizers are adapted from the PyTorch ForEach [`_multi_tensor`](https://github.com/pytorch/pytorch/tree/master/torch/optim) implementations. They are 21 to 293 percent faster relative to fastai native optimizers depending on the model. \n",
    "\n",
    "The primary difference between PyTorch's ForEach implementations and fastxtend is fastxtend's ForEach optimizers apply per-parameter weight decay in one optimizer step instead of requiring a seperate weight decay parameter group and a non-weight decay parameter group. This also allows seamless support for fastai's [discriminative learning rates](https://docs.fast.ai/callback.schedule.html#learner.fine_tune).\n",
    "\n",
    "Unlike fastai optimizers, which are made of [multiple stepper callbacks](https://docs.fast.ai/optimizer.html#basic-steppers) and share one `Optimizer`, ForEach optimizers require an optimizer specific `ForEachOptimizer` implementation. \n",
    "\n",
    "Currently `SGD`, `Adam`, `RAdam`, `Lamb`, and `Ranger` have ForEach implementations.\n",
    "\n",
    ":::{.callout-important}\n",
    "ForEach optimizers have only been tested on PyTorch 1.12 and are not guaranteed to work on older versions.\n",
    ":::\n",
    "\n",
    "ForEach optimizers are faster due to horizontal fusion across multiple parameters. Using `xresnet50` and the simplest form of `SGD` as an example, a ForEach optimizer would construct a list of all 167 `params` and their `grads` before performing one horizontally fused step.\n",
    "\n",
    "```python\n",
    "def simple_sgd_foreach(params:list[Tensor], grads:list[Tensor], lr:float)\n",
    "    torch._foreach_add_(params, grads, alpha=-lr)\n",
    "```\n",
    "\n",
    "In contrast, a standard PyTorch optimizer would call the simple `SGD` step 167 times:\n",
    "\n",
    "```python\n",
    "def simple_sgd_standard(param:Tensor, lr:float)\n",
    "    param.add_(param.grad, alpha=-lr)\n",
    "```\n",
    "\n",
    "ForEach optimizers are tested to be equal to fastai optimizers for 25 steps using [nbdev's GitHub CI](https://nbdev.fast.ai/tutorials/tutorial.html#check-out-your-workflows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from fastai.optimizer import Optimizer\n",
    "\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Utils -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "from fastai.optimizer import (weight_decay, l2_reg, average_grad, sgd_step, momentum_step, \n",
    "                              average_sqr_grad, rms_prop_step, step_stat, adam_step, radam_step, \n",
    "                              larc_layer_lr, larc_step, lamb_step, Lookahead)\n",
    "\n",
    "from fastxtend.test_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# tests are copied with light modifications from fastai\n",
    "test_steps = 25\n",
    "\n",
    "def tst_param(val, grad=None):\n",
    "    \"Create a tensor with `val` and a gradient of `grad` for testing\"\n",
    "    res = tensor([val]).float()\n",
    "    res.grad = tensor([val/10 if grad is None else grad]).float()\n",
    "    return res\n",
    "\n",
    "def tst_params():\n",
    "    r = L.range(4)\n",
    "    return r.map(tst_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ForEachOptimizer -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class ForEachOptimizer(Optimizer):\n",
    "    \"Base foreach optimizer class, updating `params` with `opt_step` instead of `Optimizer.cbs`\"\n",
    "    def __init__(self,\n",
    "        params:listified[Tensor], # Model parameters\n",
    "        opt_step:Callable, # `ForEachOptimizer` optimizer step\n",
    "        decouple_wd:bool=True, # Use true weight decay or L2 regularization, if applicable\n",
    "        **defaults # Optimizer specific hyper parameters\n",
    "    ):\n",
    "        if notmax_torch('1.12'):\n",
    "            warn(f'ForEach optimizers are untested on PyTorch {torch.__verson__}, recommended to use 1.12 or newer')\n",
    "        super().__init__(params, [None], True, **defaults)\n",
    "        self.opt_step = opt_step\n",
    "        self.decouple_wd = decouple_wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ForEachOptimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def sgd_foreach_step(p:list[Tensor], g:list[Tensor], grad_avg:list[Tensor|None], ones:list[Tensor|None], \n",
    "                     do_wd:np.ndarray[Any, bool], lr:float, wd:float, mom:float, decouple_wd:bool, **kwargs):\n",
    "\n",
    "    if wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            wd = np.where(do_wd, 1-lr*wd, 1.)\n",
    "            torch._foreach_mul_(p, scalars=wd.tolist())\n",
    "        else:\n",
    "            # l2_reg\n",
    "            wd = np.where(do_wd, wd, 1.)\n",
    "            torch._foreach_addcdiv_(g, p, ones, scalars=wd.tolist())\n",
    "            # cannot use scalers with foreach_add & multiple tensors, so divide by one with foreach_addcdiv\n",
    "\n",
    "    if mom != 0:\n",
    "        # average_grad\n",
    "        torch._foreach_mul_(grad_avg, mom)\n",
    "        torch._foreach_add_(grad_avg, g)\n",
    "\n",
    "        # momentum_step\n",
    "        torch._foreach_add_(p, grad_avg, alpha=-lr)\n",
    "    else:\n",
    "        # sgd_step\n",
    "        torch._foreach_add_(p, g, alpha=-lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class SGDForEachOptimizer(ForEachOptimizer):\n",
    "    \"A `ForEachOptimizer` with a modified step for `sgd_foreach_step`\"\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        if closure is not None: raise NotImplementedError(\"fastai optimizers currently do not support closure\")\n",
    "        for pg, hyper in zip(self.param_lists, self.hypers):\n",
    "            pl, gl, grad_avg, ones, do_wd = [], [], [], [], []\n",
    "\n",
    "            for p in pg:\n",
    "                if hasattr(p, 'grad') and p.grad is not None:\n",
    "                    state = self.state[p]\n",
    "\n",
    "                    if 'setup' not in state:\n",
    "                        if hyper['mom'] != 0:\n",
    "                            state['grad_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        if not self.decouple_wd:\n",
    "                            state['ones'] = torch.ones_like(p, memory_format=torch.preserve_format)\n",
    "                        state['setup'] = True\n",
    "                    \n",
    "                    pl.append(p)\n",
    "                    gl.append(p.grad)\n",
    "                    grad_avg.append(state.get('grad_avg', None))\n",
    "                    ones.append(state.get('ones', None))\n",
    "                    do_wd.append(state.get('do_wd', True))\n",
    "\n",
    "            self.opt_step(p=pl, g=gl, grad_avg=grad_avg, ones=ones, do_wd=np.array(do_wd, dtype=bool), \n",
    "                          decouple_wd=self.decouple_wd, **hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(SGDForEachOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def SGD(params, lr, mom=0., wd=0., decouple_wd=True, foreach=False):\n",
    "    if foreach:\n",
    "        return SGDForEachOptimizer(params, sgd_foreach_step, lr=lr, mom=mom, wd=wd, decouple_wd=decouple_wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        if mom != 0: cbs.append(average_grad)\n",
    "        cbs.append(sgd_step if mom==0 else momentum_step)\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# Vanilla SGD\n",
    "params_org = tst_params()\n",
    "opt_org = SGD(params_org, lr=0.1, foreach=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_for = tst_params()\n",
    "opt_for = SGD(params_for, lr=0.1, foreach=True)\n",
    "opt_for.step()\n",
    "\n",
    "test_close([p.item() for p in params_org], [i*0.99 for i in range(4)])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_for])\n",
    "\n",
    "opt_org.step()\n",
    "opt_for.step()\n",
    "test_close([p.item() for p in params_org], [i*0.98 for i in range(4)])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_for])\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_for.step()\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_for])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# SGD with momentum\n",
    "params_org = tst_params()\n",
    "opt_org = SGD(params_org, lr=0.1, mom=0.9, foreach=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_for = tst_params()\n",
    "opt_for = SGD(params_for, lr=0.1, mom=0.9, foreach=True)\n",
    "opt_for.step()\n",
    "test_close([p.item() for p in params_org], [i*0.99 for i in range(4)])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_for])\n",
    "\n",
    "opt_org.step()\n",
    "opt_for.step()\n",
    "test_close([p.item() for p in params_org], [i*(1 - 0.1 * (0.1 + 0.1*1.9)) for i in range(4)])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_for])\n",
    "for i,p in enumerate(params_org):\n",
    "    test_close(opt_org.state[p]['grad_avg'].item(), i*0.19)\n",
    "for i,p in enumerate(params_for):\n",
    "    test_close(opt_for.state[p]['grad_avg'].item(), i*0.19)\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_for.step()\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_for])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#Weight decay\n",
    "params_org = tst_params()\n",
    "opt_org = SGD(params_org, lr=0.1, mom=0.9, wd=0.1, foreach=False)\n",
    "opt_org.step()\n",
    "opt_org.step()\n",
    "\n",
    "params_for = tst_params()\n",
    "opt_for = SGD(params_for, lr=0.1, mom=0.9, wd=0.1, foreach=True)\n",
    "opt_for.step()\n",
    "opt_for.step()\n",
    "\n",
    "test_close([p.item() for p in params_org], [i*0.9512 for i in range(4)])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_for])\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_for.step()\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_for])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#L2 reg\n",
    "params_org = tst_params()\n",
    "opt_org = SGD(params_org, lr=0.1, mom=0.9, wd=0.1, decouple_wd=False, foreach=False)\n",
    "opt_org.step()\n",
    "opt_org.step()\n",
    "\n",
    "params_for = tst_params()\n",
    "opt_for = SGD(params_for, lr=0.1, mom=0.9, wd=0.1, decouple_wd=False, foreach=True)\n",
    "opt_for.step()\n",
    "opt_for.step()\n",
    "\n",
    "test_close([p.item() for p in params_org], [i*0.9322 for i in range(4)])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_for])\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_for.step()\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_for])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def adam_foreach_step(p:list[Tensor], g:list[Tensor], grad_avg:list[Tensor], sqr_avg:list[Tensor], ones:list[Tensor|None], \n",
    "                      steps:np.ndarray[Any, int], do_wd:np.ndarray[Any, bool], lr:float, wd:float, mom:float, sqr_mom:float, \n",
    "                      eps:float, decouple_wd:bool, **kwargs):\n",
    "\n",
    "    if wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            wd = np.where(do_wd, 1-lr*wd, 1.)\n",
    "            torch._foreach_mul_(p, scalars=wd.tolist())\n",
    "        else:\n",
    "            # l2_reg\n",
    "            wd = np.where(do_wd, wd, 1.)\n",
    "            torch._foreach_addcdiv_(g, p, ones, scalars=wd.tolist())\n",
    "            # cannot use scalers with foreach_add & multiple tensors, so divide by one with foreach_addcdiv\n",
    "\n",
    "    # average_grad, dampening=True\n",
    "    torch._foreach_mul_(grad_avg, mom)\n",
    "    torch._foreach_add_(grad_avg, g, alpha=1-mom)\n",
    "\n",
    "    # average_sqr_grad\n",
    "    torch._foreach_mul_(sqr_avg, sqr_mom)\n",
    "    torch._foreach_addcmul_(sqr_avg, g, g, value=1-sqr_mom)\n",
    "\n",
    "    # adam_step\n",
    "    debias1 = -lr / (1 - mom**steps)\n",
    "    debias2 = np.sqrt(1 - sqr_mom**steps)\n",
    "\n",
    "    sqr_avg_debias2 = torch._foreach_sqrt(sqr_avg)\n",
    "    torch._foreach_div_(sqr_avg_debias2, debias2.tolist())\n",
    "    torch._foreach_add_(sqr_avg_debias2, eps)\n",
    "\n",
    "    torch._foreach_addcdiv_(p, grad_avg, sqr_avg_debias2, debias1.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class AdamForEachOptimizer(ForEachOptimizer):\n",
    "    \"An `ForEachOptimizer` with a modified step for `adam_foreach_step`\"\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        if closure is not None: raise NotImplementedError(\"fastai optimizers currently do not support closure\")\n",
    "        for pg, hyper in zip(self.param_lists, self.hypers):\n",
    "            pl, gl, grad_avg, sqr_avg, ones, steps, do_wd = [], [], [], [], [], [], []\n",
    "\n",
    "            for p in pg:\n",
    "                if hasattr(p, 'grad') and p.grad is not None:\n",
    "                    state = self.state[p]\n",
    "\n",
    "                    if 'step' not in state:\n",
    "                        state['grad_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        state['sqr_avg']  = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        if not self.decouple_wd:\n",
    "                            state['ones'] = torch.ones_like(p, memory_format=torch.preserve_format)\n",
    "                        state['step'] = 0\n",
    "\n",
    "                    state['step'] += 1\n",
    "                    pl.append(p)\n",
    "                    gl.append(p.grad)\n",
    "                    grad_avg.append(state['grad_avg'])\n",
    "                    sqr_avg.append(state['sqr_avg'])\n",
    "                    ones.append(state.get('ones', None))\n",
    "                    steps.append(state['step'])\n",
    "                    do_wd.append(state.get('do_wd', True))\n",
    "\n",
    "            self.opt_step(p=pl, g=gl, grad_avg=grad_avg, sqr_avg=sqr_avg, ones=ones, \n",
    "                          steps=np.array(steps, dtype=np.int32), do_wd=np.array(do_wd, dtype=bool), \n",
    "                          decouple_wd=self.decouple_wd, **hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(AdamForEachOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def Adam(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0.01, decouple_wd=True, foreach=False):\n",
    "    if foreach:\n",
    "        return AdamForEachOptimizer(params, adam_foreach_step, lr=lr, mom=mom, \n",
    "                                    sqr_mom=sqr_mom, eps=eps, wd=wd, decouple_wd=decouple_wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, adam_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = Adam(params_org, lr=0.1, wd=0, foreach=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_for = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_for = Adam(params_for, lr=0.1, wd=0, foreach=True)\n",
    "opt_for.step()\n",
    "\n",
    "step = -0.1 * 0.1 / (math.sqrt(0.1**2) + 1e-8)\n",
    "test_close(params_org[0], tensor([1+step, 2+step, 3+step]))\n",
    "test_close([p[0].item() for p in params_org], [p[0].item() for p in params_for])\n",
    "\n",
    "opt_org.step()\n",
    "opt_for.step()\n",
    "test_close(params_org[0], tensor([1+2*step, 2+2*step, 3+2*step]), eps=1e-3)\n",
    "test_close([p[0].item() for p in params_org], [p[0].item() for p in params_for])\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_for.step()\n",
    "test_close([p[0].item() for p in params_org], [p[0].item() for p in params_for])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# test with weight decay\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = Adam(params_org, lr=0.1, wd=0.1, foreach=False)\n",
    "\n",
    "params_for = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_for = Adam(params_for, lr=0.1, wd=0.1, foreach=True)\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_for.step()\n",
    "\n",
    "test_close([p[0].item() for p in params_org], [p[0].item() for p in params_for])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# test with l2 reg\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = Adam(params_org, lr=0.1, wd=0.1, decouple_wd=False, foreach=False)\n",
    "\n",
    "params_for = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_for = Adam(params_for, lr=0.1, wd=0.1, decouple_wd=False, foreach=True)\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_for.step()\n",
    "\n",
    "test_close([p[0].item() for p in params_org], [p[0].item() for p in params_for])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAdam -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def radam_foreach_step(p:list[Tensor], g:list[Tensor], grad_avg:list[Tensor], sqr_avg:list[Tensor], ones:list[Tensor],\n",
    "                       steps:np.ndarray[Any, int], do_wd:np.ndarray[Any, bool], lr:float, wd:float, mom:float, sqr_mom:float,\n",
    "                       eps:float, decouple_wd:bool, **kwargs):\n",
    "\n",
    "    if wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            wd = np.where(do_wd, 1-lr*wd, 1.)\n",
    "            torch._foreach_mul_(p, scalars=wd.tolist())\n",
    "        else:\n",
    "            # l2_reg\n",
    "            wd = np.where(do_wd, wd, 1.)\n",
    "            torch._foreach_addcdiv_(g, p, ones, scalars=wd.tolist())\n",
    "            # cannot use scalers with foreach_add & multiple tensors, so divide by one with foreach_addcdiv\n",
    "\n",
    "    # average_grad, dampening=True\n",
    "    torch._foreach_mul_(grad_avg, mom)\n",
    "    torch._foreach_add_(grad_avg, g, alpha=1-mom)\n",
    "\n",
    "    # average_sqr_grad\n",
    "    torch._foreach_mul_(sqr_avg, sqr_mom)\n",
    "    torch._foreach_addcmul_(sqr_avg, g, g, value=1-sqr_mom)\n",
    "\n",
    "    # radam_step\n",
    "    debias1 = -lr / (1 - mom**steps)\n",
    "    debias2 = np.sqrt(1 - sqr_mom**steps)\n",
    "    \n",
    "    r_inf = 2/(1-sqr_mom) - 1\n",
    "    r = r_inf - 2*steps*sqr_mom**steps/(1-sqr_mom**steps)\n",
    "\n",
    "    rect   = np.where(r>5, debias1*np.sqrt(((r-4) * (r-2) * r_inf)/((r_inf-4)*(r_inf-2)*r), where=r>5), 0)\n",
    "    unrect = np.where(r<=5, debias1, 0)\n",
    "\n",
    "    # rectified step\n",
    "    sqrt_avg_debias2 = torch._foreach_sqrt(sqr_avg)\n",
    "    torch._foreach_div_(sqrt_avg_debias2, debias2.tolist())\n",
    "    torch._foreach_add_(sqrt_avg_debias2, eps)\n",
    "    torch._foreach_addcdiv_(p, grad_avg, sqrt_avg_debias2, scalars=rect.tolist())\n",
    "\n",
    "    # unrectified step. cannot use scalers with foreach_add & multiple tensors, so divide by one with foreach_addcdiv\n",
    "    torch._foreach_addcdiv_(p, grad_avg, ones, scalars=unrect.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class RAdamForEachOptimizer(ForEachOptimizer):\n",
    "    \"An `ForEachOptimizer` with a modified step for `radam_foreach_step`\"\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        if closure is not None: raise NotImplementedError(\"fastai optimizers currently do not support closure\")\n",
    "        for pg, hyper in zip(self.param_lists, self.hypers):\n",
    "            pl, gl, grad_avg, sqr_avg, steps, ones, do_wd = [], [], [], [], [], [], []\n",
    "\n",
    "            for p in pg:\n",
    "                if hasattr(p, 'grad') and p.grad is not None:\n",
    "                    state = self.state[p]\n",
    "\n",
    "                    if 'step' not in state:\n",
    "                        state['grad_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        state['sqr_avg']  = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        state['ones']     = torch.ones_like(p, memory_format=torch.preserve_format)\n",
    "                        state['step']     = 0\n",
    "\n",
    "                    state['step'] += 1\n",
    "                    pl.append(p)\n",
    "                    gl.append(p.grad)\n",
    "                    grad_avg.append(state['grad_avg'])\n",
    "                    sqr_avg.append(state['sqr_avg'])\n",
    "                    ones.append(state['ones'])\n",
    "                    steps.append(state['step'])\n",
    "                    do_wd.append(state.get('do_wd', True))\n",
    "\n",
    "            self.opt_step(p=pl, g=gl, grad_avg=grad_avg, sqr_avg=sqr_avg, ones=ones, \n",
    "                          steps=np.array(steps, dtype=np.int32), do_wd=np.array(do_wd, dtype=bool), \n",
    "                          decouple_wd=self.decouple_wd, **hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(RAdamForEachOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def RAdam(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0., beta=0., decouple_wd=True, foreach=False):\n",
    "    if foreach:\n",
    "        if beta != 0: warn('ForEach RAdam does not use beta, set foreach=False if beta!=0')\n",
    "        return RAdamForEachOptimizer(params, radam_foreach_step, lr=lr, mom=mom, sqr_mom=sqr_mom, \n",
    "                                     eps=eps, wd=wd, decouple_wd=decouple_wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, radam_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd, beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = RAdam(params_org, lr=0.1, foreach=False)\n",
    "\n",
    "params_for = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_for = RAdam(params_for, lr=0.1, foreach=True)\n",
    "\n",
    "#The r factor is lower than 5 during the first 5 steps so updates use the average of gradients (all the same)\n",
    "r_inf = 2/(1-0.99) - 1\n",
    "for i in range(5): \n",
    "    r = r_inf - 2*(i+1)*0.99**(i+1)/(1-0.99**(i+1))\n",
    "    assert r <= 5\n",
    "    opt_org.step()\n",
    "    opt_for.step()\n",
    "p = tensor([0.95, 1.9, 2.85])\n",
    "test_close(params_org[0], p)\n",
    "test_close(params_org[0], params_for[0])\n",
    "\n",
    "#The r factor is greater than 5 for the sixth step so we update with RAdam\n",
    "r = r_inf - 2*6*0.99**6/(1-0.99**6)\n",
    "assert r > 5\n",
    "opt_org.step()\n",
    "opt_for.step()\n",
    "v = math.sqrt(((r-4) * (r-2) * r_inf)/((r_inf-4)*(r_inf-2)*r))\n",
    "step = -0.1*0.1*v/(math.sqrt(0.1**2) + 1e-8)\n",
    "test_close(params_org[0], p+step)\n",
    "test_close(params_org[0], params_for[0])\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_for.step()\n",
    "\n",
    "test_close(params_org[0], params_for[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# test with weight decay\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = RAdam(params_org, lr=0.1, wd=0.1, foreach=False)\n",
    "\n",
    "params_for = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_for = RAdam(params_for, lr=0.1, wd=0.1, foreach=True)\n",
    "\n",
    "opt_org.step()\n",
    "opt_for.step()\n",
    "\n",
    "test_close(params_org[0], params_for[0])\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_for.step()\n",
    "\n",
    "test_close(params_org[0], params_for[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAMB -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@torch.jit.script\n",
    "def lamb_jit_substep(p:Tensor, lstep:Tensor, lr:float):\n",
    "    r1 = p.pow(2).mean().sqrt()\n",
    "    r2 = lstep.pow(2).mean().sqrt()\n",
    "    if r1 == 0 or r2 == 0:\n",
    "        return -lr\n",
    "    else:\n",
    "        return -lr*min(r1/r2, 10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def lamb_foreach_step(p:list[Tensor], g:list[Tensor], grad_avg:list[Tensor], sqr_avg:list[Tensor], ones:list[Tensor], \n",
    "                      steps:np.ndarray[Any, int], do_wd:np.ndarray[Any, bool], lr:float, wd:float, mom:float, sqr_mom:float, \n",
    "                      eps:float, decouple_wd:bool, **kwargs):\n",
    "\n",
    "    if wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            wd = np.where(do_wd, 1-lr*wd, 1.)\n",
    "            torch._foreach_mul_(p, scalars=wd.tolist())\n",
    "        else:\n",
    "            # l2_reg\n",
    "            wd = np.where(do_wd, wd, 1.)\n",
    "            torch._foreach_addcdiv_(g, p, ones, scalars=wd.tolist())\n",
    "            # cannot use scalers with foreach_add & multiple tensors, so divide by one with foreach_addcdiv\n",
    "\n",
    "    # average_grad, dampening=True\n",
    "    torch._foreach_mul_(grad_avg, mom)\n",
    "    torch._foreach_add_(grad_avg, g, alpha=1-mom)\n",
    "\n",
    "    # average_sqr_grad\n",
    "    torch._foreach_mul_(sqr_avg, sqr_mom)\n",
    "    torch._foreach_addcmul_(sqr_avg, g, g, value=1-sqr_mom)\n",
    "\n",
    "    # lamb_step\n",
    "    debias1 = 1 - mom**steps\n",
    "    debias2 = np.sqrt(1 - sqr_mom**steps)\n",
    "\n",
    "    debias2 = torch._foreach_div(torch._foreach_sqrt(sqr_avg), debias2.tolist())\n",
    "    torch._foreach_add_(debias2, eps)\n",
    "    lstep = torch._foreach_div(grad_avg, debias1.tolist())\n",
    "    torch._foreach_div_(lstep, debias2)\n",
    "\n",
    "    # there are no implementations for foreach_pow, foreach_mean, or foreach_where/if methods\n",
    "    q = [lamb_jit_substep(pi, ls, lr) for pi, ls in zip(p, lstep)]\n",
    "\n",
    "    # cannot use scalers with foreach_add & multiple tensors, so divide by one with foreach_addcdiv\n",
    "    torch._foreach_addcdiv_(p, lstep, ones, scalars=q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class LambForEachOptimizer(RAdamForEachOptimizer):\n",
    "    \"An `ForEachOptimizer` with a modified step for `lamb_foreach_step`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(LambForEachOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def Lamb(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0., decouple_wd=True, foreach=False):\n",
    "    if foreach:\n",
    "        return LambForEachOptimizer(params, lamb_foreach_step, lr=lr, mom=mom, sqr_mom=sqr_mom, \n",
    "                                    eps=eps, wd=wd, decouple_wd=decouple_wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, lamb_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = Lamb(params_org, lr=0.1, foreach=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_for = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_for = Lamb(params_for, lr=0.1, foreach=True)\n",
    "opt_for.step()\n",
    "\n",
    "test_close(params_org[0], tensor([0.7840,1.7840,2.7840]), eps=1e-3)\n",
    "test_close(params_org[0], params_for[0])\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_for.step()\n",
    "\n",
    "test_close(params_org[0], params_for[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# test with weight decay\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = Lamb(params_org, lr=0.1, wd=0.1, foreach=False)\n",
    "\n",
    "params_for = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_for = Lamb(params_for, lr=0.1, wd=0.1, foreach=True)\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_for.step()\n",
    "\n",
    "test_close(params_org[0], params_for[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranger -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def ranger_foreach_step(p:list[Tensor], g:list[Tensor], grad_avg:list[Tensor], sqr_avg:list[Tensor], slow_p:list[Tensor], \n",
    "                        ones:list[Tensor], steps:np.ndarray[Any, int], do_wd:np.ndarray[Any, bool], lr:float, wd:float, \n",
    "                        mom:float, sqr_mom:float, eps:float, decouple_wd:bool, count:int, k:int, alpha:float, **kwargs):\n",
    "\n",
    "    radam_foreach_step(p=p, g=g, grad_avg=grad_avg, sqr_avg=sqr_avg, ones=ones, steps=steps, do_wd=do_wd, \n",
    "                       lr=lr, wd=wd, mom=mom, sqr_mom=sqr_mom, eps=eps, decouple_wd=decouple_wd)\n",
    "\n",
    "    if count % k == 0:\n",
    "        torch._foreach_add_(slow_p, torch._foreach_sub(p, slow_p), alpha=alpha)\n",
    "        # there currently is no foreach_set method\n",
    "        [pi.set_(slow_pi.clone()) for pi, slow_pi in zip(p, slow_p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class RangerForEachOptimizer(ForEachOptimizer):\n",
    "    \"An `ForEachOptimizer` with a modified `LookAhead` step for `ranger_foreach_step`\"\n",
    "    def __init__(self, \n",
    "        params:listified[Tensor], # Model parameters\n",
    "        opt_step:Callable, # `ForEachOptimizer` optimizer step\n",
    "        decouple_wd:bool=True, # Use true weight decay or L2 regularization, if applicable\n",
    "        **defaults # Optimizer specific hyper parameters default values\n",
    "    ):\n",
    "        super().__init__(params, opt_step, decouple_wd, **defaults)\n",
    "        self._init_state()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        self.count += 1\n",
    "        if closure is not None: raise NotImplementedError(\"fastai optimizers currently do not support closure\")\n",
    "        for pg, hyper in zip(self.param_lists, self.hypers):\n",
    "            pl, gl, grad_avg, sqr_avg, slow_p, steps, ones, do_wd = [], [], [], [], [], [], [], []\n",
    "            for p in pg:\n",
    "                if hasattr(p, 'grad') and p.grad is not None:\n",
    "                    state = self.state[p]\n",
    "\n",
    "                    if 'step' not in state:\n",
    "                        state['grad_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        state['sqr_avg']  = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        state['ones']     = torch.ones_like(p, memory_format=torch.preserve_format)\n",
    "                        state['slow_p']   = p.data.clone()\n",
    "                        state['step']     = 0\n",
    "\n",
    "                    state['step'] += 1\n",
    "                    pl.append(p)\n",
    "                    gl.append(p.grad)\n",
    "                    grad_avg.append(state['grad_avg'])\n",
    "                    sqr_avg.append(state['sqr_avg'])\n",
    "                    slow_p.append(state['slow_p'])\n",
    "                    ones.append(state['ones'])\n",
    "                    steps.append(state['step'])\n",
    "                    do_wd.append(state.get('do_wd', True))\n",
    "\n",
    "            self.opt_step(p=pl, g=gl, grad_avg=grad_avg, sqr_avg=sqr_avg, slow_p=slow_p, ones=ones, \n",
    "                          steps=np.array(steps, dtype=np.int32), do_wd=np.array(do_wd, dtype=bool), \n",
    "                          decouple_wd=self.decouple_wd, count=self.count, **hyper)\n",
    "\n",
    "    def clear_state(self):\n",
    "        super().clear_state()\n",
    "        self._init_state()\n",
    "\n",
    "    def state_dict(self):\n",
    "        state = super().state_dict()\n",
    "        state.update({'count': self.count})\n",
    "        return state\n",
    "\n",
    "    def load_state_dict(self, sd):\n",
    "        self.count = sd.pop('count')\n",
    "        super().load_state_dict(sd)\n",
    "\n",
    "    def _init_state(self): \n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(RangerForEachOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def Ranger(params, lr, mom=0.95, sqr_mom=0.99, eps=1e-6, wd=0.01, beta=0., k=6, alpha=0.5, decouple_wd=True, foreach=False):\n",
    "    if foreach:\n",
    "        if beta != 0: warn('RAdam foreach does not use beta, set foreach=False if beta!=0')\n",
    "        return RangerForEachOptimizer(params, ranger_foreach_step, lr=lr, mom=mom, sqr_mom=sqr_mom,\n",
    "                                      eps=eps, wd=wd, decouple_wd=decouple_wd, k=k, alpha=alpha)\n",
    "    else:\n",
    "        return Lookahead(RAdam(params, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd, \n",
    "                               beta=beta, decouple_wd=decouple_wd, foreach=False),\n",
    "                         k=k, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "po = tensor([1,2,3])\n",
    "\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = Ranger(params_org, lr=0.1, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0., beta=0., foreach=False)\n",
    "\n",
    "params_for = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_for = Ranger(params_for, lr=0.1, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0., foreach=True)\n",
    "\n",
    "#The first 5 steps are normal RAdam steps\n",
    "#The r factor is lower than 5 during the first 5 steps so updates use the average of gradients (all the same)\n",
    "r_inf = 2/(1-0.99) - 1\n",
    "for i in range(5): \n",
    "    r = r_inf - 2*(i+1)*0.99**(i+1)/(1-0.99**(i+1))\n",
    "    assert r <= 5\n",
    "    opt_org.step()\n",
    "    opt_for.step()\n",
    "p = tensor([0.95, 1.9, 2.85])\n",
    "test_close(params_org[0], p)\n",
    "test_close(params_org[0], params_for[0])\n",
    "\n",
    "#The r factor is greater than 5 for the sixth step so we update with RAdam\n",
    "r = r_inf - 2*6*0.99**6/(1-0.99**6)\n",
    "assert r > 5\n",
    "opt_org.step()\n",
    "opt_for.step()\n",
    "v = math.sqrt(((r-4) * (r-2) * r_inf)/((r_inf-4)*(r_inf-2)*r))\n",
    "step = -0.1*0.1*v/(math.sqrt(0.1**2) + 1e-8)\n",
    "\n",
    "#Since k=6, sixth step is a moving average of the 6 RAdam steps with the initial weight\n",
    "test_close(params_org[0], po+((p+step)-po)*0.5)\n",
    "test_close(params_org[0], params_for[0])\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_for.step()\n",
    "\n",
    "test_close(params_org[0], params_for[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# test with weight decay\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = Ranger(params_org, lr=0.1, wd=0.1, foreach=False)\n",
    "\n",
    "params_for = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_for = Ranger(params_for, lr=0.1, wd=0.1, foreach=True)\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_for.step()\n",
    "\n",
    "test_close(params_org[0], params_for[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('fastai')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
