{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp optimizers.adan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adan: ADAptive Nesterov betaentum Optimizer\n",
    "> Adds the Adan optimizer to fastai from [Adan: Adaptive Nesterov betaentum Algorithm for\n",
    "Faster Optimizing Deep Models](https://arxiv.org/abs/2208.06677)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.optimizer import Optimizer\n",
    "\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def avg_grad(p, beta1, grad_avg=None, **kwargs):\n",
    "    \"Tracks average gradients (m) of `p` in `state` with `beta1`.\"\n",
    "    if grad_avg is None: \n",
    "        grad_avg = torch.zeros_like(p.grad.data, memory_format=p.memory_format)\n",
    "        beta1 = 0 # TODO: see if setting m0=g0 per paper is better than averaging, as g0 will have an outsized initial effect compared to g1, g2, etc\n",
    "    grad_avg.mul_(beta1).add_(p.grad.data, alpha=1-beta1)\n",
    "    return {'grad_avg': grad_avg}\n",
    "\n",
    "avg_grad.defaults = dict(beta1=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def step_stat(p, step=-1, **kwargs):\n",
    "    \"Register the number of steps done in `state` for `p`\"\n",
    "    step += 1\n",
    "    return {'steps': step}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def avg_diff_grad(p, beta2, steps, prior_grad=None, diff_avg=None, **kwargs):\n",
    "    \"Tracks the average difference of current and prior gradients (v) of `p` in `state` with `beta2`.\"\n",
    "    if diff_avg is None: \n",
    "        diff_avg = torch.zeros_like(p.grad.data, memory_format=p.memory_format)\n",
    "    elif steps == 1:\n",
    "        # TODO: see if setting v1=g1-g0 per paper is better than averaging, as v1 will have an outsized initial effect compared to v2, v3, etc\n",
    "        diff_avg = p.grad.data-prior_grad\n",
    "    else:\n",
    "        diff_avg.mul_(beta2).add_(p.grad.data-prior_grad, alpha=1-beta2)\n",
    "    return {'diff_avg': diff_avg}\n",
    "\n",
    "avg_diff_grad.defaults = dict(beta2=0.92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def avg_nesterov_est(p, beta2, beta3, prior_grad=None, nesterov_est=None, **kwargs):\n",
    "    \"Tracks the Nesterov momentum estimate of gradients (n) of `p` in `state` with `beta2` & `beta3`.\"\n",
    "    if nesterov_est is None: \n",
    "        nesterov_est = torch.zeros_like(p.grad.data, memory_format=p.memory_format)\n",
    "        # TODO: see if setting n1=g0^2 per paper is better than averaging, as g0 will have an outsized initial effect compared to g1, g2, etc\n",
    "        nesterov_est.addcmul_(p.grad.data, p.grad.data)\n",
    "    else:\n",
    "        nesterov_est.mul_(beta3).add_(torch.square(p.grad.data, torch.addcmul_(p.grad.data, prior_grad, alpha=beta2)), alpha=1-beta3)\n",
    "    return {'nesterov_est': nesterov_est}\n",
    "\n",
    "avg_nesterov_est.defaults = dict(beta2=0.92, beta3=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def prior_grad(p, **kwargs):\n",
    "    \"Register the current gradient of `p` for use in the next step\"\n",
    "    return {'prior_grad' : p.grad.data.clone()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def adan_step(p, lr, eps, wd, beta2, grad_avg, diff_avg, nesterov_est):\n",
    "    \"Performs the Adan step with `lr` on `p`\"\n",
    "    wd = 1/(1+lr*wd) if wd!=0 else 1\n",
    "    lr = lr/torch.sqrt(nesterov_est+eps)\n",
    "    p.data.mul_(wd).sub_(grad_avg.add_(diff_avg, alpha=beta2).matmul_(lr).mul_(wd))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def Adan(params, lr, beta1=0.98, beta2=0.92, beta3=0.99, eps=1e-8, wd=0.02):\n",
    "    \"A `Optimizer` for Adan with `lr`, `beta`s, `eps` and `params`\"\n",
    "    cbs = [avg_grad, step_stat, avg_diff_grad, avg_nesterov_est, prior_grad, adan_step]\n",
    "    return Optimizer(params, cbs, lr=lr, beta1=beta1, beta2=beta2, beta3=beta3, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def AdanLargeBatchLR(bs):\n",
    "    \"Suare root rule for scaling Adan learning rate for large-batch training\"\n",
    "    return math.sqrt(bs/256)*6.25e-3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
