{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp optimizer.torchscript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchScript Optimizers\n",
    "> fastai optimizers compiled with TorchScript for improved performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "from typing import Optional, Dict\n",
    "\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from fastcore.basics import range_of, merge\n",
    "\n",
    "from fastai.optimizer import (Optimizer, _update, weight_decay, l2_reg, average_grad, sgd_step, \n",
    "                              momentum_step, average_sqr_grad, rms_prop_step, step_stat, adam_step, \n",
    "                              radam_step, qhadam_step, larc_layer_lr, larc_step, lamb_step, Lookahead)\n",
    "\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastxtend.test_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Utils -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def tst_param(val, grad=None):\n",
    "    \"Create a tensor with `val` and a gradient of `grad` for testing\"\n",
    "    res = tensor([val]).float()\n",
    "    res.grad = tensor([val/10 if grad is None else grad]).float()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def tst_params():\n",
    "    r = L.range(4)\n",
    "    return r.map(tst_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JitOptimizer -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def _update(\n",
    "    state:dict,\n",
    "    new=None # New values to update `state` dict\n",
    "):\n",
    "    if isinstance(new, dict): state.update(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class JitOptimizer(Optimizer):\n",
    "    \"An `Optimizer` with a modified step for TorchScript optimizers\"\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        if closure is not None: raise NotImplementedError(\"fastai optimizers currently do not support closure\")\n",
    "        for pg, hyper in zip(self.param_lists, self.hypers):\n",
    "            for p in pg:\n",
    "                if hasattr(p, 'grad') and p.grad is not None:\n",
    "                    _update(self.state[p], self.cbs[0](p, p.grad, **{**self.state[p], **hyper}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    ":::{.callout-note}\n",
    "Documentation for individual optimizers lightly adapted from the [fastai optimizer documentation](https://docs.fast.ai/optimizer.html).\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@torch.jit.script\n",
    "def sgd_jit_step(p:Tensor, grad:Tensor, lr:float, wd:float, mom:float, decouple_wd:bool, grad_avg:Optional[Tensor]=None, \n",
    "                 do_wd:bool=True, dampening:bool=False, force_train:Optional[bool]=None):\n",
    "    dp = p\n",
    "    if do_wd and wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            dp = dp.mul(1 - lr*wd)\n",
    "        else:\n",
    "            # l2_reg\n",
    "            grad = grad.add(dp, alpha=wd)\n",
    "\n",
    "    if mom != 0:\n",
    "        if grad_avg is None: \n",
    "            grad_avg = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "\n",
    "        # average_grad\n",
    "        damp = 1-mom if dampening else 1.\n",
    "        grad_avg = grad_avg.mul(mom)\n",
    "        grad_avg = grad_avg.add(grad, alpha=damp)\n",
    "\n",
    "        # momentum_step\n",
    "        dp = dp.add(grad_avg, alpha=-lr)\n",
    "        p.set_(dp)\n",
    "        return {'grad_avg': grad_avg}\n",
    "    else:\n",
    "        # sgd_step\n",
    "        dp = dp.add(grad, alpha=-lr)\n",
    "        p.set_(dp)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def SGD(params, lr, mom=0., wd=0., decouple_wd=True, jit=False):\n",
    "    \"A `Optimizer` or `JitOptimizer` for SGD with `lr` and `mom` and `params`\"\n",
    "    if jit:\n",
    "        cb = partial(sgd_jit_step, decouple_wd=decouple_wd)\n",
    "        return JitOptimizer(params, cb, lr=lr, mom=mom, wd=wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        if mom != 0: cbs.append(average_grad)\n",
    "        cbs.append(sgd_step if mom==0 else momentum_step)\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, wd=wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional weight decay of `wd` is applied, as true weight decay (decay the weights directly) if `decouple_wd=True` else as L2 regularization (add the decay to the gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "\n",
    "# Test Vanilla SGD\n",
    "params_org = tst_params()\n",
    "opt_org = SGD(params_org, lr=0.1, jit=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = tst_params()\n",
    "opt_jit = SGD(params_jit, lr=0.1, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "test_close([p.item() for p in params_org], [i*0.99 for i in range(4)])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_jit])\n",
    "\n",
    "opt_org.step()\n",
    "opt_jit.step()\n",
    "test_close([p.item() for p in params_org], [i*0.98 for i in range(4)])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_jit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@torch.jit.script\n",
    "def rmsprop_jit_step(p:Tensor, grad:Tensor, lr:float, wd:float, mom:float, sqr_mom:float, eps:float, decouple_wd:bool, \n",
    "                     grad_avg:Optional[Tensor]=None, sqr_avg:Optional[Tensor]=None, do_wd:bool=True, force_train:Optional[bool]=None):\n",
    "    dp = p\n",
    "    if do_wd and wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            dp = dp.mul(1 - lr*wd)\n",
    "        else:\n",
    "            # l2_reg\n",
    "            grad = grad.add(dp, alpha=wd)\n",
    "\n",
    "    if sqr_avg is None: \n",
    "        sqr_avg = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "\n",
    "    if mom != 0:\n",
    "        if grad_avg is None: \n",
    "            grad_avg = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "\n",
    "        # average_grad, dampening=False\n",
    "        grad_avg = torch.mul(grad_avg, mom)\n",
    "        grad_avg = torch.add(grad_avg, grad)\n",
    "\n",
    "        # average_sqr_grad\n",
    "        sqr_avg = torch.mul(sqr_avg, sqr_mom)\n",
    "        sqr_avg = torch.addcmul(sqr_avg, grad, grad, value=1-sqr_mom)\n",
    "\n",
    "        # rms_prop_step\n",
    "        denom = torch.sqrt(sqr_avg)\n",
    "        denom = torch.add(denom, eps)\n",
    "        dp = torch.addcdiv(dp, grad_avg, denom, value=-lr)\n",
    "        p.set_(dp)\n",
    "        return {'grad_avg': grad_avg, 'sqr_avg': sqr_avg}\n",
    "    else:\n",
    "        # average_sqr_grad\n",
    "        sqr_avg = torch.mul(sqr_avg, sqr_mom)\n",
    "        sqr_avg = torch.addcmul(sqr_avg, grad, grad, value=1-sqr_mom)\n",
    "        \n",
    "        # rms_prop_step\n",
    "        denom = torch.sqrt(sqr_avg)\n",
    "        denom = torch.add(denom, eps)\n",
    "        dp = dp.addcdiv(grad, denom, value=-lr)\n",
    "        p.set_(dp)\n",
    "        return {'sqr_avg': sqr_avg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def RMSProp(params, lr, sqr_mom=0.99, mom=0., eps=1e-8, wd=0., decouple_wd=True, jit=False):\n",
    "    \"A `Optimizer` or `JitOptimizer` for RMSProp with `lr`, `sqr_mom`, `mom` and `params`\"\n",
    "    if jit:\n",
    "        cb = partial(rmsprop_jit_step, decouple_wd=decouple_wd)\n",
    "        return JitOptimizer(params, cb, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        cbs += ([average_sqr_grad] if mom==0. else [average_grad, average_sqr_grad])\n",
    "        cbs.append(rms_prop_step)\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, wd=wd, eps=eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSProp was introduced by Geoffrey Hinton in his [course](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf). What is named `sqr_mom` here is the `alpha` in the course. \n",
    "\n",
    "Optional weight decay of `wd` is applied, as true weight decay (decay the weights directly) if `decouple_wd=True` else as L2 regularization (add the decay to the gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#Without momentum\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = RMSProp(params_org, lr=0.1, jit=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = RMSProp(params_jit, lr=0.1, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "test_close(params_org[0], tensor([0.,1.,2.]))\n",
    "test_close(params_org[0], params_jit[0])\n",
    "\n",
    "opt_org.step()\n",
    "opt_jit.step()\n",
    "step = - 0.1 * 0.1 / (math.sqrt((0.01*0.99+0.01) * 0.1**2) + 1e-8)\n",
    "test_close(params_org[0], tensor([step, 1+step, 2+step]))\n",
    "test_close(params_org[0], params_jit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#With momentum\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = RMSProp(params_org, lr=0.1, mom=0.9, jit=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = RMSProp(params_jit, lr=0.1, mom=0.9, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "test_close(params_org[0], tensor([0.,1.,2.]))\n",
    "test_close(params_org[0], params_jit[0])\n",
    "\n",
    "opt_org.step()\n",
    "opt_jit.step()\n",
    "step = - 0.1 * (0.1 + 0.9*0.1) / (math.sqrt((0.01*0.99+0.01) * 0.1**2) + 1e-8)\n",
    "test_close(params_org[0], tensor([step, 1+step, 2+step]))\n",
    "test_close(params_org[0], params_jit[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def debias(beta:float, step:int):\n",
    "    \"Simple debias calculation\"\n",
    "    return 1-beta**step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@torch.jit.script\n",
    "def adam_jit_step(p:Tensor, grad:Tensor, lr:float, wd:float, mom:float, sqr_mom:float, eps:float, \n",
    "                  decouple_wd:bool, grad_avg:Optional[Tensor]=None, sqr_avg:Optional[Tensor]=None, \n",
    "                  do_wd:bool=True, step:int=0, force_train:Optional[bool]=None):\n",
    "    dp = p\n",
    "    step += 1\n",
    "    if do_wd and wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            dp = dp.mul(1 - lr*wd)\n",
    "        else:\n",
    "            # l2_reg\n",
    "            grad = grad.add(dp, alpha=wd)\n",
    "\n",
    "    if grad_avg is None: \n",
    "        grad_avg = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "    if sqr_avg is None: \n",
    "        sqr_avg  = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "\n",
    "    # average_grad, dampening=True\n",
    "    grad_avg = torch.mul(grad_avg, mom)\n",
    "    grad_avg = torch.add(grad_avg, grad, alpha=1-mom)\n",
    "\n",
    "    # average_sqr_grad\n",
    "    sqr_avg = torch.mul(sqr_avg, sqr_mom)\n",
    "    sqr_avg = torch.addcmul(sqr_avg, grad, grad, value=1-sqr_mom)\n",
    "\n",
    "    # adam_step\n",
    "    debias1 = debias(mom, step)\n",
    "    debias2 = debias(sqr_mom, step)\n",
    "    dp = torch.addcdiv(dp, grad_avg, torch.sqrt(sqr_avg/debias2) + eps, value = -lr / debias1)\n",
    "    p.set_(dp)\n",
    "\n",
    "    return torch.jit.annotate(Dict[str, Union[Tensor, int]], {'grad_avg': grad_avg, 'sqr_avg': sqr_avg, 'step': step})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def Adam(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0.01, decouple_wd=True, jit=False):\n",
    "    \"A `Optimizer` or `JitOptimizer` for Adam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n",
    "    if jit:\n",
    "        cb = partial(adam_jit_step, decouple_wd=decouple_wd)\n",
    "        return JitOptimizer(params, cb, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, adam_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam was introduced by Diederik P. Kingma and Jimmy Ba in [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980). For consistency across optimizers, fastai renamed `beta1` and `beta2` in the paper to `mom` and  `sqr_mom`. Note that the defaults also differ from the paper (0.99 for `sqr_mom` or `beta2`, 1e-5 for `eps`). Those values seem to be better from experiments in a wide range of situations.\n",
    "\n",
    "Optional weight decay of `wd` is applied, as true weight decay (decay the weights directly) if `decouple_wd=True` else as L2 regularization (add the decay to the gradients).\n",
    "\n",
    ":::{.callout-note}\n",
    "Don't forget that `eps` is an hyper-parameter you can change. Some models won't train without a very high `eps` like 0.1 (intuitively, the higher `eps` is, the closer we are to normal SGD). The usual default of 1e-8 is often too extreme in the sense we don't manage to get as good results as with SGD.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = Adam(params_org, lr=0.1, wd=0, jit=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = Adam(params_jit, lr=0.1, wd=0, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "step = -0.1 * 0.1 / (math.sqrt(0.1**2) + 1e-8)\n",
    "test_close(params_org[0], tensor([1+step, 2+step, 3+step]))\n",
    "test_close([p[0].item() for p in params_org], [p[0].item() for p in params_jit])\n",
    "\n",
    "opt_org.step()\n",
    "opt_jit.step()\n",
    "test_close(params_org[0], tensor([1+2*step, 2+2*step, 3+2*step]), eps=1e-3)\n",
    "test_close([p[0].item() for p in params_org], [p[0].item() for p in params_jit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAdam -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@torch.jit.script\n",
    "def radam_jit_step(p:Tensor, grad:Tensor, lr:float, wd:float, mom:float, sqr_mom:float, eps:float, beta:float,\n",
    "                   decouple_wd:bool, grad_avg:Optional[Tensor]=None, sqr_avg:Optional[Tensor]=None,\n",
    "                   do_wd:bool=True, step:int=0, force_train:Optional[bool]=None):\n",
    "    dp = p\n",
    "    step += 1\n",
    "    if do_wd and wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            dp = dp.mul(1 - lr*wd)\n",
    "        else:\n",
    "            # l2_reg\n",
    "            grad = grad.add(dp, alpha=wd)\n",
    "\n",
    "    if grad_avg is None: \n",
    "        grad_avg = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "    if sqr_avg is None: \n",
    "        sqr_avg  = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "\n",
    "    # average_grad, dampening=True\n",
    "    grad_avg = torch.mul(grad_avg, mom)\n",
    "    grad_avg = torch.add(grad_avg, grad, alpha=1-mom)\n",
    "\n",
    "    # average_sqr_grad\n",
    "    sqr_avg = torch.mul(sqr_avg, sqr_mom)\n",
    "    sqr_avg = torch.addcmul(sqr_avg, grad, grad, value=1-sqr_mom)\n",
    "\n",
    "    # radam_step\n",
    "    debias1 = debias(mom, step)\n",
    "    debias2 = debias(sqr_mom, step)\n",
    "    r_inf = 2/(1-sqr_mom) - 1\n",
    "    r = r_inf - 2*step*sqr_mom**step/(1-sqr_mom**step)\n",
    "    \n",
    "    if r > 5:\n",
    "        v = math.sqrt(((r-4) * (r-2) * r_inf)/((r_inf-4)*(r_inf-2)*r))\n",
    "        denom = torch.sqrt(sqr_avg/debias2)\n",
    "        if eps != 0: \n",
    "            denom = denom + eps\n",
    "        if beta != 0: \n",
    "            denom = F.softplus(denom, beta)\n",
    "        dp = torch.addcdiv(dp, grad_avg, denom, value = -lr*v / debias1)\n",
    "    else:\n",
    "        dp = torch.add(dp, grad_avg, alpha=-lr / debias1)\n",
    "    p.set_(dp)\n",
    "\n",
    "    return torch.jit.annotate(Dict[str, Union[Tensor, int]], {'grad_avg': grad_avg, 'sqr_avg': sqr_avg, 'step': step})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def RAdam(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0., beta=0., decouple_wd=True, jit=False):\n",
    "    \"A `Optimizer` or `JitOptimizer` for RAdam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n",
    "    if jit:\n",
    "        cb = partial(radam_jit_step, decouple_wd=decouple_wd)\n",
    "        return JitOptimizer(params, cb, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd, beta=beta)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, radam_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd, beta=beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAdam (for rectified Adam) was introduced by Zhang et al. in [On the Variance of the Adaptive Learning Rate and Beyond](https://arxiv.org/abs/1907.08610) to slightly modify the Adam optimizer to be more stable at the beginning of training (and thus not require a long warmup). They use an estimate of the variance of the moving average of the squared gradients (the term in the denominator of traditional Adam) and rescale this moving average by this term before performing the update.\n",
    "\n",
    "This version also incorporates [SAdam](https://arxiv.org/abs/1908.00700); set `beta` to enable this (definition same as in the paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = RAdam(params_org, lr=0.1, jit=False)\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = RAdam(params_jit, lr=0.1, jit=True)\n",
    "\n",
    "#The r factor is lower than 5 during the first 5 steps so updates use the average of gradients (all the same)\n",
    "r_inf = 2/(1-0.99) - 1\n",
    "for i in range(5): \n",
    "    r = r_inf - 2*(i+1)*0.99**(i+1)/(1-0.99**(i+1))\n",
    "    assert r <= 5\n",
    "    opt_org.step()\n",
    "    opt_jit.step()\n",
    "p = tensor([0.95, 1.9, 2.85])\n",
    "test_close(params_org[0], p)\n",
    "test_close(params_org[0], params_jit[0])\n",
    "\n",
    "#The r factor is greater than 5 for the sixth step so we update with RAdam\n",
    "r = r_inf - 2*6*0.99**6/(1-0.99**6)\n",
    "assert r > 5\n",
    "opt_org.step()\n",
    "opt_jit.step()\n",
    "v = math.sqrt(((r-4) * (r-2) * r_inf)/((r_inf-4)*(r_inf-2)*r))\n",
    "step = -0.1*0.1*v/(math.sqrt(0.1**2) + 1e-8)\n",
    "test_close(params_org[0], p+step)\n",
    "test_close(params_org[0], params_jit[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QHAdam -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@torch.jit.script\n",
    "def qhadam_jit_step(p:Tensor, grad:Tensor, lr:float, wd:float, mom:float, sqr_mom:float, eps:float,\n",
    "                    nu_1:float, nu_2:float, decouple_wd:bool, grad_avg:Optional[Tensor]=None, \n",
    "                    sqr_avg:Optional[Tensor]=None, do_wd:bool=True, step:int=0, force_train:Optional[bool]=None):\n",
    "    dp = p\n",
    "    step += 1\n",
    "    if do_wd and wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            dp = dp.mul(1 - lr*wd)\n",
    "        else:\n",
    "            # l2_reg\n",
    "            grad = grad.add(dp, alpha=wd)\n",
    "\n",
    "    if grad_avg is None: \n",
    "        grad_avg = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "    if sqr_avg is None: \n",
    "        sqr_avg  = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "\n",
    "    # average_grad, dampening=True\n",
    "    grad_avg = torch.mul(grad_avg, mom)\n",
    "    grad_avg = torch.add(grad_avg, grad, alpha=1-mom)\n",
    "\n",
    "    # average_sqr_grad\n",
    "    sqr_avg = torch.mul(sqr_avg, sqr_mom)\n",
    "    sqr_avg = torch.addcmul(sqr_avg, grad, grad, value=1-sqr_mom)\n",
    "\n",
    "    # qhadam_step\n",
    "    debias1 = debias(mom, step)\n",
    "    debias2 = debias(sqr_mom, step)\n",
    "    dp = torch.addcdiv(dp, ((1-nu_1) * grad) + (nu_1 * (grad_avg / debias1)),\n",
    "                       torch.sqrt(((1 - nu_2) * (grad)**2) + (nu_2 * (sqr_avg / debias2))) + eps,\n",
    "                       value = -lr)\n",
    "    p.set_(dp)\n",
    "\n",
    "    return torch.jit.annotate(Dict[str, Union[Tensor, int]], {'grad_avg': grad_avg, 'sqr_avg': sqr_avg, 'step': step})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def QHAdam(params, lr, mom=0.999, sqr_mom=0.999, nu_1=0.7, nu_2=1.0, eps=1e-8, wd=0., decouple_wd=True, jit=True):\n",
    "    \"An `Optimizer` for Adam with `lr`, `mom`, `sqr_mom`, `nus`, eps` and `params`\"\n",
    "    if jit:\n",
    "        cb = partial(qhadam_jit_step, decouple_wd=decouple_wd)\n",
    "        return JitOptimizer(params, cb, lr=lr, nu_1=nu_1, nu_2=nu_2, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, qhadam_step]\n",
    "        return Optimizer(params, cbs, lr=lr, nu_1=nu_1, nu_2=nu_2, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QHAdam (for Quasi-Hyperbolic Adam) was introduced by Ma & Yarats in [Quasi-Hyperbolic Momentum and Adam for Deep Learning](https://arxiv.org/pdf/1810.06801.pdf) as a *\"computationally cheap, intuitive to interpret, and simple to implement\"* optimizer. Additional code can be found in their [qhoptim repo](https://github.com/facebookresearch/qhoptim). QHAdam is based on QH-Momentum, which introduces the immediate discount factor `nu`, encapsulating plain SGD (`nu = 0`) and momentum (`nu = 1`). QH-Momentum is defined below, where g_t+1 is the update of the moment. An interpretation of QHM is as a nu-weighted average of the momentum update step and the plain SGD update step.\n",
    "\n",
    "> θ_t+1 ← θ_t − lr * [(1 − nu) · ∇L_t(θ_t) + nu · g_t+1]\n",
    "\n",
    "QHAdam takes the concept behind QHM above and applies it to Adam, replacing both of Adam’s moment estimators with quasi-hyperbolic terms. \n",
    "\n",
    "The paper's suggested default parameters are `mom = 0.999`, `sqr_mom = 0.999`, `nu_1 = 0.7` and `and nu_2 = 1.0`. When training is not stable, it is possible that setting `nu_2 < 1` can improve stability by imposing a tighter step size bound. Note that QHAdam recovers Adam when `nu_1 = nu_2 = 1.0`. QHAdam recovers RMSProp (Hinton et al., 2012) when `nu_1 = 0` and `nu_2 = 1`, and NAdam (Dozat, 2016) when `nu_1 = mom` and `nu_2 = 1`.\n",
    "\n",
    "Optional weight decay of `wd` is applied, as true weight decay (decay the weights directly) if `decouple_wd=True` else as L2 regularization (add the decay to the gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = QHAdam(params_org, lr=0.1, jit=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = QHAdam(params_jit, lr=0.1, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "step = -0.1 * (((1-0.7) * 0.1) + (0.7 * 0.1)) / (\n",
    "     math.sqrt(((1-1.0) * 0.1**2) + (1.0 * 0.1**2)) + 1e-8) \n",
    "\n",
    "test_close(params_org[0], tensor([1+step, 2+step, 3+step]))\n",
    "test_close(params_org[0], params_jit[0])\n",
    "\n",
    "opt_org.step()\n",
    "opt_jit.step()\n",
    "test_close(params_org[0], tensor([1+2*step, 2+2*step, 3+2*step]), eps=1e-3)\n",
    "test_close(params_org[0], params_jit[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LARS/LARC -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@torch.jit.script\n",
    "def larc_jit_step(p:Tensor, grad:Tensor, lr:float, wd:float, mom:float, eps:float, trust_coeff:float, decouple_wd:bool,\n",
    "                  clip:bool, grad_avg:Optional[Tensor]=None, do_wd:bool=True, dampening:bool=False, force_train:Optional[bool]=None):\n",
    "    dp = p\n",
    "    if do_wd and wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            dp = dp.mul(1 - lr*wd)\n",
    "        else:\n",
    "            # l2_reg\n",
    "            grad = grad.add(dp, alpha=wd)\n",
    "\n",
    "    # larc_layer_lr\n",
    "    p_norm = torch.norm(dp)\n",
    "    g_norm = torch.norm(grad)\n",
    "    local_lr = lr*trust_coeff * (p_norm) / (g_norm + p_norm * wd + eps)\n",
    "    if clip:\n",
    "        lr = min(local_lr, lr)\n",
    "    else:\n",
    "        lr = local_lr\n",
    "\n",
    "    if mom != 0:\n",
    "        if grad_avg is None: \n",
    "            grad_avg = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "\n",
    "        # average_grad, dampening=True\n",
    "        grad_avg = torch.mul(grad_avg, mom)\n",
    "        grad_avg = torch.add(grad_avg, grad)\n",
    "\n",
    "        # larc_step\n",
    "        dp = torch.add(dp, grad_avg, alpha=-lr)\n",
    "    else:\n",
    "        # larc_step\n",
    "        dp = torch.add(dp, grad, alpha=-lr)\n",
    "\n",
    "    p.set_(dp)\n",
    "    return {'grad_avg': grad_avg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def Larc(params, lr, mom=0.9, clip=True, trust_coeff=0.02, eps=1e-8, wd=0., decouple_wd=True, jit=False):\n",
    "    \"A `Optimizer` for Adam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n",
    "    if jit:\n",
    "        cb = partial(larc_jit_step, decouple_wd=decouple_wd, clip=clip)\n",
    "        return JitOptimizer(params, cb, lr=lr, mom=mom, trust_coeff=trust_coeff, eps=eps, wd=wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        if mom!=0.: cbs.append(average_grad)\n",
    "        cbs += [partial(larc_layer_lr, clip=clip), larc_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, trust_coeff=trust_coeff, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LARS optimizer was first introduced in [Large Batch Training of Convolutional Networks](https://arxiv.org/abs/1708.03888) then refined in its LARC variant (original LARS is with `clip=False`). A learning rate is computed for each individual layer with a certain `trust_coefficient`, then clipped to be always less than `lr`.\n",
    "\n",
    "Optional weight decay of `wd` is applied, as true weight decay (decay the weights directly) if `decouple_wd=True` else as L2 regularization (add the decay to the gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "params_org = [tst_param([1,2,3], [0.1,0.2,0.3]), tst_param([1,2,3], [0.01,0.02,0.03])]\n",
    "opt_org = Larc(params_org, lr=0.1, jit=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = [tst_param([1,2,3], [0.1,0.2,0.3]), tst_param([1,2,3], [0.01,0.02,0.03])]\n",
    "opt_jit = Larc(params_jit, lr=0.1, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "#First param local lr is 0.02 < lr so it's not clipped\n",
    "test_close(opt_org.state[params_org[0]]['local_lr'], 0.02)\n",
    "#Second param local lr is 0.2 > lr so it's clipped\n",
    "test_eq(opt_org.state[params_org[1]]['local_lr'], 0.1)\n",
    "\n",
    "test_close(params_org[0], tensor([0.998,1.996,2.994]))\n",
    "test_close(params_org[0], params_jit[0])\n",
    "test_close(params_org[1], tensor([0.999,1.998,2.997]))\n",
    "test_close(params_org[1], params_jit[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "params_org = [tst_param([1,2,3], [0.1,0.2,0.3]), tst_param([1,2,3], [0.01,0.02,0.03])]\n",
    "opt_org = Larc(params_org, lr=0.1, clip=False, jit=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = [tst_param([1,2,3], [0.1,0.2,0.3]), tst_param([1,2,3], [0.01,0.02,0.03])]\n",
    "opt_jit = Larc(params_jit, lr=0.1, clip=False, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "#No clipping\n",
    "test_close(opt_org.state[params_org[0]]['local_lr'], 0.02)\n",
    "test_close(opt_org.state[params_org[1]]['local_lr'], 0.2)\n",
    "test_close(params_org[0], tensor([0.998,1.996,2.994]))\n",
    "test_close(params_org[0], params_jit[0])\n",
    "test_close(params_org[1], tensor([0.998,1.996,2.994]))\n",
    "test_close(params_org[1], params_jit[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAMB -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@torch.jit.script\n",
    "def lamb_jit_step(p:Tensor, grad:Tensor, lr:float, wd:float, mom:float, sqr_mom:float, eps:float, \n",
    "                  decouple_wd:bool, grad_avg:Optional[Tensor]=None, sqr_avg:Optional[Tensor]=None, \n",
    "                  do_wd:bool=True, step:int=0, force_train:Optional[bool]=None):\n",
    "    dp = p\n",
    "    step += 1\n",
    "    if do_wd and wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            dp = dp.mul(1 - lr*wd)\n",
    "        else:\n",
    "            # l2_reg\n",
    "            grad = grad.add(dp, alpha=wd)\n",
    "\n",
    "    if grad_avg is None: \n",
    "        grad_avg = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "    if sqr_avg is None: \n",
    "        sqr_avg  = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "\n",
    "    # average_grad, dampening=True\n",
    "    grad_avg = torch.mul(grad_avg, mom)\n",
    "    grad_avg = torch.add(grad_avg, grad, alpha=1-mom)\n",
    "\n",
    "    # average_sqr_grad\n",
    "    sqr_avg = torch.mul(sqr_avg, sqr_mom)\n",
    "    sqr_avg = torch.addcmul(sqr_avg, grad, grad, value=1-sqr_mom)\n",
    "\n",
    "    # lamb_step\n",
    "    debias1 = debias(mom, step)\n",
    "    debias2 = debias(sqr_mom, step)\n",
    "    r1 = dp.pow(2).mean().sqrt()\n",
    "    lstep = (grad_avg/debias1) / ((sqr_avg/debias2).sqrt()+eps)\n",
    "    r2 = lstep.pow(2).mean().sqrt()\n",
    "    if r1 == 0 or r2 == 0:\n",
    "        q = 1.\n",
    "    else:\n",
    "        q = min(r1/r2, 10.)\n",
    "    dp = torch.add(dp,lstep, alpha = -lr * q)\n",
    "\n",
    "    p.set_(dp)\n",
    "\n",
    "    return torch.jit.annotate(Dict[str, Union[Tensor, int]], {'grad_avg': grad_avg, 'sqr_avg': sqr_avg, 'step': step})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def Lamb(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0., decouple_wd=True, jit=False):\n",
    "    \"A `Optimizer` for Adam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n",
    "    if jit:\n",
    "        cb = partial(lamb_jit_step, decouple_wd=decouple_wd)\n",
    "        return JitOptimizer(params, cb, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, lamb_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LAMB was introduced in [Large Batch Optimization for Deep Learning: Training BERT in 76 minutes](https://arxiv.org/abs/1904.00962). Intuitively, it's LARC applied to Adam. As in `Adam`, fastai renamed `beta1` and `beta2` in the paper to `mom` and  `sqr_mom`. Note that the defaults also differ from the paper (0.99 for `sqr_mom` or `beta2`, 1e-5 for `eps`). Those values seem to be better from experiments in a wide range of situations.\n",
    "\n",
    "Optional weight decay of `wd` is applied, as true weight decay (decay the weights directly) if `decouple_wd=True` else as L2 regularization (add the decay to the gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = Lamb(params_org, lr=0.1, jit=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = Lamb(params_jit, lr=0.1, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "test_close(params_org[0], tensor([0.7840,1.7840,2.7840]), eps=1e-3)\n",
    "test_close(params_org[0], params_jit[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookahead -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class JitLookahead(Optimizer):\n",
    "    \"An `Optimizer` with a modified step for Lookahead TorchScript optimizers\"\n",
    "    def __init__(self, params:Tensor, cbs:list, train_bn:bool=True, **defaults):\n",
    "        super().__init__(params, cbs, train_bn, **defaults)\n",
    "        self._init_state()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        self.count += 1\n",
    "        if closure is not None: raise NotImplementedError(\"fastai optimizers currently do not support closure\")\n",
    "        for pg, hyper in zip(self.param_lists, self.hypers):\n",
    "            for p in pg:\n",
    "                if hasattr(p, 'grad') and p.grad is not None:\n",
    "                    _update(self.state[p], self.cbs[0](p, p.grad, **{**self.state[p], **hyper}, count=self.count))\n",
    "\n",
    "    def clear_state(self):\n",
    "        super().clear_state()\n",
    "        self._init_state()\n",
    "\n",
    "    def state_dict(self):\n",
    "        state = super().state_dict()\n",
    "        state.update({'count': self.count})\n",
    "        return state\n",
    "\n",
    "    def load_state_dict(self, sd):\n",
    "        self.count = sd.pop('count')\n",
    "        super().load_state_dict(sd)\n",
    "\n",
    "    def _init_state(self): \n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lookahead was introduced by Zhang et al. in [Lookahead Optimizer: k steps forward, 1 step back](https://arxiv.org/abs/1907.08610). With Lookahead, the final weights (*slow weights*)are a moving average of the normal weights (*fast weights*). Every `k` steps, Lookahead modifieds the current weights by a moving average of the *fast weights* (normal weights) with the *slow weights* (the copy of old weights k steps ago). Those *slow weights* act like a stability mechanism.\n",
    "\n",
    ":::{.callout-important}\n",
    "While fastai's `Lookahead` can be applied to any optimizer, fastxtend's `JitLookahead` must have a custom written TorchScript optimizer callback. Currently `ranger` with `RAdam` is the optimizer with `JitLookahead` support.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ranger -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@torch.jit.script\n",
    "def ranger_jit_step(p:Tensor, grad:Tensor, lr:float, wd:float, mom:float, sqr_mom:float, eps:float, beta:float,\n",
    "                    decouple_wd:bool, count:int, k:int, alpha:float, grad_avg:Optional[Tensor]=None, sqr_avg:Optional[Tensor]=None,\n",
    "                    slow_p:Optional[Tensor]=None, do_wd:bool=True, step:int=0, force_train:Optional[bool]=None):\n",
    "    dp = p\n",
    "    step += 1\n",
    "    if slow_p is None: \n",
    "        slow_p = dp.clone().detach()\n",
    "\n",
    "    if do_wd and wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            dp = dp.mul(1 - lr*wd)\n",
    "        else:\n",
    "            # l2_reg\n",
    "            grad = grad.add(dp, alpha=wd)\n",
    "\n",
    "    if grad_avg is None: \n",
    "        grad_avg = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "    if sqr_avg is None: \n",
    "        sqr_avg  = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "\n",
    "    # average_grad, dampening=True\n",
    "    grad_avg = torch.mul(grad_avg, mom)\n",
    "    grad_avg = torch.add(grad_avg, grad, alpha=1-mom)\n",
    "\n",
    "    # average_sqr_grad\n",
    "    sqr_avg = torch.mul(sqr_avg, sqr_mom)\n",
    "    sqr_avg = torch.addcmul(sqr_avg, grad, grad, value=1-sqr_mom)\n",
    "\n",
    "    # radam_step\n",
    "    debias1 = debias(mom, step)\n",
    "    debias2 = debias(sqr_mom, step)\n",
    "    r_inf = 2/(1-sqr_mom) - 1\n",
    "    r = r_inf - 2*step*sqr_mom**step/(1-sqr_mom**step)\n",
    "    \n",
    "    if r > 5:\n",
    "        v = math.sqrt(((r-4) * (r-2) * r_inf)/((r_inf-4)*(r_inf-2)*r))\n",
    "        denom = torch.sqrt(sqr_avg/debias2)\n",
    "        if eps != 0: \n",
    "            denom = denom + eps\n",
    "        if beta != 0: \n",
    "            denom = F.softplus(denom, beta)\n",
    "        dp = torch.addcdiv(dp, grad_avg, denom, value = -lr*v / debias1)\n",
    "    else:\n",
    "        dp = torch.add(dp, grad_avg, alpha=-lr / debias1)\n",
    "\n",
    "    # lookahead step\n",
    "    if count % k != 0:\n",
    "        p.set_(dp)\n",
    "    else:\n",
    "        slow_p = torch.add(slow_p, torch.sub(dp, slow_p), alpha=alpha)\n",
    "        p.set_(slow_p)\n",
    "    \n",
    "    return torch.jit.annotate(Dict[str, Union[Tensor, int]], {'grad_avg': grad_avg, 'sqr_avg': sqr_avg, 'step': step, 'slow_p': slow_p})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def ranger(params, lr, mom=0.95, sqr_mom=0.99, eps=1e-6, wd=0.01, beta=0., k=6, alpha=0.5, decouple_wd=True, jit=False):\n",
    "    \"Convenience method for `Lookahead` with `RAdam`\"\n",
    "    if jit:\n",
    "        cb = partial(ranger_jit_step, decouple_wd=decouple_wd)\n",
    "        return JitLookahead(params, cb, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd, beta=beta, k=k, alpha=alpha)\n",
    "    else:\n",
    "        return Lookahead(RAdam(params, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd, beta=beta), k=k, alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ranger was introduced by Less Wright [New Deep Learning Optimizer, Ranger: Synergistic combination of RAdam + Lookahead for the best of both.](https://lessw.medium.com/new-deep-learning-optimizer-ranger-synergistic-combination-of-radam-lookahead-for-the-best-of-2dc83f79a48d) It combines RAdam and Lookahead together in one optimizer and reduces the need for hyperparameter tuning due to a combination of RAdam's warmup heuristic and Lookahead's interpolation of parameter weights.\n",
    "\n",
    "Ranger performs best on vision tasks when paired with the `fit_flat_cos` scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "po = tensor([1,2,3])\n",
    "\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = ranger(params_org, lr=0.1, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0., beta=0., jit=False)\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = ranger(params_jit, lr=0.1, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0., beta=0., jit=True)\n",
    "\n",
    "#The first 5 steps are normal RAdam steps\n",
    "#The r factor is lower than 5 during the first 5 steps so updates use the average of gradients (all the same)\n",
    "r_inf = 2/(1-0.99) - 1\n",
    "for i in range(5): \n",
    "    r = r_inf - 2*(i+1)*0.99**(i+1)/(1-0.99**(i+1))\n",
    "    assert r <= 5\n",
    "    opt_org.step()\n",
    "    opt_jit.step()\n",
    "p = tensor([0.95, 1.9, 2.85])\n",
    "test_close(params_org[0], p)\n",
    "test_close(params_org[0], params_jit[0])\n",
    "\n",
    "#The r factor is greater than 5 for the sixth step so we update with RAdam\n",
    "r = r_inf - 2*6*0.99**6/(1-0.99**6)\n",
    "assert r > 5\n",
    "opt_org.step()\n",
    "opt_jit.step()\n",
    "v = math.sqrt(((r-4) * (r-2) * r_inf)/((r_inf-4)*(r_inf-2)*r))\n",
    "step = -0.1*0.1*v/(math.sqrt(0.1**2) + 1e-8)\n",
    "\n",
    "#Since k=6, sixth step is a moving average of the 6 RAdam steps with the initial weight\n",
    "test_close(params_org[0], po+((p+step)-po)*0.5)\n",
    "test_close(params_org[0], params_jit[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('fastai')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
