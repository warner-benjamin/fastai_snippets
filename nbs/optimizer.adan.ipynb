{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp optimizer.adan\n",
    "#|default_cls_lvl 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "# Memory and operations reduction ported from the official Adan implementation\n",
    "# https://github.com/sail-sg/Adan - Apache License 2.0 - Copyright 2022 Xingyu Xie et al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adan: ADAptive Nesterov Momentum Optimizer\n",
    "> With fastai native, fused ForEach, and fused TorchScript implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "from typing import Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from fastai.optimizer import Optimizer\n",
    "\n",
    "from fastxtend.optimizer.foreach import ForEachOptimizer\n",
    "from fastxtend.optimizer.torchscript import JitOptimizer\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastxtend.test_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def debias(beta:float, step:int):\n",
    "    \"Simple debias calculation\"\n",
    "    return 1-beta**step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adan Fastai Callbacks -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def adan_setup(p:Tensor, step:int=0, grad_avg:Tensor|None=None, diff_avg:Tensor|None=None,\n",
    "               sqr_avg:Tensor|None=None, prior_grad:Tensor|None=None, paper_init:bool=False, **kwargs):\n",
    "    \"Handles Adan setup and keeps track of steps\"\n",
    "    if step == 0:\n",
    "        grad_avg = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "        diff_avg = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "        sqr_avg  = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "        if paper_init:\n",
    "            prior_grad = p.grad.clone().mul_(-1)\n",
    "        else:\n",
    "            prior_grad = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "        step += 1\n",
    "        return {'grad_avg':grad_avg, 'diff_avg':diff_avg, 'sqr_avg':sqr_avg, 'prior_grad':prior_grad, 'step':step}\n",
    "    else:\n",
    "        step += 1\n",
    "        return {'step':step}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def adan_step(p:Tensor, lr:float, eps:float, wd:float, beta1:float, beta2:float, beta3:float,\n",
    "              step:int, grad_avg:Tensor, diff_avg:Tensor, sqr_avg:Tensor, prior_grad:Tensor,\n",
    "              do_wd:bool=True, **kwargs):\n",
    "    \"Updates Adan moving averages and performs the Adan step with `lr` on `p`\"\n",
    "\n",
    "    # difference between current and previous gradients, prior_grad is negated in last step\n",
    "    prior_grad = torch.add(p.grad.data, prior_grad)\n",
    "\n",
    "    # update m_k\n",
    "    grad_avg.mul_(beta1).add_(p.grad.data, alpha=1-beta1)\n",
    "\n",
    "    # update v_k\n",
    "    diff_avg.mul_(beta2).add_(prior_grad, alpha=1-beta2)\n",
    "\n",
    "    # update n_k\n",
    "    prior_grad = torch.add(p.grad.data, prior_grad, alpha=beta2)\n",
    "    sqr_avg.mul_(beta3).addcmul_(prior_grad, prior_grad, value=1-beta3)\n",
    "\n",
    "    # calculate debias terms\n",
    "    db1 = 1/debias(beta1, step)\n",
    "    db2 = beta2/debias(beta2, step)\n",
    "    db3 = lr*math.sqrt(debias(beta3, step))\n",
    "\n",
    "    # calculate applied λ\n",
    "    wd = (1+lr*wd) if wd!=0 and do_wd else 1\n",
    "\n",
    "    # calculate η_k\n",
    "    lrs = torch.sqrt(sqr_avg).div(db3).add(eps)\n",
    "\n",
    "    # perform Adan step and apply to parameter `p`\n",
    "    p.data.addcdiv_(grad_avg, lrs, value=-db1)\n",
    "    p.data.addcdiv_(diff_avg, lrs, value=-db2)\n",
    "    p.data.div_(wd)\n",
    "\n",
    "    # set next step's prior_grad as negated current grad\n",
    "    prior_grad = p.grad.data.clone().mul_(-1)\n",
    "    return {'grad_avg':grad_avg, 'diff_avg':diff_avg, 'sqr_avg':sqr_avg, 'prior_grad':prior_grad}\n",
    "\n",
    "adan_step.defaults = dict(beta1=0.98, beta2=0.92, beta3=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adan Jit -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@torch.jit.script\n",
    "def adan_jit_step(p:Tensor, g:Tensor, lr:float, wd:float, beta1:float, beta2:float, beta3:float, eps:float,\n",
    "                  paper_init:bool, grad_avg:Optional[Tensor]=None, diff_avg:Optional[Tensor]=None,\n",
    "                  sqr_avg:Optional[Tensor]=None, prior_grad:Optional[Tensor]=None, do_wd:bool=True, step:int=0,\n",
    "                  force_train:Optional[bool]=None, mom:Optional[float]=None, decouple_wd:bool=False):\n",
    "    dp = p\n",
    "    grad = g\n",
    "    step += 1\n",
    "\n",
    "    if grad_avg is None:\n",
    "        grad_avg = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "    if diff_avg is None:\n",
    "        diff_avg = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "    if sqr_avg is None:\n",
    "        sqr_avg = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "    if prior_grad is None:\n",
    "        if paper_init:\n",
    "            prior_grad = grad.clone()\n",
    "        else:\n",
    "            prior_grad = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "\n",
    "    # difference between current and previous gradients\n",
    "    diff_grad = grad.sub(prior_grad)\n",
    "\n",
    "    # update m_k\n",
    "    grad_avg = grad_avg.mul(beta1).add(grad, alpha=1-beta1)\n",
    "\n",
    "    # update v_k\n",
    "    diff_avg = diff_avg.mul(beta2).add(diff_grad, alpha=1-beta2)\n",
    "\n",
    "    # update n_k\n",
    "    adjusted_grad = grad.add(diff_grad, alpha=beta2)\n",
    "    sqr_avg = sqr_avg.mul(beta3).addcmul(adjusted_grad, adjusted_grad, value=1-beta3)\n",
    "\n",
    "    # calculate debias terms\n",
    "    db1 = 1-beta1**step\n",
    "    db2 = 1-beta2**step\n",
    "    db3 = math.sqrt(1-beta3**step)\n",
    "\n",
    "    # calculate applied λ\n",
    "    if wd!=0 and do_wd:\n",
    "        wd = (1+lr*wd)\n",
    "    else:\n",
    "        wd = 1.\n",
    "\n",
    "    # calculate η_k\n",
    "    lrs = lr/torch.sqrt(sqr_avg).div(db3).add(eps)\n",
    "\n",
    "    # perform Adan step\n",
    "    dp = dp.sub(torch.add(grad_avg.div(db1), diff_avg.div(db2), alpha=beta2).mul(lrs)).div(wd)\n",
    "\n",
    "    # set next step's prior_grad as negated current grad\n",
    "    prior_grad = grad.clone()\n",
    "\n",
    "    # apply results to parameter p\n",
    "    p.set_(dp)\n",
    "    g.set_(grad)\n",
    "\n",
    "    return torch.jit.annotate(Dict[str, Union[Tensor, int]], {'grad_avg':grad_avg, 'diff_avg':diff_avg, 'sqr_avg':sqr_avg, 'prior_grad':prior_grad, 'step':step})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adan ForEach -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def adan_foreach_step(p:list[Tensor], grad:list[Tensor], grad_avg:list[Tensor], diff_avg:list[Tensor],\n",
    "                      sqr_avg:list[Tensor], prior_grad:list[Tensor], steps:np.ndarray[Any, int],\n",
    "                      do_wd:np.ndarray[Any, bool], lr:float, wd:float, beta1:float, beta2:float,\n",
    "                      beta3:float, eps:float, **kwargs):\n",
    "\n",
    "    # difference between current and previous gradients, prior_grad is negated in last step\n",
    "    torch._foreach_add_(prior_grad, grad)\n",
    "\n",
    "    # update m_k\n",
    "    torch._foreach_mul_(grad_avg, beta1)\n",
    "    torch._foreach_add_(grad_avg, grad, alpha=1-beta1)\n",
    "\n",
    "    # update v_k\n",
    "    torch._foreach_mul_(diff_avg, beta2)\n",
    "    torch._foreach_add_(diff_avg, prior_grad, alpha=1-beta2)\n",
    "\n",
    "    # update n_k\n",
    "    torch._foreach_mul_(prior_grad, scalar=beta2)\n",
    "    torch._foreach_add_(prior_grad, grad)\n",
    "    torch._foreach_mul_(sqr_avg, beta3)\n",
    "    torch._foreach_addcmul_(sqr_avg, prior_grad, prior_grad, value=1-beta3)\n",
    "\n",
    "    # calculate debias terms\n",
    "    db1 = -1/(1 - beta1**steps)\n",
    "    db2 = -beta2/(1 - beta2**steps)\n",
    "    db3 = lr*np.sqrt(1 - beta3**steps)\n",
    "\n",
    "    # calculate η_k\n",
    "    lrs = torch._foreach_sqrt(sqr_avg)\n",
    "    torch._foreach_div_(lrs, scalars=db3.tolist())\n",
    "    torch._foreach_add_(lrs, scalar=eps)\n",
    "\n",
    "    # perform Adan step\n",
    "    torch._foreach_addcdiv_(p, grad_avg, lrs, scalars=db1.tolist())\n",
    "    torch._foreach_addcdiv_(p, diff_avg, lrs, scalars=db2.tolist())\n",
    "\n",
    "    # calculate and apply λ\n",
    "    if wd != 0:\n",
    "        wd = np.where(do_wd, 1+lr*wd, 1.)\n",
    "        torch._foreach_div_(p, scalars=wd.tolist())\n",
    "\n",
    "    # set next step's prior_grad as negated current grad\n",
    "    torch._foreach_zero_(prior_grad)\n",
    "    torch._foreach_add_(prior_grad, grad, alpha=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class AdanForEachOptimizer(ForEachOptimizer):\n",
    "    \"An `Optimizer` with a modified step for Adan ForEach\"\n",
    "    def __init__(self,\n",
    "        params:Listified[Tensor], # Model parameters\n",
    "        opt_step:Callable, # `ForEachOptimizer` optimizer step\n",
    "        paper_init:bool=False, # Initialize first prior_grad to grad following paper or zeros\n",
    "        **defaults # Optimizer specific hyper parameters\n",
    "    ):\n",
    "        super().__init__(params, opt_step, **defaults)\n",
    "        self.paper_init = paper_init\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        if closure is not None: raise NotImplementedError(\"fastai optimizers currently do not support closure\")\n",
    "        for pg, hyper in zip(self.param_lists, self.hypers):\n",
    "            pl, gl, grad_avg, diff_avg, sqr_avg, prior_grad, steps, do_wd = [], [], [], [], [], [], [], []\n",
    "\n",
    "            for p in pg:\n",
    "                if hasattr(p, 'grad') and p.grad is not None:\n",
    "                    state = self.state[p]\n",
    "\n",
    "                    if 'step' not in state:\n",
    "                        state['grad_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        state['diff_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        state['sqr_avg']  = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        if self.paper_init:\n",
    "                            state['prior_grad'] = p.grad.clone().mul_(-1)\n",
    "                        else:\n",
    "                            state['prior_grad'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        state['step'] = 0\n",
    "\n",
    "                    state['step'] += 1\n",
    "                    pl.append(p)\n",
    "                    gl.append(p.grad)\n",
    "                    grad_avg.append(state['grad_avg'])\n",
    "                    diff_avg.append(state['diff_avg'])\n",
    "                    sqr_avg.append(state['sqr_avg'])\n",
    "                    prior_grad.append(state['prior_grad'])\n",
    "                    do_wd.append(state.get('do_wd', True))\n",
    "                    steps.append(state['step'])\n",
    "\n",
    "            self.opt_step(p=pl, grad=gl, grad_avg=grad_avg, diff_avg=diff_avg, sqr_avg=sqr_avg,\n",
    "                          prior_grad=prior_grad, steps=np.array(steps, dtype=np.int32), do_wd=np.array(do_wd, dtype=bool), **hyper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adan was introduced by Xie et al in *[Adan: Adaptive Nesterov momentum Algorithm for Faster Optimizing Deep Models](https://arxiv.org/abs/2208.06677)*. Adan uses a efficient Nesterov momentum estimation method to avoid the extra computation and memory overhead of calculating the extrapolation point gradient. \n",
    "\n",
    "[Nadam](https://openreview.net/forum?id=OM0jvwB8jIp57ZJjtNEZ) also estimates Nesterov momentum, but in contrast it only estimates the first-order gradient moment while Adan estimates both first- and second-order movements.\n",
    "\n",
    "For consistency with other fastai optimizers, the coefficients `beta1`, `beta2`, and `beta3` have been inversed from the paper values, e.g. `β1=0.98` instead of `β1=0.02`.\n",
    "\n",
    ":::{.callout-note}\n",
    "This implementation of `Adan` does not contain the restart condition, as it is mostly unused in the paper.\n",
    ":::\n",
    "\n",
    "In addition to a fastai native implementation, `Adan` has fused ForEach and Torchscript implementations. See the [Fused Optimizer](optimizer.fused.html) documentation for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adan -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def Adan(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    beta1:float=0.98, # Gradient moving average (β1) coefficient\n",
    "    beta2:float=0.92, # Gradient difference moving average (β2) coefficient\n",
    "    beta3:float=0.99, # Gradient squared moving average (β3) coefficient\n",
    "    eps:float=1e-8, # Added for numerical stability\n",
    "    wd:float=0.02, # True weight decay\n",
    "    paper_init:bool=False, # Initialize prior gradient with current gradient per paper, or zeroes\n",
    "    foreach:bool=False, # Use fused ForEach implementation\n",
    "    jit:bool=False # Use fused TorchScript implementation\n",
    ") -> Optimizer|AdanForEachOptimizer|JitOptimizer:\n",
    "    \"A fastai Adan optimizer with optional ForEach and TorchScript implementations\"\n",
    "    if foreach:\n",
    "        return AdanForEachOptimizer(params, adan_foreach_step, lr=lr, beta1=beta1, beta2=beta2,\n",
    "                                    beta3=beta3, eps=eps, wd=wd, paper_init=paper_init)\n",
    "    elif jit:\n",
    "        cb = partial(adan_jit_step, paper_init=paper_init)\n",
    "        return JitOptimizer(params, cb, lr=lr, beta1=beta1, beta2=beta2, beta3=beta3, eps=eps, wd=wd)\n",
    "    else:\n",
    "        cbs = [partial(adan_setup, paper_init=paper_init), adan_step]\n",
    "        return Optimizer(params, cbs, lr=lr, beta1=beta1, beta2=beta2, beta3=beta3, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def adan(\n",
    "    beta1:float=0.98, # Gradient moving average (β1) coefficient\n",
    "    beta2:float=0.92, # Gradient difference moving average (β2) coefficient\n",
    "    beta3:float=0.99, # Gradient squared moving average (β3) coefficient\n",
    "    eps:float=1e-8, # Added for numerical stability\n",
    "    wd:float=0.02, # True weight decay\n",
    "    paper_init:bool=False, # Initialize prior gradient with current gradient per paper, or zeroes\n",
    "    foreach:bool=False, # Use fused ForEach implementation\n",
    "    jit:bool=False # Use fused TorchScript implementation\n",
    ") -> Optimizer|AdanForEachOptimizer|JitOptimizer:\n",
    "    \"Partial function for the Adan optimizer with fused ForEach and TorchScript implementations\"\n",
    "    return partialler(Adan, beta1=beta1, beta2=beta2, beta3=beta3, eps=eps, wd=wd,\n",
    "                      paper_init=paper_init, foreach=foreach, jit=jit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "test_steps = 25\n",
    "\n",
    "def tst_param(val, grad=None):\n",
    "    \"Create a tensor with `val` and a gradient of `grad` for testing\"\n",
    "    res = tensor([val]).float()\n",
    "    res.grad = tensor([val/10 if grad is None else grad]).float()\n",
    "    return res\n",
    "\n",
    "def tst_params():\n",
    "    r = L.range(4)\n",
    "    return r.map(tst_param)\n",
    "\n",
    "params_org = tst_params()\n",
    "opt_org = Adan(params_org, lr=0.1, paper_init=True)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = tst_params()\n",
    "opt_jit = Adan(params_jit, lr=0.1, jit=True, paper_init=True)\n",
    "opt_jit.step()\n",
    "\n",
    "params_for = tst_params()\n",
    "opt_for = Adan(params_for, lr=0.1, foreach=True, paper_init=True)\n",
    "opt_for.step()\n",
    "\n",
    "# Test values from paper implementation: https://github.com/sail-sg/Adan\n",
    "test_close([p.item() for p in params_org], [0., 0.8982, 1.8962, 2.8942], eps=1e-4)\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_jit])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_for])\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_jit.step()\n",
    "    opt_for.step()\n",
    "\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_jit])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_for])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdanLargeBatchLR -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def AdanLargeBatchLR(bs:int) -> float:\n",
    "    \"Square root rule for scaling `Adan` learning rate for large-batch training\"\n",
    "    return math.sqrt(bs/256)*6.25e-3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Hyperparameter notes from Xie et al:\n",
    "\n",
    "1. `beta2` is the least sensitive Adan hyperparameter, default of 0.92 works for majority of tasks\n",
    "2. Xie et al primarily tune `beta3` (between 0.9-0.999) before `beta1` (between 0.9-0.98) for different tasks\n",
    "3. Adan pairs well with large learning rates. Paper and GitHub report up to 3x larger than `Lamb` and up to 5-10x larger than `Adam`\n",
    "4. Xie et al use the default weight decay of 0.02 for all tasks except fine-tuning BERT (`wd=0.01`) and reinforcement learning (`wd=0`)\n",
    "\n",
    ":::{.callout-note}\n",
    "With `paper_init=True`, fastxtend's `Adan` matches Xie et al's [Adan implementation](https://github.com/sail-sg/Adan).\n",
    ":::\n",
    "\n",
    "## Training Speed\n",
    "\n",
    ":::{.callout-important}\n",
    "ForEach and TorchScript optimizers have only been tested on PyTorch 1.12+ and are not guaranteed to work on older versions.\n",
    ":::\n",
    "\n",
    "One [critique of Adan](https://wandb.ai/capecape/adan_optimizer/reports/Adan-A-New-Optimizer-That-Challenges-Adam--VmlldzoyNTQ5NjQ5#conclusions) is the original PyTorch implementation was significantly slower than AdamW. Between 41 to 97 percent slower on tested models. However, Xie et al's implementation has been refactored to decrease memory usage and Cuda operations. These improvements have been ported to the fastxtend versions. This improved Adan implementation is benchmarked in @tbl-first below.\n",
    "\n",
    "As shown in @tbl-fused, fastxtend's fused ForEach Adan is 36 to 401 percent faster[^faster] then a standard PyTorch implementation.\n",
    "\n",
    "| Model             | Layers | Native Step | ForEach Step | ForEach Speedup | JIT Step | JIT Speedup |\n",
    "| ----------------- | :----: | :---------: | :----------: | :-------------: | :------: | :---------: |\n",
    "| XResNet18         | 1      | 31ms        | 13ms         | 150%            | 27ms     | 18%         |\n",
    "| XResNet50         | 1      | 69ms        | 33ms         | 108%            | 56ms     | 24%         |\n",
    "| XSE-ResNext50     | 1      | 94ms        | 45ms         | 110%            | 75ms     | 25%         |\n",
    "| XResNet101        | 1      | 115ms       | 46ms         | 148%            | 89ms     | 29%         |\n",
    "| ConvNext Tiny     | 2      | 139ms       | 102ms        | 36%             | 124ms    | 11%         |\n",
    "| ConvNext Small    | 2      | 225ms       | 162ms        | 39%             | 198ms    | 13%         |\n",
    "| ViT Patch16 Small | 2      | 76ms        | 46ms         | 65%             | 62ms     | 21%         |\n",
    "| DeBERTa Base      | 1      | 42ms        | 8.4ms        | 401%            | 28ms     | 46%         |\n",
    "\n",
    ": Increase in Adan `opt_step` Speed vs Native Optimizer {#tbl-fused}\n",
    "\n",
    "Now the Adan ForEach steps are only 1.6 to 17 percent slower than the AdamW ForEach steps, and the difference in performance and as a percentage of total training time is significantly smaller. An Adan ForEach step is 0.6ms to 4ms slower than an AdamW ForEach step across measured models, instead of 6ms to 29ms with Adan native as shown in @tbl-panel.\n",
    "\n",
    "::: {#tbl-panel layout-ncol=2}\n",
    "| Model             | AdamW Step | Adan Step | Slowdown |\n",
    "| :---------------: | :--------: | :-------: | :------: |\n",
    "| XResNet18         | 25ms       | 31ms      | 27%      |\n",
    "| XResNet50         | 53ms       | 69ms      | 28%      |\n",
    "| XSE-ResNext50     | 71ms       | 94ms      | 35%      |\n",
    "| XResNet101        | 85ms       | 115ms     | 33%      |\n",
    "| ConvNext Tiny     | 124ms      | 139ms     | 12%      |\n",
    "| ConvNext Small    | 196ms      | 225ms     | 15%      |\n",
    "| ViT Patch16 Small | 63ms       | 76ms      | 20%      |\n",
    "| DeBERTa Base      | 26ms       | 42ms      | 62%      |\n",
    "\n",
    ": Native Implementation {#tbl-first}\n",
    "\n",
    "| Model             | AdamW Step | Adan Step | Slowdown |\n",
    "| :---------------: | :--------: | :-------: | :------: |\n",
    "| XResNet18         | 13ms       | 13ms      | 5.0%     |\n",
    "| XResNet50         | 31ms       | 33ms      | 2.3%     |\n",
    "| XSE-ResNext50     | 43ms       | 45ms      | 5.9%     |\n",
    "| XResNet101        | 43ms       | 46ms      | 8.4%     |\n",
    "| ConvNext Tiny     | 100ms      | 102ms     | 1.6%     |\n",
    "| ConvNext Small    | 159ms      | 162ms     | 1.6%     |\n",
    "| ViT Patch16 Small | 44ms       | 46ms      | 2.9%     |\n",
    "| DeBERTa Base      | 7.2ms      | 8.4ms     | 17%      |\n",
    "\n",
    ": Fused ForEach Implementation {#tbl-second}\n",
    "\n",
    "AdamW vs Adan Training Speed\n",
    ":::\n",
    "\n",
    "[^faster]: Benchmarked on a GeForce 3080 Ti using PyTorch 1.13.1 on Cuda 11.7, Mixed Precision, [Channels Last](callback.channelslast.html) (except DeBERTa and ViT), and fastxtend's [Simple Profiler Callback](callback.simpleprofiler.html). Results may differ on other models, hardware, and across benchmarking runs. Speedup and slowdown are calculated from the total time spent on the optimization step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
