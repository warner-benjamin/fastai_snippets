{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp multiloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "from types import FunctionType\n",
    "\n",
    "from torch.distributions.beta import Beta\n",
    "\n",
    "from fastai.callback.core import Callback\n",
    "from fastai.learner import Recorder\n",
    "from fastai.layers import NoneReduce\n",
    "\n",
    "from fastxtend.basics import is_listish\n",
    "from fastxtend.metrics import AvgLossX, AvgSmoothLossX\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastxtend.test_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiLoss\n",
    "> A loss wrapper and callback to calculate and log individual losses as fastxtend metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def init_loss(l, **kwargs):\n",
    "    \"Initiatize loss class or partial loss function\"\n",
    "    return partialler(l, reduction='none') if isinstance(l, FunctionType) else l(reduction='none', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class MultiLoss(Module):\n",
    "    \"\"\"\n",
    "    Combine multiple `loss_funcs` on one prediction & target via `reduction`, with optional weighting. \n",
    "    \n",
    "    Log `loss_funcs` as metrics via `MultiLossCallback`, optionally using `loss_names`.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "        loss_funcs:listy[Callable[...,nn.Module]|FunctionType], # Uninitialized loss functions or classes. Must support PyTorch `reduction` string.\n",
    "        weights:listified[Number]|None=None, # Weight per loss. Defaults to uniform weighting.\n",
    "        loss_kwargs:listy[dict[str,Any]]|None=None, # kwargs to pass to each loss function. Defaults to None.\n",
    "        loss_names:listy[str]|None=None, # Loss names to log using `MultiLossCallback`. Defaults to loss `__name__`.\n",
    "        reduction:str|None='mean' # PyTorch loss reduction\n",
    "    ):\n",
    "        store_attr(but='loss_names')\n",
    "        assert is_listy(loss_funcs), \"`loss_funcs` must be list-like\"\n",
    "        \n",
    "        if weights is None or len(weights)==0: \n",
    "            self.weights = [1]*len(loss_funcs)\n",
    "        else:\n",
    "            assert len(loss_funcs) == len(weights), \"Must provide same number of `weights` as `loss_funcs`\"\n",
    "            self.weights = weights\n",
    "\n",
    "        if loss_kwargs is None or len(loss_kwargs)==0: loss_kwargs = [{}]*len(loss_funcs)\n",
    "        else: assert len(loss_funcs) == len(loss_kwargs), \"Must provide same number of `loss_kwargs` as `loss_funcs`\"\n",
    "\n",
    "        if loss_names is None or len(loss_names)==0: loss_names = [l.__name__ for l in loss_funcs]\n",
    "        else: assert len(loss_funcs) == len(loss_names), \"Must provide same number of `loss_names` as `loss_funcs`\"\n",
    "\n",
    "        self.loss_funcs = [init_loss(l, **k) for l, k in zip(loss_funcs, loss_kwargs)]\n",
    "        self.loss_names = L(loss_names)\n",
    "        self._reduction,self._loss = reduction,{}\n",
    "        \n",
    "        for loss in self.loss_funcs:\n",
    "            if getattr(loss, 'y_int', False):\n",
    "                self.y_int = True\n",
    "\n",
    "    def forward(self, pred, targ):\n",
    "        for i, loss_func in enumerate(self.loss_funcs):\n",
    "            l = self.weights[i]*loss_func(pred, targ)\n",
    "            if i == 0: loss = torch.zeros_like(targ).float()\n",
    "            loss += l\n",
    "            self._loss[i] = l\n",
    "\n",
    "        return loss.mean() if self._reduction=='mean' else loss.sum() if self._reduction=='sum' else loss\n",
    "\n",
    "    def forward_mixup(self, pred, targ1, targ2, lam):\n",
    "        for i, loss_func in enumerate(self.loss_funcs):\n",
    "            l = self.weights[i]*torch.lerp(loss_func(pred, targ1), loss_func(pred, targ2), lam)\n",
    "            if i == 0: loss = torch.zeros_like(targ1).float()\n",
    "            loss += l\n",
    "            self._loss[i] = l\n",
    "\n",
    "        return loss.mean() if self._reduction=='mean' else loss.sum() if self._reduction=='sum' else loss\n",
    "\n",
    "    @property\n",
    "    def losses(self): return self._loss\n",
    "\n",
    "    @property\n",
    "    def reduction(self): return self._reduction\n",
    "\n",
    "    @reduction.setter\n",
    "    def reduction(self, r): self._reduction = r\n",
    "\n",
    "    @delegates(Module.to)\n",
    "    def to(self, *args, **kwargs):\n",
    "        device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(*args, **kwargs)\n",
    "        if is_listish(self.weights) or not isinstance(self.weights, torch.Tensor): self.weights = torch.Tensor(self.weights)\n",
    "        if self.weights.device != device: self.weights = self.weights.to(device=device)\n",
    "        super().to(*args, **kwargs)\n",
    "\n",
    "    def activation(self, pred):\n",
    "        \"Returns first `loss_funcs` `activation`\"\n",
    "        return getattr(self.loss_funcs[0], 'activation', noop)(pred)\n",
    "\n",
    "    def decodes(self, pred):\n",
    "        \"Returns first `loss_funcs` `decodes`\"\n",
    "        return getattr(self.loss_funcs[0], 'decodes', noop)(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MultiLoss` is a simple multiple loss wrapper which allows logging each individual loss automatically using the `MultiLossCallback`.\n",
    "\n",
    "Pass uninitialized loss functions to `loss_funcs`, optional per loss weighting via `weights`, any loss arguments via a list of dictionaries in `loss_kwargs`, and optional names for each individual loss via `loss_names`.\n",
    "\n",
    "If passed, `weights`, `loss_kwargs`, & `loss_names` must be an iterable of the same length as `loss_funcs`.\n",
    "\n",
    "Output from each loss function must be the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "losses = [nn.MSELoss, nn.L1Loss]\n",
    "multiloss = MultiLoss(loss_funcs=losses)\n",
    "\n",
    "output = torch.sigmoid(torch.randn(32, 5, 10))\n",
    "target = torch.randint(0,2,(32, 5, 10))\n",
    "\n",
    "with torch.no_grad():\n",
    "    ml = multiloss(output, target)\n",
    "    for i, l in enumerate(losses):\n",
    "        test_close(l()(output, target), multiloss.losses[i].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastai.losses import FocalLoss\n",
    "\n",
    "losses = [nn.CrossEntropyLoss, FocalLoss]\n",
    "multiloss = MultiLoss(loss_funcs=losses)\n",
    "\n",
    "output = torch.randn(32, 5, 128, 128)\n",
    "target = torch.randint(0, 5, (32, 128, 128))\n",
    "\n",
    "with torch.no_grad():\n",
    "    ml = multiloss(output, target)\n",
    "    for i, l in enumerate(losses):\n",
    "        test_close(l()(output, target), multiloss.losses[i].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiTargetLoss -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class MultiTargetLoss(MultiLoss):\n",
    "    \"\"\"\n",
    "    Combine `loss_funcs` from multiple predictions & targets via `reduction`, with optional weighting.\n",
    "    \n",
    "    Log `loss_funcs` as metrics via `MultiLossCallback`, optionally using `loss_names`.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "        loss_funcs:listy[Callable[...,nn.Module]|FunctionType], # Uninitialized loss functions or classes. One per prediction and target. Must support PyTorch `reduction` string.\n",
    "        weights:listified[Number]|None=None, # Weight per loss. Defaults to uniform weighting.\n",
    "        loss_kwargs:listy[dict[str,Any]]|None=None, # kwargs to pass to each loss function. Defaults to None.\n",
    "        loss_names:listy[str]|None=None, # Loss names to log using `MultiLossCallback`. Defaults to loss `__name__`.\n",
    "        reduction:str|None='mean' # PyTorch loss reduction\n",
    "    ):\n",
    "        super().__init__(loss_funcs, weights, loss_kwargs, loss_names, reduction)\n",
    "\n",
    "    def forward(self, preds, targs):\n",
    "        for i, (loss_func, pred, targ) in enumerate(zip(self.loss_funcs, preds, targs)):\n",
    "            l = TensorBase(self.weights[i]*loss_func(pred, targ))\n",
    "            if i == 0: loss = TensorBase(torch.zeros_like(targ)).float()\n",
    "            loss += l\n",
    "            self._loss[i] = l\n",
    "\n",
    "        return loss.mean() if self._reduction=='mean' else loss.sum() if self._reduction=='sum' else loss\n",
    "\n",
    "    def forward_mixup(self, **kwargs):\n",
    "        raise NotImplementedError(\"Mixup doesn't support Multi-Target training\")\n",
    "\n",
    "    def activation(self, preds):\n",
    "        \"Returns list of `activation`\"\n",
    "        return [getattr(self.loss_funcs[i], 'activation', noop)(pred) for i, pred in enumerate(preds)]\n",
    "\n",
    "    def decodes(self, preds):\n",
    "        \"Returns list of `decodes`\"\n",
    "        return [getattr(self.loss_funcs[i], 'decodes', noop)(pred) for i, pred in enumerate(preds)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MultiTargetLoss` a single loss per multiple target version of `Multiloss`. It is a simple multiple loss wrapper which allows logging each individual loss automatically using the `MultiLossCallback`.\n",
    "\n",
    "Pass uninitialized loss functions to `loss_funcs`, optional per loss weighting via `weights`, any loss arguments via a list of dictionaries in `loss_kwargs`, and optional names for each individual loss via `loss_names`.\n",
    "\n",
    "If passed, `weights`, `loss_kwargs`, & `loss_names` must be an iterable of the same length as `loss_funcs`.\n",
    "\n",
    "Output from each loss function must be the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "losses = [nn.MSELoss, nn.L1Loss]\n",
    "multitargloss = MultiTargetLoss(loss_funcs=losses)\n",
    "\n",
    "outputs = [torch.sigmoid(torch.randn(32, 5, 10)),torch.sigmoid(torch.randn(32, 5, 10))]\n",
    "targets = [torch.randint(0,2,(32, 5, 10)),torch.randint(0,2,(32, 5, 10))]\n",
    "\n",
    "with torch.no_grad():\n",
    "    ml = multitargloss(outputs, targets)\n",
    "    for i, (l, out, targ) in enumerate(zip(losses, outputs, targets)):\n",
    "        test_close(l()(out, targ), multitargloss.losses[i].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastai.losses import FocalLoss\n",
    "\n",
    "losses = [nn.CrossEntropyLoss, FocalLoss]\n",
    "multitargloss = MultiTargetLoss(loss_funcs=losses)\n",
    "\n",
    "outputs = [torch.randn(32, 5, 128, 128), torch.randn(32, 5, 128, 128)]\n",
    "targets = [torch.randint(0, 5, (32, 128, 128)), torch.randint(0, 5, (32, 128, 128))]\n",
    "\n",
    "with torch.no_grad():\n",
    "    ml = multitargloss(outputs, targets)\n",
    "    for i, (l, out, targ) in enumerate(zip(losses, outputs, targets)):\n",
    "        test_close(l()(out, targ), multitargloss.losses[i].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiloss Metrics -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class MultiAvgLoss(AvgLossX):\n",
    "    \"Average the MultiLoss losses taking into account potential different batch sizes\"\n",
    "    def __init__(self, \n",
    "        i, # `Multiloss` loss function location\n",
    "        name, # Loss function name\n",
    "        reduction:str|None='mean' # Override loss reduction for logging\n",
    "    ):\n",
    "        store_attr(but='name')\n",
    "        self._name = name\n",
    "    \n",
    "    def accumulate(self, learn):\n",
    "        bs = find_bs(learn.yb)\n",
    "        loss = learn.loss_func.losses[self.i]\n",
    "        loss = loss.mean() if self.reduction=='mean' else loss.sum() if self.reduction=='sum' else loss\n",
    "        self.total += learn.to_detach(loss)*bs\n",
    "        self.count += bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class MultiAvgSmoothLoss(AvgSmoothLossX):\n",
    "    \"Smooth average of the MultiLoss losses (exponentially weighted with `beta`)\"\n",
    "    def __init__(self, \n",
    "        i, # `Multiloss` loss function location\n",
    "        name, # Loss function name\n",
    "        beta:float=0.98, # Smoothing beta\n",
    "        reduction:str|None='mean' # Override loss reduction for logging\n",
    "    ):\n",
    "        super().__init__()\n",
    "        store_attr(but='name')\n",
    "        self._name = name\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        self.count += 1\n",
    "        loss = learn.loss_func.losses[self.i]\n",
    "        loss = loss.mean() if self.reduction=='mean' else loss.sum() if self.reduction=='sum' else loss\n",
    "        self.val = torch.lerp(to_detach(loss, gather=False), self.val, self.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class MultiAvgSmoothLossMixup(AvgSmoothLossX):\n",
    "    \"Smooth average of the MultiLoss losses (exponentially weighted with `beta`)\"\n",
    "    def __init__(self, \n",
    "        i, # `Multiloss` loss function location\n",
    "        name, # Loss function name\n",
    "        beta:float=0.98, # Smoothing beta\n",
    "        reduction:str|None='mean' # Override loss reduction for logging\n",
    "    ):\n",
    "        super().__init__()\n",
    "        store_attr(but='name')\n",
    "        self._name = name\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        self.count += 1\n",
    "        loss = learn.loss_func_mixup.losses[self.i]\n",
    "        loss = loss.mean() if self.reduction=='mean' else loss.sum() if self.reduction=='sum' else loss\n",
    "        self.val = torch.lerp(to_detach(loss, gather=False), self.val, self.beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MixHandlerX -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class MixHandlerX(Callback):\n",
    "    \"A handler class for implementing `MixUp` style scheduling. Like fastai's `MixHandler` but supports `MultiLoss`.\"\n",
    "    run_valid = False\n",
    "    def __init__(self,\n",
    "        alpha:float=0.5, # Alpha & beta parametrization for `Beta` distribution\n",
    "        interp_label:bool|None=None # Blend or stack labels. Defaults to `loss_func.y_int` if None\n",
    "    ):\n",
    "        store_attr()\n",
    "        self.distrib = Beta(tensor(alpha), tensor(alpha))\n",
    "\n",
    "    def before_fit(self):\n",
    "        \"Determine whether to stack or interpolate labels\"\n",
    "        self.multiloss = isinstance(self.learn.loss_func, MultiLoss)\n",
    "        if self.interp_label is None: \n",
    "            self.stack_y = getattr(self.learn.loss_func, 'y_int', False)\n",
    "        else:\n",
    "            self.stack_y = not self.interp_label \n",
    "\n",
    "    def before_train(self):\n",
    "        \"Determine whether to stack y\"\n",
    "        if self.stack_y:\n",
    "            if self.multiloss:\n",
    "                self.learn.loss_func_mixup = self.learn.loss_func\n",
    "                self.learn.loss_func = self.multi_lf\n",
    "            else:\n",
    "                self.old_lf = self.learn.loss_func\n",
    "                self.learn.loss_func = self.solo_lf\n",
    "\n",
    "    def after_train(self):\n",
    "        \"Set the loss function back to the original loss\"\n",
    "        if self.stack_y: \n",
    "            if self.multiloss:\n",
    "                self.learn.loss_func = self.learn.loss_func_mixup\n",
    "            else: \n",
    "                self.learn.loss_func = self.old_lf\n",
    "\n",
    "    def after_cancel_train(self):\n",
    "        \"If training is canceled, still set the loss function back\"\n",
    "        self.after_train()\n",
    "\n",
    "    def after_cancel_fit(self):\n",
    "        \"If fit is canceled, still set the loss function back\"\n",
    "        self.after_train()\n",
    "\n",
    "    def solo_lf(self, pred, *yb):\n",
    "        \"Interpolates losses on stacked labels by `self.lam` during training\"\n",
    "        if not self.training: return self.old_lf(pred, *yb)\n",
    "        with NoneReduce(self.old_lf) as lf:\n",
    "            loss = torch.lerp(lf(pred,*self.yb1), lf(pred,*yb), self.lam)\n",
    "        return reduce_loss(loss, getattr(self.old_lf, 'reduction', 'mean'))\n",
    "\n",
    "    def multi_lf(self, pred, *yb):\n",
    "        if not self.training: \n",
    "            return self.learn.loss_func_mixup(pred, *yb)\n",
    "        else:\n",
    "            return self.learn.loss_func_mixup.forward_mixup(pred, *self.yb1, *yb, self.lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MixHandlerX` is defined here to prevent a circular import between multiloss and cutmixup modules. If `interp_label` is false, then labels will be blended together. Use with losses that prefer floats as labels such as BCE. If `interp_label` is false, then `MixHandlerX` will call the loss function twice, once with each label, and blend the losses together. Use with losses that prefer class integers as labels such as CE. If `interp_label` is None, then it is set via `loss_func.y_int`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiLossCallback -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class MultiLossCallback(Callback):\n",
    "    \"Callback to automatically log and name `MultiLoss` losses as fastxtend metrics\"\n",
    "    run_valid,order = False,Recorder.order-1\n",
    "\n",
    "    def __init__(self, \n",
    "        beta:float=0.98, # Smoothing beta\n",
    "        reduction:str|None='mean' # Override loss reduction for logging\n",
    "    ):\n",
    "        store_attr()\n",
    "\n",
    "    def before_fit(self):\n",
    "        if not isinstance(self.loss_func, MultiLoss):\n",
    "            raise ValueError(\"`MultiLossCallback` requires loss to be `MultiLoss` class\")\n",
    "\n",
    "        mixup = len(self.learn._grab_cbs(MixHandlerX)) > 0 and getattr(self.learn.loss_func, 'y_int', False)\n",
    "\n",
    "        mets= L()\n",
    "        reduction = self.loss_func.reduction if self.reduction is None else self.reduction\n",
    "        for i in range(len(self.loss_func.loss_funcs)):\n",
    "            if mixup: mets += MultiAvgSmoothLossMixup(i, self.loss_func.loss_names[i], self.beta, reduction)\n",
    "            else:     mets += MultiAvgSmoothLoss(i, self.loss_func.loss_names[i], self.beta, reduction)\n",
    "            mets += MultiAvgLoss(i, self.loss_func.loss_names[i], reduction)\n",
    "        \n",
    "        self.learn.metrics = mets + self.learn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|slow\n",
    "from fastai.learner import Learner\n",
    "from fastai.optimizer import SGD\n",
    "from fastxtend.metrics import RMSE\n",
    "\n",
    "@delegates(Learner.__init__)\n",
    "def synth_learner(n_trn=10, n_val=2, cuda=False, lr=1e-3, data=None, model=None, **kwargs):\n",
    "    if data is None: data=synth_dbunch(n_train=n_trn,n_valid=n_val, cuda=cuda)\n",
    "    if model is None: model=RegModel()\n",
    "    return Learner(data, model, lr=lr, opt_func=partial(SGD, mom=0.9), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_mse_loss</th>\n",
       "      <th>train_l1_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_mse_loss</th>\n",
       "      <th>valid_l1_loss</th>\n",
       "      <th>valid_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>23.598301</td>\n",
       "      <td>12.719514</td>\n",
       "      <td>10.878788</td>\n",
       "      <td>17.910727</td>\n",
       "      <td>9.067028</td>\n",
       "      <td>8.843699</td>\n",
       "      <td>3.011151</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>22.448792</td>\n",
       "      <td>11.937573</td>\n",
       "      <td>10.511218</td>\n",
       "      <td>15.481797</td>\n",
       "      <td>7.464430</td>\n",
       "      <td>8.017367</td>\n",
       "      <td>2.732111</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>20.827835</td>\n",
       "      <td>10.837888</td>\n",
       "      <td>9.989948</td>\n",
       "      <td>12.756706</td>\n",
       "      <td>5.756156</td>\n",
       "      <td>7.000550</td>\n",
       "      <td>2.399199</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>19.028177</td>\n",
       "      <td>9.657351</td>\n",
       "      <td>9.370827</td>\n",
       "      <td>10.031281</td>\n",
       "      <td>4.145008</td>\n",
       "      <td>5.886274</td>\n",
       "      <td>2.035929</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>17.167393</td>\n",
       "      <td>8.481768</td>\n",
       "      <td>8.685625</td>\n",
       "      <td>7.581020</td>\n",
       "      <td>2.787561</td>\n",
       "      <td>4.793459</td>\n",
       "      <td>1.669599</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|slow\n",
    "with no_random():\n",
    "    mloss = MultiLoss(loss_funcs=[nn.MSELoss, nn.L1Loss], \n",
    "                      weights=[1, 3.5],\n",
    "                      loss_names=['mse_loss', 'l1_loss'])\n",
    "\n",
    "\n",
    "    learn = synth_learner(n_trn=5, loss_func=mloss, metrics=RMSE(), cbs=MultiLossCallback)\n",
    "    learn.fit(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
