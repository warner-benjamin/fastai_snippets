{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp callback.channelslast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Channels Last\n",
    "> A Callback which converts a fastai `Learner` and input to channels_last format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Mixed Precision, image models trained in channels last format on Nvidia Tensor Cores can achieve 8%-35% increased performance over contiguous format. \n",
    "\n",
    "Channels last memory format is only implemented for 4D NCHW Tensors. Not all PyTorch operators have been converted to support channels last. See [(Beta) Channels Last Memory Format in PyTorch](https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html) for more details.\n",
    "\n",
    "Channels Last format can error out if `torch.backends.cudnn.benchmark = False`, e.g. via fast.ai's [no_random](https://docs.fast.ai/torch_core.html#no_random) context manager. If this occurs the `less_random` context manager instead. This will allow reproducable training on the same GPU, PyTorch, and CUDA setup at the expense of less reproducablity should any of those change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "from fastai.torch_core import TensorImageBase, TensorMask\n",
    "from fastai.learner import Learner\n",
    "from fastai.vision.augment import DisplayedTransform\n",
    "from fastai.callback.core import Callback\n",
    "from fastai.callback.fp16 import MixedPrecision\n",
    "from fastai.basics import Pipeline\n",
    "\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channels Last Transform -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ChannelsLastTfm(DisplayedTransform):\n",
    "    \"Sets image-like inputs to `channels_last` format. For use in ChannelsLastCallback\"\n",
    "    order = 110 # run after all other transforms if added to batch_tfms\n",
    "    def encodes(self, x:TensorImageBase|TensorMask):\n",
    "        return x.to(memory_format=torch.channels_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Channels last format requires inputs to be 4D NCHW Tensors, so `ChannelsLastTfm` only encodes `TensorImageBase` and `TensorMask` inputs to channels last using fastcore's type dispatch.\n",
    "\n",
    "To set another input type as channels last format, patch `ChannelsLastTfm.encodes` to dispatch for that type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channels Last -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ChannelsLastCallback(Callback):\n",
    "    \"Channels last training using PyTorch's Channels Last Memory Format (beta)\"\n",
    "    order = MixedPrecision.order+1\n",
    "    def __init__(self):\n",
    "        self._channels_last = Pipeline([ChannelsLastTfm()])\n",
    "\n",
    "    def before_fit(self):\n",
    "        self.learn.model.to(memory_format=torch.channels_last)\n",
    "\n",
    "    def before_batch(self):\n",
    "        self.learn.xb = self._channels_last(self.xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convenience Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "@delegates(GradScaler)\n",
    "def to_channelslast(self:Learner, to_fp16=True, **kwargs):\n",
    "    \"Set `Learner` and inputs to `channels_last` format and Mixed Precision by default\"\n",
    "    if to_fp16 and not hasattr(self, 'mixed_precision') and not hasattr(self, 'channels_last'): \n",
    "        return self.add_cbs([ChannelsLastCallback(), MixedPrecision(**kwargs)])\n",
    "    elif not hasattr(self, 'channels_last'):\n",
    "        return self.add_cb(ChannelsLastCallback())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def to_contiguous(self:Learner, to_fp32=False):\n",
    "    \"Set `Learner` and inputs to `contiguous_format` (default format), optionally to single precision\"\n",
    "    self.model.to(memory_format=torch.contiguous_format)\n",
    "    if to_fp32: return self.remove_cbs([ChannelsLastCallback, MixedPrecision])\n",
    "    else:       return self.remove_cb(ChannelsLastCallback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastxtend.test_utils import *\n",
    "from fastai.data.core import TfmdDL, DataLoaders\n",
    "from fastai.optimizer import SGD\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "class ChannelsLastInputTest(Callback):\n",
    "    \"Asserts that inputs are in channels last format\"\n",
    "    order = ChannelsLastCallback.order+1\n",
    "    def before_batch(self):\n",
    "        assert self.x.is_contiguous(memory_format=torch.channels_last), \"Input isn't channels last\"\n",
    "\n",
    "class ChannelsLastPredTest(Callback):\n",
    "    \"Asserts that predictions are in channels last format\"\n",
    "    order = MixedPrecision.order-1\n",
    "    def after_pred(self): \n",
    "        assert self.pred.is_contiguous(memory_format=torch.channels_last), \"Model and/or output isn't channels last\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "def synth_dbunch(bs=16, n_train=10, n_valid=2, cuda=True):\n",
    "    def get_data(n):\n",
    "        return TensorDataset(TensorImage(torch.randn(bs*n, 3, 32, 32)))\n",
    "    train_ds = get_data(n_train)\n",
    "    valid_ds = get_data(n_valid)\n",
    "    device = default_device() if cuda else None\n",
    "    train_dl = TfmdDL(train_ds, bs=bs, shuffle=True, num_workers=0)\n",
    "    valid_dl = TfmdDL(valid_ds, bs=bs, num_workers=0)\n",
    "    return DataLoaders(train_dl, valid_dl, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>nan</td>\n",
       "      <td>None</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>None</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>nan</td>\n",
       "      <td>None</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "with no_random():\n",
    "    learn = synth_learner(cbs=[MixedPrecision,ChannelsLastCallback,ChannelsLastInputTest,ChannelsLastPredTest], cuda=True, data=synth_dbunch())\n",
    "    class ConvModel(Module):\n",
    "        def __init__(self): self.conv = nn.Conv2d(3, 32, 1)\n",
    "        def forward(self,x): return self.conv(x)\n",
    "    def fakeloss(): pass\n",
    "    learn.model = ConvModel()\n",
    "    learn.opt_func = partial(SGD, mom=0.)\n",
    "    learn.loss_func=fakeloss\n",
    "    learn.fit(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
