{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp callback.channelslast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Channels Last\n",
    "> A Callback which converts a fastai `Learner` and input to channels_last format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Mixed Precision, image models trained in channels last format on Nvidia Tensor Cores can achieve 8%-35% increased performance over contiguous format. \n",
    "\n",
    "Channels last memory format is only implemented for 4D NCHW Tensors. Not all PyTorch operators have been converted to support channels last. See [(Beta) Channels Last Memory Format in PyTorch](https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html) for more details.\n",
    "\n",
    "Channels Last format can error out if `torch.backends.cudnn.benchmark = False`, e.g. via fast.ai's [no_random](https://docs.fast.ai/torch_core.html#no_random) context manager. If this occurs the `less_random` context manager instead. This will allow reproducable training on the same GPU, PyTorch, and CUDA setup at the expense of less reproducablity should any of those change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "import torch\n",
    "from fastai.torch_core import TensorImageBase, TensorMask\n",
    "from fastai.learner import Learner\n",
    "from fastai.vision.augment import DisplayedTransform\n",
    "from fastai.callback.core import Callback\n",
    "from fastai.callback.fp16 import MixedPrecision\n",
    "from fastai.callback.mixup import MixHandler\n",
    "from fastai.basics import Pipeline\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channels Last Transform -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ChannelsLastTfm(DisplayedTransform):\n",
    "    \"Sets image inputs to `channels_last` format. For use in ChannelsLastCallback\"\n",
    "    def encodes(self, x:TensorImageBase|TensorMask):\n",
    "        return x.to(memory_format=torch.channels_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channels Last Callback -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ChannelsLastCallback(Callback):\n",
    "    \"Channels last training using PyTorch's Channels Last Memory Format (beta)\"\n",
    "    order = MixHandler.order+1\n",
    "    def __init__(self):\n",
    "        self._channels_last = Pipeline([ChannelsLastTfm()])\n",
    "\n",
    "    def before_fit(self):\n",
    "        self.learn.model.to(memory_format=torch.channels_last)\n",
    "\n",
    "    def before_batch(self):\n",
    "        self.learn.xb = self._channels_last(self.xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convenience Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "@delegates(GradScaler)\n",
    "def to_channelslast(self:Learner, to_fp16=True, **kwargs):\n",
    "    \"Set `Learner` and inputs to `channels_last` format and Mixed Precision by default\"\n",
    "    if to_fp16 and not hasattr(self, 'mixed_precision'): \n",
    "        return self.add_cbs([ChannelsLastCallback(), MixedPrecision(**kwargs)])\n",
    "    else:\n",
    "        return self.add_cb(ChannelsLastCallback())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def to_contiguous(self:Learner, to_fp32=False):\n",
    "    \"Set `Learner` and inputs to `contiguous_format` (default format), optionally to single precision\"\n",
    "    self.model.to(memory_format=torch.contiguous_format)\n",
    "    if to_fp32: return self.remove_cbs([ChannelsLastCallback, MixedPrecision])\n",
    "    else:       return self.remove_cb(ChannelsLastCallback())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
