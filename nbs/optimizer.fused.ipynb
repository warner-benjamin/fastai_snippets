{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp optimizer.fused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "# Contains code from:\n",
    "# fastai - Apache License 2.0 - Copyright (c) 2023 fast.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fused Optimizers\n",
    "> Fused fastai optimizers using ForEach methods and TorchScript"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastxtend's fused optimizers are 21 to 293 percent faster, drop-in replacements for fastai native optimizers.\n",
    "\n",
    "Like fastai optimizers, fastxtend fused optimizers support both discriminative learning rates across multiple parameter groups and per-parameter weight decay without any extra setup.\n",
    "\n",
    "While all fastai optimizers have vertically fused TorchScript implementations, only a subset have horizontally fused ForEach implementations. These optimizers, [SGD](#sgd-optimizer), [Adam](#adam-optimizer), [RAdam](#radam-optimizer), [Lamb](#lamb-optimizer), and [Ranger](#ranger-optimizer), usually outperform their TorchScript counterparts in all but the tiniest models.\n",
    "\n",
    "fastxtend ForEach optimizers are equivalent in performance to PyTorch ForEach optimizers with two parameter groups, one for applying weight decay and one for parameters without weight decay.\n",
    "\n",
    ":::{.callout-important}\n",
    "ForEach and TorchScript optimizers have only been tested on PyTorch 1.12+ and are not guaranteed to work on older versions.\n",
    ":::\n",
    "\n",
    ":::{.callout-note}\n",
    "Documentation for individual optimizers are lightly adapted from the [fastai optimizer documentation](https://docs.fast.ai/optimizer.html). [Docments](https://nbdev.fast.ai/tutorials/best_practices.html#document-parameters-with-docments) and type hints have been  [upstreamed to fastai](https://github.com/fastai/fastai/pull/3847).\n",
    "\n",
    "For implementation details, see the [ForEach](optimizer.foreach.html) or [TorchScript](optimizer.torchscript.html) documentation.\n",
    "\n",
    "fastxtend ForEach optimizers are adapted from the PyTorch ForEach [`_multi_tensor`](https://github.com/pytorch/pytorch/tree/master/torch/optim) implementations, but seamlessly work with fastai features.\n",
    ":::\n",
    "\n",
    "## Fused Performance\n",
    "\n",
    "As shown in @tbl-single, ForEach Optimizers are 21 to 293 percent faster[^faster] in [AdamW](#adam-optimizer) optimizer step performance relative to fastai implementations across benchmarked models. Complex optimizers without ForEach implementations, such as [QHAdam](#qhadam-optimizer), are up to 137 percent faster using TorchScript implementations.\n",
    "\n",
    "| Model         | fastai Step | ForEach Step | ForEach Speedup | JIT Step | JIT Speedup |\n",
    "| :-----------: | :---------: | :----------: | :-------------: | :------: | :---------: |\n",
    "| XResNet18     | 26ms        | 12ms         | 109%            | 20ms     | 29%         |\n",
    "| XResNet50     | 56ms        | 32ms         | 74%             | 46ms     | 20%         |\n",
    "| XSE-ResNeXt50 | 72ms        | 43ms         | 68%             | 61ms     | 18%         |\n",
    "| XResNet101    | 88ms        | 47ms         | 84%             | 68ms     | 30%         |\n",
    "| DeBERTa Base  | 27ms        | 6.9ms        | 293%            | 19ms     | 46%         |\n",
    "\n",
    ": Increase in [AdamW](#adam-optimizer) `opt_step` Speed vs fastai Native Optimizer {#tbl-single} {tbl-colwidths=\"[10,8,8,8,8,8]\"}\n",
    "\n",
    "This speedup persists with single or multiple parameter groups. Although more groups can lead to a small decrease in optimizer step speed, as shown by DeBERTa in @tbl-layers.\n",
    "\n",
    "| Model             | Layers | fastai Step | ForEach Step | ForEach Speedup | JIT Step | JIT Speedup |\n",
    "| :---------------: | :----: | :---------: | :----------: | :-------------: | :------: | :---------: |\n",
    "| XResNet18         | 2      | 25ms        | 12ms         | 103%            | 19ms     | 30%         |\n",
    "| XResNet50         | 2      | 56ms        | 32ms         | 76%             | 46ms     | 24%         |\n",
    "| XSE-ResNeXt50     | 2      | 72ms        | 45ms         | 85%             | 61ms     | 29%         |\n",
    "| XResNet101        | 2      | 87ms        | 47ms         | 60%             | 67ms     | 17%         |\n",
    "| ConvNeXt Tiny     | 2      | 125ms       | 102ms        | 22%             | 115ms    | 9.4%        |\n",
    "| ConvNeXt Small    | 2      | 200ms       | 165ms        | 21%             | 181ms    | 10%         |\n",
    "| ViT Patch16 Small | 2      | 62ms        | 38ms         | 62%             | 52ms     | 20%         |\n",
    "| DeBERTa Base      | 4      | 27ms        | 7.7ms        | 254%            | 19ms     | 47%         |\n",
    "\n",
    ": Increase in [AdamW](#adam-optimizer) `opt_step` Speed With Multiple Param Groups vs fastai Native Optimizer {#tbl-layers} {tbl-colwidths=\"[10,2,8,8,8,8,8]\"}\n",
    "\n",
    "[^faster]: All optimizers benchmarked on a GeForce 3080 Ti using PyTorch 1.12.1, Cuda 11.6, Mixed Precision, [Channels Last](callback.channelslast.html) (except ViT and DeBERTa), and fastxtend's [Simple Profiler Callback](callback.simpleprofiler.html). Results may differ with other optimizers, models, hardware, and across benchmarking runs. Speedup is calculated from the total time spent on the optimization step.\n",
    "\n",
    "## Examples\n",
    "\n",
    "For backwards compatibility, all fastxtend optimizers return a fastai native optimizer by default. To use a fused version set `foreach=True` or `jit=True`.\n",
    "\n",
    "```python\n",
    "from fastai.vision.all import *\n",
    "from fastxtend.vision.all import *\n",
    "\n",
    "# Use ForEach AdamW\n",
    "opt_func = adam(foreach=True)\n",
    "\n",
    "# Or use TorchScript AdamW\n",
    "opt_func = adam(jit=True)\n",
    "\n",
    "Learner(..., opt_func=opt_func)\n",
    "```\n",
    "\n",
    "Or import fused optimizers independent of other fastxtend features.\n",
    "\n",
    "```python\n",
    "from fastai.vision.all import *\n",
    "from fastxtend.optimizer.all import *\n",
    "\n",
    "Learner(..., opt_func=partial(Adam, foreach=True))\n",
    "```\n",
    "\n",
    ":::{.callout-note}\n",
    "`adam(...)` is a fastxtend convenience method equivalent to `partial(Adam, ...)`. fastextend adds lowercase convenience methods for all fastai optimizers.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "from typing import Optional, Dict\n",
    "\n",
    "from fastcore.basics import partialler\n",
    "\n",
    "from fastai.optimizer import (Optimizer, weight_decay, l2_reg, average_grad, sgd_step, momentum_step,\n",
    "                              average_sqr_grad, rms_prop_step, step_stat, adam_step, radam_step,\n",
    "                              qhadam_step, larc_layer_lr, larc_step, lamb_step, Lookahead)\n",
    "\n",
    "from fastxtend.optimizer.torchscript import (JitOptimizer, radam_jit_step, sgd_jit_step, rmsprop_jit_step,\n",
    "                                             adam_jit_step, radam_jit_step, qhadam_jit_step, larc_jit_step,\n",
    "                                             lamb_jit_step, JitLookahead, ranger_jit_step)\n",
    "\n",
    "from fastxtend.optimizer.foreach import (SGDForEachOptimizer, sgd_foreach_step, AdamForEachOptimizer,\n",
    "                                         adam_foreach_step, RAdamForEachOptimizer, radam_foreach_step,\n",
    "                                         LambForEachOptimizer, lamb_foreach_step, RangerForEachOptimizer,\n",
    "                                         ranger_foreach_step)\n",
    "\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Utils -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastxtend.test_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# full tests in the optimizer.torchscript and optimizer.foreach notebooks\n",
    "# tests are copied with light modifications from fastai\n",
    "def tst_param(val, grad=None):\n",
    "    \"Create a tensor with `val` and a gradient of `grad` for testing\"\n",
    "    res = tensor([val]).float()\n",
    "    res.grad = tensor([val/10 if grad is None else grad]).float()\n",
    "    return res\n",
    "\n",
    "def tst_params():\n",
    "    r = L.range(4)\n",
    "    return r.map(tst_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Optimizer\n",
    "\n",
    "Stochastic gradient descent, optionally with momentum.\n",
    "\n",
    "Optional weight decay of `wd` is applied, as true weight decay (decay the weights directly) if `decouple_wd=True` else as L2 regularization (add the decay to the gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def SGD(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    mom:float=0., # Gradient moving average (β1) coefficient\n",
    "    wd:float=0., # Optional weight decay (true or L2)\n",
    "    decouple_wd:bool=True, # Apply true weight decay (SGDW) or L2 regularization (SGD)\n",
    "    foreach:bool=False, # Use fused ForEach implementation\n",
    "    jit:bool=False # Use fused TorchScript implementation\n",
    ") -> Optimizer|SGDForEachOptimizer|JitOptimizer:\n",
    "    \"A fastai SGD/SGDW optimizer with fused ForEach and TorchScript implementations\"\n",
    "    if foreach:\n",
    "        return SGDForEachOptimizer(params, sgd_foreach_step, lr=lr, mom=mom, wd=wd, decouple_wd=decouple_wd)\n",
    "    elif jit:\n",
    "        return JitOptimizer(params, sgd_jit_step, lr=lr, mom=mom, wd=wd, decouple_wd=decouple_wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        if mom != 0: cbs.append(average_grad)\n",
    "        cbs.append(sgd_step if mom==0 else momentum_step)\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def sgd(\n",
    "    mom:float=0., # Gradient moving average (β1) coefficient\n",
    "    wd:float=0., # Optional weight decay (true or L2)\n",
    "    decouple_wd:bool=True, # Apply true weight decay (SGDW) or L2 regularization (SGD)\n",
    "    foreach:bool=False, # Use fused ForEach implementation\n",
    "    jit:bool=False # Use fused TorchScript implementation\n",
    ") -> Optimizer|SGDForEachOptimizer|JitOptimizer:\n",
    "    \"Partial function for the SGD/SGDW optimizer with fused ForEach and TorchScript implementations\"\n",
    "    return partialler(SGD, mom=mom, wd=wd, decouple_wd=decouple_wd, jit=jit, foreach=foreach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params_org = tst_params()\n",
    "opt_org = SGD(params_org, lr=0.1)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = tst_params()\n",
    "opt_jit = SGD(params_jit, lr=0.1, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "params_for = tst_params()\n",
    "opt_for = SGD(params_for, lr=0.1, foreach=True)\n",
    "opt_for.step()\n",
    "\n",
    "test_close([p.item() for p in params_org], [i*0.99 for i in range(4)])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_jit])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_for])\n",
    "\n",
    "opt_org.step()\n",
    "opt_jit.step()\n",
    "opt_for.step()\n",
    "test_close([p.item() for p in params_org], [i*0.98 for i in range(4)])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_jit])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_for])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp Optimizer\n",
    "\n",
    "RMSProp was introduced by Geoffrey Hinton in his [course](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf). What is named `sqr_mom` here is the `alpha` in the course. \n",
    "\n",
    "Optional weight decay of `wd` is applied as true weight decay (decay the weights directly) if `decouple_wd=True` else as L2 regularization (add the decay to the gradients).\n",
    "\n",
    ":::{.callout-note}\n",
    "The order of the `mom` and `sqr_mom` hyperparameters has been swapped from fastai to follow the order of all the other fastai and fastxtend optimizers.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def RMSProp(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    mom:float=0., # Gradient moving average (β1) coefficient\n",
    "    sqr_mom:float=0.99, # Gradient squared moving average (β2) coefficient\n",
    "    eps:float=1e-8, # Added for numerical stability\n",
    "    wd:float=0., # Optional weight decay (true or L2)\n",
    "    decouple_wd:bool=True, # Apply true weight decay (RMSPropW) or L2 regularization (RMSProp)\n",
    "    jit:bool=False # Use fused TorchScript implementation\n",
    ") -> Optimizer|JitOptimizer:\n",
    "    \"A fastai RMSProp/RMSPropW optimizer with a fused TorchScript implementation\"\n",
    "    if jit:\n",
    "        return JitOptimizer(params, rmsprop_jit_step, lr=lr, mom=mom, sqr_mom=sqr_mom,\n",
    "                            eps=eps, wd=wd, decouple_wd=decouple_wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        cbs += ([average_sqr_grad] if mom==0. else [average_grad, average_sqr_grad])\n",
    "        cbs.append(rms_prop_step)\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, wd=wd, eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def rmsprop(\n",
    "    mom:float=0., # Gradient moving average (β1) coefficient\n",
    "    sqr_mom:float=0.99, # Gradient squared moving average (β2) coefficient\n",
    "    eps:float=1e-8, # Added for numerical stability\n",
    "    wd:float=0., # Optional weight decay (true or L2)\n",
    "    decouple_wd:bool=True, # Apply true weight decay (RMSPropW) or L2 regularization (RMSProp)\n",
    "    jit:bool=False # Use fused TorchScript implementation\n",
    ") -> Optimizer|JitOptimizer:\n",
    "    \"Partial function for the RMSProp/RMSPropW optimizer with a fused TorchScript implementation\"\n",
    "    return partialler(RMSProp, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd, decouple_wd=decouple_wd, jit=jit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = RMSProp(params_org, lr=0.1, mom=0.9)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = RMSProp(params_jit, lr=0.1, mom=0.9, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "test_close(params_org[0], tensor([0.,1.,2.]))\n",
    "test_close(params_org[0], params_jit[0])\n",
    "\n",
    "opt_org.step()\n",
    "opt_jit.step()\n",
    "step = - 0.1 * (0.1 + 0.9*0.1) / (math.sqrt((0.01*0.99+0.01) * 0.1**2) + 1e-8)\n",
    "test_close(params_org[0], tensor([step, 1+step, 2+step]))\n",
    "test_close(params_org[0], params_jit[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam Optimizer\n",
    "\n",
    "Adam was introduced by Diederik P. Kingma and Jimmy Ba in *[Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)*. For consistency across optimizers, fastai renamed `beta1` and `beta2` in the paper to `mom` and  `sqr_mom`. Note that the defaults also differ from the paper (0.99 for `sqr_mom` or `beta2`, 1e-5 for `eps`). Those values seem to be better from experimentation in a wide range of situations.\n",
    "\n",
    "Optional weight decay of `wd` is applied, as true weight decay (decay the weights directly) if `decouple_wd=True` else as L2 regularization (add the decay to the gradients).\n",
    "\n",
    ":::{.callout-note}\n",
    "Don't forget that `eps` is an hyper-parameter you can change. Some models won't train without a very high `eps` like 0.1 (intuitively, the higher `eps` is, the closer Adam is to normal SGD). The usual default of 1e-8 is often too extreme in the sense Adam does't manage to get as good results as with SGD.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def Adam(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    mom:float=0.9, # Gradient moving average (β1) coefficient\n",
    "    sqr_mom:float=0.99, # Gradient squared moving average (β2) coefficient\n",
    "    eps:float=1e-5, # Added for numerical stability\n",
    "    wd:float=0.01, # Optional weight decay (true or L2)\n",
    "    decouple_wd:bool=True, # Apply true weight decay (AdamW) or L2 regularization (Adam)\n",
    "    foreach:bool=False, # Use fused ForEach implementation\n",
    "    jit:bool=False # Use fused TorchScript implementation\n",
    ") -> Optimizer|AdamForEachOptimizer|JitOptimizer:\n",
    "    \"A fastai Adam/AdamW optimizer with fused ForEach and TorchScript implementations\"\n",
    "    if foreach:\n",
    "        return AdamForEachOptimizer(params, adam_foreach_step, lr=lr, mom=mom,\n",
    "                                    sqr_mom=sqr_mom, eps=eps, wd=wd, decouple_wd=decouple_wd)\n",
    "    elif jit:\n",
    "        return JitOptimizer(params, adam_jit_step, lr=lr, mom=mom, sqr_mom=sqr_mom,\n",
    "                            eps=eps, wd=wd, decouple_wd=decouple_wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, adam_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def adam(\n",
    "    mom:float=0.9, # Gradient moving average (β1) coefficient\n",
    "    sqr_mom:float=0.99, # Gradient squared moving average (β2) coefficient\n",
    "    eps:float=1e-5, # Added for numerical stability\n",
    "    wd:float=0.01, # Optional weight decay (true or L2)\n",
    "    decouple_wd:bool=True, # Apply true weight decay (RMSPropW) or L2 regularization (RMSProp)\n",
    "    foreach:bool=False, # Use fused ForEach implementation\n",
    "    jit:bool=False # Use fused TorchScript implementation\n",
    ") -> Optimizer|AdamForEachOptimizer|JitOptimizer:\n",
    "    \"Partial function for the Adam/AdamW optimizer with fused ForEach and TorchScript implementations\"\n",
    "    return partialler(Adam, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd,\n",
    "                      decouple_wd=decouple_wd, foreach=foreach, jit=jit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = Adam(params_org, lr=0.1, wd=0)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = Adam(params_jit, lr=0.1, wd=0, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "params_for = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_for = Adam(params_for, lr=0.1, wd=0, foreach=True)\n",
    "opt_for.step()\n",
    "\n",
    "step = -0.1 * 0.1 / (math.sqrt(0.1**2) + 1e-8)\n",
    "test_close(params_org[0], tensor([1+step, 2+step, 3+step]))\n",
    "test_close([p[0].item() for p in params_org], [p[0].item() for p in params_jit])\n",
    "test_close([p[0].item() for p in params_org], [p[0].item() for p in params_for])\n",
    "\n",
    "opt_org.step()\n",
    "opt_jit.step()\n",
    "opt_for.step()\n",
    "test_close(params_org[0], tensor([1+2*step, 2+2*step, 3+2*step]), eps=1e-3)\n",
    "test_close([p[0].item() for p in params_org], [p[0].item() for p in params_jit])\n",
    "test_close([p[0].item() for p in params_org], [p[0].item() for p in params_for])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAdam Optimizer\n",
    "\n",
    "RAdam (for rectified Adam) was introduced by Zhang et al. in *[On the Variance of the Adaptive Default learning rate and Beyond](https://arxiv.org/abs/1908.03265)* to slightly modify the Adam optimizer to be more stable at the beginning of training (and thus not require a long warmup). They use an estimate of the variance of the moving average of the squared gradients (the term in the denominator of traditional Adam) and rescale this moving average by this term before performing the update.\n",
    "\n",
    "The native fastai implementation also incorporates [SAdam](https://arxiv.org/abs/1908.00700); set `beta` to enable this (definition same as in the paper).\n",
    "\n",
    ":::{.callout-note}\n",
    "fastxtend ForEach and TorchScript implementations do not support `beta` and SAdam.\n",
    ":::\n",
    "\n",
    "Optional weight decay of `wd` is applied, as true weight decay (decay the weights directly) if `decouple_wd=True` else as L2 regularization (add the decay to the gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def RAdam(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    mom:float=0.9, # Gradient moving average (β1) coefficient\n",
    "    sqr_mom:float=0.99, # Gradient squared moving average (β2) coefficient\n",
    "    eps:float=1e-5, # Added for numerical stability\n",
    "    wd:float=0., # Optional weight decay (true or L2)\n",
    "    beta:float=0., # Set to enable SAdam with native fastai RAdam\n",
    "    decouple_wd:bool=True, # Apply true weight decay (RAdamW) or L2 regularization (RAdam)\n",
    "    foreach:bool=False, # Use fused ForEach implementation\n",
    "    jit:bool=False # Use fused TorchScript implementation\n",
    ") -> Optimizer|RAdamForEachOptimizer|JitOptimizer:\n",
    "    \"A fastai RAdam/RAdamW optimizer with fused ForEach and TorchScript implementations\"\n",
    "    if (foreach or jit) and beta != 0:\n",
    "        raise ValueError(f'ForEach and TorchScript RAdam does not use {beta=}, set `jit` & `foreach` to False if beta!=0')\n",
    "    if foreach:\n",
    "        return RAdamForEachOptimizer(params, radam_foreach_step, lr=lr, mom=mom, sqr_mom=sqr_mom,\n",
    "                                     eps=eps, wd=wd, decouple_wd=decouple_wd)\n",
    "    if jit:\n",
    "        return JitOptimizer(params, radam_jit_step, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps,\n",
    "                            wd=wd, decouple_wd=decouple_wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, radam_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd, beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def radam(\n",
    "    mom:float=0.9, # Gradient moving average (β1) coefficient\n",
    "    sqr_mom:float=0.99, # Gradient squared moving average (β2) coefficient\n",
    "    eps:float=1e-5, # Added for numerical stability\n",
    "    wd:float=0., # Optional weight decay (true or L2)\n",
    "    beta:float=0., # Set to enable SAdam with native fastai RAdam\n",
    "    decouple_wd:bool=True, # Apply true weight decay (RMSPropW) or L2 regularization (RMSProp)\n",
    "    foreach:bool=False, # Use fused ForEach implementation\n",
    "    jit:bool=False # Use fused TorchScript implementation\n",
    ") -> Optimizer|RAdamForEachOptimizer|JitOptimizer:\n",
    "    \"Partial function for the RAdam/RAdamW optimizer with fused ForEach and TorchScript implementations\"\n",
    "    return partialler(RAdam, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd, beta=beta,\n",
    "                      decouple_wd=decouple_wd, foreach=foreach, jit=jit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = RAdam(params_org, lr=0.1)\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = RAdam(params_jit, lr=0.1, jit=True)\n",
    "\n",
    "params_for = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_for = RAdam(params_for, lr=0.1, foreach=True)\n",
    "\n",
    "#The r factor is lower than 5 during the first 5 steps so updates use the average of gradients (all the same)\n",
    "r_inf = 2/(1-0.99) - 1\n",
    "for i in range(5):\n",
    "    r = r_inf - 2*(i+1)*0.99**(i+1)/(1-0.99**(i+1))\n",
    "    assert r <= 5\n",
    "    opt_org.step()\n",
    "    opt_jit.step()\n",
    "    opt_for.step()\n",
    "p = tensor([0.95, 1.9, 2.85])\n",
    "test_close(params_org[0], p)\n",
    "test_close(params_org[0], params_jit[0])\n",
    "test_close(params_org[0], params_for[0])\n",
    "\n",
    "#The r factor is greater than 5 for the sixth step so we update with RAdam\n",
    "r = r_inf - 2*6*0.99**6/(1-0.99**6)\n",
    "assert r > 5\n",
    "opt_org.step()\n",
    "opt_jit.step()\n",
    "opt_for.step()\n",
    "v = math.sqrt(((r-4) * (r-2) * r_inf)/((r_inf-4)*(r_inf-2)*r))\n",
    "step = -0.1*0.1*v/(math.sqrt(0.1**2) + 1e-8)\n",
    "test_close(params_org[0], p+step)\n",
    "test_close(params_org[0], params_jit[0])\n",
    "test_close(params_org[0], params_for[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QHAdam Optimizer\n",
    "\n",
    "QHAdam (for Quasi-Hyperbolic Adam) was introduced by Ma & Yarats in *[Quasi-Hyperbolic Momentum and Adam for Deep Learning](https://arxiv.org/abs/1810.06801)* as a *\"computationally cheap, intuitive to interpret, and simple to implement\"* optimizer. Additional code can be found in their [qhoptim repo](https://github.com/facebookresearch/qhoptim). QHAdam is based on QH-Momentum, which introduces the immediate discount factor `nu`, encapsulating plain SGD (`nu = 0`) and momentum (`nu = 1`). QH-Momentum is defined below, where $g_t+1$ is the update of the moment. An interpretation of QHM is as a nu-weighted average of the momentum update step and the plain SGD update step.\n",
    "\n",
    "$$θ_t+1 ← θ_t − lr * [(1 − nu) · ∇L_t(θ_t) + nu · g_t+1]$$\n",
    "\n",
    "QHAdam takes the concept behind QHM above and applies it to Adam, replacing both of Adam’s moment estimators with quasi-hyperbolic terms. \n",
    "\n",
    "The paper's suggested default parameters are `mom = 0.999`, `sqr_mom = 0.999`, `nu_1 = 0.7` and `and nu_2 = 1.0`. When training is not stable, it is possible that setting `nu_2 < 1` can improve stability by imposing a tighter step size bound. Note that QHAdam recovers Adam when `nu_1 = nu_2 = 1.0`. QHAdam recovers RMSProp (Hinton et al., 2012) when `nu_1 = 0` and `nu_2 = 1`, and NAdam (Dozat, 2016) when `nu_1 = mom` and `nu_2 = 1`.\n",
    "\n",
    "Optional weight decay of `wd` is applied, as true weight decay (decay the weights directly) if `decouple_wd=True` else as L2 regularization (add the decay to the gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def QHAdam(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    mom:float=0.999, # Gradient moving average (β1) coefficient\n",
    "    sqr_mom:float=0.999, # Gradient squared moving average (β2) coefficient\n",
    "    nu_1:float=0.7, # QH immediate discount factor\n",
    "    nu_2:float=1.0, # QH momentum discount factor\n",
    "    eps:float=1e-8, # Added for numerical stability\n",
    "    wd:float=0., # Optional weight decay (true or L2)\n",
    "    decouple_wd:bool=True, # Apply true weight decay (QHAdamW) or L2 regularization (QHAdam)\n",
    "    jit:bool=False # Use fused TorchScript implementation\n",
    ") -> Optimizer|JitOptimizer:\n",
    "    \"A fastai QHAdam/QHAdamW optimizer with a fused TorchScript implementation\"\n",
    "    if jit:\n",
    "        return JitOptimizer(params, qhadam_jit_step, lr=lr, nu_1=nu_1, nu_2=nu_2, mom=mom,\n",
    "                            sqr_mom=sqr_mom, eps=eps, wd=wd, decouple_wd=decouple_wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, qhadam_step]\n",
    "        return Optimizer(params, cbs, lr=lr, nu_1=nu_1, nu_2=nu_2, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def qhadam(\n",
    "    mom:float=0.999, # Gradient moving average (β1) coefficient\n",
    "    sqr_mom:float=0.999, # Gradient squared moving average (β2) coefficient\n",
    "    nu_1:float=0.7, # QH immediate discount factor\n",
    "    nu_2:float=1.0, # QH momentum discount factor\n",
    "    eps:float=1e-8, # Added for numerical stability\n",
    "    wd:float=0., # Optional weight decay (true or L2)\n",
    "    decouple_wd:bool=True, # Apply true weight decay (RMSPropW) or L2 regularization (RMSProp)\n",
    "    jit:bool=False # Use fused TorchScript implementation\n",
    ") -> Optimizer|JitOptimizer:\n",
    "    \"Partial function for the QHAdam/QHAdamW optimizer with a fused TorchScript implementation\"\n",
    "    return partialler(QHAdam, mom=mom, sqr_mom=sqr_mom, nu_1=nu_1, nu_2=nu_2, eps=eps,\n",
    "                      wd=wd, decouple_wd=decouple_wd, jit=jit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = QHAdam(params_org, lr=0.1)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = QHAdam(params_jit, lr=0.1, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "step = -0.1 * (((1-0.7) * 0.1) + (0.7 * 0.1)) / (\n",
    "     math.sqrt(((1-1.0) * 0.1**2) + (1.0 * 0.1**2)) + 1e-8)\n",
    "\n",
    "test_close(params_org[0], tensor([1+step, 2+step, 3+step]))\n",
    "test_close(params_org[0], params_jit[0])\n",
    "\n",
    "opt_org.step()\n",
    "opt_jit.step()\n",
    "test_close(params_org[0], tensor([1+2*step, 2+2*step, 3+2*step]), eps=1e-3)\n",
    "test_close(params_org[0], params_jit[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LARS/LARC Optimizer\n",
    "\n",
    "The LARS optimizer was first introduced in *[Large Batch Training of Convolutional Networks](https://arxiv.org/abs/1708.03888)* then refined in its LARC variant (original LARS is with `clip=False`). A Default learning rate is computed for each individual layer with a certain `trust_coefficient`, then clipped to be always less than `lr`.\n",
    "\n",
    "Optional weight decay of `wd` is applied, as true weight decay (decay the weights directly) if `decouple_wd=True` else as L2 regularization (add the decay to the gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def Larc(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    mom:float=0.9, # Gradient moving average (β1) coefficient\n",
    "    clip:bool=True, # LARC if clip=True, LARS if clip=False\n",
    "    trust_coeff:float=0.02, # Trust coeffiecnet for calculating layerwise LR\n",
    "    eps:float=1e-8, # Added for numerical stability\n",
    "    wd:float=0., # Optional weight decay (true or L2)\n",
    "    decouple_wd:bool=True, # Apply true weight decay or L2 regularization\n",
    "    jit:bool=False # Use fused TorchScript implementation\n",
    ") -> Optimizer|JitOptimizer:\n",
    "    \"A fastai LARC/LARS optimizer with a fused TorchScript implementation\"\n",
    "    if jit:\n",
    "        cb = partial(larc_jit_step, clip=clip)\n",
    "        return JitOptimizer(params, cb, lr=lr, mom=mom, trust_coeff=trust_coeff,\n",
    "                            eps=eps, wd=wd, decouple_wd=decouple_wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        if mom!=0.: cbs.append(average_grad)\n",
    "        cbs += [partial(larc_layer_lr, clip=clip), larc_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, trust_coeff=trust_coeff, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def larc(\n",
    "    mom:float=0.9, # Gradient moving average (β1) coefficient\n",
    "    clip:bool=True, # LARC if clip=True, LARS if clip=False\n",
    "    trust_coeff:float=0.02, # Trust coeffiecnet for calculating layerwise LR\n",
    "    eps:float=1e-8, # Added for numerical stability\n",
    "    wd:float=0., # Optional weight decay (true or L2)\n",
    "    decouple_wd:bool=True, # Apply true weight decay (RMSPropW) or L2 regularization (RMSProp)\n",
    "    jit:bool=False # Use fused TorchScript implementation\n",
    ") -> Optimizer|JitOptimizer:\n",
    "    \"Partial function for the LARC/LARS optimizer with a fused TorchScript implementation\"\n",
    "    return partialler(Larc, mom=mom, clip=clip, eps=eps, trust_coeff=trust_coeff,\n",
    "                      wd=wd, decouple_wd=decouple_wd, jit=jit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params_org = [tst_param([1,2,3], [0.1,0.2,0.3]), tst_param([1,2,3], [0.01,0.02,0.03])]\n",
    "opt_org = Larc(params_org, lr=0.1)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = [tst_param([1,2,3], [0.1,0.2,0.3]), tst_param([1,2,3], [0.01,0.02,0.03])]\n",
    "opt_jit = Larc(params_jit, lr=0.1, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "#First param local lr is 0.02 < lr so it's not clipped\n",
    "test_close(opt_org.state[params_org[0]]['local_lr'], 0.02)\n",
    "#Second param local lr is 0.2 > lr so it's clipped\n",
    "test_eq(opt_org.state[params_org[1]]['local_lr'], 0.1)\n",
    "\n",
    "test_close(params_org[0], tensor([0.998,1.996,2.994]))\n",
    "test_close(params_org[0], params_jit[0])\n",
    "test_close(params_org[1], tensor([0.999,1.998,2.997]))\n",
    "test_close(params_org[1], params_jit[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAMB Optimizer\n",
    "\n",
    "LAMB was introduced in *[Large Batch Optimization for Deep Learning: Training BERT in 76 minutes](https://arxiv.org/abs/1904.00962)*. Intuitively, it's LARC applied to Adam. As in `Adam`, `beta1` and `beta2` in the paper is renamed to `mom` and  `sqr_mom`. Note that the defaults also differ from the paper (0.99 for `sqr_mom` or `beta2`, 1e-5 for `eps`). Those values seem to be better from experimentation in a wide range of situations.\n",
    "\n",
    "Optional weight decay of `wd` is applied, as true weight decay (decay the weights directly) if `decouple_wd=True` else as L2 regularization (add the decay to the gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def Lamb(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    mom:float=0.9, # Gradient moving average (β1) coefficient\n",
    "    sqr_mom:float=0.99, # Gradient squared moving average (β2) coefficient\n",
    "    eps:float=1e-5, # Added for numerical stability\n",
    "    wd:float=0., # Optional weight decay (true or L2)\n",
    "    decouple_wd:bool=True, # Apply true weight decay or L2 regularization\n",
    "    foreach:bool=False, # Use fused ForEach implementation\n",
    "    jit:bool=False # Use fused TorchScript implementation\n",
    ") -> Optimizer|LambForEachOptimizer|JitOptimizer:\n",
    "    \"A fastai LAMB optimizer with fused ForEach and TorchScript implementations\"\n",
    "    if foreach:\n",
    "        return LambForEachOptimizer(params, lamb_foreach_step, lr=lr, mom=mom, sqr_mom=sqr_mom,\n",
    "                                    eps=eps, wd=wd, decouple_wd=decouple_wd)\n",
    "    if jit:\n",
    "        return JitOptimizer(params, lamb_jit_step, lr=lr, mom=mom, sqr_mom=sqr_mom,\n",
    "                            eps=eps, wd=wd, decouple_wd=decouple_wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, lamb_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def lamb(\n",
    "    mom:float=0.9, # Gradient moving average (β1) coefficient\n",
    "    sqr_mom:float=0.99, # Gradient squared moving average (β2) coefficient\n",
    "    eps:float=1e-5, # Added for numerical stability\n",
    "    wd:float=0., # Optional weight decay (true or L2)\n",
    "    decouple_wd:bool=True, # Apply true weight decay (RMSPropW) or L2 regularization (RMSProp)\n",
    "    foreach:bool=False, # Use fused ForEach implementation\n",
    "    jit:bool=False # Use fused TorchScript implementation\n",
    ") -> Optimizer|LambForEachOptimizer|JitOptimizer:\n",
    "    \"Partial function for the LAMB optimizer with fused ForEach and TorchScript implementations\"\n",
    "    return partialler(Lamb, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd,\n",
    "                      decouple_wd=decouple_wd, foreach=foreach, jit=jit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = Lamb(params_org, lr=0.1, wd=0.)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = Lamb(params_jit, lr=0.1, wd=0., jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "params_for = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_for = Lamb(params_for, lr=0.1, wd=0., foreach=True)\n",
    "opt_for.step()\n",
    "\n",
    "test_close(params_org[0], tensor([0.7840,1.7840,2.7840]), eps=1e-3)\n",
    "test_close(params_org[0], params_jit[0])\n",
    "test_close(params_org[0], params_for[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranger Optimizer\n",
    "\n",
    ":::{.callout-warning}\n",
    "Ranger is the only non-backward compatible fastxtend Optimizer. `Ranger` is equivalent fastai's [<code>ranger</code>](https://docs.fast.ai/optimizer.html#ranger) while fastxtend's `ranger` is a partial function which returns `Ranger`. Most fastai code should be uneffected by this change.\n",
    ":::\n",
    "\n",
    "Lookahead was introduced by Zhang et al. in *[Lookahead Optimizer: k steps forward, 1 step back](https://arxiv.org/abs/1907.08610)*. With Lookahead, the final weights (slow weights) are a moving average of the normal weights (fast weights). Every k steps, Lookahead modifieds the current weights by a moving average of the fast weights (normal weights) with the slow weights (the copy of old weights k steps ago). Those slow weights act like a stability mechanism.\n",
    "\n",
    "Ranger was introduced by Less Wright in *[New Deep Learning Optimizer, Ranger: Synergistic combination of RAdam + Lookahead for the best of both](https://lessw.medium.com/new-deep-learning-optimizer-ranger-synergistic-combination-of-radam-lookahead-for-the-best-of-2dc83f79a48d)*. It combines RAdam and Lookahead together in one optimizer and reduces the need for hyperparameter tuning due to a combination of RAdam's warmup heuristic and Lookahead's interpolation of parameter weights.\n",
    "\n",
    "Ranger performs best on vision tasks when paired with the `fit_flat_cos` or `fit_flat_varied` schedulers.\n",
    "\n",
    "Optional weight decay of `wd` is applied, as true weight decay (decay the weights directly) if `decouple_wd=True` else as L2 regularization (add the decay to the gradients).\n",
    "\n",
    ":::{.callout-important}\n",
    "While fastai's `Lookahead` can be applied to any optimizer, fastxtend's `JitLookahead` must have a custom written TorchScript callback and `ForEachOptimizer` a custom Lookahead optimizer step. Currently ranger with RAdam is the only TorchScript and ForEach optimizer with Lookahead support.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def Ranger(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    mom:float=0.95, # Gradient moving average (β1) coefficient\n",
    "    sqr_mom:float=0.99, # Gradient squared moving average (β2) coefficient\n",
    "    eps:float=1e-6, # Added for numerical stability\n",
    "    wd:float=0.01, # Optional weight decay (true or L2)\n",
    "    k:int=6, # How often to conduct Lookahead step\n",
    "    alpha:float=0.5, # Slow weight moving average coefficient\n",
    "    decouple_wd:bool=True, # Apply true weight decay (RAdamW) or L2 regularization (RAdam)\n",
    "    foreach:bool=False, # Use fused ForEach implementation\n",
    "    jit:bool=False # Use fused TorchScript implementation\n",
    ") -> Lookahead|RangerForEachOptimizer|JitLookahead:\n",
    "    \"Convenience method for `Lookahead` with `RAdam` fused ForEach and TorchScript implementations\"\n",
    "    if foreach:\n",
    "        return RangerForEachOptimizer(params, ranger_foreach_step, lr=lr, mom=mom, sqr_mom=sqr_mom,\n",
    "                                      eps=eps, wd=wd, decouple_wd=decouple_wd, k=k, alpha=alpha)\n",
    "    elif jit:\n",
    "        return JitLookahead(params, ranger_jit_step, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps,\n",
    "                            wd=wd, k=k, alpha=alpha, decouple_wd=decouple_wd)\n",
    "    else:\n",
    "        return Lookahead(RAdam(params, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd,\n",
    "                               decouple_wd=decouple_wd),\n",
    "                         k=k, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def ranger(\n",
    "    mom:float=0.95, # Gradient moving average (β1) coefficient\n",
    "    sqr_mom:float=0.99, # Gradient squared moving average (β2) coefficient\n",
    "    eps:float=1e-6, # Added for numerical stability\n",
    "    wd:float=0.01, # Optional weight decay (true or L2)\n",
    "    k:int=6, # How often to conduct Lookahead step\n",
    "    alpha:float=0.5, # Slow weight moving average coefficient\n",
    "    decouple_wd:bool=True, # Apply true weight decay (RAdamW) or L2 regularization (RAdam)\n",
    "    foreach:bool=False, # Use fused ForEach implementation\n",
    "    jit:bool=False # Use fused TorchScript implementation\n",
    ") -> Lookahead|RangerForEachOptimizer|JitLookahead:\n",
    "    \"Partial function of the onvenience method for `Lookahead` with `RAdam` fused ForEach and TorchScript implementations\"\n",
    "    return partialler(Ranger, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd, k=k,\n",
    "                      alpha=alpha, decouple_wd=decouple_wd, foreach=foreach, jit=jit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "po = tensor([1,2,3])\n",
    "\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = Ranger(params_org, lr=0.1, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0.)\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = Ranger(params_jit, lr=0.1, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0., jit=True)\n",
    "\n",
    "params_for = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_for = Ranger(params_for, lr=0.1, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0., foreach=True)\n",
    "\n",
    "#The first 5 steps are normal RAdam steps\n",
    "#The r factor is lower than 5 during the first 5 steps so updates use the average of gradients (all the same)\n",
    "r_inf = 2/(1-0.99) - 1\n",
    "for i in range(5):\n",
    "    r = r_inf - 2*(i+1)*0.99**(i+1)/(1-0.99**(i+1))\n",
    "    assert r <= 5\n",
    "    opt_org.step()\n",
    "    opt_jit.step()\n",
    "    opt_for.step()\n",
    "p = tensor([0.95, 1.9, 2.85])\n",
    "test_close(params_org[0], p)\n",
    "test_close(params_org[0], params_jit[0])\n",
    "test_close(params_org[0], params_for[0])\n",
    "\n",
    "#The r factor is greater than 5 for the sixth step so we update with RAdam\n",
    "r = r_inf - 2*6*0.99**6/(1-0.99**6)\n",
    "assert r > 5\n",
    "opt_org.step()\n",
    "opt_jit.step()\n",
    "opt_for.step()\n",
    "v = math.sqrt(((r-4) * (r-2) * r_inf)/((r_inf-4)*(r_inf-2)*r))\n",
    "step = -0.1*0.1*v/(math.sqrt(0.1**2) + 1e-8)\n",
    "\n",
    "#Since k=6, sixth step is a moving average of the 6 RAdam steps with the initial weight\n",
    "test_close(params_org[0], po+((p+step)-po)*0.5)\n",
    "test_close(params_org[0], params_jit[0])\n",
    "test_close(params_org[0], params_for[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
