{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp ffcv.loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "# Contains code from:\n",
    "# fastai - Apache License 2.0 - Copyright (c) 2023 fast.ai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFCV Loader\n",
    "> fastxtend's fastai+FFCV Integrated DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastxtend's `Loader` adds fastai features to [FFCV's Loader](https://docs.ffcv.io/making_dataloaders.html), including [<code>one_batch</code>](https://docs.fast.ai/data.core.html#dataloader.one_batch), [<code>show_batch</code>](https://docs.fast.ai/data.core.html#tfmddl.show_batch), [<code>show_results</code>](https://docs.fast.ai/data.core.html#tfmddl.show_results), and support for GPU batch transforms, to name a few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Mapping, Sequence\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from ffcv.fields.base import Field\n",
    "from ffcv.loader.loader import Loader as _Loader\n",
    "from ffcv.loader.loader import OrderOption, ORDER_TYPE, DEFAULT_OS_CACHE, ORDER_MAP\n",
    "from ffcv.pipeline.compiler import Compiler\n",
    "from ffcv.pipeline.operation import Operation\n",
    "from ffcv.transforms.ops import ToDevice as _ToDevice\n",
    "\n",
    "from fastxtend.data.loader import DataLoaderMixin\n",
    "from fastxtend.ffcv.epoch_iterator import EpochIterator, AsyncEpochIterator\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "_all_ = ['OrderOption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Loader(DataLoaderMixin, _Loader):\n",
    "    \"FFCV `Loader` with fastai Transformed DataLoader `TfmdDL` batch transforms\"\n",
    "    def __init__(self,\n",
    "        fname:str|Path, # Path to the location of the dataset (FFCV beton format)\n",
    "        batch_size:int, # Batch size\n",
    "        num_workers:int=-1, # Number of CPU cores to use in parallel (default: All available up to 16)\n",
    "        os_cache:bool=DEFAULT_OS_CACHE, # Leverage the OS for caching. Beneficial when there is enough memory to cache the dataset\n",
    "        order:ORDER_TYPE=OrderOption.SEQUENTIAL, # Dataset traversal order, one of: `SEQEUNTIAL`, `RANDOM`, `QUASI_RANDOM`\n",
    "        distributed:bool=False, # Emulates the behavior of PyTorch's DistributedSampler for distributed training\n",
    "        seed:int|None=None, # Random seed for batch ordering\n",
    "        indices:Sequence[int]|None=None, # Subset dataset by returning only these indices\n",
    "        pipelines:Mapping[str, Sequence[Operation|nn.Module]]={}, # Dictionary defining for each field the sequence of Decoders and transforms to apply\n",
    "        custom_fields:Mapping[str, Field]={}, # Dictonary informing `Loader` of the types associated to fields that are using a custom type\n",
    "        drop_last:bool|None=None, # Drop non-full batch in each epoch. Defaults to True if order is `SEQEUNTIAL`\n",
    "        batches_ahead:int=2, # Number of batches prepared in advance; balances latency and memory\n",
    "        recompile:bool=False, # Recompile at every epoch. Required if FFCV augmentations change during training\n",
    "        device:str|int|torch.device|None=None, # Device to place batch. Defaults to fastai's `default_device`\n",
    "        async_tfms:bool=False, # Asynchronously run `batch_tfms` before batch is drawn.\n",
    "        n_inp:int|None=None, # Number of inputs to the model. Defaults to pipelines length minus 1\n",
    "        split_idx:int|None=None, # Apply batch transform(s) to training (0) or validation (1) set. Defaults to valid if order is `SEQEUNTIAL`\n",
    "        do_setup:bool=True, # Run `setup()` for batch transform(s)\n",
    "        **kwargs\n",
    "    ):\n",
    "        if split_idx is None:\n",
    "            split_idx = int(order==OrderOption.SEQUENTIAL)\n",
    "\n",
    "        self.async_tfms = async_tfms and len(kwargs['after_batch'].fs) > 0\n",
    "        self.cuda_streams = None\n",
    "\n",
    "        if drop_last is None:\n",
    "            drop_last != order==OrderOption.SEQUENTIAL\n",
    "\n",
    "        _Loader.__init__(self,\n",
    "            fname=str(Path(fname)), batch_size=batch_size, num_workers=num_workers,\n",
    "            os_cache=os_cache, order=order, distributed=distributed, seed=seed,\n",
    "            indices=indices, pipelines=pipelines, custom_fields=custom_fields,\n",
    "            drop_last=drop_last, batches_ahead=batches_ahead, recompile=recompile\n",
    "        )\n",
    "        DataLoaderMixin.__init__(self,\n",
    "            device=device, n_inp=n_inp, split_idx=split_idx, do_setup=do_setup, **kwargs\n",
    "        )\n",
    "        if self._n_inp is None:\n",
    "            self._n_inp = len(pipelines) - 1\n",
    "\n",
    "    def one_batch(self, batches_ahead:bool=False):\n",
    "        \"Return one processed batch of input(s) and target(s), optionally loading `batches_ahead`\"\n",
    "        for b in self._n_batches(self.batches_ahead + 2 if batches_ahead else 1):\n",
    "            # need to return the yield from _n_batches so `Loader` can reset to iterate the entire epoch\n",
    "            pass\n",
    "        return b\n",
    "\n",
    "    @property\n",
    "    def bs(self) -> int:\n",
    "        \"Number of items a batch\"\n",
    "        return self.batch_size\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return super().device\n",
    "\n",
    "    @device.setter\n",
    "    def device(self, device:int|str|torch.device):\n",
    "        # parse device\n",
    "        device, *_ = torch._C._nn._parse_to(device=device)\n",
    "        self._device = device\n",
    "        # Device setter for Loader.batch_tfms\n",
    "        if hasattr(self.after_batch, 'fs'):\n",
    "            self._pipeline_device(self.after_batch.fs)\n",
    "        # Device setter for FFCV Pipeline\n",
    "        for p in self.pipeline_specs.values():\n",
    "            for t in p.transforms:\n",
    "                if isinstance(t, _ToDevice):\n",
    "                    t.device = self._device\n",
    "\n",
    "    def _iter(self):\n",
    "        Compiler.set_num_threads(self.num_workers)\n",
    "        order = self.next_traversal_order()\n",
    "        selected_order = order[:len(self) * self.batch_size]\n",
    "        self.next_epoch += 1\n",
    "\n",
    "        # Compile at the first epoch\n",
    "        if self.code is None or self.recompile:\n",
    "            self.generate_code()\n",
    "\n",
    "        # Asynchronous transforms require using the same Cuda streams for the entire run\n",
    "        if self.cuda_streams is None:\n",
    "            self.cuda_streams = [(torch.cuda.Stream() if torch.cuda.is_available() else None)\n",
    "                                  for _ in range(self.batches_ahead + 2)]\n",
    "        if self.async_tfms:\n",
    "            return AsyncEpochIterator(self, selected_order, self.after_batch)\n",
    "        else:\n",
    "            return EpochIterator(self, selected_order)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.before_iter()\n",
    "        if self.async_tfms:\n",
    "            yield from self._iter()\n",
    "        else:\n",
    "            for b in self._iter():\n",
    "                yield self.after_batch(b)\n",
    "        self.after_iter()\n",
    "        if hasattr(self, 'it'):\n",
    "            del(self.it)\n",
    "\n",
    "    def _n_batches(self, num_batches:int=1):\n",
    "        orig_traversal_order = self.traversal_order\n",
    "        orig_indices = self.indices\n",
    "        orig_drop_last = self.drop_last\n",
    "\n",
    "        # Set Loader to only return one batch per epoch\n",
    "        if self._args['order'] == OrderOption.SEQUENTIAL:\n",
    "            self.indices = np.arange(0, self.batch_size*num_batches)\n",
    "        else:\n",
    "            self.indices = np.random.random_integers(0, self.reader.num_samples, self.batch_size*num_batches)\n",
    "        self.traversal_order = ORDER_MAP[OrderOption.SEQUENTIAL](self)\n",
    "        self.drop_last = False\n",
    "\n",
    "        # yield num_batches\n",
    "        yield from self.__iter__()\n",
    "\n",
    "        # Reset Loader state to its original status\n",
    "        self.next_epoch -= 1\n",
    "        self.indices = orig_indices\n",
    "        self.drop_last = orig_drop_last\n",
    "        self.traversal_order = orig_traversal_order\n",
    "\n",
    "    def _callback_warning(self, kwargs):\n",
    "        for name in ['item_tfms', 'after_item', 'before_batch']:\n",
    "            if name in kwargs:\n",
    "                if name != 'before_batch':\n",
    "                    msg = f\"fastxtend's `Loader` will not call any {name} methods. \" \\\n",
    "                          f\"{name} is for use with a fastai DataLoader.\\n\" \\\n",
    "                          f\"Instead of passing fastai Item Transforms to {name},\" \\\n",
    "                          f\"initialize the fastxtend `Loader` pipeline with FFCV transforms.\"\n",
    "                else:\n",
    "                    msg = f\"fastxtend's `Loader` will not call any {name} methods. \" \\\n",
    "                          f\"{name} are for use with a fastai DataLoader.\"\n",
    "                warn(msg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important `Loader` arguments:\n",
    "\n",
    "- `order`: Controls how much memory is used for dataset caching and whether the dataset is randomly shuffled. Can be one of `RANDOM`, `QUASI_RANDOM`, or `SEQUENTIAL`. See the note below for more details. Defaults to `SEQUENTIAL`, which is unrandomized.\n",
    "\n",
    "- `os_cache`: By default, FFCV will attempt to cache the entire dataset into RAM using the operating system's caching. This can be changed by setting `os_cache=False` or setting the enviroment variable 'FFCV_DEFAULT_CACHE_PROCESS' to \"True\" or \"1\". If `os_cache=False` then `order` must be set to `QUASI_RANDOM` for the training `Loader`.\n",
    "\n",
    "- `num_workers`: If not set, will use all CPU cores up to 16 by default.\n",
    "\n",
    "- `batches_ahead`: Controls the number of batches ahead the `Loader` works. Increasing uses more RAM, both CPU and GPU. Defaults to 2.\n",
    "\n",
    "- `n_inp`: Controls which inputs to pass to the model. By default, set to number of pipelines minus 1.\n",
    "\n",
    "- `drop_last`: Whether to drop the last partial batch. By default, will set to True if `order` is `RANDOM` or `QUASI_RANDOM`, False if `SEQUENTIAL`.\n",
    "\n",
    "- `device`: The device to place the processed batches of data on. Defaults to `fastai.torch_core.default_device` if not set.\n",
    "\n",
    "- `async_tfms`: Asynchronously apply `batch_tfms` before the batch is drawn. Can accelerate training if GPU compute isn't fully saturated (95% or less) or if only using `IntToFloatTensor` and `Normalize`.\n",
    "\n",
    "- `split_idx`: This tells the fastai batch transforms what dataset they are operating on. By default will use 0 (train) if `order` is `RANDOM` or `QUASI_RANDOM`, 1 (valid) if `SEQUENTIAL`.\n",
    "\n",
    "- `distributed`: For distributed training on multiple GPUs. Emulates the behavior of PyTorch's [`DistributedSampler`](https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler). `QUASI_RANDOM` is unavailable with distributed training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note collapse=\"false\"}\n",
    "#### Note: Order Memory Usage\n",
    "Each `order` option requires differing amounts of system memory.\n",
    "\n",
    "- `RANDOM` caches the entire dataset in memory for fast random sampling. `RANDOM` uses the most memory.\n",
    "\n",
    "- `QUASI_RANDOM` caches a subset of the dataset at a time in memory and randomly samples from the subset. Use when the entire dataset cannot fit into memory.\n",
    "\n",
    "- `SEQUENTIAL` requires least memory. It loads a few samples ahead of time. As the name suggests, it is not random, and primarly is for validation.\n",
    ":::\n",
    "\n",
    "Asynchronous batch transforms can accelerate training by decreasing the draw time at the expense of slightly longer batch step. If the GPU isn't fully saturated, usually 95% or less compute use, this will be a net gain in training performance. `async_tfms=True` pairs well with `ProgressiveResize`, as the GPU is almost never saturated when training on smaller then full sized images. When near or fully saturated, asynchronous batch transforms usually result a wash in training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Loader.one_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Loader.show_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Loader.show_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Loader.to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Loader.n_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Loader.decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Loader.decode_batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
