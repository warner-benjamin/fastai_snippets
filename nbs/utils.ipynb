{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "# Contains code from:\n",
    "# fastai - Apache License 2.0 - Copyright (c) 2023 fast.ai\n",
    "# mish-cuda - MIT License - Copyright (c) 2019 thomasbrandon https://github.com/thomasbrandon/mish-cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility\n",
    "> A collection of utility methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "\n",
    "import torch, random, gc\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "\n",
    "from fastcore.foundation import contextmanager\n",
    "\n",
    "from fastai.learner import Learner\n",
    "from fastai.data.core import DataLoaders\n",
    "from fastai.callback.core import set_random_states, get_random_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def free_gpu_memory(learn:Learner, dls:DataLoaders=None):\n",
    "    \"Frees GPU memory using `gc.collect` and `torch.cuda.empty_cache`\"\n",
    "    learn.dls, learn, dls = None, None, None\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@contextmanager\n",
    "def less_random(\n",
    "    seed:int=42, # Seed for `random`, `torch`, and `numpy`\n",
    "    deterministic:bool|None=None, # Set `torch.backends.cudnn.deterministic` if not None\n",
    "    benchmark:bool|None=None # Set `torch.backends.cudnn.benchmark` if not None\n",
    "):\n",
    "    \"\"\"\n",
    "    Stores and retrieves state of random number generators. Sets random seed for `random`, `torch`, and `numpy`.\n",
    "\n",
    "    Does not set `torch.backends.cudnn.benchmark` or `torch.backends.cudnn.deterministic` by default.\n",
    "    \"\"\"\n",
    "    states = get_random_states()\n",
    "\n",
    "    try: torch.manual_seed(seed)\n",
    "    except NameError: pass\n",
    "    try: torch.cuda.manual_seed_all(seed)\n",
    "    except NameError: pass\n",
    "    try: np.random.seed(seed%(2**32-1))\n",
    "    except NameError: pass\n",
    "\n",
    "    random.seed(seed)\n",
    "    if deterministic is not None:\n",
    "        torch.backends.cudnn.deterministic = deterministic\n",
    "    if benchmark is not None:\n",
    "        torch.backends.cudnn.benchmark = benchmark\n",
    "\n",
    "    try:\n",
    "        yield # we are managing global variables\n",
    "    finally:\n",
    "        set_random_states(**states)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random state manager which provides some reproducibility without sacrificing potential training speed.\n",
    "\n",
    "Unlike `fastai.torch_core.no_random`, `less_random` does not set `torch.backends.cudnn.benchmark` or `torch.backends.cudnn.deterministic` by default.\n",
    "\n",
    "`less_random` training runs on the same GPU, PyTorch, & Cuda setup should be close to `no_random` reproducibility, but different hardware/software setup will have less reproducibility than using `no_random`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "# modified from https://github.com/thomasbrandon/mish-cuda/blob/master/test/perftest.py\n",
    "def scale_time(val:float, spec:str=\"#0.4G\"):\n",
    "    \"Scale fractional second `time` values and return formatted to `spec`\"\n",
    "    if val == 0:\n",
    "        return '-'\n",
    "    PREFIXES = np.array([c for c in u\"yzafpnÂµm kMGTPEZY\"])\n",
    "    exp = np.int8(np.log10(np.abs(val)) // 3 * 3 * np.sign(val))\n",
    "    val /= 10.**exp\n",
    "    prefix = PREFIXES[exp//3 + len(PREFIXES)//2]\n",
    "    return f\"{val:{spec}}{prefix}s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# From https://uploadcare.com/blog/fast-import-of-pillow-images-to-numpy-opencv-arrays/\n",
    "# Up to 2.5 times faster with the same functionality and a smaller number of allocations than numpy.asarray(img)\n",
    "def pil_to_numpy(img:Image.Image) -> np.ndarray:\n",
    "    \"Fast conversion of Pillow `Image` to NumPy NDArray\"\n",
    "    img.load()\n",
    "    # unpack data\n",
    "    enc = Image._getencoder(img.mode, 'raw', img.mode)\n",
    "    enc.setimage(img.im)\n",
    "\n",
    "    # NumPy buffer for the result\n",
    "    shape, typestr = Image._conv_type_shape(img)\n",
    "    data = np.empty(shape, dtype=np.dtype(typestr))\n",
    "    mem = data.data.cast('B', (data.data.nbytes,))\n",
    "\n",
    "    bufsize, s, offset = 65536, 0, 0\n",
    "    while not s:\n",
    "        l, s, d = enc.encode(bufsize)\n",
    "        mem[offset:offset + len(d)] = d\n",
    "        offset += len(d)\n",
    "    if s < 0:\n",
    "        raise RuntimeError(\"encoder error %d in tobytes\" % s)\n",
    "    return data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
