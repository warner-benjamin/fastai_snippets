{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp optimizer.eightbit\n",
    "#|default_cls_lvl 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "# Contains code from:\n",
    "# bitsandbytes - MIT License - Copyright (c) Facebook, Inc. and its affiliates.\n",
    "# fastai - Apache License 2.0 - Copyright (c) 2023 fast.ai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8-Bit Optimizers\n",
    "> bitsandbytes 8-bit optimizers with full fastai compatibility"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[bitsandbytes](https://github.com/TimDettmers/bitsandbytes) 8-bit optimizers can reduce optimizer memory usage up to 75% compared to 32-bit optimizers.\n",
    "\n",
    "While it is possible to use bitsandbytes optimizers[^PyTorch] with fastai via `fastai.optimizer.OptimWrapper`, this doesn't provide compatibility with all fastai optimizer features. fastxtend adds full fastai compatibility to bitsandbytes 8-bit optimizers, including per-parameter weight decay, automatic weight decay exclusion for normalization and bias terms, and discriminative learning rate support.\n",
    "\n",
    ":::{.callout-note}\n",
    "#### Note: 8-bit Optimizer Usage\n",
    "While 8-bit optimizer support is defined and detailed here, they are integrated into and intended to be used via fastxtend's [fused fastai optimizers](optimizer.fused.html) for [SGD](#sgd-optimizer), [Adam](#adam-optimizer), [LARS](#larc-optimizer), and [LAMB](#lamb-optimizer), and via fastxtend's [Lion optimizer](optimizer.lion.html) as shown below.\n",
    ":::\n",
    "\n",
    "To use 8-bit optimizers, install bitsandbytes on a machine with a Cuda device\n",
    "\n",
    "```bash\n",
    "pip install bitandbytes\n",
    "```\n",
    "\n",
    "then import fastxtend optimizers after importing fastai\n",
    "\n",
    "```python\n",
    "from fastxtend.vision.all import *\n",
    "# or just import fastxtend optimizers\n",
    "from fastxtend.optimizer.all import *\n",
    "\n",
    "opt_func = adam(eightbit=True)\n",
    "Learner(..., opt_func=opt_func)\n",
    "```\n",
    "\n",
    "If training NLP models, you may need to replace the PyTorch embedding layer with a bitsandbytes layer : `torch.nn.Embedding(..) -> bnb.nn.Embedding(..)`.\n",
    "\n",
    "Check out the bitsandbytes [readme](https://github.com/TimDettmers/bitsandbytes#using-the-8-bit-optimizers) for more details on using 8-bit optimizers.\n",
    "\n",
    ":::{.callout-note collapse=\"true\"}\n",
    "#### Note: Modification of Synchronize\n",
    "bitsandbytes calls [`torch.cuda.synchronize`](https://pytorch.org/docs/stable/generated/torch.cuda.synchronize.html) after each optimizer step. This prevents starting the next optimizer step until the current step finishes, which may increase optimizer wallclock time.\n",
    "\n",
    "fastxtend adds `sync_each_step=False` as an argument to both all 8-bit optimizers, disabling the per-step `torch.cuda.synchronize`. Set to `sync_each_step=True` to match bitsandbytes behavior.\n",
    ":::\n",
    "\n",
    "[^PyTorch]: Or any PyTorch-compatible optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "\n",
    "from bitsandbytes.optim.optimizer import Optimizer1State, Optimizer2State, MockArgs\n",
    "import bitsandbytes.functional as BF\n",
    "\n",
    "from fastcore.basics import even_mults\n",
    "\n",
    "from fastxtend.optimizer.utils import FastaiOptimizerAdapter, _convert_params\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastai and bitsandbytes Compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class EightBitFastaiAdapter(FastaiOptimizerAdapter):\n",
    "    \"Base for adding fastai optimizer functionality to eight-bit optimizers\"\n",
    "\n",
    "    def get_config(self, gindex, pindex, group):\n",
    "        config = {}\n",
    "        config[\"mom\"] = group[\"mom\"]\n",
    "        config[\"sqr_mom\"] = group[\"sqr_mom\"]\n",
    "        config[\"wd\"] = group[\"wd\"]\n",
    "        config[\"eps\"] = group[\"eps\"]\n",
    "        config[\"lr\"] = group[\"lr\"]\n",
    "        config[\"optim_bits\"] = self.args.optim_bits\n",
    "        config[\"min_8bit_size\"] = self.args.min_8bit_size\n",
    "        config[\"percentile_clipping\"] = self.args.percentile_clipping\n",
    "        config[\"block_wise\"] = self.args.block_wise\n",
    "        config[\"max_unorm\"] = self.args.max_unorm\n",
    "        config[\"skip_zeros\"] = self.args.skip_zeros\n",
    "\n",
    "        if (gindex, pindex) in self.mng.index2config:\n",
    "            config.update(self.mng.index2config[(gindex, pindex)])\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(EightBitFastaiAdapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class EightBitCommon:\n",
    "    \"Common changes to EightBit Optimizers\"\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        if not self.initialized:\n",
    "            self.check_overrides()\n",
    "            self.to_gpu()  # needed for fairseq pure fp16 training\n",
    "            self.initialized = True\n",
    "\n",
    "        #if self.is_paged: self.page_mng.prefetch_all()\n",
    "        for gindex, group in enumerate(self.param_groups):\n",
    "            for pindex, p in enumerate(group[\"params\"]):\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state = self.state[p]\n",
    "                if 'step' not in state:\n",
    "                    self.init_state(group, p, gindex, pindex)\n",
    "\n",
    "                self.prefetch_state(p)\n",
    "                self.update_step(group, p, gindex, pindex)\n",
    "                if self.sync_each_step:\n",
    "                    torch.cuda.synchronize()\n",
    "        if self.is_paged or not self.sync_each_step:\n",
    "            # all paged operation are asynchronous, we need\n",
    "            # to sync to make sure all tensors are in the right state\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(EightBitCommon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class EightBit1StateOptimizer(EightBitCommon, EightBitFastaiAdapter, Optimizer1State):\n",
    "    \"Adds fastai optimizer functionality & compatability to `Optimizer1State`\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer_name,\n",
    "        params,\n",
    "        lr=1e-3,\n",
    "        mom=0.9,\n",
    "        sqr_mom=0.0,\n",
    "        eps=1e-8,\n",
    "        wd=0.0,\n",
    "        optim_bits=8,\n",
    "        args=None,\n",
    "        min_8bit_size=4096,\n",
    "        percentile_clipping=100,\n",
    "        block_wise=True,\n",
    "        max_unorm=0.0,\n",
    "        skip_zeros=False,\n",
    "        is_paged=False,\n",
    "        sync_each_step=False\n",
    "    ):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(f\"Invalid epsilon value: {eps}\")\n",
    "        if not 0.0 <= mom < 1.0:\n",
    "            raise ValueError(f\"Invalid mom value: {mom}\")\n",
    "        if not 0.0 <= sqr_mom < 1.0:\n",
    "            raise ValueError(f\"Invalid sqr_mom value: {sqr_mom}\")\n",
    "        if not 0.0 <= wd:\n",
    "            raise ValueError(f\"Invalid weight_decay value: {wd}\")\n",
    "        defaults = dict(lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)\n",
    "        params = super().convert_params(params)\n",
    "        super(Optimizer1State, self).__init__(params=params, defaults=defaults, optim_bits=optim_bits, is_paged=is_paged)\n",
    "\n",
    "        if args is None:\n",
    "            args = {}\n",
    "            args[\"optim_bits\"] = optim_bits\n",
    "            args[\"percentile_clipping\"] = 100\n",
    "            args[\"min_8bit_size\"] = min_8bit_size\n",
    "            args[\"percentile_clipping\"] = percentile_clipping\n",
    "            args[\"block_wise\"] = block_wise\n",
    "            args[\"max_unorm\"] = max_unorm\n",
    "            args[\"skip_zeros\"] = skip_zeros\n",
    "\n",
    "            self.args = MockArgs(args)\n",
    "        else:\n",
    "            self.args = args\n",
    "\n",
    "        self.optimizer_name = optimizer_name\n",
    "        self.sync_each_step = sync_each_step\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_step(self, group, p, gindex, pindex):\n",
    "        state = self.state[p]\n",
    "        grad = p.grad\n",
    "\n",
    "        config = self.get_config(gindex, pindex, group)\n",
    "\n",
    "        state[\"step\"] += 1\n",
    "        step = state[\"step\"]\n",
    "\n",
    "        if config[\"percentile_clipping\"] < 100:\n",
    "            current_gnorm, clip_value, gnorm_scale = BF.percentile_clipping(\n",
    "                grad, state[\"gnorm_vec\"], step, config[\"percentile_clipping\"]\n",
    "            )\n",
    "        else:\n",
    "            gnorm_scale = 1.0\n",
    "\n",
    "        if state[\"state1\"].dtype == torch.float:\n",
    "            BF.optimizer_update_32bit(\n",
    "                optimizer_name=self.optimizer_name,\n",
    "                g=grad,\n",
    "                p=p,\n",
    "                state1=state[\"state1\"],\n",
    "                beta1=config[\"mom\"],\n",
    "                eps=config[\"eps\"],\n",
    "                step=step,\n",
    "                lr=config[\"lr\"],\n",
    "                state2=None,\n",
    "                beta2=config[\"sqr_mom\"],\n",
    "                weight_decay=state.get('wd', config['wd']) if state.get('do_wd', True) else 0.0,\n",
    "                gnorm_scale=gnorm_scale,\n",
    "                unorm_vec=state[\"unorm_vec\"] if config[\"max_unorm\"] > 0.0 else None,\n",
    "                max_unorm=config[\"max_unorm\"],\n",
    "                skip_zeros=config[\"skip_zeros\"],\n",
    "            )\n",
    "\n",
    "        elif state[\"state1\"].dtype == torch.uint8 and not config[\"block_wise\"]:\n",
    "            BF.optimizer_update_8bit(\n",
    "                optimizer_name=self.optimizer_name,\n",
    "                g=grad,\n",
    "                p=p,\n",
    "                state1=state[\"state1\"],\n",
    "                state2=None,\n",
    "                beta1=config[\"mom\"],\n",
    "                beta2=config['sqr_mom'],\n",
    "                eps=config[\"eps\"],\n",
    "                step=step,\n",
    "                lr=config[\"lr\"],\n",
    "                qmap1=state[\"qmap1\"],\n",
    "                qmap2=None,\n",
    "                max1=state[\"max1\"],\n",
    "                max2=None,\n",
    "                new_max1=state[\"new_max1\"],\n",
    "                new_max2=None,\n",
    "                weight_decay=state.get('wd', config['wd']) if state.get('do_wd', True) else 0.0,\n",
    "                gnorm_scale=gnorm_scale,\n",
    "                unorm_vec=state[\"unorm_vec\"] if config[\"max_unorm\"] > 0.0 else None,\n",
    "                max_unorm=config[\"max_unorm\"],\n",
    "            )\n",
    "\n",
    "            state[\"max1\"], state[\"new_max1\"] = state[\"new_max1\"], state[\"max1\"]\n",
    "        elif state[\"state1\"].dtype == torch.uint8 and config[\"block_wise\"]:\n",
    "            BF.optimizer_update_8bit_blockwise(\n",
    "                optimizer_name=self.optimizer_name,\n",
    "                g=grad,\n",
    "                p=p,\n",
    "                state1=state[\"state1\"],\n",
    "                state2=None,\n",
    "                beta1=config[\"mom\"],\n",
    "                beta2=config['sqr_mom'],\n",
    "                eps=config[\"eps\"],\n",
    "                step=step,\n",
    "                lr=config[\"lr\"],\n",
    "                qmap1=state[\"qmap1\"],\n",
    "                qmap2=None,\n",
    "                absmax1=state[\"absmax1\"],\n",
    "                absmax2=None,\n",
    "                weight_decay=state.get('wd', config['wd']) if state.get('do_wd', True) else 0.0,\n",
    "                gnorm_scale=gnorm_scale,\n",
    "                skip_zeros=config[\"skip_zeros\"],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(EightBit1StateOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class EightBit2StateOptimizer(EightBitCommon, EightBitFastaiAdapter, Optimizer2State):\n",
    "    \"Adds fastai optimizer functionality & compatability to `Optimizer2State`\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer_name,\n",
    "        params,\n",
    "        lr=1e-3,\n",
    "        mom=0.9,\n",
    "        sqr_mom=0.999,\n",
    "        eps=1e-8,\n",
    "        wd=0.0,\n",
    "        optim_bits=8,\n",
    "        args=None,\n",
    "        min_8bit_size=4096,\n",
    "        percentile_clipping=100,\n",
    "        block_wise=True,\n",
    "        max_unorm=0.0,\n",
    "        skip_zeros=False,\n",
    "        is_paged=False,\n",
    "        sync_each_step=False\n",
    "    ):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(f\"Invalid epsilon value: {eps}\")\n",
    "        if not 0.0 <= mom < 1.0:\n",
    "            raise ValueError(f\"Invalid mom value: {mom}\")\n",
    "        if not 0.0 <= sqr_mom < 1.0:\n",
    "            raise ValueError(f\"Invalid sqr_mom value: {sqr_mom}\")\n",
    "        if not 0.0 <= wd:\n",
    "            raise ValueError(f\"Invalid weight_decay value: {wd}\")\n",
    "        defaults = dict(lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)\n",
    "        params = super().convert_params(params)\n",
    "        super(Optimizer2State, self).__init__(params, defaults, optim_bits, is_paged)\n",
    "\n",
    "        if args is None:\n",
    "            args = {}\n",
    "            args[\"optim_bits\"] = optim_bits\n",
    "            args[\"percentile_clipping\"] = 100\n",
    "            args[\"min_8bit_size\"] = min_8bit_size\n",
    "            args[\"percentile_clipping\"] = percentile_clipping\n",
    "            args[\"block_wise\"] = block_wise\n",
    "            args[\"max_unorm\"] = max_unorm\n",
    "            args[\"skip_zeros\"] = skip_zeros\n",
    "\n",
    "            self.args = MockArgs(args)\n",
    "        else:\n",
    "            self.args = args\n",
    "\n",
    "        self.optimizer_name = optimizer_name\n",
    "        self.sync_each_step = sync_each_step\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_step(self, group, p, gindex, pindex):\n",
    "        state = self.state[p]\n",
    "        grad = p.grad\n",
    "\n",
    "        config = self.get_config(gindex, pindex, group)\n",
    "\n",
    "        state[\"step\"] += 1\n",
    "        step = state[\"step\"]\n",
    "\n",
    "        if config[\"percentile_clipping\"] < 100:\n",
    "            current_gnorm, clip_value, gnorm_scale = BF.percentile_clipping(\n",
    "                grad, state[\"gnorm_vec\"], step, config[\"percentile_clipping\"]\n",
    "            )\n",
    "        else:\n",
    "            gnorm_scale = 1.0\n",
    "\n",
    "        if state[\"state1\"].dtype == torch.float:\n",
    "            BF.optimizer_update_32bit(\n",
    "                optimizer_name=self.optimizer_name,\n",
    "                g=grad,\n",
    "                p=p,\n",
    "                state1=state[\"state1\"],\n",
    "                beta1=config[\"mom\"],\n",
    "                eps=config[\"eps\"],\n",
    "                step=step,\n",
    "                lr=config[\"lr\"],\n",
    "                state2=state[\"state2\"],\n",
    "                beta2=config[\"sqr_mom\"],\n",
    "                weight_decay=state.get('wd', config['wd']) if state.get('do_wd', True) else 0.0,\n",
    "                gnorm_scale=gnorm_scale,\n",
    "                unorm_vec=state[\"unorm_vec\"] if config[\"max_unorm\"] > 0.0 else None,\n",
    "                max_unorm=config[\"max_unorm\"],\n",
    "                skip_zeros=config[\"skip_zeros\"],\n",
    "            )\n",
    "\n",
    "        elif state[\"state1\"].dtype == torch.uint8 and not config[\"block_wise\"]:\n",
    "            BF.optimizer_update_8bit(\n",
    "                optimizer_name=self.optimizer_name,\n",
    "                g=grad,\n",
    "                p=p,\n",
    "                state1=state[\"state1\"],\n",
    "                state2=state[\"state2\"],\n",
    "                beta1=config[\"mom\"],\n",
    "                beta2=config['sqr_mom'],\n",
    "                eps=config[\"eps\"],\n",
    "                step=step,\n",
    "                lr=config[\"lr\"],\n",
    "                qmap1=state[\"qmap1\"],\n",
    "                qmap2=state[\"qmap2\"],\n",
    "                max1=state[\"max1\"],\n",
    "                max2=state[\"max2\"],\n",
    "                new_max1=state[\"new_max1\"],\n",
    "                new_max2=state[\"new_max2\"],\n",
    "                weight_decay=state.get('wd', config['wd']) if state.get('do_wd', True) else 0.0,\n",
    "                gnorm_scale=gnorm_scale,\n",
    "                unorm_vec=state[\"unorm_vec\"] if config[\"max_unorm\"] > 0.0 else None,\n",
    "                max_unorm=config[\"max_unorm\"],\n",
    "            )\n",
    "\n",
    "            # swap maxes\n",
    "            state[\"max1\"], state[\"new_max1\"] = state[\"new_max1\"], state[\"max1\"]\n",
    "            state[\"max2\"], state[\"new_max2\"] = state[\"new_max2\"], state[\"max2\"]\n",
    "        elif state[\"state1\"].dtype == torch.uint8 and config[\"block_wise\"]:\n",
    "            BF.optimizer_update_8bit_blockwise(\n",
    "                optimizer_name=self.optimizer_name,\n",
    "                g=grad,\n",
    "                p=p,\n",
    "                state1=state[\"state1\"],\n",
    "                state2=state[\"state2\"],\n",
    "                beta1=config[\"mom\"],\n",
    "                beta2=config['sqr_mom'],\n",
    "                eps=config[\"eps\"],\n",
    "                step=step,\n",
    "                lr=config[\"lr\"],\n",
    "                qmap1=state[\"qmap1\"],\n",
    "                qmap2=state[\"qmap2\"],\n",
    "                absmax1=state[\"absmax1\"],\n",
    "                absmax2=state[\"absmax2\"],\n",
    "                weight_decay=state.get('wd', config['wd']) if state.get('do_wd', True) else 0.0,\n",
    "                gnorm_scale=gnorm_scale,\n",
    "                skip_zeros=config[\"skip_zeros\"],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(EightBit2StateOptimizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8-bit Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class SGD8bitOptimizer(EightBit1StateOptimizer):\n",
    "    \"A fastai-compatible bitsandbytes 8-bit SGD optimizer\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr,\n",
    "        mom,\n",
    "        wd=0,\n",
    "        args=None,\n",
    "        min_8bit_size=4096,\n",
    "        percentile_clipping=100,\n",
    "        block_wise=True,\n",
    "        sync_each_step=False\n",
    "    ):\n",
    "        if mom == 0:\n",
    "            raise NotImplementedError(f\"8-bit SGD without momentum {mom=} is not supported\")\n",
    "        super().__init__(\n",
    "            optimizer_name=\"momentum\",\n",
    "            params=params,\n",
    "            lr=lr,\n",
    "            mom=mom,\n",
    "            sqr_mom=0.0,\n",
    "            eps=0.0,\n",
    "            wd=wd,\n",
    "            optim_bits=8,\n",
    "            args=args,\n",
    "            min_8bit_size=min_8bit_size,\n",
    "            percentile_clipping=percentile_clipping,\n",
    "            block_wise=block_wise,\n",
    "            sync_each_step=sync_each_step\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class RMSProp8bitOptimizer(EightBit1StateOptimizer):\n",
    "    \"A fastai-compatible bitsandbytes 8-bit RMSProb optimizer\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr=1e-2,\n",
    "        sqr_mom=0.99,\n",
    "        eps=1e-8,\n",
    "        wd=0,\n",
    "        args=None,\n",
    "        min_8bit_size=4096,\n",
    "        percentile_clipping=100,\n",
    "        block_wise=True,\n",
    "        sync_each_step=False\n",
    "    ):\n",
    "        if sqr_mom == 0:\n",
    "            raise NotImplementedError(f\"8-bit RMSProp with {sqr_mom=} is not supported\")\n",
    "        super().__init__(\n",
    "            optimizer_name=\"rmsprop\",\n",
    "            params=params,\n",
    "            lr=lr,\n",
    "            mom=sqr_mom,\n",
    "            sqr_mom=0.0,\n",
    "            eps=eps,\n",
    "            wd=wd,\n",
    "            optim_bits=8,\n",
    "            args=args,\n",
    "            min_8bit_size=min_8bit_size,\n",
    "            percentile_clipping=percentile_clipping,\n",
    "            block_wise=block_wise,\n",
    "            sync_each_step=sync_each_step\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AdamW8bitOptimizer(EightBit2StateOptimizer):\n",
    "    \"A fastai-compatible bitsandbytes 8-bit AdamW optimizer\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr=1e-3,\n",
    "        mom=0.9,\n",
    "        sqr_mom=0.99,\n",
    "        eps=1e-8,\n",
    "        wd=1e-2,\n",
    "        args=None,\n",
    "        min_8bit_size=4096,\n",
    "        percentile_clipping=100,\n",
    "        block_wise=True,\n",
    "        is_paged=False,\n",
    "        sync_each_step=False\n",
    "    ):\n",
    "        super().__init__(\n",
    "            optimizer_name=\"adam\",\n",
    "            params=params,\n",
    "            lr=lr,\n",
    "            mom=mom,\n",
    "            sqr_mom=sqr_mom,\n",
    "            eps=eps,\n",
    "            wd=wd,\n",
    "            optim_bits=8,\n",
    "            args=args,\n",
    "            min_8bit_size=min_8bit_size,\n",
    "            percentile_clipping=percentile_clipping,\n",
    "            block_wise=block_wise,\n",
    "            is_paged=is_paged,\n",
    "            sync_each_step=sync_each_step\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LARS8bitOptimizer(EightBit1StateOptimizer):\n",
    "    \"A fastai-compatible bitsandbytes 8-bit LARS optimizer\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr,\n",
    "        mom=0,\n",
    "        wd=0,\n",
    "        args=None,\n",
    "        min_8bit_size=4096,\n",
    "        percentile_clipping=100,\n",
    "        trust_coeff=0.02,\n",
    "        sync_each_step=False\n",
    "    ):\n",
    "        if mom == 0:\n",
    "            raise NotImplementedError(f\"8-bit LARS without momentum {mom=} is not supported\")\n",
    "        super().__init__(\n",
    "            optimizer_name=\"lars\",\n",
    "            params=params,\n",
    "            lr=lr,\n",
    "            mom=mom,\n",
    "            sqr_mom=0.0,\n",
    "            eps=0.0,\n",
    "            wd=wd,\n",
    "            optim_bits=8,\n",
    "            args=args,\n",
    "            min_8bit_size=min_8bit_size,\n",
    "            percentile_clipping=percentile_clipping,\n",
    "            max_unorm=trust_coeff,\n",
    "            block_wise=False,\n",
    "            sync_each_step=sync_each_step,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LAMB8bitOptimizer(EightBit2StateOptimizer):\n",
    "    \"A fastai-compatible bitsandbytes 8-bit LAMB optimizer\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr=1e-3,\n",
    "        mom=0.9,\n",
    "        sqr_mom=0.999,\n",
    "        eps=1e-8,\n",
    "        wd=0,\n",
    "        args=None,\n",
    "        min_8bit_size=4096,\n",
    "        percentile_clipping=100,\n",
    "        block_wise=False,\n",
    "        sync_each_step=False\n",
    "    ):\n",
    "        super().__init__(\n",
    "            optimizer_name=\"lamb\",\n",
    "            params=params,\n",
    "            lr=lr,\n",
    "            mom=mom,\n",
    "            sqr_mom=sqr_mom,\n",
    "            eps=eps,\n",
    "            wd=wd,\n",
    "            optim_bits=8,\n",
    "            args=args,\n",
    "            min_8bit_size=min_8bit_size,\n",
    "            percentile_clipping=percentile_clipping,\n",
    "            block_wise=block_wise,\n",
    "            max_unorm=1.0,\n",
    "            sync_each_step=sync_each_step,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Lion8bitOptimizer(EightBit1StateOptimizer):\n",
    "    \"A fastai-compatible bitsandbytes 8-bit Lion optimizer\"\n",
    "    def __init__(self,\n",
    "        params,\n",
    "        lr=1e-4,\n",
    "        beta1=0.9,\n",
    "        beta2=0.99,\n",
    "        wd=0,\n",
    "        args=None,\n",
    "        min_8bit_size=4096,\n",
    "        percentile_clipping=100,\n",
    "        block_wise=True,\n",
    "        is_paged=False,\n",
    "        sync_each_step=False\n",
    "    ):\n",
    "        super().__init__(\n",
    "            optimizer_name=\"lion\",\n",
    "            params=params,\n",
    "            lr=lr,\n",
    "            mom=beta1,\n",
    "            sqr_mom=beta2,\n",
    "            eps=0.0,\n",
    "            wd=wd,\n",
    "            optim_bits=8,\n",
    "            args=args,\n",
    "            min_8bit_size=min_8bit_size,\n",
    "            percentile_clipping=percentile_clipping,\n",
    "            block_wise=block_wise,\n",
    "            is_paged=is_paged,\n",
    "            sync_each_step=sync_each_step,\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastai Compatiblity Tests -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastxtend.test_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params = [tst_params(), tst_params(), tst_params()]\n",
    "for opt in [EightBit1StateOptimizer('momentum', params, lr=0.1), EightBit2StateOptimizer('adam', params, lr=0.1)]:\n",
    "    #Freezing the first layer\n",
    "    opt.freeze_to(1)\n",
    "    req_grad = Self.requires_grad()\n",
    "    test_eq(L(params[0]).map(req_grad), [False]*4)\n",
    "    for i in {1,2}:\n",
    "        test_eq(L(params[i]).map(req_grad), [True]*4)\n",
    "\n",
    "    #Unfreezing\n",
    "    opt.unfreeze()\n",
    "    for i in range(2):\n",
    "        test_eq(L(params[i]).map(req_grad), [True]*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params = [tst_params(), tst_params(), tst_params()]\n",
    "for opt in [EightBit1StateOptimizer('momentum', params, lr=0.1), EightBit2StateOptimizer('adam', params, lr=0.1)]:\n",
    "    for p in L(params[1])[[1,3]]:\n",
    "        opt.state[p] = {'force_train': True}\n",
    "    opt.freeze()\n",
    "    test_eq(L(params[0]).map(req_grad), [False]*4)\n",
    "    test_eq(L(params[1]).map(req_grad), [False, True, False, True])\n",
    "    test_eq(L(params[2]).map(req_grad), [True]*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params = [tst_params(), tst_params(), tst_params()]\n",
    "for opt in [EightBit1StateOptimizer('momentum', params), EightBit2StateOptimizer('adam', params)]:\n",
    "    opt.set_hypers(lr=3e-3, mom=0.98, sqr_mom=0.9999, eps=1e-6, wd=0.01)\n",
    "    test_eq(opt.hypers, [dict(lr=3e-3, mom=0.98, sqr_mom=0.9999, eps=1e-6, wd=0.01)]*3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8-bit Optimizer Tests -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "from fastai.optimizer import (weight_decay, l2_reg, average_grad, momentum_step,\n",
    "                              average_sqr_grad, rms_prop_step, step_stat, adam_step,\n",
    "                              larc_layer_lr, larc_step, lamb_step, Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "def assert_most_approx_close(a, b, rtol=1e-3, atol=1e-3, max_error_count=10):\n",
    "    idx = torch.isclose(a, b, rtol=rtol, atol=atol)\n",
    "    error_count = (idx == 0).sum().item()\n",
    "    if error_count > max_error_count:\n",
    "        print(f\"Too many values not close: assert {error_count} < {max_error_count}\")\n",
    "        torch.testing.assert_close(a, b, rtol=rtol, atol=atol)\n",
    "\n",
    "def tst_param(shape):\n",
    "    \"Create a tensor with `val` and a gradient of `grad` for testing\"\n",
    "    param = torch.rand(shape, device='cuda', dtype=torch.float32) * 0.1\n",
    "    param.grad = torch.rand(shape, device='cuda', dtype=param.dtype) * 0.01\n",
    "    return param\n",
    "\n",
    "def tst_params():\n",
    "    params1, params2 = [], []\n",
    "    for i in range(4):\n",
    "        param1 = tst_param(4096)\n",
    "        param2 = param1.clone()\n",
    "        param2.grad = param1.grad.clone()\n",
    "        params1.append(param1)\n",
    "        params2.append(param2)\n",
    "    return params1, params2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "def SGD(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    mom:float=0.9, # Gradient moving average (β1) coefficient\n",
    "    wd:float=0., # Optional L2 weight decay\n",
    "    eightbit:bool=False, # Use fused 8-bit implementation\n",
    "    **eightbitargs\n",
    ") -> Optimizer|SGD8bitOptimizer:\n",
    "    if eightbit:\n",
    "        return SGD8bitOptimizer(params, lr=lr, mom=mom, wd=wd, **eightbitargs)\n",
    "    else:\n",
    "        cbs = [l2_reg, average_grad, momentum_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "params_org, params_8bit = tst_params()\n",
    "for p, e in zip(params_org, params_8bit):\n",
    "    assert_most_approx_close(p, e)\n",
    "\n",
    "opt_org = SGD(params_org, lr=0.01)\n",
    "opt_org.step()\n",
    "\n",
    "opt_8bit = SGD(params_8bit, lr=0.01, eightbit=True)\n",
    "opt_8bit.step()\n",
    "for p, e in zip(params_org, params_8bit):\n",
    "    assert_most_approx_close(p, e)\n",
    "\n",
    "opt_org.step()\n",
    "opt_8bit.step()\n",
    "for p, e in zip(params_org, params_8bit):\n",
    "    assert_most_approx_close(p, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "def RMSProp(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    sqr_mom:float=0.99, # Gradient squared moving average (β2) coefficient\n",
    "    eps:float=1e-8, # Added for numerical stability\n",
    "    wd:float=0., # Optional weight decay (true or L2)\n",
    "    eightbit:bool=False, # Use fused 8-bit implementation\n",
    "    **eightbitargs\n",
    ") -> Optimizer|RMSProp8bitOptimizer:\n",
    "    \"A fastai RMSProp/RMSPropW optimizer with fused TorchScript and 8-bit implementations\"\n",
    "    if eightbit:\n",
    "        return RMSProp8bitOptimizer(params, lr=lr, sqr_mom=sqr_mom, eps=eps, wd=wd, **eightbitargs)\n",
    "    else:\n",
    "        cbs = [l2_reg, average_sqr_grad, rms_prop_step]\n",
    "        return Optimizer(params, cbs, lr=lr, sqr_mom=sqr_mom, wd=wd, eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "params_org, params_8bit = tst_params()\n",
    "for p, e in zip(params_org, params_8bit):\n",
    "    assert_most_approx_close(p, e)\n",
    "\n",
    "opt_org = RMSProp(params_org, lr=1e-3, wd=0)\n",
    "opt_org.step()\n",
    "\n",
    "opt_8bit = RMSProp(params_8bit, lr=1e-3, wd=0, eightbit=True)\n",
    "opt_8bit.step()\n",
    "for p, e in zip(params_org, params_8bit):\n",
    "    assert_most_approx_close(p, e)\n",
    "\n",
    "opt_org.step()\n",
    "opt_8bit.step()\n",
    "for p, e in zip(params_org, params_8bit):\n",
    "    assert_most_approx_close(p, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "def Adam(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    mom:float=0.9, # Gradient moving average (β1) coefficient\n",
    "    sqr_mom:float=0.99, # Gradient squared moving average (β2) coefficient\n",
    "    eps:float=1e-5, # Added for numerical stability\n",
    "    wd:float=0.01, # Optional weight decay (true or L2)\n",
    "    eightbit:bool=False, # Use fused 8-bit implementation\n",
    "    **eightbitargs\n",
    ") -> Optimizer|AdamW8bitOptimizer:\n",
    "    if eightbit:\n",
    "        return AdamW8bitOptimizer(params, lr=lr, mom=mom, sqr_mom=sqr_mom,\n",
    "                                eps=eps, wd=wd, **eightbitargs)\n",
    "    else:\n",
    "        cbs = [weight_decay, partial(average_grad, dampening=True), average_sqr_grad, step_stat, adam_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "params_org, params_8bit = tst_params()\n",
    "for p, e in zip(params_org, params_8bit):\n",
    "    assert_most_approx_close(p, e)\n",
    "\n",
    "opt_org = Adam(params_org, lr=1e-3)\n",
    "opt_org.step()\n",
    "\n",
    "opt_8bit = Adam(params_8bit, lr=1e-3, eightbit=True)\n",
    "opt_8bit.step()\n",
    "for p, e in zip(params_org, params_8bit):\n",
    "    assert_most_approx_close(p, e)\n",
    "\n",
    "opt_org.step()\n",
    "opt_8bit.step()\n",
    "for p, e in zip(params_org, params_8bit):\n",
    "    assert_most_approx_close(p, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "def Lars(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    mom:float=0.9, # Gradient moving average (β1) coefficient\n",
    "    trust_coeff:float=0.02, # Trust coeffiecnet for calculating layerwise LR\n",
    "    eps:float=1e-8, # Added for numerical stability\n",
    "    wd:float=0., # Optional weight decay (true or L2)\n",
    "    eightbit:bool=False, # Use fused 8-bit implementation. Only supports LARS: `clip=False`\n",
    "    **eightbitargs\n",
    ") -> Optimizer|LARS8bitOptimizer:\n",
    "    if eightbit:\n",
    "        return LARS8bitOptimizer(params, lr=lr, mom=mom, wd=wd, trust_coeff=trust_coeff, **eightbitargs)\n",
    "    else:\n",
    "        cbs = [l2_reg, average_grad,partial(larc_layer_lr, clip=False), larc_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, trust_coeff=trust_coeff, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "params_org, params_8bit = tst_params()\n",
    "for p, e in zip(params_org, params_8bit):\n",
    "    assert_most_approx_close(p, e)\n",
    "\n",
    "opt_org = Lars(params_org, lr=0.01)\n",
    "opt_org.step()\n",
    "\n",
    "opt_8bit = Lars(params_8bit, lr=0.01, eightbit=True)\n",
    "opt_8bit.step()\n",
    "for p, e in zip(params_org, params_8bit):\n",
    "    assert_most_approx_close(p, e)\n",
    "\n",
    "opt_org.step()\n",
    "opt_8bit.step()\n",
    "for p, e in zip(params_org, params_8bit):\n",
    "    assert_most_approx_close(p, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "def Lamb(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    mom:float=0.9, # Gradient moving average (β1) coefficient\n",
    "    sqr_mom:float=0.99, # Gradient squared moving average (β2) coefficient\n",
    "    eps:float=1e-5, # Added for numerical stability\n",
    "    wd:float=0., # Optional weight decay (true or L2)\n",
    "    eightbit:bool=False, # Use fused 8-bit implementation. Only supports true weight decay\n",
    "    **eightbitargs\n",
    ") -> Optimizer|LAMB8bitOptimizer:\n",
    "    if eightbit:\n",
    "        return LAMB8bitOptimizer(params, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd, **eightbitargs)\n",
    "    else:\n",
    "        cbs = [weight_decay, partial(average_grad, dampening=True), average_sqr_grad, step_stat, lamb_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "params_org, params_8bit = tst_params()\n",
    "for p, e in zip(params_org, params_8bit):\n",
    "    assert_most_approx_close(p, e)\n",
    "\n",
    "opt_org = Lamb(params_org, lr=0.01)\n",
    "opt_org.step()\n",
    "\n",
    "opt_8bit = Lamb(params_8bit, lr=0.01, eightbit=True)\n",
    "opt_8bit.step()\n",
    "for p, e in zip(params_org, params_8bit):\n",
    "    assert_most_approx_close(p, e)\n",
    "\n",
    "opt_org.step()\n",
    "opt_8bit.step()\n",
    "for p, e in zip(params_org, params_8bit):\n",
    "    assert_most_approx_close(p, e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8-bit Training Test -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "from packaging.version import parse\n",
    "import fastai\n",
    "\n",
    "from fastcore.basics import num_cpus\n",
    "\n",
    "if parse(fastai.__version__) < parse('2.7.11'):\n",
    "    from fastxtend.callback.channelslast import *\n",
    "else:\n",
    "    from fastai.callback.channelslast import *\n",
    "from fastai.data.external import URLs, untar_data\n",
    "from fastai.data.block import DataBlock, CategoryBlock\n",
    "from fastai.data.transforms import GrandparentSplitter, get_image_files, parent_label, Normalize\n",
    "from fastai.learner import Learner\n",
    "from fastai.vision.augment import Resize\n",
    "from fastai.vision.core import imagenet_stats\n",
    "from fastai.vision.data import ImageBlock\n",
    "from fastxtend.metrics import *\n",
    "from fastxtend.vision.models.xresnet import xresnext34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastxtend.optimizer import adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "imagenette = untar_data(URLs.IMAGENETTE_160)\n",
    "\n",
    "dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "                    splitter=GrandparentSplitter(valid_name='val'),\n",
    "                    get_items=get_image_files, get_y=parent_label,\n",
    "                    item_tfms=Resize(128),\n",
    "                    batch_tfms=[Normalize.from_stats(*imagenet_stats)])\n",
    "\n",
    "dls = dblock.dataloaders(imagenette, bs=64, num_workers=num_cpus())\n",
    "\n",
    "learn = Learner(dls, xresnext34(n_out=dls.c), opt_func=adam(eightbit=True),\n",
    "                loss_func=nn.CrossEntropyLoss(label_smoothing=0.1),\n",
    "                metrics=Accuracy()).to_channelslast()\n",
    "\n",
    "learn.fit_one_cycle(5, 3e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
