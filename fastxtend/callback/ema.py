# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/callback.ema.ipynb.

# %% ../../nbs/callback.ema.ipynb 1
# EMA Callbacks are inspired by timm's ModelEmaV2: https://github.com/rwightman/pytorch-image-models/blob/main/timm/utils/model_ema.py
# PyTorch Image Models - Apache License 2.0 - Copyright (c) 2020 Ross Wightman

# %% ../../nbs/callback.ema.ipynb 3
from __future__ import annotations

from copy import deepcopy

from fastai.callback.core import Callback
from fastai.callback.fp16 import MixedPrecision
from fastai.callback.schedule import SchedCos, _Annealer

from .utils import *
from ..imports import *

# %% auto 0
__all__ = ['EMACallback', 'EMASchedule', 'EMAWarmupCallback']

# %% ../../nbs/callback.ema.ipynb 6
class EMACallback(Callback):
    "Exponential Moving Average (EMA) of model weights with a fused update step"
    order,run_valid = MixedPrecision.order+1,False
    def __init__(self,
        decay:float=0.9998, # EMA decay value
        start:Numeric=0, # Start EMA in percent of training steps (float) or epochs (int, index 0)
        ema_device:torch.device|str|None=None, # Device to store EMA weights. Defaults to model device
        validate_ema:bool=True, # Run validation metrics using EMA weights instead of model weights. If true, `ema_device` must match model device
        replace_weights:bool=False, # Replace model weights with EMA weights when finished training. If false, sets `Learner.model_ema` to EMA weights
        foreach:bool|None=None, # Fuse EMA update step with PyTorch ForEach methods or use a standard for loop. Defaults to true if PyTorch 1.12+ and Cuda device detected
        resume:bool=False, # Resume from EMA weights from previous training saved to `Learner.model_ema`
        all_parameters:bool=False, # Apply EMA step to all parameters or only those with `requires_grad`
        all_buffers:bool=False, # Apply EMA step to persistent model buffers or all buffers
        skip_ema:bool=True, # Skip EMA step if callbacks, such as GradientAccumulation or MixedPrecision, skip the Optimizer update step
    ):
        store_attr()
        self.inverse_decay = 1-decay
        if self.foreach is None and ema_device is None:
            self.foreach = ismin_torch('1.12') and torch.cuda.is_available()

        if self.foreach:
            if notmax_torch('1.12'):
                warn(f'EMACallback with foreach=True is untested on PyTorch {torch.__verson__}, recommended to use 1.12 or newer')

        if resume and self.start > 0:
            warn(f'Resuming from prior EMA weights but delaying EMA until {start=}')

    @torch.no_grad()
    def before_fit(self):
        if hasattr(self.learn, 'lr_finder') or hasattr(self.learn, "gather_preds"):
            self.run = False
            return

        self._do_ema, self._restore_ema = False, False

        if self.start >= 1 and isinstance(self.start, int):
            self.start = self.start/self.n_epoch
        if self.start >= 1:
            warn(f'EMA start {self.start} is equal or greater than one and will not start in this training run')

        if self.resume:
            self.ema_model = self.learn.model_ema.eval()
        else:
            self.ema_model = deepcopy(self.learn.model).eval()

        model_device = next(self.learn.model.parameters()).device
        self.ema_model.to(self.ema_device if self.ema_device is not None else model_device)
        ema_device = next(self.ema_model.parameters()).device

        self.model_tensors, self.ema_tensors = [], []
        for mt, et in zip(self.learn.model.parameters(), self.ema_model.parameters()):
            if self.all_parameters or mt.requires_grad:
                self.model_tensors.append(mt)
                self.ema_tensors.append(et)

        self.model_buffers, self.ema_buffers = [], []
        state_names = self.model.state_dict().keys()
        for (n, mb), (_, eb) in zip(self.learn.model.named_buffers(), self.ema_model.named_buffers()):
            if self.all_buffers or n in state_names:
                # foreach methods cannot convert non-floats back to original type and error out
                if self.foreach and torch.is_floating_point(mb):
                    self.model_tensors.append(mb)
                    self.ema_tensors.append(eb)
                else:
                    self.model_buffers.append(mb)
                    self.ema_buffers.append(mb)

        self._validate_ema = model_device == ema_device if self.validate_ema else False
        if self.foreach:
            assert model_device == ema_device, f"{ema_device=} must equal {model_device=} if using foreach"

    @torch.no_grad()
    def before_batch(self):
        if self.pct_train >= self.start:
            if self.start > 0 and not self._do_ema and not self.resume:
                self.ema_model.load_state_dict(self.learn.model.state_dict())
            self._do_ema = True

    def after_cancel_batch(self):
        # if a callback (such as GradientAccumulation) raises a CancelBatchException, don't do EMA step and potentially turn EMA back on
        if self.skip_ema:
            self._restore_ema = self._do_ema
            self._do_ema = False

    def after_cancel_step(self):
        # if a callback (such as MixedPrecision) raises a CancelStepException, don't do EMA step and potentially turn EMA back on
        if self.skip_ema:
            self._restore_ema = self._do_ema
            self._do_ema = False

    @torch.no_grad()
    def after_batch(self):
        if self._do_ema:
            if self.foreach:
                torch._foreach_mul_(self.ema_tensors, scalar=self.decay)
                torch._foreach_add_(self.ema_tensors, self.model_tensors, alpha=self.inverse_decay)
                # foreach methods cannot convert non-floats back to original type and error out
                for mb, eb in zip(self.model_buffers, self.ema_buffers):
                    eb.copy_(self.decay * eb + self.inverse_decay * mb)
            else:
                for mt, et in zip(self.model_tensors, self.ema_tensors):
                    et.copy_(self.decay * et + self.inverse_decay * mt)
        # handle a Cancel Exception while self._do_ema was set to True
        if self._restore_ema:
            self._do_ema = True
            self._restore_ema = False

    @torch.no_grad()
    def before_validate(self):
        if self._do_ema and self._validate_ema:
            self.temp_model = self.learn.model
            self.learn.model = self.ema_model

    @torch.no_grad()
    def after_validate(self):
        if self._do_ema and self._validate_ema:
            self.learn.model = self.temp_model

    def after_fit(self):
        if self.replace_weights:
            self.learn.model = self.ema_model
            self.ema_model = None
        else:
            self.learn.model_ema = self.ema_model

# %% ../../nbs/callback.ema.ipynb 9
class EMASchedule(EMACallback, CallbackScheduler):
    "Exponential Moving Average (EMA) of model weights with a warmup schedule and fused update step"
    order,run_valid = MixedPrecision.order+1,False
    def __init__(self,
        start_decay:float=0.9, # Initial EMA decay value
        final_decay:float=0.9998, # Final EMA decay value
        start:Numeric=0, # Start EMA warmup in percent of training steps (float) or epochs (int, index 0)
        finish:Numeric=0.3, # Finish EMA warmup in percent of training steps (float) or epochs (int, index 0)
        schedule:Callable[..., _Annealer]=SchedCos, # EMA decay warmup schedule
        ema_device:torch.device|str|None=None, # Device to store EMA weights. Defaults to model device
        validate_ema:bool=True, # Run validation metrics using EMA weights instead of model weights. If true, `ema_device` must match model device
        replace_weights:bool=False, # Replace model weights with EMA weights when finished training. If false, set `Learner.model_ema` to EMA weights
        foreach:bool|None=None, # Fuse EMA update step with PyTorch ForEach methods or use a standard for loop. Defaults to true if PyTorch 1.12+ and Cuda device detected
        resume:bool=False, # Resume from EMA weights from previous training saved to `Learner.model_ema`
        all_parameters:bool=False, # Apply EMA step to all parameters or only those with `requires_grad`
        all_buffers:bool=False, # Apply EMA step to persistent model buffers or all buffers
        skip_ema:bool=True, # Skip EMA step if callbacks, such as GradientAccumulation or MixedPrecision, skip the Optimizer update step
    ):
        EMACallback.__init__(self,
            decay=final_decay,
            start=start,
            ema_device=ema_device,
            validate_ema=validate_ema,
            replace_weights=replace_weights,
            foreach=foreach,
            resume=resume,
            all_parameters=all_parameters,
            all_buffers=all_buffers,
            skip_ema=skip_ema)

        CallbackScheduler.__init__(self)
        store_attr(names='start_decay,final_decay,finish,schedule')

    def before_fit(self):
        super().setup_schedule(self.n_epoch, len(self.dls.train), self.start_decay,
                               self.final_decay, self.start, self.finish, self.schedule,
                               callback_name='EMA Schedule')
        super().before_fit()

        # negate decay so at least one ema scheduling step will occur
        self.decay = -1*self.decay

    def after_batch(self):
        if self._do_ema:
            self.decay = super().schedule_step(self.decay, self.pct_train)
            self.inverse_decay = 1-self.decay

            super().after_batch()

        self.learn._log_values(ema_decay=self.decay if self._do_ema else 0)

# %% ../../nbs/callback.ema.ipynb 11
class EMAWarmupCallback(EMASchedule):
    "EMAWarmpCallback has been renamed to EMASchedule"
    @delegates(EMASchedule.__init__)
    def __init__(self, **kwargs):
        warn("EMAWarmpCallback has been renamed to EMASchedule")
        super().__init__(**kwargs)
