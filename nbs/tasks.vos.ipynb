{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp tasks.vos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Beta] Virtual Outlier Synthesis\n",
    "> Means to train a fastai model using [Virtual Outlier Synthesis](https://arxiv.org/abs/2202.01197)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VOS requires the metrics refactor from this draft PR to work. You need to pip install from git.\n",
    "\n",
    "```\n",
    "pip install git+https://github.com/warner-benjamin/fastai.git@metrics_rewrite\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "\n",
    "# Original VOS implementation from https://github.com/deeplearning-wisc/vos\n",
    "\n",
    "# Original code, Copyright 2022 by Xuefeng Du, Zhaoning Wang, Mu Cai & Yixuan Li\n",
    "# Licensed under the Apache License, Version 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "from fastai.learner import Learner, replacing_yield\n",
    "from fastai.torch_core import Module, defaults, default_device\n",
    "from fastai.callback.core import Callback\n",
    "from fastai.callback.fp16 import MixedPrecision\n",
    "from fastai.data.transforms import get_c\n",
    "from fastcore.basics import store_attr, ifnone\n",
    "from fastcore.foundation import L, mk_class, patch\n",
    "from fastcore.meta import delegates, contextmanager\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from fastai.vision.learner import create_body, num_features_model, apply_init, model_meta, _default_meta\n",
    "from fastai.losses import CrossEntropyLossFlat, LabelSmoothingCrossEntropy, LabelSmoothingCrossEntropyFlat\n",
    "from fastai.layers import *\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporary Learner Stuff -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "from fastai.basics import Path\n",
    "from fastai.optimizer import Adam\n",
    "from fastai.learner import trainable_params, CancelFitException\n",
    "from fastcore.xtras import ContextManagers\n",
    "\n",
    "# This section adds LearnerState to `Learner` via patches, which currently is a draft pr in the fastai repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "mk_class('LearnerState', **{o:o.lower() for o in ['Fit', 'Eval', 'LRFind', 'Idle']},\n",
    "         doc=\"All built in states for `Learner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "@patch\n",
    "def __init__(self:Learner, dls, model, loss_func=None, opt_func=Adam, lr=defaults.lr, splitter=trainable_params, cbs=None,\n",
    "                metrics=None, path=None, model_dir='models', wd=None, wd_bn_bias=False, train_bn=True,\n",
    "                moms=(0.95,0.85,0.95)):\n",
    "    path = Path(path) if path is not None else getattr(dls, 'path', Path('.'))\n",
    "    if loss_func is None:\n",
    "        loss_func = getattr(dls.train_ds, 'loss_func', None)\n",
    "        assert loss_func is not None, \"Could not infer loss function from the data, please pass a loss function.\"\n",
    "    self.dls,self.model = dls,model\n",
    "    store_attr(but='dls,model,cbs')\n",
    "    self.training,self.create_mbar,self.logger,self.opt,self.cbs = False,True,print,None,L()\n",
    "    self.add_cbs(L(defaults.callbacks)+L(cbs))\n",
    "    self(\"after_create\")\n",
    "    self.state=LearnerState.Idle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "@patch\n",
    "@contextmanager\n",
    "def set_state(self:Learner, state): return replacing_yield(self, 'state', state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "@patch\n",
    "def fit(self:Learner, n_epoch, lr=None, wd=None, cbs=None, reset_opt=False):\n",
    "    with self.added_cbs(cbs), self.set_state(LearnerState.Fit):\n",
    "        if reset_opt or not self.opt: self.create_opt()\n",
    "        if wd is None: wd = self.wd\n",
    "        if wd is not None: self.opt.set_hypers(wd=wd)\n",
    "        self.opt.set_hypers(lr=self.lr if lr is None else lr)\n",
    "        self.n_epoch = n_epoch\n",
    "        self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "@patch\n",
    "def validation_context(self:Learner, cbs=None, inner=False):\n",
    "    cms = [self.no_logging(),self.no_mbar(),self.set_state(LearnerState.Eval)]\n",
    "    if cbs: cms.append(self.added_cbs(cbs))\n",
    "    if not inner: cms.append(self)\n",
    "    return ContextManagers(cms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOS Model Heads -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VOSHead(Module):\n",
    "    def __init__(self, \n",
    "        nf:int,\n",
    "        n_out:int,\n",
    "        lin_ftrs:list[int]|None=None,\n",
    "        p:float|None=None,\n",
    "        concat_pool:bool=True,\n",
    "        first_bn:bool=True,\n",
    "        bn_final:bool=False,\n",
    "        lin_first:bool=False,\n",
    "        y_range:float|None=None,\n",
    "        act_cls:Callable[...,nn.Module]=defaults.activation,\n",
    "        fastai_head:bool=False,\n",
    "        head_pool:Callable[[int,int],nn.Module]=AdaptiveAvgPool\n",
    "    ):\n",
    "        if fastai_head:\n",
    "            if concat_pool: nf *= 2\n",
    "            lin_ftrs = [nf, 512, n_out] if lin_ftrs is None else [nf] + lin_ftrs + [n_out]\n",
    "            bns = [first_bn] + [True]*len(lin_ftrs[1:])\n",
    "            ps = L(p if p is not None else 0.5)\n",
    "            if len(ps) == 1: ps = [ps[0]/2] * (len(lin_ftrs)-2) + ps\n",
    "            actns = [act_cls(inplace=True)] * (len(lin_ftrs)-2) + [None]\n",
    "            pool = AdaptiveConcatPool2d() if concat_pool else nn.AdaptiveAvgPool2d(1)\n",
    "            pool_layers = [pool, Flatten()]\n",
    "            if lin_first: pool_layers.append(nn.Dropout(ps.pop(0)))\n",
    "            self.pool = nn.Sequential(*pool_layers)\n",
    "            layers = []\n",
    "            for ni,no,bn,p,actn in zip(lin_ftrs[:-1], lin_ftrs[1:], bns, ps, actns):\n",
    "                layers += LinBnDrop(ni, no, bn=bn, p=p, act=actn, lin_first=lin_first)\n",
    "            if lin_first: layers.append(nn.Linear(lin_ftrs[-2], n_out))\n",
    "            if bn_final: layers.append(nn.BatchNorm1d(lin_ftrs[-1], momentum=0.01))\n",
    "            if y_range is not None: layers.append(SigmoidRange(*y_range))\n",
    "            self.head = nn.Sequential(*layers)\n",
    "        else:\n",
    "            p = p if p is not None else 0.0\n",
    "            self.pool = nn.Sequential(*[head_pool(sz=1, ndim=2), Flatten()])\n",
    "            self.head = nn.Sequential(*[nn.Dropout(p), nn.Linear(nf, n_out)])\n",
    "        self.weight_energy = torch.nn.Linear(n_out, 1)\n",
    "        self.log_reg = torch.nn.Linear(1, 2)\n",
    "        self.nf = nf\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        return self.head(x), x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(VOSHead.__init__)\n",
    "def create_vos_model(\n",
    "    arch:nn.Module, \n",
    "    n_out:int, \n",
    "    pretrained:bool=True, \n",
    "    cut:int|None=None, \n",
    "    n_in:int=3, \n",
    "    init:Callable=nn.init.kaiming_normal_, \n",
    "    custom_head:nn.Module|None=None,\n",
    "    concat_pool:bool=True, \n",
    "    **kwargs\n",
    "):\n",
    "    \"Create custom convnet architecture\"\n",
    "    meta = model_meta.get(arch, _default_meta)\n",
    "    body = create_body(arch, n_in, pretrained, ifnone(cut, meta['cut']))\n",
    "    if custom_head is None:\n",
    "        nf = num_features_model(nn.Sequential(*body.children()))\n",
    "        head = VOSHead(nf, n_out, concat_pool=concat_pool, **kwargs)\n",
    "    else: head = custom_head\n",
    "    model = nn.Sequential(body, head)\n",
    "    if init is not None: apply_init(model[1], init)\n",
    "    torch.nn.init.uniform_(model[1].weight_energy.weight)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOS Callback -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "mk_class('VOSDistSample', **{o:o.lower() for o in ['List', 'Fork', 'Batch']},\n",
    "         doc=\"Sample features via simple for loop, torch.jit.fork for loop, as a batch. Batch will affect variance of samples.\")\n",
    "\n",
    "_all_=['VOSDistSample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def list_dist(mean_embed_id, temp_precision, sample_from, select, nf, n_out):\n",
    "    ood_samples = []\n",
    "    temp_precision = torch.linalg.cholesky(temp_precision)\n",
    "    for idx in range(n_out):\n",
    "        new_dis = MultivariateNormal(mean_embed_id, scale_tril=temp_precision)\n",
    "        negative_samples = new_dis.rsample((sample_from,))\n",
    "        prob_density = new_dis.log_prob(negative_samples)\n",
    "        # keep the data in the low density area.\n",
    "        _, index_prob = torch.topk(-prob_density, select)\n",
    "        ood_samples.append(negative_samples[index_prob])\n",
    "    return torch.cat(ood_samples, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def calc_forked_dist(mean_embed_id, temp_precision, sample_from, select):\n",
    "    new_dis = MultivariateNormal(mean_embed_id, scale_tril=temp_precision)\n",
    "    negative_samples = new_dis.rsample((sample_from,))\n",
    "    prob_density = new_dis.log_prob(negative_samples)\n",
    "    # keep the data in the low density area.\n",
    "    _, index_prob = torch.topk(-prob_density, select)\n",
    "    return negative_samples[index_prob]\n",
    "\n",
    "def forked_dist(mean_embed_id, temp_precision, sample_from, select, nf, n_out):\n",
    "    temp_precision = torch.linalg.cholesky(temp_precision)\n",
    "    futures:list[torch.jit.Future[torch.Tensor]] = []\n",
    "    ood_samples = []\n",
    "    for idx in range(n_out):\n",
    "        futures.append(torch.jit.fork(calc_forked_dist, mean_embed_id[idx],\n",
    "                                      temp_precision, sample_from, select))\n",
    "    for future in futures:\n",
    "        ood_samples.append(torch.jit.wait(future))\n",
    "    return torch.cat(ood_samples, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def batched_dist(mean_embed_id, temp_precision, sample_from, select, nf, n_out):\n",
    "    new_dis = MultivariateNormal(mean_embed_id, covariance_matrix=temp_precision)\n",
    "    negative_samples = new_dis.rsample((sample_from,))\n",
    "    prob_density = new_dis.log_prob(negative_samples)\n",
    "\n",
    "    # keep the data in the low density area\n",
    "    _, idx_prob = torch.topk(-prob_density.t(), select)\n",
    "    negative_samples = negative_samples.permute(1,0,2)\n",
    "    return negative_samples.gather(dim=1, index=idx_prob.repeat(1,1,nf)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VOSCallback(Callback):\n",
    "    order = MixedPrecision.order-1\n",
    "    \"Base class with shared initializtion for Virtual Outlier Synthesis\"\n",
    "    def __init__(self, \n",
    "        n_out:int|None=None, # Number of labels, defaults to `learn.dls.c``\n",
    "        start_epoch:int|None=None, # Epoch to start Virtual Outlier Synthesis, defaults to `start_pct`\n",
    "        start_pct:float=0.40, # Iteration percent to start Virtual Outlier Synthesis\n",
    "        n_sample:int=1000, # Number of virtual outliers to generate\n",
    "        sample_from:int=10000,\n",
    "        energy_samples:int=1,\n",
    "        energy_loss_weight:float=0.1,\n",
    "        energy_loss_func:nn.Module|None=None,\n",
    "        dist_sample:DistSample=VOSDistSample.Fork\n",
    "    ):\n",
    "        store_attr(but='dist_sample')\n",
    "        if dist_sample==VOSDistSample.List:    self._sample_dist = list_dist\n",
    "        elif dist_sample==VOSDistSample.Fork:  self._sample_dist = forked_dist\n",
    "        elif dist_sample==VOSDistSample.Batch: self._sample_dist = batched_dist\n",
    "\n",
    "    def before_fit(self):\n",
    "        assert isinstance(self.learn.model[1], VOSHead), 'VOSCallback requires model have a be `VOSHead'\n",
    "        if self.learn.state == LearnerState.Fit:\n",
    "            self.device = device = getattr(self.learn.dls, 'device', default_device())\n",
    "            self.n_out = ifnone(self.n_out, get_c(self.learn.dls))\n",
    "            self.nf = self.learn.model[1].nf\n",
    "            assert self.n_out, \"`n_out` is not defined, and could not be inferred from data, set `dls.c` or pass `n_out`\"\n",
    "            self.eye_matrix = torch.eye(self.nf, device=self.device)\n",
    "            self.features = torch.zeros(self.n_out, self.n_sample, self.nf).to(device=self.device)\n",
    "            self.number_dict = {}\n",
    "            for i in range(self.n_out): self.number_dict[i] = 0\n",
    "            self.sample_count, self.total_samples = 0, self.n_out * self.n_sample\n",
    "            if isinstance(self.learn.loss_func, (nn.CrossEntropyLoss,CrossEntropyLossFlat,LabelSmoothingCrossEntropy,LabelSmoothingCrossEntropyFlat)):\n",
    "                self.energy_loss_func = self.learn.loss_func\n",
    "            elif self.energy_loss_func is None:\n",
    "                self.energy_loss_func = torch.nn.CrossEntropyLoss()\n",
    "        self.train_vos = self.learn.state != LearnerState.LRFind\n",
    "        self.eval_vos  = self.learn.state == LearnerState.Eval\n",
    "        self.targ_vos  = len(self.learn.dls.valid.one_batch()) - self.learn.dls.valid.n_inp > 1\n",
    "\n",
    "    def after_pred(self):\n",
    "        pred, self.batch_features = self.learn.pred\n",
    "        self.batch_features = self.batch_features.detach().clone()\n",
    "        self.learn.pred = pred\n",
    "\n",
    "    def after_loss(self):\n",
    "        if self.train_vos:\n",
    "            if self.training: self._vos_train_step()\n",
    "            else:             self._vos_valid_step()\n",
    "\n",
    "    def _vos_train_step(self):\n",
    "        if self.targ_vos:\n",
    "            self.orig_loss = self.learn.loss.detach().clone()\n",
    "            targs, self.vos_targs = self.learn.yb\n",
    "            self.learn.yb = (targs,)\n",
    "        else:\n",
    "            self.orig_loss, targs = self.learn.loss.detach().clone(), *self.learn.yb\n",
    "        \n",
    "        cpu_targs = targs.cpu().int().numpy()\n",
    "        if self.sample_count >= self.total_samples:\n",
    "            # Enque and deque ID features\n",
    "            for idx, targ in enumerate(cpu_targs):\n",
    "                self.features[targ] = torch.cat((self.features[targ][1:], self.batch_features[idx].view(1, -1)), dim=0)\n",
    "\n",
    "            if self.learn.pct_train >= self.start_pct:\n",
    "                # the covariance finder needs the data to be centered\n",
    "                fs = self.features.shape\n",
    "                mean_embed_id = self.features.mean(1)\n",
    "                X = (self.features - mean_embed_id[:, None, :]).view(fs[0]*fs[1], fs[2])\n",
    "                \n",
    "                # add the variance.\n",
    "                temp_precision = torch.mm(X.t(), X) / len(X)\n",
    "                temp_precision += 0.0001 * self.eye_matrix\n",
    "                \n",
    "                # gausian sample ood features\n",
    "                ood_samples = self._sample_dist(mean_embed_id, temp_precision, self.sample_from, \n",
    "                                                self.energy_samples, self.nf, self.n_out)\n",
    "\n",
    "                id_energy = self._log_sum_exp(self.learn.pred, 1)\n",
    "                ood_pred = self.learn.model[1].head(ood_samples)\n",
    "                ood_energy = self._log_sum_exp(ood_pred, 1)\n",
    "\n",
    "                self.energy_pred = torch.cat((id_energy, ood_energy), -1)\n",
    "                self.energy_targ = torch.cat((torch.ones(len(id_energy), device=self.device, dtype=torch.long), \n",
    "                                              torch.zeros(len(ood_pred), device=self.device, dtype=torch.long)), -1)\n",
    "\n",
    "                self.energy_pred = self.learn.model[1].log_reg(self.energy_pred.view(-1, 1))\n",
    "                self.energy_loss = self.energy_loss_weight * self.energy_loss_func(self.energy_pred, self.energy_targ)\n",
    "                self.learn.loss_grad += self.energy_loss\n",
    "                self.learn.loss += self.energy_loss.detach()\n",
    "\n",
    "        else:\n",
    "            for idx, targ in enumerate(cpu_targs):\n",
    "                if self.number_dict[targ] < self.n_sample:\n",
    "                    self.features[targ][self.number_dict[targ]] = self.batch_features[idx]\n",
    "                    self.number_dict[targ] += 1\n",
    "                    self.sample_count += 1\n",
    "                else:\n",
    "                    self.features[targ] = torch.cat((self.features[targ][1:], self.batch_features[idx].view(1, -1)), dim=0)\n",
    "            self.energy_loss = torch.tensor(0., device=self.device)\n",
    "\n",
    "    def _vos_valid_step(self):\n",
    "        self.orig_loss = self.learn.loss.detach().clone()\n",
    "        if self.eval_vos or (self.learn.pct_train >= self.start_pct and self.sample_count >= self.total_samples):\n",
    "            id_energy = self._log_sum_exp(self.learn.pred, 1)\n",
    "            if self.targ_vos: self.energy_targ = self.vos_targs\n",
    "            else:             self.energy_targ = torch.ones(len(id_energy), device=self.device, dtype=torch.long)\n",
    "            self.energy_pred = self.learn.model[1].log_reg(id_energy.view(-1, 1))\n",
    "\n",
    "            self.energy_loss = self.energy_loss_weight * self.energy_loss_func(self.energy_pred, self.energy_targ)\n",
    "            self.learn.loss += self.energy_loss.detach()\n",
    "        else:\n",
    "            self.energy_loss = torch.tensor(0., device=self.device)\n",
    "            self.energy_pred = torch.tensor(0., device=self.device)\n",
    "            self.energy_targ = torch.tensor(1., device=self.device)\n",
    "\n",
    "    # def after_batch(self):\n",
    "    #     if self.learn.state==LearnerState.Eval or (self.learn.pct_train >= self.start_pct and self.sample_count >= self.total_samples):\n",
    "    #         self.learn.pred = self.learn.pred, self.energy_pred\n",
    "\n",
    "    def _log_sum_exp(self, value, dim=None, keepdim=False):\n",
    "        \"Numerically stable implementation of the operation value.exp().sum(dim, keepdim).log()\"\n",
    "        # TODO: torch.max(value, dim=None) threw an error at time of writing\n",
    "        if dim is not None:\n",
    "            m, _ = torch.max(value, dim=dim, keepdim=True)\n",
    "            value = value - m\n",
    "            if keepdim is False:\n",
    "                m = m.squeeze(dim)\n",
    "            return m + torch.log(torch.sum(F.relu(self.learn.model[1].weight_energy.weight) * torch.exp(value), dim=dim, keepdim=keepdim))\n",
    "        else:\n",
    "            m = torch.max(value)\n",
    "            sum_exp = torch.sum(torch.exp(value - m))\n",
    "            return m + torch.log(sum_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOS Metrics -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "from fastai.learner import Metric, AvgMetric, ActivationType, MetricType, LogMetric, find_bs, AvgLoss, AvgSmoothLoss, to_detach\n",
    "from fastai.metrics import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "class VOSMetric(Metric):\n",
    "    \"Blueprint for defining a metric with accumulate\"\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        self.pred, self.targ = learn.vos.energy_pred, learn.vos.energy_targ\n",
    "        if self.activation in [ActivationType.Softmax, ActivationType.BinarySoftmax]:\n",
    "            self.pred = F.softmax(self.pred, dim=self.dim_argmax)\n",
    "            if self.activation == ActivationType.BinarySoftmax: self.pred = self.pred[:, -1]\n",
    "        elif self.activation == ActivationType.Sigmoid: self.pred = torch.sigmoid(self.pred)\n",
    "        elif self.dim_argmax: self.pred = self.pred.argmax(dim=self.dim_argmax)\n",
    "        if self.thresh: self.pred = (self.pred >= self.thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "@delegates(VOSMetric)\n",
    "class VOSAvgMetric(VOSMetric):\n",
    "    \"Average the values of `func` taking into account potential different batch sizes\"\n",
    "    def __init__(self, func, to_np=False, invert_arg=False, **kwargs):\n",
    "        super().__init__(**self._split_kwargs(VOSMetric.__init__, **kwargs))\n",
    "        self.func, self.fkwargs = func, self._split_kwargs(func, **kwargs)\n",
    "        self.to_np, self.invert_arg = to_np, invert_arg\n",
    "        self._name = ifnone(kwargs.get('name', None), self.func.func.__name__ if hasattr(self.func, 'func') else self.func.__name__)\n",
    "\n",
    "    def reset(self): self.total,self.count = 0.,0\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        super().accumulate(learn)\n",
    "        bs = find_bs(learn.yb)\n",
    "        if self.to_np: self.pred,self.targ = learn.to_detach(self.pred).numpy(),learn.to_detach(self.targ).numpy()\n",
    "        self.total += (self.func(self.targ, self.pred, **self.fkwargs) if self.invert_arg else self.func(self.pred, self.targ, **self.fkwargs))*bs\n",
    "        self.count += bs\n",
    "    \n",
    "    @property\n",
    "    def value(self): return self.total/self.count if self.count != 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "@delegates(VOSMetric)\n",
    "def func_to_metric(func, metric_type, is_class, thresh=None, axis=-1, activation=None, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Convert `func` metric to a fastai metric\"\n",
    "\n",
    "    dim_argmax = axis if is_class and thresh is None else None\n",
    "    if activation is None:\n",
    "        activation = ActivationType.Sigmoid if (is_class and thresh is not None) else ActivationType.No\n",
    "\n",
    "    if metric_type==MetricType.Avg:\n",
    "        return VOSAvgMetric(func, dim_argmax=dim_argmax, activation=activation,\n",
    "                         thresh=thresh, log_metric=log_metric, **kwargs)\n",
    "    else:\n",
    "        name = func.func.__name__ if hasattr(func, 'func') else  func.__name__\n",
    "        raise ValueError(f\"Unsupported `metric_type` {metric_type} for metric {name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def VOSAccuracy(axis=-1, metric_type=MetricType.Avg, log_metric=LogMetric.Valid, **kwargs):\n",
    "    \"Compute accuracy with `targ` when `pred` is bs * n_classes\"\n",
    "    return func_to_metric(accuracy, metric_type, True, axis=axis, log_metric=log_metric, name='vos_accuracy', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "class VOSAvgLoss(AvgLoss):\n",
    "    \"Average the MultiLoss losses taking into account potential different batch sizes\"\n",
    "    def __init__(self, name='vos_loss'): \n",
    "        self._name = name\n",
    "    \n",
    "    def accumulate(self, learn):\n",
    "        bs = find_bs(learn.yb)\n",
    "        self.total += learn.to_detach(learn.vos.energy_loss.mean())*bs\n",
    "        self.count += bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "class OrigAvgLoss(AvgLoss):\n",
    "    \"Average the MultiLoss losses taking into account potential different batch sizes\"\n",
    "    def __init__(self, name='orig_loss'):\n",
    "        self._name = name\n",
    "    \n",
    "    def accumulate(self, learn):\n",
    "        bs = find_bs(learn.yb)\n",
    "        self.total += learn.to_detach(learn.vos.orig_loss.mean())*bs\n",
    "        self.count += bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "class VOSAvgSmoothLoss(AvgSmoothLoss):\n",
    "    \"Smooth average of the MultiLoss losses (exponentially weighted with `beta`)\"\n",
    "    def __init__(self, name='vos_loss', beta=0.98):\n",
    "        super().__init__(beta)\n",
    "        self._name = name\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        self.count += 1\n",
    "        self.val = torch.lerp(to_detach(learn.vos.energy_loss.mean(), gather=False), self.val, self.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "class OrigAvgSmoothLoss(AvgSmoothLoss):\n",
    "    \"Smooth average of the MultiLoss losses (exponentially weighted with `beta`)\"\n",
    "    def __init__(self, name='orig_loss', beta=0.98): \n",
    "        super().__init__(beta)\n",
    "        self._name = name\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        self.count += 1\n",
    "        self.val = torch.lerp(to_detach(learn.vos.orig_loss.mean(), gather=False), self.val, self.beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOS Learner -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(create_vos_model)\n",
    "def vos_learner(dls, arch, n_out=None, pretrained=True, fastai_head=False, vos_metrics=[VOSAccuracy()],\n",
    "                # vos args\n",
    "                start_epoch:int|None=None, start_pct:float=0.40, n_sample:int=1000, sample_from:int=10000,\n",
    "                energy_samples:int=1, energy_loss_weight:float=0.1, energy_loss_func:nn.Module|None=None,\n",
    "                dist_sample:VOSDistSample=VOSDistSample.Fork,\n",
    "                # learner args\n",
    "                loss_func=None, opt_func=Adam, lr=defaults.lr, splitter=None, cbs=None, metrics=None, path=None,\n",
    "                model_dir='models', wd=None, wd_bn_bias=False, train_bn=True, moms=(0.95,0.85,0.95),\n",
    "                # other model args\n",
    "                **kwargs):\n",
    "    \"Build a convnet style learner from `dls` and `arch`\"\n",
    "    meta = model_meta.get(arch, _default_meta)\n",
    "\n",
    "    if n_out is None: n_out = get_c(dls)\n",
    "    assert n_out, \"`n_out` is not defined, and could not be inferred from data, set `dls.c` or pass `n_out`\"\n",
    "    model = create_vos_model(arch, n_out, pretrained=pretrained, fastai_head=fastai_head, **kwargs)\n",
    "\n",
    "    if metrics is None: metrics = L()\n",
    "    else: metrics=L(metrics)\n",
    "    metrics = L([OrigAvgSmoothLoss(), VOSAvgSmoothLoss(), OrigAvgLoss(), VOSAvgLoss()]) + metrics\n",
    "    for m in vos_metrics:\n",
    "        if m not in metrics:\n",
    "            metrics.append(m)\n",
    "    \n",
    "    vos_cb = [VOSCallback(n_out, start_epoch, start_pct, n_sample, sample_from, \n",
    "                          energy_samples, energy_loss_weight, energy_loss_func, dist_sample)]\n",
    "    if cbs is None: cbs = vos_cb\n",
    "    else: cbs += vos_cb\n",
    "\n",
    "    splitter=ifnone(splitter, meta['split'])\n",
    "    learn = Learner(dls=dls, model=model, loss_func=loss_func, opt_func=opt_func, lr=lr, splitter=splitter, cbs=cbs,\n",
    "                   metrics=metrics, path=path, model_dir=model_dir, wd=wd, wd_bn_bias=wd_bn_bias, train_bn=train_bn,\n",
    "                   moms=moms)\n",
    "    if pretrained: learn.freeze()\n",
    "    # keep track of args for loggers\n",
    "    store_attr('arch,pretrained', self=learn, **kwargs)\n",
    "    return learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
