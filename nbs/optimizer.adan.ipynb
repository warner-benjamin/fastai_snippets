{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp optimizer.adan\n",
    "#|default_cls_lvl 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adan: ADAptive Nesterov Momentum Optimizer\n",
    "> With fastai native, fused ForEach, and fused TorchScript implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "from typing import Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from fastai.optimizer import Optimizer\n",
    "\n",
    "from fastxtend.optimizer.foreach import ForEachOptimizer\n",
    "from fastxtend.optimizer.torchscript import JitOptimizer\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastxtend.test_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def debias(beta:float, step:int):\n",
    "    \"Simple debias calculation\"\n",
    "    return 1-beta**step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adan Fastai Callbacks -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def adan_setup(p:Tensor, step:int=0, grad_avg:Tensor|None=None, diff_avg:Tensor|None=None, \n",
    "               sqr_avg:Tensor|None=None, prior_grad:Tensor|None=None, paper_init:bool=False, **kwargs):\n",
    "    \"Handles Adan setup and keeps track of steps\"\n",
    "    if step == 0: \n",
    "        grad_avg = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "        diff_avg = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "        sqr_avg  = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "        if paper_init:\n",
    "            prior_grad = p.grad.clone()\n",
    "        else:\n",
    "            prior_grad = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "        step += 1\n",
    "        return {'grad_avg':grad_avg, 'diff_avg':diff_avg, 'sqr_avg':sqr_avg, 'prior_grad':prior_grad, 'step':step}\n",
    "    else:\n",
    "        step += 1\n",
    "        return {'step':step}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def adan_step(p:Tensor, lr:float, eps:float, wd:float, beta1:float, beta2:float, beta3:float, \n",
    "              step:int, grad_avg:Tensor, diff_avg:Tensor, sqr_avg:Tensor, prior_grad:Tensor, \n",
    "              do_wd:bool=True, **kwargs):\n",
    "    \"Updates Adan moving averages and performs the Adan step with `lr` on `p`\"\n",
    "\n",
    "    # difference between current and previous gradients\n",
    "    grad_diff = torch.sub(p.grad.data, prior_grad)\n",
    "\n",
    "    # update m_k\n",
    "    grad_avg.mul_(beta1).add_(p.grad.data, alpha=1-beta1)\n",
    "\n",
    "    # update v_k\n",
    "    diff_avg.mul_(beta2).add_(grad_diff, alpha=1-beta2)\n",
    "    \n",
    "    # update n_k\n",
    "    adjusted_grad = torch.add(p.grad.data, grad_diff, alpha=beta2)\n",
    "    sqr_avg.mul_(beta3).addcmul_(adjusted_grad, adjusted_grad, value=1-beta3)\n",
    "\n",
    "    # calculate debias terms\n",
    "    db1, db2, db3 = debias(beta1, step), debias(beta2, step), debias(beta3, step)\n",
    "\n",
    "    # calculate applied λ \n",
    "    wd = (1+lr*wd) if wd!=0 and do_wd else 1 \n",
    "\n",
    "    # calculate η_k\n",
    "    lr = lr/torch.sqrt(sqr_avg.div(db3)).add(eps) \n",
    "\n",
    "    # perform Adan step and apply to parameter `p`\n",
    "    p.data.sub_(torch.add(grad_avg.div(db1), diff_avg.div(db2), alpha=beta2).mul_(lr)).div_(wd)\n",
    "\n",
    "    # set current grad as next step's prior_grad\n",
    "    prior_grad = p.grad.data.clone()\n",
    "    return {'grad_avg':grad_avg, 'diff_avg':diff_avg, 'sqr_avg':sqr_avg, 'prior_grad':prior_grad}\n",
    "\n",
    "adan_step.defaults = dict(beta1=0.98, beta2=0.92, beta3=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adan Jit -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@torch.jit.script\n",
    "def adan_jit_step(p:Tensor, g:Tensor, lr:float, wd:float, beta1:float, beta2:float, beta3:float, eps:float,\n",
    "                  paper_init:bool, grad_avg:Optional[Tensor]=None, diff_avg:Optional[Tensor]=None, \n",
    "                  sqr_avg:Optional[Tensor]=None, prior_grad:Optional[Tensor]=None, do_wd:bool=True, step:int=0, \n",
    "                  force_train:Optional[bool]=None, mom:Optional[float]=None, decouple_wd:bool=False):\n",
    "    dp = p\n",
    "    grad = g\n",
    "    step += 1\n",
    "\n",
    "    if grad_avg is None: \n",
    "        grad_avg = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "    if diff_avg is None: \n",
    "        diff_avg = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "    if sqr_avg is None: \n",
    "        sqr_avg = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "    if prior_grad is None:\n",
    "        if paper_init:\n",
    "            prior_grad = grad.clone()\n",
    "        else:\n",
    "            prior_grad = torch.zeros_like(dp, memory_format=torch.preserve_format)\n",
    "\n",
    "    # difference between current and previous gradients\n",
    "    diff_grad = grad.sub(prior_grad)\n",
    "\n",
    "    # update m_k\n",
    "    grad_avg = grad_avg.mul(beta1).add(grad, alpha=1-beta1)\n",
    "\n",
    "    # update v_k\n",
    "    diff_avg = diff_avg.mul(beta2).add(diff_grad, alpha=1-beta2)\n",
    "    \n",
    "    # update n_k\n",
    "    adjusted_grad = grad.add(diff_grad, alpha=beta2)\n",
    "    sqr_avg = sqr_avg.mul(beta3).addcmul(adjusted_grad, adjusted_grad, value=1-beta3)\n",
    "\n",
    "    # calculate debias terms\n",
    "    db1 = debias(beta1, step)\n",
    "    db2 = debias(beta2, step)\n",
    "    db3 = debias(beta3, step)\n",
    "\n",
    "    # calculate applied λ\n",
    "    if wd!=0 and do_wd:\n",
    "        wd = (1+lr*wd) \n",
    "    else:\n",
    "        wd = 1. \n",
    "\n",
    "    # calculate η_k\n",
    "    lr = lr/torch.sqrt(sqr_avg.div(db3)).add(eps)\n",
    "\n",
    "    # perform Adan step\n",
    "    dp = dp.sub(torch.add(grad_avg.div(db1), diff_avg.div(db2), alpha=beta2).mul(lr)).div(wd)\n",
    "\n",
    "    # set current grad as next step's prior_grad\n",
    "    prior_grad = grad.clone()\n",
    "\n",
    "    # apply results to parameter p\n",
    "    p.set_(dp)\n",
    "    g.set_(grad)\n",
    "\n",
    "    return torch.jit.annotate(Dict[str, Union[Tensor, int]], {'grad_avg':grad_avg, 'diff_avg':diff_avg, 'sqr_avg':sqr_avg, 'prior_grad':prior_grad, 'step':step})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adan ForEach -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def adan_foreach_step(p:Tensor, grad:Tensor, grad_avg:list[Tensor], diff_avg:list[Tensor], sqr_avg:list[Tensor], \n",
    "                      prior_grad:list[Tensor], steps:np.ndarray[Any, int], do_wd:np.ndarray[Any, bool], lr:float, \n",
    "                      wd:float, beta1:float, beta2:float, beta3:float, eps:float, **kwargs):\n",
    "\n",
    "    # difference between current and previous gradients\n",
    "    grad_diff = torch._foreach_sub(grad, prior_grad)\n",
    "\n",
    "    # update m_k\n",
    "    torch._foreach_mul_(grad_avg, beta1)\n",
    "    torch._foreach_add_(grad_avg, grad, alpha=1-beta1)\n",
    "\n",
    "    # update v_k\n",
    "    torch._foreach_mul_(diff_avg, beta2)\n",
    "    torch._foreach_add_(diff_avg, grad_diff, alpha=1-beta2)\n",
    "    \n",
    "    # update n_k\n",
    "    adjusted_grad = torch._foreach_add(grad, grad_diff, alpha=beta2)\n",
    "    torch._foreach_mul_(sqr_avg, beta3)\n",
    "    torch._foreach_addcmul_(sqr_avg, adjusted_grad, adjusted_grad, value=1-beta3)\n",
    "\n",
    "    # calculate debias terms\n",
    "    db1 = 1 - beta1**steps\n",
    "    db2 = 1 - beta2**steps\n",
    "    db3 = 1 - beta3**steps\n",
    "\n",
    "    # calculate η_k\n",
    "    db3 = torch._foreach_div(sqr_avg, scalars=db3.tolist())\n",
    "    torch._foreach_sqrt_(db3)\n",
    "    torch._foreach_add_(db3, eps)\n",
    "    p_lrs = torch._foreach_div(db3, lr)\n",
    "    torch._foreach_reciprocal_(p_lrs)\n",
    "    # currently foreach_div doesn't allow a scalar as the first arg\n",
    "\n",
    "    # perform Adan step\n",
    "    db1 = torch._foreach_div(grad_avg, scalars=db1.tolist())\n",
    "    db2 = torch._foreach_div(diff_avg, scalars=db2.tolist())\n",
    "    torch._foreach_sub_(p, torch._foreach_mul(torch._foreach_add(db1, db2, alpha=beta2), p_lrs))\n",
    "    \n",
    "    # calculate and apply λ\n",
    "    if wd != 0:\n",
    "        wd = np.where(do_wd, 1+lr*wd, 1.)\n",
    "        torch._foreach_div_(p, scalars=wd.tolist())\n",
    "\n",
    "    # set current grad as next step's prior_grad, currently no foreach_set method\n",
    "    [pg.set_(gd.clone()) for pg, gd in zip(prior_grad, grad)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class AdanForEachOptimizer(ForEachOptimizer):\n",
    "    \"An `Optimizer` with a modified step for Adan ForEach\"\n",
    "    def __init__(self,\n",
    "        params:listified[Tensor], # Model parameters\n",
    "        opt_step:Callable, # `ForEachOptimizer` optimizer step\n",
    "        paper_init:bool=False, # Initialize first prior_grad to grad following paper or zeros\n",
    "        **defaults # Optimizer specific hyper parameters\n",
    "    ):\n",
    "        super().__init__(params, opt_step, **defaults)\n",
    "        self.paper_init = paper_init\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        if closure is not None: raise NotImplementedError(\"fastai optimizers currently do not support closure\")\n",
    "        for pg, hyper in zip(self.param_lists, self.hypers):\n",
    "            pl, gl, grad_avg, diff_avg, sqr_avg, prior_grad, steps, do_wd = [], [], [], [], [], [], [], []\n",
    "\n",
    "            for p in pg:\n",
    "                if hasattr(p, 'grad') and p.grad is not None:\n",
    "                    state = self.state[p]\n",
    "\n",
    "                    if 'step' not in state:\n",
    "                        state['grad_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        state['diff_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        state['sqr_avg']  = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        if self.paper_init:\n",
    "                            state['prior_grad'] = p.grad.clone()\n",
    "                        else:\n",
    "                            state['prior_grad'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        state['step'] = 0\n",
    "\n",
    "                    state['step'] += 1\n",
    "                    pl.append(p)\n",
    "                    gl.append(p.grad)\n",
    "                    grad_avg.append(state['grad_avg'])\n",
    "                    diff_avg.append(state['diff_avg'])\n",
    "                    sqr_avg.append(state['sqr_avg'])\n",
    "                    prior_grad.append(state['prior_grad'])\n",
    "                    do_wd.append(state.get('do_wd', True))\n",
    "                    steps.append(state['step'])\n",
    "\n",
    "            self.opt_step(p=pl, grad=gl, grad_avg=grad_avg, diff_avg=diff_avg, sqr_avg=sqr_avg, \n",
    "                          prior_grad=prior_grad, steps=np.array(steps, dtype=np.int32), do_wd=np.array(do_wd, dtype=bool), **hyper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adan was introduced by Xie et al in *[Adan: Adaptive Nesterov momentum Algorithm for Faster Optimizing Deep Models](https://arxiv.org/abs/2208.06677)*. Adan uses a efficient Nesterov momentum estimation method to avoid the extra computation and memory overhead of calculating the extrapolation point gradient. \n",
    "\n",
    "[Nadam](https://openreview.net/forum?id=OM0jvwB8jIp57ZJjtNEZ) also estimates Nesterov momentum, but in contrast it only estimates the first-order gradient moment while Adan estimates both first- and second-order movements.\n",
    "\n",
    "For consistency with other fastai optimizers, the coefficients `beta1`, `beta2`, and `beta3` have been inversed from the paper values, e.g. `β1=0.98` instead of `β1=0.02`.\n",
    "\n",
    ":::{.callout-note}\n",
    "This implementation of `Adan` does not contain the restart condition, as it is mostly unused in the paper.\n",
    ":::\n",
    "\n",
    "In addition to a fastai native implementation, `Adan` has fused ForEach and Torchscript implementations. See the [Fused Optimizer](optimizer.fused.html) documentation for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adan -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def Adan(\n",
    "    params:listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    beta1:float=0.98, # Gradient moving average (β1) coefficient\n",
    "    beta2:float=0.92, # Gradient difference moving average (β2) coefficient\n",
    "    beta3:float=0.99, # Gradient squared moving average (β3) coefficient\n",
    "    eps:float=1e-8, # Added for numerical stability\n",
    "    wd:float=0.02, # True weight decay\n",
    "    paper_init:bool=False, # Initialize prior gradient with current gradient per paper, or zeroes\n",
    "    foreach:bool=False, # Use fused ForEach implementation\n",
    "    jit:bool=False # Use fused TorchScript implementation\n",
    ") -> Optimizer|AdanForEachOptimizer|JitOptimizer:\n",
    "    \"A fastai Adan optimizer with optional ForEach and TorchScript implementations\"\n",
    "    if foreach:\n",
    "        return AdanForEachOptimizer(params, adan_foreach_step, lr=lr, beta1=beta1, beta2=beta2, \n",
    "                                    beta3=beta3, eps=eps, wd=wd, paper_init=paper_init)\n",
    "    elif jit:\n",
    "        cb = partial(adan_jit_step, paper_init=paper_init)\n",
    "        return JitOptimizer(params, cb, lr=lr, beta1=beta1, beta2=beta2, beta3=beta3, eps=eps, wd=wd)\n",
    "    else:\n",
    "        cbs = [partial(adan_setup, paper_init=paper_init), adan_step]\n",
    "        return Optimizer(params, cbs, lr=lr, beta1=beta1, beta2=beta2, beta3=beta3, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def adan(\n",
    "    beta1:float=0.98, # Gradient moving average (β1) coefficient\n",
    "    beta2:float=0.92, # Gradient difference moving average (β2) coefficient\n",
    "    beta3:float=0.99, # Gradient squared moving average (β3) coefficient\n",
    "    eps:float=1e-8, # Added for numerical stability\n",
    "    wd:float=0.02, # True weight decay\n",
    "    paper_init:bool=False, # Initialize prior gradient with current gradient per paper, or zeroes\n",
    "    foreach:bool=False, # Use fused ForEach implementation\n",
    "    jit:bool=False # Use fused TorchScript implementation\n",
    ") -> Optimizer|AdanForEachOptimizer|JitOptimizer:\n",
    "    \"Partial function for the Adan optimizer with fused ForEach and TorchScript implementations\"\n",
    "    return partialler(Adan, beta1=beta1, beta2=beta2, beta3=beta3, eps=eps, wd=wd, \n",
    "                      paper_init=paper_init, foreach=foreach, jit=jit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "test_steps = 25\n",
    "\n",
    "def tst_param(val, grad=None):\n",
    "    \"Create a tensor with `val` and a gradient of `grad` for testing\"\n",
    "    res = tensor([val]).float()\n",
    "    res.grad = tensor([val/10 if grad is None else grad]).float()\n",
    "    return res\n",
    "\n",
    "def tst_params():\n",
    "    r = L.range(4)\n",
    "    return r.map(tst_param)\n",
    "\n",
    "params_org = tst_params()\n",
    "opt_org = Adan(params_org, lr=0.1)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = tst_params()\n",
    "opt_jit = Adan(params_jit, lr=0.1, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "params_for = tst_params()\n",
    "opt_for = Adan(params_for, lr=0.1, foreach=True)\n",
    "opt_for.step()\n",
    "\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_jit])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_for])\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_jit.step()\n",
    "    opt_for.step()\n",
    "\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_jit])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_for])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdanLargeBatchLR -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def AdanLargeBatchLR(bs:int) -> float:\n",
    "    \"Square root rule for scaling `Adan` learning rate for large-batch training\"\n",
    "    return math.sqrt(bs/256)*6.25e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Hyperparameter notes from Xie et al:\n",
    "\n",
    "1. `beta2` is the least sensitive Adan hyperparameter, default of 0.92 works for majority of tasks\n",
    "2. Xie et al primarily tune `beta3` (between 0.9-0.999) before `beta1` (between 0.9-0.98) for different tasks\n",
    "3. Adan pairs well with large learning rates. Paper and GitHub report up to 3x larger than `Lamb` and up to 5-10x larger than `Adam`\n",
    "4. Xie et al use the default weight decay of 0.02 for all tasks except fine-tuning Bert `wd=0.01` and reinforcement learning `wd=0`\n",
    "\n",
    ":::{.callout-note}\n",
    "With `paper_init=True`, fastxtend's `Adan` matches Xie et al's [Adan implementation](https://github.com/sail-sg/Adan).\n",
    ":::\n",
    "\n",
    "## Training Speed\n",
    "\n",
    ":::{.callout-important}\n",
    "ForEach and TorchScript optimizers have only been tested on PyTorch 1.12 and are not guaranteed to work on older versions.\n",
    ":::\n",
    "\n",
    "One [critque of Adan](https://wandb.ai/capecape/adan_optimizer/reports/Adan-A-New-Optimizer-That-Challenges-Adam--VmlldzoyNTQ5NjQ5#conclusions) is when using a standard PyTorch implementation, Adan is significantly slower than AdamW. Between 41 to 97 percent slower on the models benchmarked in @tbl-first below.\n",
    "\n",
    "As shown in @tbl-fused, fastxtend's fused ForEach Adan is 95 to 284 percent faster[^faster] then a standard PyTorch implementation.\n",
    "\n",
    "| Model         | Native Step | ForEach Step | ForEach Speedup | JIT Step | JIT Speedup |\n",
    "| ------------- | :---------: | :----------: | :-------------: | :------: | :---------: |\n",
    "| XResNet18     | 36ms        | 17ms         | 112%            | 23ms     | 60%         |\n",
    "| XResNet50     | 78ms        | 41ms         | 95%             | 50ms     | 59%         |\n",
    "| XSE-ResNext50 | 109ms       | 52ms         | 108%            | 74ms     | 48%         |\n",
    "| XResNet101    | 131ms       | 59ms         | 120%            | 75ms     | 75%         |\n",
    "| DeBERTa Base  | 53ms        | 13ms         | 284%            | 22ms     | 137%        |\n",
    "\n",
    ": Increase in Adan `opt_step` Speed vs Native Optimizer {#tbl-fused}\n",
    "\n",
    "While the Adan ForEach methods are still 17 to 82 percent slower than the AdamW ForEach methods, the difference in performance and as a percentage of total training time is significantly smaller. Adan ForEach is 6ms to 11ms slower then AdamW ForEach instead of 11ms to 43ms with Adan native as shown in @tbl-panel.\n",
    "\n",
    "::: {#tbl-panel layout-ncol=2}\n",
    "| Model         | AdamW Step | Adan Step | Slowdown |\n",
    "| :-----------: | :--------: | :-------: | :------: |\n",
    "| XResNet18     | 25ms       | 36ms      | 44%      |\n",
    "| XResNet50     | 55ms       | 78ms      | 41%      |\n",
    "| XSE-ResNext50 | 72ms       | 109ms     | 52%      |\n",
    "| XResNet101    | 88ms       | 131ms     | 51%      |\n",
    "| DeBERTa Base  | 27ms       | 53ms      | 97%      |\n",
    "\n",
    ": Native Implementation {#tbl-first}\n",
    "\n",
    "| Model         | AdamW Step | Adan Step | Slowdown |\n",
    "| :-----------: | :--------: | :-------: | :------: |\n",
    "| XResNet18     | 13ms       | 17ms      | 38%      |\n",
    "| XResNet50     | 31ms       | 41ms      | 28%      |\n",
    "| XSE-ResNext50 | 43ms       | 52ms      | 17%      |\n",
    "| XResNet101    | 48ms       | 59ms      | 27%      |\n",
    "| DeBERTa Base  | 6.9ms      | 13ms      | 82%      |\n",
    "\n",
    ": Fused ForEach Implementation {#tbl-second}\n",
    "\n",
    "AdamW vs Adan Training Speed\n",
    ":::\n",
    "\n",
    "[^faster]: Benchmarked on a GeForce 3080 Ti using PyTorch 1.12.1, Mixed Precision, [Channels Last](callback.channelslast.html) (except DeBERTa), and fastxtend's [Simple Profiler Callback](callback.simpleprofiler.html). Results may differ on other models, hardware, and across benchmarking runs. Speedup and slowdown are calculated from the total time spent on the optimization step.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('fastai')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
