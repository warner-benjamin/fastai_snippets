{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp optimizer.lion\n",
    "#|default_cls_lvl 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "# Lion implementation based on the paper's code release\n",
    "# https://github.com/google/automl/tree/master/lion - Apache License 2.0 - Copyright 2020 Google Research"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lion: Evo**L**ved S**i**gn M**o**me**n**tum Optimizer\n",
    "> With fastai native and fused ForEach implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from fastai.optimizer import Optimizer\n",
    "\n",
    "from fastxtend.optimizer.foreach import ForEachOptimizer\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastxtend.test_utils import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lion Fastai Callback -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def lion_step(p:Tensor, lr:float, wd:float, beta1:float, beta2:float,\n",
    "              grad_avg:Tensor|None=None, do_wd:bool=True, **kwargs):\n",
    "\n",
    "    if grad_avg is None:\n",
    "        grad_avg = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "\n",
    "    # weight decay\n",
    "    if do_wd and wd != 0:\n",
    "        p.data.mul_(1-lr*wd)\n",
    "\n",
    "    # lion step\n",
    "    update = grad_avg.mul(beta1) + p.grad.data.mul(1 - beta1)\n",
    "    p.data.add_(torch.sign(update), alpha=-lr)\n",
    "\n",
    "    # Update m_k\n",
    "    grad_avg.mul_(beta2).add_(p.grad.data, alpha=1-beta2)\n",
    "\n",
    "    return {'grad_avg': grad_avg}\n",
    "\n",
    "lion_step.defaults = dict(beta1=0.9, beta2=0.99)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lion ForEach -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def lion_foreach_step(p:list[Tensor], g:list[Tensor], grad_avg:list[Tensor],\n",
    "                      do_wd:np.ndarray[Any, bool], lr:float, wd:float, beta1:float,\n",
    "                      beta2:float, **kwargs):\n",
    "\n",
    "    # weight_decay\n",
    "    if wd != 0:\n",
    "        wd = np.where(do_wd, 1-lr*wd, 1.)\n",
    "        torch._foreach_mul_(p, scalars=wd.tolist())\n",
    "\n",
    "    # lion update step\n",
    "    update = torch._foreach_mul(grad_avg, scalar=beta1)\n",
    "    torch._foreach_add_(update, g, alpha=1-beta1)\n",
    "    for u in update: u.sign_()\n",
    "    torch._foreach_add_(p, update, alpha=-lr)\n",
    "\n",
    "    # update m_k\n",
    "    torch._foreach_mul_(grad_avg, scalar=beta2)\n",
    "    torch._foreach_add_(grad_avg, g, alpha=1-beta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class LionForEachOptimizer(ForEachOptimizer):\n",
    "    \"An `Optimizer` with a modified step for Lion ForEach\"\n",
    "    def __init__(self,\n",
    "        params:Listified[Tensor], # Model parameters\n",
    "        opt_step:Callable, # `ForEachOptimizer` optimizer step\n",
    "        **defaults # Optimizer specific hyper parameters\n",
    "    ):\n",
    "        super().__init__(params, opt_step, **defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        if closure is not None: raise NotImplementedError(\"fastai optimizers currently do not support closure\")\n",
    "        for pg, hyper in zip(self.param_lists, self.hypers):\n",
    "            pl, gl, grad_avg, do_wd = [], [], [], []\n",
    "\n",
    "            for p in pg:\n",
    "                if hasattr(p, 'grad') and p.grad is not None:\n",
    "                    state = self.state[p]\n",
    "\n",
    "                    if 'grad_avg' not in state:\n",
    "                        state['grad_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "\n",
    "                    pl.append(p)\n",
    "                    gl.append(p.grad)\n",
    "                    grad_avg.append(state['grad_avg'])\n",
    "                    do_wd.append(state.get('do_wd', True))\n",
    "\n",
    "            self.opt_step(p=pl, g=gl, grad_avg=grad_avg, do_wd=np.array(do_wd, dtype=bool), **hyper)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lion was introduced by Chen et al in *[Symbolic Discovery of Optimization Algorithms](https://arxiv.org/abs/2302.06675)*. Lion only keeps track of the gradient moving average (momentum) which reduces memory usage compared to [`AdamW`](optimizer.fused.html#adam-optimizer). Lion uses two momentum EMA factors, one for tracking momentum and another for using momentum in the update step. Using default hyperparmeters, this allows up to ten times longer history for momentum tracking while leveraging more of the current gradient for the model update. Unlike most optimizers, Lion uses the same magnitude for each parameter update calculated using the sign operation.\n",
    "\n",
    "In addition to a fastai native implementation, `Lion` has a fused ForEach implementation. See the [Fused Optimizer](optimizer.fused.html) documentation for more details."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lion -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def Lion(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    beta1:float=0.9, # Update gradient moving average (β1) coefficient\n",
    "    beta2:float=0.99, # Gradient moving average (β2) coefficient\n",
    "    wd:float=0.1, # True weight decay\n",
    "    foreach:bool=False, # Use fused ForEach implementation\n",
    ") -> Optimizer|LionForEachOptimizer:\n",
    "    \"A fastai Lion optimizer with a fused ForEach implementation\"\n",
    "    if foreach:\n",
    "        return LionForEachOptimizer(params, lion_foreach_step, lr=lr,\n",
    "                                    beta1=beta1, beta2=beta2, wd=wd)\n",
    "    else:\n",
    "        return Optimizer(params, [lion_step], lr=lr, beta1=beta1, beta2=beta2, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def lion(\n",
    "    beta1:float=0.9, # Update gradient moving average (β1) coefficient\n",
    "    beta2:float=0.99, # Gradient moving average (β2) coefficient\n",
    "    wd:float=0.1, # True weight decay\n",
    "    foreach:bool=False, # Use fused ForEach implementation\n",
    ") -> Optimizer|LionForEachOptimizer:\n",
    "    \"Partial function for the Lion optimizer with a fused ForEach implementation\"\n",
    "    return partialler(Lion, beta1=beta1, beta2=beta2, wd=wd, foreach=foreach)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Hyperparameter notes from Chen et al:\n",
    "\n",
    "1. Except for language modeling, `beta1` and `beta2` are held at 0.9 and 0.99, respectively. When traing T5 they set `beta1=0.95` and `beta2=0.98`.\n",
    "2. Due to the larger update norm from the sign operation, the Lion learning rate is typically 10X smaller than [`AdamW`](optimizer.fused.html#adam-optimizer), with 3X smaller sometimes performing better.\n",
    "3. Since the effective weight decay is multiplied by the learning rate, weight decay should be increased by the learning rate decrease (10X or 3X).\n",
    "4. The optimal batch size for Lion is 4096 (vs AdamW's 256), but Lion still performs well at a batch size of 64 and matches or exceeds [`AdamW`](optimizer.fused.html#adam-optimizer) on all tested batch sizes.\n",
    "\n",
    "## Training Speed\n",
    "\n",
    ":::{.callout-important}\n",
    "The ForEach optimizer has only been tested on PyTorch 1.12+ and are not guaranteed to work on older versions.\n",
    ":::\n",
    "\n",
    "As shown in @tbl-fused, fastxtend's fused ForEach Lion is 13 to 195 percent faster[^faster] then a standard PyTorch implementation. This training speed advantage could increase in a future PyTorch release, as PyTorch doesn't have a ForEach implementation of `sign`[^equivalent], so the implementation falls back to a for loop in the middle of the Lion update step.\n",
    "\n",
    "| Model             | Layers | Native Step | ForEach Step | ForEach Speedup |\n",
    "| ----------------- | :----: | :---------: | :----------: | :-------------: |\n",
    "| XResNet18         | 1      | 23ms        | 13ms         | 73%             |\n",
    "| XResNet50         | 1      | 50ms        | 34ms         | 47%             |\n",
    "| XSE-ResNeXt50     | 1      | 66ms        | 47ms         | 41%             |\n",
    "| XResNet101        | 1      | 76ms        | 48ms         | 59%             |\n",
    "| ConvNeXt Tiny     | 2      | 118ms       | 104ms        | 13%             |\n",
    "| ConvNeXt Small    | 2      | 189ms       | 164ms        | 16%             |\n",
    "| ViT Patch16 Small | 2      | 57ms        | 45ms         | 26%             |\n",
    "| DeBERTa Base      | 1      | 22ms        | 7.5ms        | 195%            |\n",
    "\n",
    ": Increase in Lion ForEach `opt_step` Speed vs Native Optimizer {#tbl-fused}\n",
    "\n",
    "Due a simpler update and only tracking momentum, the native implementation of Lion is both faster than the native implementation of AdamW and uses less memory. However, since the ForEach implementation requires use of a for loop, Lion ForEach is equal or slower than AdamW ForEach. However, Lion ForEach should still use less memory than AdamW ForEach.\n",
    "\n",
    "::: {#tbl-panel layout-ncol=2}\n",
    "| Model             | AdamW Step | Lion Step | Speedup   |\n",
    "| :---------------: | :--------: | :-------: | :-------: |\n",
    "| XResNet18         | 26ms       | 23ms      | 15%       |\n",
    "| XResNet50         | 54ms       | 50ms      | 8.2%      |\n",
    "| XSE-ResNeXt50     | 72ms       | 66ms      | 8.3%      |\n",
    "| XResNet101        | 91ms       | 76ms      | 19%       |\n",
    "| ConvNeXt Tiny     | 125ms      | 118ms     | 6.1%      |\n",
    "| ConvNeXt Small    | 202ms      | 189ms     | 6.8%      |\n",
    "| ViT Patch16 Small | 63ms       | 57ms      | 9.4%      |\n",
    "| DeBERTa Base      | 26ms       | 22ms      | 25%       |\n",
    "\n",
    ": Native Implementation {#tbl-native}\n",
    "\n",
    "| Model             | AdamW Step | Lion Step | Slowdown |\n",
    "| :---------------: | :--------: | :-------: | :------: |\n",
    "| XResNet18         | 13ms       | 13ms      | 0.9%     |\n",
    "| XResNet50         | 33ms       | 34ms      | 4.6%     |\n",
    "| XSE-ResNeXt50     | 42ms       | 47ms      | 12%      |\n",
    "| XResNet101        | 46ms       | 48ms      | 4.2%     |\n",
    "| ConvNeXt Tiny     | 102ms      | 104ms     | 2.5%     |\n",
    "| ConvNeXt Small    | 161ms      | 164ms     | 1.6%     |\n",
    "| ViT Patch16 Small | 42ms       | 45ms      | 4.9%     |\n",
    "| DeBERTa Base      | 7.4ms      | 7.5ms     | 1.5%     |\n",
    "\n",
    ": Fused ForEach Implementation {#tbl-foreach}\n",
    "\n",
    "AdamW vs Lion Training Speed\n",
    ":::\n",
    "\n",
    "[^faster]: Benchmarked on a GeForce 3080 Ti using PyTorch 1.13.1, Cuda 11.7, Mixed Precision, [Channels Last](callback.channelslast.html) (except DeBERTa and ViT), and fastxtend's [Simple Profiler Callback](callback.simpleprofiler.html). Results may differ on other models, hardware, and across benchmarking runs. Speedup and slowdown are calculated from the total time spent on the optimization step.\n",
    "\n",
    "[^equivalent]: Numerically equivalent approximations of `sign` using ForEach operators ended up using more memory and were a wash on training speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# Test contain code from:\n",
    "# fastai - Apache License 2.0 - Copyright (c) 2023 fast.ai\n",
    "\n",
    "test_steps = 25\n",
    "\n",
    "def tst_param(val, grad=None):\n",
    "    \"Create a tensor with `val` and a gradient of `grad` for testing\"\n",
    "    res = tensor([val]).float()\n",
    "    res.grad = tensor([val/10 if grad is None else grad]).float()\n",
    "    return res\n",
    "\n",
    "def tst_params():\n",
    "    r = L.range(4)\n",
    "    return r.map(tst_param)\n",
    "\n",
    "params_org = tst_params()\n",
    "opt_org = Lion(params_org, lr=0.01)\n",
    "opt_org.step()\n",
    "\n",
    "params_for = tst_params()\n",
    "opt_for = Lion(params_for, lr=0.01, foreach=True)\n",
    "opt_for.step()\n",
    "\n",
    "# Test values from paper implementation: https://github.com/google/automl/tree/master/lion\n",
    "test_close([p.item() for p in params_org], [0., 0.9890, 1.9880, 2.9869999])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_for])\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_for.step()\n",
    "\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_for])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
