{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp optimizer.sophia\n",
    "#|default_cls_lvl 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "# Sophia implementation based on the paper's code release\n",
    "# https://github.com/Liuhong99/Sophia - MIT License - Copyright 2023 Hong Liu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sophia: **S**econd-**o**rder Cli**p**ped Stoc**h**astic Optimiz**a**tion\n",
    "> With fastai native and fused ForEach implementations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sophia was introduced by Liu et al in *[Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training](https://arxiv.org/abs/2305.14342)*. Sophia is a second-order optimizer that leverages a light-weight Hessian estimate as a pre-conditioner, which is supposed to handle the Large Language Model (LLM) loss landscape better than [AdamW](https://openreview.net/forum?id=Bkg6RiCqY7). The Hessian pre-conditioner is more aggressive than [`AdamW`](optimizer.fused.html#adam-optimizer), with stronger update penalties sharp dimensions, which can lead to a more uniform loss decrease across parameters and faster convergence. Additionally, Sophia applies element-wise clipping to updates which allows infrequent and stochastic updates to the Hessian estimate, reducing optimizer wall-clock time.\n",
    "\n",
    ":::{.callout-important}\n",
    "`Sophia` will not update the Hessian estimate unless the `SophiaCallback` is added to `fastai.learner.Learner`.\n",
    ":::\n",
    "\n",
    "In addition to a fastai native implementation, `Sophia` has a fused ForEach implementation. See the [Fused Optimizer](optimizer.fused.html) documentation for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "from typing import Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.distributions import Categorical\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from fastai.callback.core import Callback\n",
    "from fastai.callback.fp16 import MixedPrecision\n",
    "from fastai.losses import CrossEntropyLossFlat, LabelSmoothingCrossEntropy, LabelSmoothingCrossEntropyFlat\n",
    "from fastai.optimizer import Optimizer, _update\n",
    "\n",
    "from fastxtend.optimizer.foreach import ForEachOptimizer\n",
    "\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def sophia_step(p:Tensor, lr:float, eps:float, wd:float, mom:float, hess_mom:float,\n",
    "                rho:float, bs:int, hessian_step:bool, grad_avg:Tensor|None=None,\n",
    "                hessian:Tensor|None=None, do_wd:bool=True, **kwargs):\n",
    "    \"Updates Stable Adam moving averages and performs the Stable Adam step with `lr` on `p`\"\n",
    "    if grad_avg is None:\n",
    "        grad_avg = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "        hessian  = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "\n",
    "    if hessian_step:\n",
    "        hessian.mul_(hess_mom).addcmul_(p.grad.data, p.grad.data, value=1-hess_mom)\n",
    "    else:\n",
    "        if wd!=0 and do_wd:\n",
    "            p.data.mul_(1-lr*wd)\n",
    "\n",
    "        # update moving average\n",
    "        grad_avg.mul_(mom).add_(p.grad.data, alpha=1-mom)\n",
    "\n",
    "        # compute sophia update ratio\n",
    "        ratio = grad_avg.abs().div(hessian.mul(rho * bs).add(eps)).clamp(None, 1)\n",
    "\n",
    "        # sophia update step\n",
    "        p.data.addcmul_(grad_avg.sign(), ratio, value=-lr)\n",
    "\n",
    "    return {'grad_avg': grad_avg, 'hessian': hessian}\n",
    "\n",
    "sophia_step.defaults = dict(mom=0.9, hess_mom=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class SophiaOptimizer(Optimizer):\n",
    "    def __init__(self,\n",
    "        params:Tensor|Iterable, # Model parameters\n",
    "        cbs:callable|MutableSequence, # `Optimizer` step callbacks\n",
    "        **defaults # Hyper parameters default values\n",
    "    ):\n",
    "        super().__init__(params, cbs, **defaults)\n",
    "        self.update_sophia_hypers(0, False)\n",
    "\n",
    "    def update_sophia_hypers(self, bs, hessian_step):\n",
    "        self._bs = bs\n",
    "        self._hessian_step = hessian_step\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        if closure is not None: raise NotImplementedError(\"fastai optimizers currently do not support closure\")\n",
    "        for p,pg,state,hyper in self.all_params(with_grad=True):\n",
    "            for cb in self.cbs:\n",
    "                state = _update(state, cb(p, **{**state, **hyper}, bs=self._bs, hessian_step=self._hessian_step))\n",
    "            self.state[p] = state\n",
    "\n",
    "    def clear_state(self):\n",
    "        super().clear_state()\n",
    "        self.update_sophia_hypers(0, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def sophia_foreach_step(p:list[Tensor], g:list[Tensor], grad_avg:list[Tensor], hessian:list[Tensor],\n",
    "                        do_wd:np.ndarray[Any, bool], lr:float, wd:float, mom:float, hess_mom:float,\n",
    "                        eps:float, rho:float, bs:int, hessian_step:bool, **kwargs):\n",
    "    if hessian_step:\n",
    "        torch._foreach_mul_(hessian, scalar=hess_mom)\n",
    "        torch._foreach_addcmul_(hessian, g, g, value=1-hess_mom)\n",
    "    else:\n",
    "        # weight_decay\n",
    "        if wd != 0:\n",
    "            wd = np.where(do_wd, 1-lr*wd, 1.)\n",
    "            torch._foreach_mul_(p, scalars=wd.tolist())\n",
    "\n",
    "        # update moving average\n",
    "        torch._foreach_mul_(grad_avg, scalar=mom)\n",
    "        torch._foreach_add_(grad_avg, g, alpha=1-mom)\n",
    "\n",
    "        # compute sophia update ratio\n",
    "        ratio = torch._foreach_abs(grad_avg)\n",
    "        temp = torch._foreach_mul(hessian, scalar=rho*bs)\n",
    "        torch._foreach_add_(temp, scalar=eps)\n",
    "        torch._foreach_div_(ratio, temp)\n",
    "        torch._foreach_clamp_max_(ratio, scalar=1)\n",
    "\n",
    "        # sophia update step\n",
    "        temp = [ga.sign() for ga in grad_avg]\n",
    "        torch._foreach_addcmul_(p, temp, ratio, value=-lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class SophiaForEachOptimizer(ForEachOptimizer):\n",
    "    \"An `ForEachOptimizer` with a modified step for `sophia_foreach_step`\"\n",
    "    def __init__(self,\n",
    "        params:Listified[Tensor], # Model parameters\n",
    "        opt_step:Callable, # `ForEachOptimizer` optimizer step\n",
    "        **defaults # Optimizer specific hyper parameters default values\n",
    "    ):\n",
    "        super().__init__(params, opt_step, True, **defaults)\n",
    "        self.update_sophia_hypers(0, False)\n",
    "\n",
    "    def update_sophia_hypers(self, bs, hessian_step):\n",
    "        self._bs = bs\n",
    "        self._hessian_step = hessian_step\n",
    "\n",
    "    def clear_state(self):\n",
    "        super().clear_state()\n",
    "        self.update_sophia_hypers(0, False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        if closure is not None:\n",
    "            raise NotImplementedError(\"fastai optimizers currently do not support closure\")\n",
    "        for pg, hyper in zip(self.param_lists, self.hypers):\n",
    "            pl, gl, grad_avg, hessian, do_wd = [], [], [], [], []\n",
    "\n",
    "            for p in pg:\n",
    "                if hasattr(p, 'grad') and p.grad is not None:\n",
    "                    state = self.state[p]\n",
    "\n",
    "                    if 'grad_avg' not in state.keys():\n",
    "                        state['grad_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        state['hessian']  = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "\n",
    "                    pl.append(p)\n",
    "                    gl.append(p.grad)\n",
    "                    grad_avg.append(state['grad_avg'])\n",
    "                    hessian.append(state['hessian'])\n",
    "                    do_wd.append(state.get('do_wd', True))\n",
    "\n",
    "            self.opt_step(p=pl, g=gl, grad_avg=grad_avg, hessian=hessian,\n",
    "                          do_wd=np.array(do_wd, dtype=bool), bs=self._bs,\n",
    "                          hessian_step=self._hessian_step, **hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def Sophia(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    mom:float=0.965, # Gradient moving average (β1) coefficient\n",
    "    hess_mom:float=0.99, # Hessian moving average (β2) coefficient\n",
    "    rho:float=0.4, # Maximum update size, set higher for more agressive updates\n",
    "    eps:float=1e-15, # Added for numerical stability\n",
    "    wd:float=0.01, # Optional weight decay\n",
    "    foreach:bool=False, # Use fused ForEach implementation\n",
    ") -> SophiaOptimizer|SophiaForEachOptimizer:\n",
    "    \"A fastai Sophia optimizer with a fused ForEach implementation\"\n",
    "    if foreach:\n",
    "        return SophiaForEachOptimizer(params, sophia_foreach_step, lr=lr, mom=mom,\n",
    "                                      hess_mom=hess_mom, rho=rho, eps=eps, wd=wd)\n",
    "    else:\n",
    "        return SophiaOptimizer(params, [sophia_step], lr=lr, mom=mom,\n",
    "                               hess_mom=hess_mom, rho=rho, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def sophia(\n",
    "    mom:float=0.965, # Gradient moving average (β1) coefficient\n",
    "    hess_mom:float=0.99, # Hessian moving average (β2) coefficient\n",
    "    rho:float=0.4, # Maximum update size, set higher for more agressive updates\n",
    "    eps:float=1e-15, # Added for numerical stability\n",
    "    wd:float=0.01, # Optional weight decay\n",
    "    foreach:bool=False, # Use fused ForEach implementation\n",
    ") -> SophiaOptimizer|SophiaForEachOptimizer:\n",
    "    \"Partial function for the Sophia optimizer with a fused ForEach implementation\"\n",
    "    return partialler(Sophia, mom=mom, hess_mom=hess_mom, eps=eps,\n",
    "                      rho=rho, wd=wd, foreach=foreach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class SophiaHessian(str, Enum):\n",
    "    \"Hessian estimator for the Sophia optimizer for autocomplete\"\n",
    "    sophiag = 'sophiag'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class SophiaCallback(Callback):\n",
    "    \"Modifies the training loop for the Sophia Optimizer. Required for Sophia to run.\"\n",
    "    order,run_valid = MixedPrecision.order+1,False\n",
    "    def __init__(self,\n",
    "        hessian_update:int=10, # Update Sophia's Hessian estimate every `hessian_update` Optimizer steps\n",
    "        # hessian_est:str|SophiaHessian=SophiaHessian.sophiag # Sophia's Hessian estimator. Defaults to SophiaG's Gauss-Newton-Bartlett\n",
    "    ):\n",
    "        store_attr()\n",
    "\n",
    "    def before_fit(self):\n",
    "        if not isinstance(self.learn.opt, (SophiaOptimizer, SophiaForEachOptimizer)):\n",
    "            raise ValueError(\"`SophiaCallback` only supports the `Sophia` optimizer\")\n",
    "        if not isinstance(self.learn.loss_fn, (CrossEntropyLoss, CrossEntropyLossFlat,\n",
    "                                               LabelSmoothingCrossEntropy,\n",
    "                                               LabelSmoothingCrossEntropyFlat)):\n",
    "            warn('Non-CrossEntropy loss detected, SophiaG assumes data is in a categorical distrobution.')\n",
    "        self._step_iter = 0\n",
    "        self._hessian_step = False\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def before_loss(self):\n",
    "        if self._step_iter % self.hessian_update == self.hessian_update:\n",
    "            dist = Categorical(logits=self.pred)\n",
    "            self.learn.yb = dist.sample()\n",
    "            self._hessian_step = True\n",
    "\n",
    "    def before_step(self):\n",
    "        self.learn.opt.update_sophia_hypers(find_bs(self.learn.yb), self._hessian_step)\n",
    "\n",
    "    def after_step(self):\n",
    "        self._step_iter += 1\n",
    "        self._hessian_step = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SophiaCallback` expects the loss function to be a CrossEntropy loss, and only supports single target and single loss function training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Hyperparameter notes from Liu et al:\n",
    "\n",
    "1) Sophia hyperparameters should be similar to AdamW\n",
    "2) $\\rho$ (`rho`) should be in $[0.01, 0.1]$. A larger $\\rho$ means more aggressive updates\n",
    "3) Sophia may benefit from slightly higher weight decay and learning rate compared to AdamW"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastxtend.test_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.757878, 1.755481, 2.753083]\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "# Test contain code from:\n",
    "# fastai - Apache License 2.0 - Copyright (c) 2023 fast.ai\n",
    "\n",
    "test_steps = 25\n",
    "\n",
    "params_org = tst_params()\n",
    "opt_org = Sophia(params_org, lr=0.01)\n",
    "opt_org.update_sophia_hypers(64, False)\n",
    "opt_org.step()\n",
    "\n",
    "params_for = tst_params()\n",
    "opt_for = Sophia(params_for, lr=0.01, foreach=True)\n",
    "opt_for.update_sophia_hypers(64, False)\n",
    "opt_for.step()\n",
    "\n",
    "test_close([p.item() for p in params_org], [0.0, 0.9899, 1.9898, 2.9897])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_for])\n",
    "\n",
    "for i in range(test_steps):\n",
    "    if i % 10 == 9:\n",
    "        opt_org.update_sophia_hypers(64, True)\n",
    "        opt_for.update_sophia_hypers(64, True)\n",
    "    else:\n",
    "        opt_org.update_sophia_hypers(64, False)\n",
    "        opt_for.update_sophia_hypers(64, False)\n",
    "    opt_org.step()\n",
    "    opt_for.step()\n",
    "\n",
    "# Sophia numerical values from SophiaG https://github.com/Liuhong99/Sophia\n",
    "test_close([p.item() for p in params_org], [0.0, 0.757878, 1.755481, 2.753083])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_for])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
