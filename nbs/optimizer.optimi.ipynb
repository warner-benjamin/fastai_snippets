{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp optimizer.optimi\n",
    "#|default_cls_lvl 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "# Contains code from:\n",
    "# optimi - MIT Licence - Copyright (c) 2023 Benjamin Warner\n",
    "# fastai - Apache License 2.0 - Copyright (c) 2023 fast.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimi Optimizers\n",
    "> Fast, modern, and low precision optimizers with full fastai compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "\n",
    "from optimi import (Adam, adam, Adan, adan, Lion, lion, RAdam, radam,\n",
    "                    Ranger, ranger, SGD, sgd, StableAdamW, stableadamw)\n",
    "\n",
    "from torch.optim.optimizer import _default_to_fused_or_foreach\n",
    "\n",
    "from fastxtend.optimizer.utils import FastaiOptimizerAdapter, _convert_params\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class AdamOptimiOptimizer(Adam, FastaiOptimizerAdapter):\n",
    "    \"\"\"Adam optimizer. Optionally with decoupled weight decay (AdamW).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params\n",
    "        Iterable of parameters to optimize or dicts defining parameter groups\n",
    "    lr\n",
    "        Learning rate\n",
    "    betas\n",
    "        Coefficients for gradient and squared gradient moving averages (default: (0.9, 0.99))\n",
    "    weight_decay\n",
    "        Weight decay coefficient. If `decouple_wd` and `decouple_lr` are False, applies L2 penalty (default: 0)\n",
    "    eps\n",
    "        Added to denominator to improve numerical stability (default: 1e-6)\n",
    "    decouple_wd\n",
    "        Apply decoupled weight decay instead of L2 penalty (default: False)\n",
    "    decouple_lr\n",
    "        Apply fully decoupled weight decay instead of L2 penalty (default: False)\n",
    "    max_lr\n",
    "        Maximum scheduled learning rate. Set if `lr` is not the maximum scheduled learning rate and `decouple_lr` is True (default: None)\n",
    "    kahan_sum\n",
    "        Enables Kahan summation for more accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters (default: None)\n",
    "    foreach\n",
    "        Enables the foreach implementation. If unspecified, tries to use foreach over for-loop implementation since it is significantly faster (default: None)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable[Tensor] | Iterable[dict],\n",
    "        lr: float,\n",
    "        mom: float = 0.9,\n",
    "        sqr_mom: float = 0.99,\n",
    "        wd: float = 0,\n",
    "        eps: float = 1e-6,\n",
    "        decouple_wd: bool = False,\n",
    "        decouple_lr: bool = False,\n",
    "        max_lr: float | None = None,\n",
    "        kahan_sum: bool | None = None,\n",
    "        foreach: bool | None = None,\n",
    "    ):\n",
    "        params = super().convert_params(params)\n",
    "\n",
    "        if not 0.0 <= mom < 1.0:\n",
    "            raise ValueError(f\"Invalid momentum parameter: {mom=}\")\n",
    "        if not 0.0 <= sqr_mom< 1.0:\n",
    "            raise ValueError(f\"Invalid squared momentum parameter: {sqr_mom=}\")\n",
    "        if not 0.0 <= wd:\n",
    "            raise ValueError(f\"Invalid weight decay: {wd=}\")\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(f\"Invalid epsilon: {eps=}\")\n",
    "\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            mom=mom,\n",
    "            sqr_mom=sqr_mom,\n",
    "            wd=wd,\n",
    "            eps=eps,\n",
    "            weight_decay=wd,\n",
    "            decouple_wd=decouple_wd,\n",
    "            decouple_lr=decouple_lr,\n",
    "            max_lr=max_lr,\n",
    "            kahan_sum=kahan_sum,\n",
    "            foreach=foreach,\n",
    "            gradient_release=False,\n",
    "            setup=False,\n",
    "        )\n",
    "        super(Adam, self).__init__(params, defaults)\n",
    "\n",
    "    def _init_group(\n",
    "        self,\n",
    "        group: dict[str, Any],\n",
    "        params_wd: list[Tensor],\n",
    "        params_no_wd: list[Tensor],\n",
    "        grads_wd: list[Tensor],\n",
    "        grads_no_wd: list[Tensor],\n",
    "        exp_avgs_wd: list[Tensor],\n",
    "        exp_avgs_no_wd: list[Tensor],\n",
    "        exp_avg_sqs_wd: list[Tensor],\n",
    "        exp_avg_sqs_no_wd: list[Tensor],\n",
    "        kahan_comps_wd: list[Tensor],\n",
    "        kahan_comps_no_wd: list[Tensor],\n",
    "    ):\n",
    "        for p in group[\"params\"]:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "\n",
    "            state = self.state[p]\n",
    "            self._init_state(group, state, p)\n",
    "\n",
    "            if state.get(\"do_wd\", True):\n",
    "                params_wd.append(p)\n",
    "                grads_wd.append(p.grad)\n",
    "                exp_avgs_wd.append(state[\"exp_avg\"])\n",
    "                exp_avg_sqs_wd.append(state[\"exp_avg_sq\"])\n",
    "                kahan_comps_wd.append(state[\"kahan_comp\"])\n",
    "            else:\n",
    "                params_no_wd.append(p)\n",
    "                grads_no_wd.append(p.grad)\n",
    "                exp_avgs_no_wd.append(state[\"exp_avg\"])\n",
    "                exp_avg_sqs_no_wd.append(state[\"exp_avg_sq\"])\n",
    "                kahan_comps_no_wd.append(state[\"kahan_comp\"])\n",
    "\n",
    "        if not group[\"setup\"]:\n",
    "            group[\"setup\"] = True\n",
    "            group[\"step\"] = torch.tensor(0, dtype=torch.int32)\n",
    "\n",
    "            if group[\"foreach\"] is None:\n",
    "                _, group[\"foreach\"] = _default_to_fused_or_foreach(params_wd + params_no_wd, False, False)\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure: Callable | None = None, param: Tensor | None = None):\n",
    "        loss = None\n",
    "        if closure is not None and param is None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        if param is None:\n",
    "            for group in self.param_groups:\n",
    "                params_wd, params_no_wd, grads_wd, grads_no_wd = [], [], [], []\n",
    "                exp_avgs_wd, exp_avgs_no_wd, exp_avg_sqs_wd, exp_avg_sqs_no_wd = [], [], [], []\n",
    "                kahan_comps_wd, kahan_comps_no_wd = [], []\n",
    "\n",
    "                self._init_group(\n",
    "                    group,\n",
    "                    params_wd,\n",
    "                    params_no_wd,\n",
    "                    grads_wd,\n",
    "                    grads_no_wd,\n",
    "                    exp_avgs_wd,\n",
    "                    exp_avgs_no_wd,\n",
    "                    exp_avg_sqs_wd,\n",
    "                    exp_avg_sqs_no_wd,\n",
    "                    kahan_comps_wd,\n",
    "                    kahan_comps_no_wd\n",
    "                )\n",
    "\n",
    "                # Call adam for parameters with weight decay\n",
    "                if len(params_wd) > 0:\n",
    "                    adam(\n",
    "                        params=params_wd,\n",
    "                        grads=grads_wd,\n",
    "                        exp_avgs=exp_avgs_wd,\n",
    "                        exp_avg_sqs=exp_avg_sqs_wd,\n",
    "                        kahan_comps=kahan_comps_wd,\n",
    "                        lr=group[\"lr\"],\n",
    "                        beta1=group[\"mom\"],\n",
    "                        beta2=group[\"sqr_mom\"],\n",
    "                        weight_decay=group[\"wd\"],\n",
    "                        eps=group[\"eps\"],\n",
    "                        step=group[\"step\"],\n",
    "                        decouple_wd=group[\"decouple_wd\"],\n",
    "                        decouple_lr=group[\"decouple_lr\"],\n",
    "                        max_lr=group[\"max_lr\"],\n",
    "                        kahan_sum=group[\"kahan_sum\"],\n",
    "                        foreach=group[\"foreach\"],\n",
    "                        gradient_release=False,\n",
    "                        optimizer_accumulation=False,\n",
    "                    )\n",
    "\n",
    "                # Call adam for parameters without weight decay\n",
    "                if len(params_no_wd) > 0:\n",
    "                    adam(\n",
    "                        params=params_no_wd,\n",
    "                        grads=grads_no_wd,\n",
    "                        exp_avgs=exp_avgs_no_wd,\n",
    "                        exp_avg_sqs=exp_avg_sqs_no_wd,\n",
    "                        kahan_comps=kahan_comps_no_wd,\n",
    "                        lr=group[\"lr\"],\n",
    "                        beta1=group[\"mom\"],\n",
    "                        beta2=group[\"sqr_mom\"],\n",
    "                        weight_decay=0,\n",
    "                        eps=group[\"eps\"],\n",
    "                        step=group[\"step\"],\n",
    "                        decouple_wd=group[\"decouple_wd\"],\n",
    "                        decouple_lr=group[\"decouple_lr\"],\n",
    "                        max_lr=group[\"max_lr\"],\n",
    "                        kahan_sum=group[\"kahan_sum\"],\n",
    "                        foreach=group[\"foreach\"],\n",
    "                        gradient_release=False,\n",
    "                        optimizer_accumulation=False,\n",
    "                    )\n",
    "        else:\n",
    "            state = self.state[param]\n",
    "            group = state[\"group\"]\n",
    "            self._init_state(group, state, param)\n",
    "\n",
    "            adam(\n",
    "                params=param,\n",
    "                grads=param.grad,\n",
    "                exp_avgs=state[\"exp_avg\"],\n",
    "                exp_avg_sqs=state[\"exp_avg_sq\"],\n",
    "                kahan_comps=state[\"kahan_comp\"],\n",
    "                lr=group[\"lr\"],\n",
    "                beta1=group[\"mom\"],\n",
    "                beta2=group[\"sqr_mom\"],\n",
    "                weight_decay=group[\"wd\"] if state.get(\"do_wd\", True) else 0,\n",
    "                eps=group[\"eps\"],\n",
    "                step=state[\"step\"],\n",
    "                decouple_wd=group[\"decouple_wd\"],\n",
    "                decouple_lr=group[\"decouple_lr\"],\n",
    "                max_lr=group[\"max_lr\"],\n",
    "                kahan_sum=group[\"kahan_sum\"],\n",
    "                foreach=False,\n",
    "                gradient_release=True,\n",
    "                optimizer_accumulation=self._optimizer_accumulation,\n",
    "            )\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(AdamOptimiOptimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdamW -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class AdamWOptimiOptimizer(AdamOptimiOptimizer):\n",
    "    \"\"\"AdamW optimizer. Adam with decoupled weight decay.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params\n",
    "        Iterable of parameters to optimize or dicts defining parameter groups\n",
    "    lr\n",
    "        Learning rate\n",
    "    betas\n",
    "        Coefficients for gradient and squared gradient moving averages (default: (0.9, 0.99))\n",
    "    weight_decay\n",
    "        Weight decay coefficient. If `decouple_wd` and `decouple_lr` are False, applies L2 penalty (default: 0)\n",
    "    eps\n",
    "        Added to denominator to improve numerical stability (default: 1e-6)\n",
    "    decouple_lr\n",
    "        Apply fully decoupled weight decay instead of L2 penalty (default: False)\n",
    "    max_lr\n",
    "        Maximum scheduled learning rate. Set if `lr` is not the maximum scheduled learning rate and `decouple_lr` is True (default: None)\n",
    "    kahan_sum\n",
    "        Enables Kahan summation for more accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters (default: None)\n",
    "    foreach\n",
    "        Enables the foreach implementation. If unspecified, tries to use foreach over for-loop implementation since it is significantly faster (default: None)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable[Tensor] | Iterable[dict],\n",
    "        lr: float,\n",
    "        mom: float = 0.9,\n",
    "        sqr_mom: float = 0.99,\n",
    "        wd: float = 1e-2,\n",
    "        eps: float = 1e-6,\n",
    "        decouple_lr: bool = False,\n",
    "        max_lr: float | None = None,\n",
    "        kahan_sum: bool | None = None,\n",
    "        foreach: bool | None = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            params=params,\n",
    "            lr=lr,\n",
    "            mom=mom,\n",
    "            sqr_mom=sqr_mom,\n",
    "            wd=wd,\n",
    "            eps=eps,\n",
    "            decouple_wd=True,\n",
    "            decouple_lr=decouple_lr,\n",
    "            max_lr=max_lr,\n",
    "            kahan_sum=kahan_sum,\n",
    "            foreach=foreach,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(AdamWOptimiOptimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adan -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class AdanOptimiOptimizer(Adan, FastaiOptimizerAdapter):\n",
    "    \"\"\"Adan Optimizer: Adaptive Nesterov Momentum Algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params\n",
    "        Iterable of parameters to optimize or dicts defining parameter groups\n",
    "    lr\n",
    "        Learning rate\n",
    "    betas\n",
    "        Coefficients for gradient, gradient difference, and squared gradient moving averages\n",
    "    weight_decay\n",
    "        Weight decay coefficient. If `decouple_lr` is False, applies decoupled weight decay\n",
    "    eps\n",
    "        Added to denominator to improve numerical stability\n",
    "    decouple_lr\n",
    "        Apply fully decoupled weight decay instead of decoupled weight decay\n",
    "    max_lr\n",
    "        Maximum scheduled learning rate. Set if `lr` is not the maximum scheduled learning rate and `decouple_lr` is True\n",
    "    adam_wd\n",
    "        Apply weight decay before parameter update (Adam-style), instead of after the update per Adan algorithm\n",
    "    kahan_sum\n",
    "        Enables Kahan summation for more accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters\n",
    "    foreach\n",
    "        Enables the foreach implementation. If unspecified, tries to use foreach over for-loop implementation since it is significantly faster\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable[Tensor] | Iterable[dict],\n",
    "        lr: float,\n",
    "        beta1: float = 0.98,\n",
    "        beta2: float = 0.92,\n",
    "        beta3: float = 0.99,\n",
    "        wd: float = 2e-2,\n",
    "        eps: float = 1e-6,\n",
    "        decouple_lr: bool = False,\n",
    "        max_lr: float | None = None,\n",
    "        adam_wd: bool = False,\n",
    "        kahan_sum: bool | None = False,\n",
    "        foreach: bool | None = None,\n",
    "    ):\n",
    "        params = super().convert_params(params)\n",
    "\n",
    "        if not 0.0 <= beta1 < 1.0:\n",
    "            raise ValueError(f\"Invalid beta1 parameter: {beta1=}\")\n",
    "        if not 0.0 <= beta2 < 1.0:\n",
    "            raise ValueError(f\"Invalid beta2 parameter: {beta2=}\")\n",
    "        if not 0.0 <= beta3 < 1.0:\n",
    "            raise ValueError(f\"Invalid beta3 parameter: {beta3=}\")\n",
    "        if not 0.0 <= wd:\n",
    "            raise ValueError(f\"Invalid weight decay: {wd=}\")\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(f\"Invalid epsilon: {eps=}\")\n",
    "\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            beta1=beta1,\n",
    "            beta2=beta2,\n",
    "            beta3=beta3,\n",
    "            wd=wd,\n",
    "            eps=eps,\n",
    "            weight_decay=wd,\n",
    "            decouple_lr=decouple_lr,\n",
    "            max_lr=max_lr,\n",
    "            kahan_sum=kahan_sum,\n",
    "            foreach=foreach,\n",
    "            adam_wd=adam_wd,\n",
    "            gradient_release=False,\n",
    "            setup=False,\n",
    "        )\n",
    "        super(Adan, self).__init__(params, defaults)\n",
    "\n",
    "    def _init_group(\n",
    "        self,\n",
    "        group: dict[str, Any],\n",
    "        params_wd: list[Tensor],\n",
    "        params_no_wd: list[Tensor],\n",
    "        grads_wd: list[Tensor],\n",
    "        grads_no_wd: list[Tensor],\n",
    "        exp_avgs_wd: list[Tensor],\n",
    "        exp_avgs_no_wd: list[Tensor],\n",
    "        exp_avg_diffs_wd: list[Tensor],\n",
    "        exp_avg_diffs_no_wd: list[Tensor],\n",
    "        exp_avg_sqs_wd: list[Tensor],\n",
    "        exp_avg_sqs_no_wd: list[Tensor],\n",
    "        prev_grads_wd: list[Tensor],\n",
    "        prev_grads_no_wd: list[Tensor],\n",
    "        kahan_comps_wd: list[Tensor],\n",
    "        kahan_comps_no_wd: list[Tensor],\n",
    "    ):\n",
    "        for p in group[\"params\"]:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "\n",
    "            state = self.state[p]\n",
    "            self._init_state(group, state, p)\n",
    "\n",
    "            if state.get(\"do_wd\", False):\n",
    "                params_wd.append(p)\n",
    "                grads_wd.append(p.grad)\n",
    "                exp_avgs_wd.append(state[\"exp_avg\"])\n",
    "                exp_avg_diffs_wd.append(state[\"exp_avg_diff\"])\n",
    "                exp_avg_sqs_wd.append(state[\"exp_avg_sq\"])\n",
    "                prev_grads_wd.append(state[\"prev_grad\"])\n",
    "                kahan_comps_wd.append(state[\"kahan_comp\"])\n",
    "            else:\n",
    "                params_no_wd.append(p)\n",
    "                grads_no_wd.append(p.grad)\n",
    "                exp_avgs_no_wd.append(state[\"exp_avg\"])\n",
    "                exp_avg_diffs_no_wd.append(state[\"exp_avg_diff\"])\n",
    "                exp_avg_sqs_no_wd.append(state[\"exp_avg_sq\"])\n",
    "                prev_grads_no_wd.append(state[\"prev_grad\"])\n",
    "                kahan_comps_no_wd.append(state[\"kahan_comp\"])\n",
    "\n",
    "        if not group[\"setup\"]:\n",
    "            group[\"setup\"] = True\n",
    "            group[\"step\"] = torch.tensor(0, dtype=torch.int32)\n",
    "\n",
    "            if group[\"foreach\"] is None:\n",
    "                _, group[\"foreach\"] = _default_to_fused_or_foreach(params_wd + params_no_wd, False, False)\n",
    "            if group[\"kahan_sum\"]:\n",
    "                group[\"adam_wd\"] = False\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure: Callable | None = None, param: Tensor | None = None):\n",
    "        loss = None\n",
    "        if closure is not None and param is None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        if param is None:\n",
    "            for group in self.param_groups:\n",
    "                params_wd, params_no_wd, grads_wd, grads_no_wd = [], [], [], []\n",
    "                exp_avgs_wd, exp_avgs_no_wd, exp_avg_diffs_wd, exp_avg_diffs_no_wd = [], [], [], []\n",
    "                exp_avg_sqs_wd, exp_avg_sqs_no_wd, prev_grads_wd, prev_grads_no_wd = [], [], [], []\n",
    "                kahan_comps_wd, kahan_comps_no_wd = [], []\n",
    "\n",
    "                self._init_group(\n",
    "                    group,\n",
    "                    params_wd,\n",
    "                    params_no_wd,\n",
    "                    grads_wd,\n",
    "                    grads_no_wd,\n",
    "                    exp_avgs_wd,\n",
    "                    exp_avgs_no_wd,\n",
    "                    exp_avg_diffs_wd,\n",
    "                    exp_avg_diffs_no_wd,\n",
    "                    exp_avg_sqs_wd,\n",
    "                    exp_avg_sqs_no_wd,\n",
    "                    prev_grads_wd,\n",
    "                    prev_grads_no_wd,\n",
    "                    kahan_comps_wd,\n",
    "                    kahan_comps_no_wd\n",
    "                )\n",
    "\n",
    "                # Call adan for parameters with weight decay\n",
    "                if len(params_wd) > 0:\n",
    "                    adan(\n",
    "                        params=params_wd,\n",
    "                        grads=grads_wd,\n",
    "                        exp_avgs=exp_avgs_wd,\n",
    "                        exp_avg_diffs=exp_avg_diffs_wd,\n",
    "                        exp_avg_sqs=exp_avg_sqs_wd,\n",
    "                        prev_grads=prev_grads_wd,\n",
    "                        kahan_comps=kahan_comps_wd,\n",
    "                        lr=group[\"lr\"],\n",
    "                        beta1=group[\"beta1\"],\n",
    "                        beta2=group[\"beta2\"],\n",
    "                        beta3=group[\"beta3\"],\n",
    "                        eps=group[\"eps\"],\n",
    "                        weight_decay=group[\"wd\"],\n",
    "                        step=group[\"step\"],\n",
    "                        decouple_lr=group[\"decouple_lr\"],\n",
    "                        max_lr=group[\"max_lr\"],\n",
    "                        kahan_sum=group[\"kahan_sum\"],\n",
    "                        foreach=group[\"foreach\"],\n",
    "                        gradient_release=False,\n",
    "                        optimizer_accumulation=False,\n",
    "                    )\n",
    "\n",
    "                # Call adan for parameters without weight decay\n",
    "                if len(params_no_wd) > 0:\n",
    "                    adan(\n",
    "                        params=params_no_wd,\n",
    "                        grads=grads_no_wd,\n",
    "                        exp_avgs=exp_avgs_no_wd,\n",
    "                        exp_avg_diffs=exp_avg_diffs_no_wd,\n",
    "                        exp_avg_sqs=exp_avg_sqs_no_wd,\n",
    "                        prev_grads=prev_grads_no_wd,\n",
    "                        kahan_comps=kahan_comps_no_wd,\n",
    "                        lr=group[\"lr\"],\n",
    "                        beta1=group[\"beta1\"],\n",
    "                        beta2=group[\"beta2\"],\n",
    "                        beta3=group[\"beta3\"],\n",
    "                        eps=group[\"eps\"],\n",
    "                        weight_decay=0.0,\n",
    "                        step=group[\"step\"],\n",
    "                        decouple_lr=group[\"decouple_lr\"],\n",
    "                        max_lr=group[\"max_lr\"],\n",
    "                        kahan_sum=group[\"kahan_sum\"],\n",
    "                        foreach=group[\"foreach\"],\n",
    "                        gradient_release=False,\n",
    "                        optimizer_accumulation=False,\n",
    "                    )\n",
    "        else:\n",
    "            state = self.state[param]\n",
    "            group = state[\"group\"]\n",
    "            self._init_state(group, state, param)\n",
    "\n",
    "            adan(\n",
    "                params=[param],\n",
    "                grads=[param.grad],\n",
    "                exp_avgs=[state[\"exp_avg\"]],\n",
    "                exp_avg_diffs=[state[\"exp_avg_diff\"]],\n",
    "                exp_avg_sqs=[state[\"exp_avg_sq\"]],\n",
    "                prev_grads=[state[\"prev_grad\"]],\n",
    "                kahan_comps=[state[\"kahan_comp\"]],\n",
    "                lr=group[\"lr\"],\n",
    "                beta1=group[\"beta1\"],\n",
    "                beta2=group[\"beta2\"],\n",
    "                beta3=group[\"beta3\"],\n",
    "                eps=group[\"eps\"],\n",
    "                weight_decay=group[\"wd\"] if state.get(\"do_wd\", True) else 0,\n",
    "                step=state[\"step\"],\n",
    "                decouple_lr=group[\"decouple_lr\"],\n",
    "                max_lr=group[\"max_lr\"],\n",
    "                kahan_sum=group[\"kahan_sum\"],\n",
    "                foreach=False,\n",
    "                gradient_release=True,\n",
    "                optimizer_accumulation=self._optimizer_accumulation,\n",
    "            )\n",
    "\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(AdanOptimiOptimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lion -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class LionOptimiOptimizer(Lion, FastaiOptimizerAdapter):\n",
    "    \"\"\"Lion optimizer. Evolved Sign Momentum.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params\n",
    "        Iterable of parameters to optimize or dicts defining parameter groups\n",
    "    lr\n",
    "        Learning rate\n",
    "    betas\n",
    "        Coefficients for update moving average and gradient moving average (default: (0.9, 0.99))\n",
    "    weight_decay\n",
    "        Weight decay coefficient. If `decouple_lr` is False, applies decoupled weight decay (default: 0)\n",
    "    decouple_lr\n",
    "        Apply fully decoupled weight decay instead of decoupled weight decay (default: False)\n",
    "    max_lr\n",
    "        Maximum scheduled learning rate. Set if `lr` is not the maximum scheduled learning rate and `decouple_lr` is True (default: None)\n",
    "    kahan_sum\n",
    "        Enables Kahan summation for more accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters (default: None)\n",
    "    foreach\n",
    "        Enables the foreach implementation. If unspecified, tries to use foreach over for-loop implementation since it is significantly faster (default: None)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable[Tensor] | Iterable[dict],\n",
    "        lr: float,\n",
    "        beta1: float = 0.9,\n",
    "        beta2: float = 0.99,\n",
    "        wd: float = 0,\n",
    "        decouple_lr: bool = False,\n",
    "        max_lr: float | None = None,\n",
    "        kahan_sum: bool | None = None,\n",
    "        foreach: bool | None = None,\n",
    "    ):\n",
    "        params = super().convert_params(params)\n",
    "\n",
    "        if not 0.0 <= beta1 < 1.0:\n",
    "            raise ValueError(f\"Invalid beta1 parameter: {beta1=}\")\n",
    "        if not 0.0 <= beta2 < 1.0:\n",
    "            raise ValueError(f\"Invalid beta2 parameter: {beta2=}\")\n",
    "        if not 0.0 <= wd:\n",
    "            raise ValueError(f\"Invalid weight decay: {wd=}\")\n",
    "\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            beta1=beta1,\n",
    "            beta2=beta2,\n",
    "            wd=wd,\n",
    "            weight_decay=wd,\n",
    "            decouple_lr=decouple_lr,\n",
    "            max_lr=max_lr,\n",
    "            kahan_sum=kahan_sum,\n",
    "            foreach=foreach,\n",
    "            gradient_release=False,\n",
    "            setup=False,\n",
    "        )\n",
    "        super(Lion, self).__init__(params, defaults)\n",
    "\n",
    "    def _init_group(\n",
    "        self,\n",
    "        group: dict[str, Any],\n",
    "        params_wd: list[Tensor],\n",
    "        params_no_wd: list[Tensor],\n",
    "        grads_wd: list[Tensor],\n",
    "        grads_no_wd: list[Tensor],\n",
    "        exp_avgs_wd: list[Tensor],\n",
    "        exp_avgs_no_wd: list[Tensor],\n",
    "        kahan_comps_wd: list[Tensor],\n",
    "        kahan_comps_no_wd: list[Tensor],\n",
    "    ):\n",
    "        for p in group[\"params\"]:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "\n",
    "            state = self.state[p]\n",
    "            self._init_state(group, state, p)\n",
    "\n",
    "            if state.get(\"do_wd\", True):\n",
    "                params_wd.append(p)\n",
    "                grads_wd.append(p.grad)\n",
    "                exp_avgs_wd.append(state[\"exp_avg\"])\n",
    "                kahan_comps_wd.append(state[\"kahan_comp\"])\n",
    "            else:\n",
    "                params_no_wd.append(p)\n",
    "                grads_no_wd.append(p.grad)\n",
    "                exp_avgs_no_wd.append(state[\"exp_avg\"])\n",
    "                kahan_comps_no_wd.append(state[\"kahan_comp\"])\n",
    "\n",
    "        if not group[\"setup\"]:\n",
    "            group[\"setup\"] = True\n",
    "\n",
    "            if group[\"foreach\"] is None:\n",
    "                _, group[\"foreach\"] = _default_to_fused_or_foreach(params_wd + params_no_wd, False, False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure: Callable | None = None, param: Tensor | None = None):\n",
    "        loss = None\n",
    "        if closure is not None and param is None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        if param is None:\n",
    "            for group in self.param_groups:\n",
    "                params_wd, params_no_wd, grads_wd, grads_no_wd = [], [], [], []\n",
    "                exp_avgs_wd, exp_avgs_no_wd, kahan_comps_wd, kahan_comps_no_wd = [], [], [], []\n",
    "\n",
    "                self._init_group(\n",
    "                    group,\n",
    "                    params_wd,\n",
    "                    params_no_wd,\n",
    "                    grads_wd,\n",
    "                    grads_no_wd,\n",
    "                    exp_avgs_wd,\n",
    "                    exp_avgs_no_wd,\n",
    "                    kahan_comps_wd,\n",
    "                    kahan_comps_no_wd\n",
    "                )\n",
    "\n",
    "                # Call lion for parameters with weight decay\n",
    "                if len(params_wd) > 0:\n",
    "                    lion(\n",
    "                        params=params_wd,\n",
    "                        grads=grads_wd,\n",
    "                        exp_avgs=exp_avgs_wd,\n",
    "                        kahan_comps=kahan_comps_wd,\n",
    "                        lr=group[\"lr\"],\n",
    "                        beta1=group[\"beta1\"],\n",
    "                        beta2=group[\"beta2\"],\n",
    "                        weight_decay=group[\"wd\"],\n",
    "                        decouple_lr=group[\"decouple_lr\"],\n",
    "                        max_lr=group[\"max_lr\"],\n",
    "                        kahan_sum=group[\"kahan_sum\"],\n",
    "                        foreach=group[\"foreach\"],\n",
    "                        gradient_release=False,\n",
    "                        optimizer_accumulation=False,\n",
    "                    )\n",
    "\n",
    "                # Call lion for parameters without weight decay\n",
    "                if len(params_no_wd) > 0:\n",
    "                    lion(\n",
    "                        params=params_no_wd,\n",
    "                        grads=grads_no_wd,\n",
    "                        exp_avgs=exp_avgs_no_wd,\n",
    "                        kahan_comps=kahan_comps_no_wd,\n",
    "                        lr=group[\"lr\"],\n",
    "                        beta1=group[\"beta1\"],\n",
    "                        beta2=group[\"beta2\"],\n",
    "                        weight_decay=0,\n",
    "                        decouple_lr=group[\"decouple_lr\"],\n",
    "                        max_lr=group[\"max_lr\"],\n",
    "                        kahan_sum=group[\"kahan_sum\"],\n",
    "                        foreach=group[\"foreach\"],\n",
    "                        gradient_release=False,\n",
    "                        optimizer_accumulation=False,\n",
    "                    )\n",
    "        else:\n",
    "            state = self.state[param]\n",
    "            group = state[\"group\"]\n",
    "            self._init_state(group, state, param)\n",
    "\n",
    "            lion(\n",
    "                params=param,\n",
    "                grads=param.grad,\n",
    "                exp_avgs=state[\"exp_avg\"],\n",
    "                kahan_comps=state[\"kahan_comp\"],\n",
    "                lr=group[\"lr\"],\n",
    "                beta1=group[\"beta1\"],\n",
    "                beta2=group[\"beta2\"],\n",
    "                weight_decay=group[\"wd\"] if state.get(\"do_wd\", True) else 0,\n",
    "                decouple_lr=group[\"decouple_lr\"],\n",
    "                max_lr=group[\"max_lr\"],\n",
    "                kahan_sum=group[\"kahan_sum\"],\n",
    "                foreach=False,\n",
    "                gradient_release=True,\n",
    "                optimizer_accumulation=self._optimizer_accumulation,\n",
    "            )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(LionOptimiOptimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAdam -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class RAdamOptimiOptimizer(RAdam, FastaiOptimizerAdapter):\n",
    "    \"\"\"Rectified Adam optimizer. Optionally with decoupled weight decay.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params\n",
    "        Iterable of parameters to optimize or dicts defining parameter groups\n",
    "    lr\n",
    "        Learning rate\n",
    "    betas\n",
    "        Coefficients for gradient and squared gradient moving averages (default: (0.9, 0.99))\n",
    "    weight_decay\n",
    "        Weight decay coefficient. If `decouple_wd` and `decouple_lr` are False, applies L2 penalty (default: 0)\n",
    "    eps\n",
    "        Added to denominator to improve numerical stability (default: 1e-6)\n",
    "    decouple_wd\n",
    "        Apply decoupled weight decay instead of L2 penalty (default: True)\n",
    "    decouple_lr\n",
    "        Apply fully decoupled weight decay instead of L2 penalty (default: False)\n",
    "    max_lr\n",
    "        Maximum scheduled learning rate. Set if `lr` is not the maximum scheduled learning rate and `decouple_lr` is True (default: None)\n",
    "    kahan_sum\n",
    "        Enables Kahan summation for more accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters (default: None)\n",
    "    foreach\n",
    "        Enables the foreach implementation. If unspecified, tries to use foreach over for-loop implementation since it is significantly faster (default: None)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable[Tensor] | Iterable[dict],\n",
    "        lr: float,\n",
    "        mom: float = 0.9,\n",
    "        sqr_mom: float = 0.99,\n",
    "        wd: float = 0,\n",
    "        eps: float = 1e-6,\n",
    "        decouple_wd: bool = False,\n",
    "        decouple_lr: bool = False,\n",
    "        max_lr: float | None = None,\n",
    "        kahan_sum: bool | None = None,\n",
    "        foreach: bool | None = None,\n",
    "    ):\n",
    "        params = super().convert_params(params)\n",
    "\n",
    "        if not 0.0 <= mom < 1.0:\n",
    "            raise ValueError(f\"Invalid momentum parameter: {mom=}\")\n",
    "        if not 0.0 <= sqr_mom< 1.0:\n",
    "            raise ValueError(f\"Invalid squared momentum parameter: {sqr_mom=}\")\n",
    "        if not 0.0 <= wd:\n",
    "            raise ValueError(f\"Invalid weight decay: {wd=}\")\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(f\"Invalid epsilon: {eps=}\")\n",
    "\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            mom=mom,\n",
    "            sqr_mom=sqr_mom,\n",
    "            eps=eps,\n",
    "            wd=wd,\n",
    "            weight_decay=wd,\n",
    "            decouple_wd=decouple_wd,\n",
    "            decouple_lr=decouple_lr,\n",
    "            max_lr=max_lr,\n",
    "            kahan_sum=kahan_sum,\n",
    "            foreach=foreach,\n",
    "            gradient_release=False,\n",
    "            setup=False,\n",
    "        )\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def _init_group(\n",
    "        self,\n",
    "        group: dict[str, Any],\n",
    "        params_wd: list[Tensor],\n",
    "        params_no_wd: list[Tensor],\n",
    "        grads_wd: list[Tensor],\n",
    "        grads_no_wd: list[Tensor],\n",
    "        exp_avgs_wd: list[Tensor],\n",
    "        exp_avgs_no_wd: list[Tensor],\n",
    "        exp_avg_sqs_wd: list[Tensor],\n",
    "        exp_avg_sqs_no_wd: list[Tensor],\n",
    "        kahan_comps_wd: list[Tensor],\n",
    "        kahan_comps_no_wd: list[Tensor],\n",
    "    ):\n",
    "        for p in group[\"params\"]:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "\n",
    "            state = self.state[p]\n",
    "            self._init_state(group, state, p)\n",
    "\n",
    "            if state.get(\"do_wd\", True):\n",
    "                params_wd.append(p)\n",
    "                grads_wd.append(p.grad)\n",
    "                exp_avgs_wd.append(state[\"exp_avg\"])\n",
    "                exp_avg_sqs_wd.append(state[\"exp_avg_sq\"])\n",
    "                kahan_comps_wd.append(state[\"kahan_comp\"])\n",
    "            else:\n",
    "                params_no_wd.append(p)\n",
    "                grads_no_wd.append(p.grad)\n",
    "                exp_avgs_no_wd.append(state[\"exp_avg\"])\n",
    "                exp_avg_sqs_no_wd.append(state[\"exp_avg_sq\"])\n",
    "                kahan_comps_no_wd.append(state[\"kahan_comp\"])\n",
    "\n",
    "        if not group[\"setup\"]:\n",
    "            group[\"setup\"] = True\n",
    "            group[\"step\"] = torch.tensor(0, dtype=torch.int32)\n",
    "\n",
    "            if group[\"foreach\"] is None:\n",
    "                _, group[\"foreach\"] = _default_to_fused_or_foreach(params_wd + params_no_wd, False, False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure: Callable | None = None, param: Tensor | None = None):\n",
    "        loss = None\n",
    "        if closure is not None and param is None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        if param is None:\n",
    "            for group in self.param_groups:\n",
    "                params_wd, params_no_wd, grads_wd, grads_no_wd = [], [], [], []\n",
    "                exp_avgs_wd, exp_avgs_no_wd, exp_avg_sqs_wd, exp_avg_sqs_no_wd = [], [], [], []\n",
    "                kahan_comps_wd, kahan_comps_no_wd = [], []\n",
    "\n",
    "                self._init_group(\n",
    "                    group,\n",
    "                    params_wd,\n",
    "                    params_no_wd,\n",
    "                    grads_wd,\n",
    "                    grads_no_wd,\n",
    "                    exp_avgs_wd,\n",
    "                    exp_avgs_no_wd,\n",
    "                    exp_avg_sqs_wd,\n",
    "                    exp_avg_sqs_no_wd,\n",
    "                    kahan_comps_wd,\n",
    "                    kahan_comps_no_wd\n",
    "                )\n",
    "\n",
    "                # Call radam for parameters with weight decay\n",
    "                if len(params_wd) > 0:\n",
    "                    radam(\n",
    "                        params=params_wd,\n",
    "                        grads=grads_wd,\n",
    "                        exp_avgs=exp_avgs_wd,\n",
    "                        exp_avg_sqs=exp_avg_sqs_wd,\n",
    "                        kahan_comps=kahan_comps_wd,\n",
    "                        lr=group[\"lr\"],\n",
    "                        beta1=group[\"mom\"],\n",
    "                        beta2=group[\"sqr_mom\"],\n",
    "                        weight_decay=group[\"wd\"],\n",
    "                        eps=group[\"eps\"],\n",
    "                        step=group[\"step\"],\n",
    "                        decouple_wd=group[\"decouple_wd\"],\n",
    "                        decouple_lr=group[\"decouple_lr\"],\n",
    "                        max_lr=group[\"max_lr\"],\n",
    "                        kahan_sum=group[\"kahan_sum\"],\n",
    "                        foreach=group[\"foreach\"],\n",
    "                        gradient_release=False,\n",
    "                        optimizer_accumulation=False,\n",
    "                    )\n",
    "\n",
    "                # Call radam for parameters without weight decay\n",
    "                if len(params_no_wd) > 0:\n",
    "                    radam(\n",
    "                        params=params_no_wd,\n",
    "                        grads=grads_no_wd,\n",
    "                        exp_avgs=exp_avgs_no_wd,\n",
    "                        exp_avg_sqs=exp_avg_sqs_no_wd,\n",
    "                        kahan_comps=kahan_comps_no_wd,\n",
    "                        lr=group[\"lr\"],\n",
    "                        beta1=group[\"mom\"],\n",
    "                        beta2=group[\"sqr_mom\"],\n",
    "                        weight_decay=0,\n",
    "                        eps=group[\"eps\"],\n",
    "                        step=group[\"step\"],\n",
    "                        decouple_wd=group[\"decouple_wd\"],\n",
    "                        decouple_lr=group[\"decouple_lr\"],\n",
    "                        max_lr=group[\"max_lr\"],\n",
    "                        kahan_sum=group[\"kahan_sum\"],\n",
    "                        foreach=group[\"foreach\"],\n",
    "                        gradient_release=False,\n",
    "                        optimizer_accumulation=False,\n",
    "                    )\n",
    "        else:\n",
    "            state = self.state[param]\n",
    "            group = state[\"group\"]\n",
    "            self._init_state(group, state, param)\n",
    "\n",
    "            radam(\n",
    "                params=param,\n",
    "                grads=param.grad,\n",
    "                exp_avgs=state[\"exp_avg\"],\n",
    "                exp_avg_sqs=state[\"exp_avg_sq\"],\n",
    "                kahan_comps=state[\"kahan_comp\"],\n",
    "                lr=group[\"lr\"],\n",
    "                beta1=group[\"mom\"],\n",
    "                beta2=group[\"sqr_mom\"],\n",
    "                weight_decay=group[\"wd\"] if state.get(\"do_wd\", True) else 0,\n",
    "                eps=group[\"eps\"],\n",
    "                step=state[\"step\"],\n",
    "                decouple_wd=group[\"decouple_wd\"],\n",
    "                decouple_lr=group[\"decouple_lr\"],\n",
    "                max_lr=group[\"max_lr\"],\n",
    "                kahan_sum=group[\"kahan_sum\"],\n",
    "                foreach=False,\n",
    "                gradient_release=True,\n",
    "                optimizer_accumulation=self._optimizer_accumulation,\n",
    "            )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(RAdamOptimiOptimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranger -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class RangerOptimiOptimizer(Ranger, FastaiOptimizerAdapter):\n",
    "    \"\"\"Ranger optimizer. RAdam with Lookahead.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params\n",
    "        Iterable of parameters to optimize or dicts defining parameter groups\n",
    "    lr\n",
    "        Learning rate\n",
    "    betas\n",
    "        Coefficients for gradient and squared gradient moving averages (default: (0.9, 0.99))\n",
    "    weight_decay\n",
    "        Weight decay coefficient. If `decouple_wd` and `decouple_lr` are False, applies L2 penalty (default: 0)\n",
    "    eps\n",
    "        Added to denominator to improve numerical stability (default: 1e-6)\n",
    "    k\n",
    "        Lookahead synchronization period (default: 6)\n",
    "    alpha\n",
    "        Lookahead weight interpolation coefficient (default: 0.5)\n",
    "    decouple_wd\n",
    "        Apply decoupled weight decay instead of L2 penalty (default: True)\n",
    "    decouple_lr\n",
    "        Apply fully decoupled weight decay instead of L2 penalty (default: False)\n",
    "    max_lr\n",
    "        Maximum scheduled learning rate. Set if `lr` is not the maximum scheduled learning rate and `decouple_lr` is True (default: None)\n",
    "    kahan_sum\n",
    "        Enables Kahan summation for more accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters (default: None)\n",
    "    foreach\n",
    "        Enables the foreach implementation. If unspecified, tries to use foreach over for-loop implementation since it is significantly faster (default: None)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable[Tensor] | Iterable[dict],\n",
    "        lr: float,\n",
    "        mom: float = 0.9,\n",
    "        sqr_mom: float = 0.99,\n",
    "        wd: float = 0,\n",
    "        eps: float = 1e-6,\n",
    "        k: int = 6,\n",
    "        alpha: float = 0.5,\n",
    "        decouple_wd: bool = True,\n",
    "        decouple_lr: bool = False,\n",
    "        max_lr: float | None = None,\n",
    "        kahan_sum: bool | None = None,\n",
    "        foreach: bool | None = None,\n",
    "    ):\n",
    "        params = super().convert_params(params)\n",
    "\n",
    "        if not 0.0 <= mom < 1.0:\n",
    "            raise ValueError(f\"Invalid momentum parameter: {mom=}\")\n",
    "        if not 0.0 <= sqr_mom< 1.0:\n",
    "            raise ValueError(f\"Invalid squared momentum parameter: {sqr_mom=}\")\n",
    "        if not 0.0 <= wd:\n",
    "            raise ValueError(f\"Invalid weight decay: {wd=}\")\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(f\"Invalid epsilon: {eps=}\")\n",
    "\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            mom=mom,\n",
    "            sqr_mom=sqr_mom,\n",
    "            wd=wd,\n",
    "            eps=eps,\n",
    "            k=k,\n",
    "            alpha=alpha,\n",
    "            weight_decay=wd,\n",
    "            decouple_wd=decouple_wd,\n",
    "            decouple_lr=decouple_lr,\n",
    "            max_lr=max_lr,\n",
    "            kahan_sum=kahan_sum,\n",
    "            foreach=foreach,\n",
    "            gradient_release=False,\n",
    "            setup=False,\n",
    "        )\n",
    "        super(Ranger, self).__init__(params, defaults)\n",
    "\n",
    "    def _init_group(\n",
    "        self,\n",
    "        group: dict[str, Any],\n",
    "        params_wd: list[Tensor],\n",
    "        params_no_wd: list[Tensor],\n",
    "        grads_wd: list[Tensor],\n",
    "        grads_no_wd: list[Tensor],\n",
    "        exp_avgs_wd: list[Tensor],\n",
    "        exp_avgs_no_wd: list[Tensor],\n",
    "        exp_avg_sqs_wd: list[Tensor],\n",
    "        exp_avg_sqs_no_wd: list[Tensor],\n",
    "        la_params_wd: list[Tensor],\n",
    "        la_params_no_wd: list[Tensor],\n",
    "        kahan_comps_wd: list[Tensor],\n",
    "        kahan_comps_no_wd: list[Tensor],\n",
    "    ):\n",
    "        for p in group[\"params\"]:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "\n",
    "            state = self.state[p]\n",
    "            self._init_state(group, state, p)\n",
    "\n",
    "            if state.get(\"do_wd\", True):\n",
    "                params_wd.append(p)\n",
    "                grads_wd.append(p.grad)\n",
    "                exp_avgs_wd.append(state[\"exp_avg\"])\n",
    "                exp_avg_sqs_wd.append(state[\"exp_avg_sq\"])\n",
    "                la_params_wd.append(state[\"la_param\"])\n",
    "                kahan_comps_wd.append(state[\"kahan_comp\"])\n",
    "            else:\n",
    "                params_no_wd.append(p)\n",
    "                grads_no_wd.append(p.grad)\n",
    "                exp_avgs_no_wd.append(state[\"exp_avg\"])\n",
    "                exp_avg_sqs_no_wd.append(state[\"exp_avg_sq\"])\n",
    "                la_params_no_wd.append(state[\"la_param\"])\n",
    "                kahan_comps_no_wd.append(state[\"kahan_comp\"])\n",
    "\n",
    "        if not group[\"setup\"]:\n",
    "            group[\"setup\"] = True\n",
    "            group[\"step\"] = torch.tensor(0, dtype=torch.int32)\n",
    "\n",
    "            if group[\"foreach\"] is None:\n",
    "                _, group[\"foreach\"] = _default_to_fused_or_foreach(params_wd + params_no_wd, False, False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure: Callable | None = None, param: Tensor | None = None):\n",
    "        loss = None\n",
    "        if closure is not None and param is None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        if param is None:\n",
    "            for group in self.param_groups:\n",
    "                params_wd, params_no_wd, grads_wd, grads_no_wd = [], [], [], []\n",
    "                exp_avgs_wd, exp_avgs_no_wd, exp_avg_sqs_wd, exp_avg_sqs_no_wd = [], [], [], []\n",
    "                la_params_wd, la_params_no_wd, kahan_comps_wd, kahan_comps_no_wd = [], [], [], []\n",
    "\n",
    "                self._init_group(\n",
    "                    group,\n",
    "                    params_wd,\n",
    "                    params_no_wd,\n",
    "                    grads_wd,\n",
    "                    grads_no_wd,\n",
    "                    exp_avgs_wd,\n",
    "                    exp_avgs_no_wd,\n",
    "                    exp_avg_sqs_wd,\n",
    "                    exp_avg_sqs_no_wd,\n",
    "                    la_params_wd,\n",
    "                    la_params_no_wd,\n",
    "                    kahan_comps_wd,\n",
    "                    kahan_comps_no_wd\n",
    "                )\n",
    "\n",
    "                # Call ranger for parameters with weight decay\n",
    "                if len(params_wd) > 0:\n",
    "                    ranger(\n",
    "                        params=params_wd,\n",
    "                        grads=grads_wd,\n",
    "                        exp_avgs=exp_avgs_wd,\n",
    "                        exp_avg_sqs=exp_avg_sqs_wd,\n",
    "                        la_params=la_params_wd,\n",
    "                        kahan_comps=kahan_comps_wd,\n",
    "                        lr=group[\"lr\"],\n",
    "                        beta1=group[\"mom\"],\n",
    "                        beta2=group[\"sqr_mom\"],\n",
    "                        weight_decay=group[\"wd\"],\n",
    "                        eps=group[\"eps\"],\n",
    "                        k=group[\"k\"],\n",
    "                        alpha=group[\"alpha\"],\n",
    "                        step=group[\"step\"],\n",
    "                        decouple_wd=group[\"decouple_wd\"],\n",
    "                        decouple_lr=group[\"decouple_lr\"],\n",
    "                        max_lr=group[\"max_lr\"],\n",
    "                        kahan_sum=group[\"kahan_sum\"],\n",
    "                        foreach=group[\"foreach\"],\n",
    "                        gradient_release=False,\n",
    "                        optimizer_accumulation=False,\n",
    "                    )\n",
    "\n",
    "                # Call ranger for parameters without weight decay\n",
    "                if len(params_no_wd) > 0:\n",
    "                    ranger(\n",
    "                        params=params_no_wd,\n",
    "                        grads=grads_no_wd,\n",
    "                        exp_avgs=exp_avgs_no_wd,\n",
    "                        exp_avg_sqs=exp_avg_sqs_no_wd,\n",
    "                        la_params=la_params_no_wd,\n",
    "                        kahan_comps=kahan_comps_no_wd,\n",
    "                        lr=group[\"lr\"],\n",
    "                        beta1=group[\"mom\"],\n",
    "                        beta2=group[\"sqr_mom\"],\n",
    "                        weight_decay=0,\n",
    "                        eps=group[\"eps\"],\n",
    "                        k=group[\"k\"],\n",
    "                        alpha=group[\"alpha\"],\n",
    "                        step=group[\"step\"],\n",
    "                        decouple_wd=group[\"decouple_wd\"],\n",
    "                        decouple_lr=group[\"decouple_lr\"],\n",
    "                        max_lr=group[\"max_lr\"],\n",
    "                        kahan_sum=group[\"kahan_sum\"],\n",
    "                        foreach=group[\"foreach\"],\n",
    "                        gradient_release=False,\n",
    "                        optimizer_accumulation=False,\n",
    "                    )\n",
    "        else:\n",
    "            state = self.state[param]\n",
    "            group = state[\"group\"]\n",
    "            self._init_state(group, state, param)\n",
    "\n",
    "            ranger(\n",
    "                params=param,\n",
    "                grads=param.grad,\n",
    "                exp_avgs=state[\"exp_avg\"],\n",
    "                exp_avg_sqs=state[\"exp_avg_sq\"],\n",
    "                la_params=state[\"la_param\"],\n",
    "                kahan_comps=state[\"kahan_comp\"],\n",
    "                lr=group[\"lr\"],\n",
    "                beta1=group[\"mom\"],\n",
    "                beta2=group[\"sqr_mom\"],\n",
    "                weight_decay=group[\"wd\"] if state.get(\"do_wd\", True) else 0,\n",
    "                eps=group[\"eps\"],\n",
    "                k=group[\"k\"],\n",
    "                alpha=group[\"alpha\"],\n",
    "                step=state[\"step\"],\n",
    "                decouple_wd=group[\"decouple_wd\"],\n",
    "                decouple_lr=group[\"decouple_lr\"],\n",
    "                max_lr=group[\"max_lr\"],\n",
    "                kahan_sum=group[\"kahan_sum\"],\n",
    "                foreach=False,\n",
    "                gradient_release=True,\n",
    "                optimizer_accumulation=self._optimizer_accumulation,\n",
    "            )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(RangerOptimiOptimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class SGDOptimiOptimizer(SGD, FastaiOptimizerAdapter):\n",
    "    \"\"\"SGD optimizer. Optionally with momentum and decoupled weight decay.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params\n",
    "        Iterable of parameters to optimize or dicts defining parameter groups\n",
    "    lr\n",
    "        Learning rate\n",
    "    momentum\n",
    "        Momentum factor. Gradient moving average coefficient if `dampening` is True (default: 0)\n",
    "    weight_decay\n",
    "        Weight decay coefficient. If `decouple_wd` and `decouple_lr` are False, applies L2 penalty (default: 0)\n",
    "    dampening\n",
    "        Use dampening for momentum update (default: False)\n",
    "    decouple_wd\n",
    "        Apply decoupled weight decay instead of L2 penalty (default: False)\n",
    "    decouple_lr\n",
    "        Apply fully decoupled weight decay instead of L2 penalty (default: False)\n",
    "    max_lr\n",
    "        Maximum scheduled learning rate. Set if `lr` is not the maximum scheduled learning rate and `decouple_lr` is True (default: None)\n",
    "    torch_init\n",
    "        Initialize momentum buffer with first gradient instead of zeroes. Enable to match PyTorch SGD when using dampening (default: False)\n",
    "    kahan_sum\n",
    "        Enables Kahan summation for more accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters (default: None)\n",
    "    foreach\n",
    "        Enables the foreach implementation. If unspecified, tries to use foreach over for-loop implementation since it is significantly faster (default: None)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable[Tensor] | Iterable[dict],\n",
    "        lr: float,\n",
    "        mom: float = 0,\n",
    "        wd: float = 0,\n",
    "        dampening: bool = False,\n",
    "        decouple_wd: bool = False,\n",
    "        decouple_lr: bool = False,\n",
    "        max_lr: float | None = None,\n",
    "        torch_init: bool = False,\n",
    "        kahan_sum: bool | None = None,\n",
    "        foreach: bool | None = None,\n",
    "    ):\n",
    "        params = super().convert_params(params)\n",
    "\n",
    "        if not 0.0 <= wd:\n",
    "            raise ValueError(f\"Invalid weight decay: {wd=}\")\n",
    "\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            mom=mom,\n",
    "            wd=wd,\n",
    "            dampening=dampening,\n",
    "            weight_decay=wd,\n",
    "            decouple_wd=decouple_wd,\n",
    "            decouple_lr=decouple_lr,\n",
    "            max_lr=max_lr,\n",
    "            torch_init=torch_init,\n",
    "            kahan_sum=kahan_sum,\n",
    "            foreach=foreach,\n",
    "            gradient_release=False,\n",
    "            setup=False,\n",
    "        )\n",
    "        super(SGD, self).__init__(params, defaults)\n",
    "\n",
    "    def _init_group(\n",
    "        self,\n",
    "        group: dict[str, Any],\n",
    "        params_wd: list[Tensor],\n",
    "        params_no_wd: list[Tensor],\n",
    "        grads_wd: list[Tensor],\n",
    "        grads_no_wd: list[Tensor],\n",
    "        exp_avgs_wd: list[Tensor],\n",
    "        exp_avgs_no_wd: list[Tensor],\n",
    "        kahan_comps_wd: list[Tensor],\n",
    "        kahan_comps_no_wd: list[Tensor],\n",
    "    ):\n",
    "        for p in group[\"params\"]:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "\n",
    "            state = self.state[p]\n",
    "            self._init_state(group, state, p)\n",
    "\n",
    "            if state.get(\"do_wd\", True):\n",
    "                params_wd.append(p)\n",
    "                grads_wd.append(p.grad)\n",
    "                exp_avgs_wd.append(state[\"exp_avg\"])\n",
    "                kahan_comps_wd.append(state[\"kahan_comp\"])\n",
    "            else:\n",
    "                params_no_wd.append(p)\n",
    "                grads_no_wd.append(p.grad)\n",
    "                exp_avgs_no_wd.append(state[\"exp_avg\"])\n",
    "                kahan_comps_no_wd.append(state[\"kahan_comp\"])\n",
    "\n",
    "        if not group[\"setup\"]:\n",
    "            group[\"setup\"] = True\n",
    "\n",
    "            if group[\"foreach\"] is None:\n",
    "                _, group[\"foreach\"] = _default_to_fused_or_foreach(params_wd + params_no_wd, False, False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure: Callable | None = None, param: Tensor | None = None):\n",
    "        loss = None\n",
    "        if closure is not None and param is None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        if param is None:\n",
    "            for group in self.param_groups:\n",
    "                params_wd, params_no_wd, grads_wd, grads_no_wd = [], [], [], []\n",
    "                exp_avgs_wd, exp_avgs_no_wd, kahan_comps_wd, kahan_comps_no_wd = [], [], [], []\n",
    "\n",
    "                self._init_group(\n",
    "                    group,\n",
    "                    params_wd,\n",
    "                    params_no_wd,\n",
    "                    grads_wd,\n",
    "                    grads_no_wd,\n",
    "                    exp_avgs_wd,\n",
    "                    exp_avgs_no_wd,\n",
    "                    kahan_comps_wd,\n",
    "                    kahan_comps_no_wd\n",
    "                )\n",
    "\n",
    "                # Call sgd for parameters with weight decay\n",
    "                if len(params_wd) > 0:\n",
    "                    sgd(\n",
    "                        params=params_wd,\n",
    "                        grads=grads_wd,\n",
    "                        exp_avgs=exp_avgs_wd,\n",
    "                        kahan_comps=kahan_comps_wd,\n",
    "                        lr=group[\"lr\"],\n",
    "                        momentum=group[\"mom\"],\n",
    "                        weight_decay=group[\"wd\"],\n",
    "                        dampening=group[\"dampening\"],\n",
    "                        decouple_wd=group[\"decouple_wd\"],\n",
    "                        decouple_lr=group[\"decouple_lr\"],\n",
    "                        max_lr=group[\"max_lr\"],\n",
    "                        kahan_sum=group[\"kahan_sum\"],\n",
    "                        foreach=group[\"foreach\"],\n",
    "                        gradient_release=False,\n",
    "                        optimizer_accumulation=False,\n",
    "                    )\n",
    "\n",
    "                # Call sgd for parameters without weight decay\n",
    "                if len(params_no_wd) > 0:\n",
    "                    sgd(\n",
    "                        params=params_no_wd,\n",
    "                        grads=grads_no_wd,\n",
    "                        exp_avgs=exp_avgs_no_wd,\n",
    "                        kahan_comps=kahan_comps_no_wd,\n",
    "                        lr=group[\"lr\"],\n",
    "                        momentum=group[\"mom\"],\n",
    "                        weight_decay=0,\n",
    "                        dampening=group[\"dampening\"],\n",
    "                        decouple_wd=group[\"decouple_wd\"],\n",
    "                        decouple_lr=group[\"decouple_lr\"],\n",
    "                        max_lr=group[\"max_lr\"],\n",
    "                        kahan_sum=group[\"kahan_sum\"],\n",
    "                        foreach=group[\"foreach\"],\n",
    "                        gradient_release=False,\n",
    "                        optimizer_accumulation=False,\n",
    "                    )\n",
    "        else:\n",
    "            state = self.state[param]\n",
    "            group = state[\"group\"]\n",
    "            self._init_state(group, state, param)\n",
    "\n",
    "            sgd(\n",
    "                params=param,\n",
    "                grads=param.grad,\n",
    "                exp_avgs=state[\"exp_avg\"],\n",
    "                kahan_comps=state[\"kahan_comp\"],\n",
    "                lr=group[\"lr\"],\n",
    "                momentum=group[\"mom\"],\n",
    "                weight_decay=group[\"wd\"] if state.get(\"do_wd\", True) else 0,\n",
    "                dampening=group[\"dampening\"],\n",
    "                decouple_wd=group[\"decouple_wd\"],\n",
    "                decouple_lr=group[\"decouple_lr\"],\n",
    "                max_lr=group[\"max_lr\"],\n",
    "                kahan_sum=group[\"kahan_sum\"],\n",
    "                foreach=False,\n",
    "                gradient_release=True,\n",
    "                optimizer_accumulation=self._optimizer_accumulation,\n",
    "            )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(SGDOptimiOptimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StableAdamW -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class StableAdamWOptimiOptimizer(StableAdamW, FastaiOptimizerAdapter):\n",
    "    \"\"\"StableAdamW optimizer. An AdamW-Adafactor hybrid with learning rate update clipping.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params\n",
    "        Iterable of parameters to optimize or dicts defining parameter groups\n",
    "    lr\n",
    "        Learning rate\n",
    "    betas\n",
    "        Coefficients for gradient and squared gradient moving averages (default: (0.9, 0.99))\n",
    "    weight_decay\n",
    "        Weight decay coefficient. If `decouple_lr` is False, applies decoupled weight decay (default: 1e-2)\n",
    "    eps\n",
    "        Added to denominator to improve numerical stability (default: 1e-6)\n",
    "    decouple_lr\n",
    "        Apply fully decoupled weight decay instead of decoupled weight decay (default: False)\n",
    "    max_lr\n",
    "        Maximum scheduled learning rate. Set if `lr` is not the maximum scheduled learning rate and `decouple_lr` is True (default: None)\n",
    "    kahan_sum\n",
    "        Enables Kahan summation for more accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters (default: None)\n",
    "    foreach\n",
    "        Enables the foreach implementation. If unspecified, tries to use foreach over for-loop implementation since it is significantly faster (default: None)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable[Tensor] | Iterable[dict],\n",
    "        lr: float,\n",
    "        mom: float = 0.9,\n",
    "        sqr_mom: float = 0.99,\n",
    "        wd: float = 1e-2,\n",
    "        eps: float = 1e-6,\n",
    "        decouple_lr: bool = False,\n",
    "        max_lr: float | None = None,\n",
    "        kahan_sum: bool | None = None,\n",
    "        foreach: bool | None = None,\n",
    "    ):\n",
    "        params = super().convert_params(params)\n",
    "\n",
    "        if not 0.0 <= mom < 1.0:\n",
    "            raise ValueError(f\"Invalid momentum parameter: {mom=}\")\n",
    "        if not 0.0 <= sqr_mom< 1.0:\n",
    "            raise ValueError(f\"Invalid squared momentum parameter: {sqr_mom=}\")\n",
    "        if not 0.0 <= wd:\n",
    "            raise ValueError(f\"Invalid weight decay: {wd=}\")\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(f\"Invalid epsilon: {eps=}\")\n",
    "\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            mom=mom,\n",
    "            sqr_mom=sqr_mom,\n",
    "            wd=wd,\n",
    "            eps=eps,\n",
    "            weight_decay=wd,\n",
    "            decouple_lr=decouple_lr,\n",
    "            max_lr=max_lr,\n",
    "            kahan_sum=kahan_sum,\n",
    "            foreach=foreach,\n",
    "            gradient_release=False,\n",
    "            setup=False,\n",
    "        )\n",
    "        super(StableAdamW, self).__init__(params, defaults)\n",
    "\n",
    "    def _init_group(\n",
    "        self,\n",
    "        group: dict[str, Any],\n",
    "        params_wd: list[Tensor],\n",
    "        params_no_wd: list[Tensor],\n",
    "        grads_wd: list[Tensor],\n",
    "        grads_no_wd: list[Tensor],\n",
    "        exp_avgs_wd: list[Tensor],\n",
    "        exp_avgs_no_wd: list[Tensor],\n",
    "        exp_avg_sqs_wd: list[Tensor],\n",
    "        exp_avg_sqs_no_wd: list[Tensor],\n",
    "        eps_sqs_wd: list[Tensor],\n",
    "        eps_sqs_no_wd: list[Tensor],\n",
    "        kahan_comps_wd: list[Tensor],\n",
    "        kahan_comps_no_wd: list[Tensor],\n",
    "    ):\n",
    "        for p in group[\"params\"]:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "\n",
    "            state = self.state[p]\n",
    "            self._init_state(group, state, p)\n",
    "\n",
    "            if state.get(\"do_wd\", True):\n",
    "                params_wd.append(p)\n",
    "                grads_wd.append(p.grad)\n",
    "                exp_avgs_wd.append(state[\"exp_avg\"])\n",
    "                exp_avg_sqs_wd.append(state[\"exp_avg_sq\"])\n",
    "                eps_sqs_wd.append(state[\"eps_sq\"])\n",
    "                kahan_comps_wd.append(state[\"kahan_comp\"])\n",
    "            else:\n",
    "                params_no_wd.append(p)\n",
    "                grads_no_wd.append(p.grad)\n",
    "                exp_avgs_no_wd.append(state[\"exp_avg\"])\n",
    "                exp_avg_sqs_no_wd.append(state[\"exp_avg_sq\"])\n",
    "                eps_sqs_no_wd.append(state[\"eps_sq\"])\n",
    "                kahan_comps_no_wd.append(state[\"kahan_comp\"])\n",
    "\n",
    "        if not group[\"setup\"]:\n",
    "            group[\"setup\"] = True\n",
    "            group[\"step\"] = torch.tensor(0, dtype=torch.int32)\n",
    "\n",
    "            if group[\"foreach\"] is None:\n",
    "                _, group[\"foreach\"] = _default_to_fused_or_foreach(params_wd + params_no_wd, False, False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure: Callable | None = None, param: Tensor | None = None):\n",
    "        loss = None\n",
    "        if closure is not None and param is None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        if param is None:\n",
    "            for group in self.param_groups:\n",
    "                params_wd, params_no_wd, grads_wd, grads_no_wd = [], [], [], []\n",
    "                exp_avgs_wd, exp_avgs_no_wd, exp_avg_sqs_wd, exp_avg_sqs_no_wd = [], [], [], []\n",
    "                eps_sqs_wd, eps_sqs_no_wd, kahan_comps_wd, kahan_comps_no_wd = [], [], [], []\n",
    "\n",
    "                self._init_group(\n",
    "                    group,\n",
    "                    params_wd,\n",
    "                    params_no_wd,\n",
    "                    grads_wd,\n",
    "                    grads_no_wd,\n",
    "                    exp_avgs_wd,\n",
    "                    exp_avgs_no_wd,\n",
    "                    exp_avg_sqs_wd,\n",
    "                    exp_avg_sqs_no_wd,\n",
    "                    eps_sqs_wd,\n",
    "                    eps_sqs_no_wd,\n",
    "                    kahan_comps_wd,\n",
    "                    kahan_comps_no_wd\n",
    "                )\n",
    "\n",
    "                # Call stableadamw for parameters with weight decay\n",
    "                if len(params_wd) > 0:\n",
    "                    stableadamw(\n",
    "                        params=params_wd,\n",
    "                        grads=grads_wd,\n",
    "                        exp_avgs=exp_avgs_wd,\n",
    "                        exp_avg_sqs=exp_avg_sqs_wd,\n",
    "                        eps_sqs=eps_sqs_wd,\n",
    "                        kahan_comps=kahan_comps_wd,\n",
    "                        lr=group[\"lr\"],\n",
    "                        beta1=group[\"mom\"],\n",
    "                        beta2=group[\"sqr_mom\"],\n",
    "                        weight_decay=group[\"wd\"],\n",
    "                        eps=group[\"eps\"],\n",
    "                        step=group[\"step\"],\n",
    "                        decouple_lr=group[\"decouple_lr\"],\n",
    "                        max_lr=group[\"max_lr\"],\n",
    "                        kahan_sum=group[\"kahan_sum\"],\n",
    "                        foreach=group[\"foreach\"],\n",
    "                        gradient_release=False,\n",
    "                        optimizer_accumulation=False,\n",
    "                    )\n",
    "\n",
    "                # Call stableadamw for parameters without weight decay\n",
    "                if len(params_no_wd) > 0:\n",
    "                    stableadamw(\n",
    "                        params=params_no_wd,\n",
    "                        grads=grads_no_wd,\n",
    "                        exp_avgs=exp_avgs_no_wd,\n",
    "                        exp_avg_sqs=exp_avg_sqs_no_wd,\n",
    "                        eps_sqs=eps_sqs_no_wd,\n",
    "                        kahan_comps=kahan_comps_no_wd,\n",
    "                        lr=group[\"lr\"],\n",
    "                        beta1=group[\"mom\"],\n",
    "                        beta2=group[\"sqr_mom\"],\n",
    "                        weight_decay=0,\n",
    "                        eps=group[\"eps\"],\n",
    "                        step=group[\"step\"],\n",
    "                        decouple_lr=group[\"decouple_lr\"],\n",
    "                        max_lr=group[\"max_lr\"],\n",
    "                        kahan_sum=group[\"kahan_sum\"],\n",
    "                        foreach=group[\"foreach\"],\n",
    "                        gradient_release=False,\n",
    "                        optimizer_accumulation=False,\n",
    "                    )\n",
    "        else:\n",
    "            state = self.state[param]\n",
    "            group = state[\"group\"]\n",
    "            self._init_state(group, state, param)\n",
    "\n",
    "            stableadamw(\n",
    "                params=param,\n",
    "                grads=param.grad,\n",
    "                exp_avgs=state[\"exp_avg\"],\n",
    "                exp_avg_sqs=state[\"exp_avg_sq\"],\n",
    "                eps_sqs=state[\"eps_sq\"],\n",
    "                kahan_comps=state[\"kahan_comp\"],\n",
    "                lr=group[\"lr\"],\n",
    "                beta1=group[\"mom\"],\n",
    "                beta2=group[\"sqr_mom\"],\n",
    "                weight_decay=group[\"wd\"] if state.get(\"do_wd\", True) else 0,\n",
    "                eps=group[\"eps\"],\n",
    "                step=state[\"step\"],\n",
    "                decouple_lr=group[\"decouple_lr\"],\n",
    "                max_lr=group[\"max_lr\"],\n",
    "                kahan_sum=group[\"kahan_sum\"],\n",
    "                foreach=False,\n",
    "                gradient_release=True,\n",
    "                optimizer_accumulation=self._optimizer_accumulation,\n",
    "            )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(StableAdamWOptimiOptimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastai Compatiblity Tests -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastxtend.test_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params = [tst_params(), tst_params(), tst_params()]\n",
    "for opt in [SGDOptimiOptimizer(params, lr=0.1), AdamWOptimiOptimizer(params, lr=0.1)]:\n",
    "    #Freezing the first layer\n",
    "    opt.freeze_to(1)\n",
    "    req_grad = Self.requires_grad()\n",
    "    test_eq(L(params[0]).map(req_grad), [False]*4)\n",
    "    for i in {1,2}:\n",
    "        test_eq(L(params[i]).map(req_grad), [True]*4)\n",
    "\n",
    "    #Unfreezing\n",
    "    opt.unfreeze()\n",
    "    for i in range(2):\n",
    "        test_eq(L(params[i]).map(req_grad), [True]*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params = [tst_params(), tst_params(), tst_params()]\n",
    "for opt in [AdamOptimiOptimizer(params, lr=0.1), LionOptimiOptimizer(params, lr=0.1)]:\n",
    "    for p in L(params[1])[[1,3]]:\n",
    "        opt.state[p] = {'force_train': True}\n",
    "    opt.freeze()\n",
    "    test_eq(L(params[0]).map(req_grad), [False]*4)\n",
    "    test_eq(L(params[1]).map(req_grad), [False, True, False, True])\n",
    "    test_eq(L(params[2]).map(req_grad), [True]*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params = [tst_params(), tst_params(), tst_params()]\n",
    "for opt in [RAdamOptimiOptimizer(params, lr=1e-3), RangerOptimiOptimizer(params, lr=1e-3)]:\n",
    "    opt.set_hypers(lr=3e-3, mom=0.98, sqr_mom=0.9999, eps=1e-9, wd=0.01)\n",
    "    for opt_dict in opt.hypers:\n",
    "        assert opt_dict.items() >= dict(lr=3e-3, mom=0.98, sqr_mom=0.9999, eps=1e-9, wd=0.01).items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimi Optimizer Tests -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "from fastai.optimizer import (weight_decay, l2_reg, average_grad, momentum_step,\n",
    "                              average_sqr_grad,step_stat, adam_step, radam_step, Lookahead, Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "def assert_most_approx_close(a, b, rtol=1e-3, atol=1e-3, max_error_count=10):\n",
    "    idx = torch.isclose(a, b, rtol=rtol, atol=atol)\n",
    "    error_count = (idx == 0).sum().item()\n",
    "    if error_count > max_error_count:\n",
    "        print(f\"Too many values not close: assert {error_count} < {max_error_count}\")\n",
    "        torch.testing.assert_close(a, b, rtol=rtol, atol=atol)\n",
    "\n",
    "def tst_param(shape):\n",
    "    \"Create a tensor with `val` and a gradient of `grad` for testing\"\n",
    "    param = torch.rand(shape, device='cuda', dtype=torch.float32) * 0.1\n",
    "    param.grad = torch.rand(shape, device='cuda', dtype=param.dtype) * 0.01\n",
    "    return param\n",
    "\n",
    "def tst_params():\n",
    "    params1, params2 = [], []\n",
    "    for i in range(4):\n",
    "        param1 = tst_param(4096)\n",
    "        param2 = param1.clone()\n",
    "        param2.grad = param1.grad.clone()\n",
    "        params1.append(param1)\n",
    "        params2.append(param2)\n",
    "    return params1, params2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "def test_adam(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    mom:float=0.9, # Gradient moving average (β1) coefficient\n",
    "    sqr_mom:float=0.99, # Gradient squared moving average (β2) coefficient\n",
    "    eps:float=1e-6, # Added for numerical stability\n",
    "    wd:float=0.01, # Optional weight decay (true or L2)\n",
    "    optimi:bool=False, # Use fused optimi implementation\n",
    ") -> Optimizer|AdamOptimiOptimizer:\n",
    "    if optimi:\n",
    "        return AdamOptimiOptimizer(params, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)\n",
    "    else:\n",
    "        cbs = [weight_decay, partial(average_grad, dampening=True), average_sqr_grad, step_stat, adam_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "params_org, params_optimi = tst_params()\n",
    "for p, e in zip(params_org, params_optimi):\n",
    "    assert_most_approx_close(p, e)\n",
    "\n",
    "opt_org = test_adam(params_org, lr=1e-3)\n",
    "opt_optimi = test_adam(params_optimi, lr=1e-3, optimi=True)\n",
    "\n",
    "for i in range(10):\n",
    "    opt_org.step()\n",
    "    opt_optimi.step()\n",
    "\n",
    "for p, e in zip(params_org, params_optimi):\n",
    "    assert_most_approx_close(p, e)\n",
    "\n",
    "opt_org.step()\n",
    "opt_optimi.step()\n",
    "for p, e in zip(params_org, params_optimi):\n",
    "    assert_most_approx_close(p, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_radam(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    mom:float=0.9, # Gradient moving average (β1) coefficient\n",
    "    sqr_mom:float=0.99, # Gradient squared moving average (β2) coefficient\n",
    "    eps:float=1e-6, # Added for numerical stability\n",
    "    wd:float=0.01, # Optional weight decay (true or L2)\n",
    "    beta:float=0., # Set to enable SAdam\n",
    "    optimi:bool=False, # Use fused optimi implementation\n",
    ") -> Optimizer|RAdamOptimiOptimizer:\n",
    "    if optimi:\n",
    "        return RAdamOptimiOptimizer(params, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)\n",
    "    else:\n",
    "        cbs = [weight_decay, partial(average_grad, dampening=True), average_sqr_grad, step_stat, radam_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd, beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "params_org, params_optimi = tst_params()\n",
    "for p, e in zip(params_org, params_optimi):\n",
    "    assert_most_approx_close(p, e)\n",
    "\n",
    "opt_org = test_radam(params_org, lr=1e-3)\n",
    "\n",
    "opt_optimi = test_radam(params_optimi, lr=1e-3, optimi=True)\n",
    "\n",
    "for i in range(10):\n",
    "    opt_org.step()\n",
    "    opt_optimi.step()\n",
    "\n",
    "for p, e in zip(params_org, params_optimi):\n",
    "    assert_most_approx_close(p, e)\n",
    "\n",
    "opt_org.step()\n",
    "opt_optimi.step()\n",
    "for p, e in zip(params_org, params_optimi):\n",
    "    assert_most_approx_close(p, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ranger(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    mom:float=0.9, # Gradient moving average (β1) coefficient\n",
    "    sqr_mom:float=0.99, # Gradient squared moving average (β2) coefficient\n",
    "    eps:float=1e-6, # Added for numerical stability\n",
    "    wd:float=0.01, # Optional weight decay (true or L2)\n",
    "    k:int=6, # How often to conduct Lookahead step\n",
    "    alpha:float=0.5, # Slow weight moving average coefficient\n",
    "    optimi:bool=False, # Use fused optimi implementation\n",
    ") -> Lookahead|RangerOptimiOptimizer:\n",
    "    if optimi:\n",
    "        return RangerOptimiOptimizer(params, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)\n",
    "    else:\n",
    "        return Lookahead(test_radam(params, lr=lr, mom=mom, sqr_mom=sqr_mom, wd=wd, eps=eps), k=k, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "params_org, params_optimi = tst_params()\n",
    "for p, e in zip(params_org, params_optimi):\n",
    "    assert_most_approx_close(p, e)\n",
    "\n",
    "opt_org = test_ranger(params_org, lr=1e-3)\n",
    "opt_optimi = test_ranger(params_optimi, lr=1e-3, optimi=True)\n",
    "\n",
    "for i in range(10):\n",
    "    opt_org.step()\n",
    "    opt_optimi.step()\n",
    "\n",
    "for p, e in zip(params_org, params_optimi):\n",
    "    assert_most_approx_close(p, e)\n",
    "\n",
    "opt_org.step()\n",
    "opt_optimi.step()\n",
    "for p, e in zip(params_org, params_optimi):\n",
    "    assert_most_approx_close(p, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "def test_sgd(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    mom:float=0.9, # Gradient moving average (β1) coefficient\n",
    "    wd:float=0., # Optional L2 weight decay\n",
    "    optimi:bool=False, # Use fused optimi implementation\n",
    ") -> Optimizer|SGDOptimiOptimizer:\n",
    "    if optimi:\n",
    "        return SGDOptimiOptimizer(params, lr=lr, mom=mom, wd=wd)\n",
    "    else:\n",
    "        cbs = [l2_reg, average_grad, momentum_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "params_org, params_optimi = tst_params()\n",
    "for p, e in zip(params_org, params_optimi):\n",
    "    assert_most_approx_close(p, e)\n",
    "\n",
    "opt_org = test_sgd(params_org, lr=0.01)\n",
    "opt_optimi = test_sgd(params_optimi, lr=0.01, optimi=True)\n",
    "\n",
    "for i in range(10):\n",
    "    opt_org.step()\n",
    "    opt_optimi.step()\n",
    "\n",
    "for p, e in zip(params_org, params_optimi):\n",
    "    assert_most_approx_close(p, e)\n",
    "\n",
    "opt_org.step()\n",
    "opt_optimi.step()\n",
    "for p, e in zip(params_org, params_optimi):\n",
    "    assert_most_approx_close(p, e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
