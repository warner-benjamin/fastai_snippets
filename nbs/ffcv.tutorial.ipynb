{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp ffcv.tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# Contains code from:\n",
    "# fastai - Apache License 2.0 - Copyright (c) 2023 fast.ai\n",
    "# FFCV - Apache License 2.0 - Copyright (c) 2022 FFCV Team"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with fastxtend and FFCV\n",
    "> Use fastxtend's FFCV integration to accelerate fastai training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastxtend integrates [FFCV](https://ffcv.io) with fastai. You can now use the speed of the highly optimized FFCV DataLoader natively with fastai batch transforms, callbacks, and other DataLoader features.\n",
    "\n",
    "![fastxtend accelerates fastai](images/imagenette_benchmark.svg)\n",
    "\n",
    "[FFCV is](https://ffcv.io) \"a drop-in data loading system that dramatically increases data throughput in model training.\" It accelerates[^mosiac] DataLoader throughput by combining Numba compiled item transforms with a custom data format and cached data loading.\n",
    "\n",
    "FFCV has a [getting started tutorial](https://docs.ffcv.io/quickstart.html) which pairs well with this guide, providing additional context and depth.\n",
    "\n",
    "While fastxtend's FFCV integration is currently in beta, it is fully functional. Expect new features and quality of life improvements in future fastxtend releases.\n",
    "\n",
    ":::{.callout-note collapse=\"true\"}\n",
    "#### Note: Benchmark Details\n",
    "The benchmark uses [fused optimizers](optimizer.fused.html), [Progressive Resizing](callback.progresize.html) callback, and the integrated FFCV DataLoader.\n",
    ":::\n",
    "\n",
    "[^mosiac]: MosiacML [found](https://www.mosaicml.com/blog/composer-ffcv-faster-together) that using FFCV led to a ~1.85x increase in throughput, from ~17,800 images/sec to ~30,000 images/sec on a 2x 32-core CPU and 8x A100 system."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing fastai, FFCV, and fastxtend\n",
    "\n",
    "The easiest way to install fastai, fastxtend, and FFCV is to use [Conda](https://docs.conda.io/en/latest/) or [Miniconda](https://docs.conda.io/en/latest/miniconda.html)[^solver] on Linux (or WSL):\n",
    "\n",
    "```bash\n",
    "conda create -n fastxtend python=3.10 pkg-config libjpeg-turbo opencv \\\n",
    "tqdm terminaltables psutil numpy=1.23.5 numba fastai timm kornia pytorch \\\n",
    "torchvision pytorch-cuda=11.8 -c pytorch -c nvidia -c fastai -c conda-forge\n",
    "\n",
    "# Switch to the newly created conda environment\n",
    "conda activate fastxtend\n",
    "```\n",
    "\n",
    "replacing `pytorch-cuda=11.8` with your prefered [supported version of Cuda](https://pytorch.org/get-started/locally). In rare[^rare] cases, you may need to add the `compilers` package to the conda install. \n",
    "\n",
    "And then install fastxtend via pip:\n",
    "\n",
    "```bash\n",
    "# Install fastxtend with Vision & FFCV support\n",
    "pip install fastxtend[ffcv]\n",
    "\n",
    "# or all fastxtend features\n",
    "pip install fastxtend[all]\n",
    "```\n",
    "\n",
    "If you are using Windows[^windows], please follow the [FFCV Windows installation guide](https://github.com/libffcv/ffcv#windows), then install fastxtend via pip.\n",
    "\n",
    "[^solver]: Miniconda with the faster [libmamba solver](https://www.anaconda.com/blog/a-faster-conda-for-a-growing-community) is recommended.\n",
    "\n",
    "[^rare]: The FFCV Linux [installation guide states](https://github.com/libffcv/ffcv#linux) the `compilers` package is rarely needed.\n",
    "\n",
    "[^windows]: fastxtend Windows support is currently untested, but it should work. It is recommended to use WSL on Windows.\n",
    "\n",
    ":::{.callout-important collapse=\"true\"}\n",
    "#### Important: Numba & NumPy Compatibility\n",
    "At the time of publication, the latest release of Numba (0.56.4) is not compatible with the latest release of NumPy (1.24.x).\n",
    "\n",
    "You need to use the prior release of NumPy (1.23.x).\n",
    ":::\n",
    "\n",
    ":::{.callout-note collapse=\"true\"}\n",
    "#### Note: Suggested Optional Packages\n",
    "If you want to use notebooks with fastxtend, it's recommended to add these three packages `jupyter jupyterlab ipykernel` to the conda install packages.\n",
    "\n",
    "`cupy` is not listed in the conda packages as it's only needed if you want to use FFCV's [`NormalizeImage`](https://docs.ffcv.io/api/transforms.html#ffcv.transforms.NormalizeImage) on the GPU. It's recommended to use fastai's Normalize instead. See [adding batch transforms](#adding-required-optional-batch-transforms) section for more details.\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing FFCV via fastxtend\n",
    "\n",
    "fastxtend's FFCV integration has been designed to use `__all__` to safely import everything needed to use FFCV with fastai and fastxtend.\n",
    "\n",
    "Run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|slow\n",
    "from fastai.vision.all import *\n",
    "from fastxtend.vision.all import *\n",
    "from fastxtend.ffcv.all import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and you are ready to go.\n",
    "\n",
    ":::{.callout-note collapse=\"false\"}\n",
    "#### Note: Importing Transforms & Operations\n",
    "fastxtend's FFCV transforms and operations are imported under the `ft` prefix, since they sometimes overlap with fastai batch transforms.\n",
    "\n",
    "For example, the FFCV augmenation `RandomErasing` is an Numba FFCV version of the batch transform `fastai.vision.augment.RandomErasing`.\n",
    ":::\n",
    "\n",
    "You can also import the FFCV integration individually. This will require you to mix imports from `fastxtend.ffcv` and `ffcv`.\n",
    "\n",
    "```python\n",
    "from ffcv.fields.decoders import CenterCropDecoder\n",
    "from fastxtend.ffcv.loader import Loader, OrderOption\n",
    "from fastxtend.ffcv.transforms import RandomHorizontalFlip\n",
    "# etc\n",
    "```\n",
    "\n",
    "However, during the beta it's recommended to use `from fastxtend.ffcv.all import *`, as imports may change between fastxtend releases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a FFCV Dataset\n",
    "\n",
    "Before we can start training with FFCV, our dataset needs to be converted into FFCV's custom beaton format. This can be done using the `DatasetWriter`.\n",
    "\n",
    "fastxtend provides the `rgb_dataset_to_ffcv` convenience method for easy FFCV image dataset creation. `rgb_dataset_to_ffcv` expects a PyTorch compatible `Dataset` or any Python iterator.\n",
    "\n",
    "First, create an Imagenette dataset using the [fastai DataBlock API](https://docs.fast.ai/tutorial.datablock.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|slow\n",
    "path = URLs.IMAGENETTE_320\n",
    "source = untar_data(path)\n",
    "\n",
    "dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "                   splitter=GrandparentSplitter(valid_name='val'),\n",
    "                   get_items=get_image_files, get_y=parent_label)\n",
    "dset = dblock.datasets(source)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use `rgb_dataset_to_ffcv` to create two FFCV files: one for the training dataset and one for the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|slow\n",
    "path = Path.home()/'.cache/fastxtend'\n",
    "path.mkdir(exist_ok=True)\n",
    "\n",
    "rgb_dataset_to_ffcv(dset.train, path/'imagenette_320_train.ffcv')\n",
    "\n",
    "rgb_dataset_to_ffcv(dset.valid, path/'imagenette_320_valid.ffcv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-tip collapse=\"true\"}\n",
    "#### Tip: Change Chunk Size\n",
    "If you have more (or less) memory, you can increase (or decrease) `DatasetWriter`'s `chunk_size` from the default of 100.\n",
    ":::\n",
    "\n",
    "If Imagenette was not already resized, we could pass `max_resolution` or `min_resolution` to resize the images. To recreate Imagenette 320 from the full size dataset, pass `min_resolution=320`:\n",
    "\n",
    "```python\n",
    "rgb_dataset_to_ffcv(\n",
    "    dset.train, path/'imagenette_320_valid.ffcv', min_resolution=320)\n",
    "```\n",
    "\n",
    "By default, `rgb_dataset_to_ffcv` will use Pillow and the [`LANCZOS`](https://pillow.readthedocs.io/en/stable/handbook/concepts.html#PIL.Image.LANCZOS) resample method to resize the image, and `DatasetWriter` will use OpenCV with [`INTER_AREA`](https://docs.opencv.org/3.4/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121).\n",
    "\n",
    ":::{.callout-tip collapse=\"true\"}\n",
    "#### Tip: Pillow-SIMD\n",
    "To accelerate image resizing, you'll probably want [Pillow-SIMD](https://github.com/uploadcare/pillow-simd) installed.\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a fastxtend Loader\n",
    "\n",
    "fastxtend adds fastai features to [FFCV's Loader](https://docs.ffcv.io/making_dataloaders.html), including `one_batch`, `show_batch`, `show_results`, and support for batch transforms, to name a few.\n",
    "\n",
    "Currently `fastai.data.block.DataBlock` is unsupported for creating a fastxtend `Loader`, so we'll have to create it from scratch.\n",
    "\n",
    "For reference, here is the fastai DataBlock we'll be recreating[^recreating] using the fastxtend `Loader`.\n",
    "\n",
    "```python\n",
    "dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "                   splitter=GrandparentSplitter(valid_name='val'),\n",
    "                   get_items=get_image_files, get_y=parent_label,\n",
    "                   item_tfms=[RandomResizedCrop(224), FlipItem(0.5)],\n",
    "                   batch_tfms=[*aug_transforms(do_flip=False), Normalize(*imagenet_stats)])\n",
    "dls = dblock.dataloaders(source, bs=64, num_workers=num_cpus())\n",
    "```\n",
    "\n",
    ":::{.callout-important collapse=\"true\"}\n",
    "#### Important: Loader Transform Type & Order Matters\n",
    "Unlike fastai, fastxtend's FFCV integration, and FFCV itself, does not automatically select between training and validation versions of transforms.\n",
    "\n",
    "Neither does it automatically create a validation pipeline or automatically reorder transforms and operations.\n",
    "\n",
    "You are responsible for adding the correct decoders, transforms, and operations to the correct pipelines in the correct order.\n",
    ":::\n",
    "\n",
    "[^recreating]: This will not be a one-to-one recreation, as `fastai.vision.augment.RandomResizedCrop` adds padding and squishes the validation images, while the fastxend `Loader` will use a standard center crop for validation. See the [Identical fastai and FFCV Dataloaders](#identical-fastai-and-ffcv-dataloaders) section for how to create an identical fastai dataloader."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Pipelines\n",
    "\n",
    "FFCV uses [pipelines](https://docs.ffcv.io/making_dataloaders.html#pipelines) to declare what input fields to read, how to decode them, and which operations and transforms to apply on them.\n",
    "\n",
    "The dataloader will need three pipelines: one for the training images, validation images, and a shared pipeline for labels.\n",
    "\n",
    "We need to make sure that decoders, transforms, and operations are all in the correct order, as they will be executed sequentially."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Pipeline\n",
    "\n",
    "Reading a FFCV dataset requires a FFCV decoder. FFCV has [multiple decoders](https://docs.ffcv.io/api/decoders.html), but we'll use [`RandomResizedCropRGBImageDecoder`](https://docs.ffcv.io/api/decoders.html#ffcv.fields.decoders.RandomResizedCropRGBImageDecoder) which integrates a random resizing crop into image loading.\n",
    "\n",
    "And we'll add `RandomHorizontalFlip` to flip the image.\n",
    "\n",
    "```python\n",
    "train_pipe = [\n",
    "    RandomResizedCropRGBImageDecoder(output_size=(224,224)),\n",
    "    ft.RandomHorizontalFlip(0.5)\n",
    "]\n",
    "```\n",
    ":::{.callout-tip collapse=\"true\"}\n",
    "#### Tip: fastxtend has Image Transforms\n",
    "fastxtend provides multiple FFCV [transforms](ffcv.transforms.html), including existing FFCV transforms with [harmonized arguments](#ffcv-transforms-reference), fastai transforms [implemented as FFCV transforms](#color-transforms), and [additional FFCV transforms](#additional-ffcv-transforms).\n",
    ":::\n",
    "\n",
    "After passing through FFCV's Numba compiled transforms, the Imagenette images are still in CPU memory as NumPy arrays. Before we can pass them to our model, they need to be converted to `fastai.torch_core.TensorImage` and moved to the GPU.\n",
    "\n",
    "We'll extend our training pipeline by adding the `ToTensorImage` and `ToDevice` operations.\n",
    "\n",
    "```python\n",
    "train_pipe.extend([ft.ToTensorImage(), ft.ToDevice()])\n",
    "```\n",
    "\n",
    "`Loader` will now asynchronously transfer each training image batch to the GPU.\n",
    "\n",
    ":::{.callout-warning collapse=\"true\"}\n",
    "#### Warning: Don't use FFCV's Tensor Operations\n",
    "FFCV has [`ToTensor`](https://docs.ffcv.io/api/transforms.html#ffcv.transforms.ToTensor), [`ToTorchImage`](https://docs.ffcv.io/api/transforms.html#ffcv.transforms.ToTorchImage), and [<code>ToDevice</code>](https://docs.ffcv.io/api/transforms.html#ffcv.transforms.ToDevice) operations for converting NumPy arrays to PyTorch Tensors and moving to the GPU. These are compatible with PyTorch dataloaders but they are not compatible with fastai as they will strip the required types for fastai features, such as batch transforms, callbacks, plotting, etc.\n",
    "\n",
    "Use fastxtend's `ToTensorImage` and `ToDevice` for compatibility with fastai features.\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Pipeline\n",
    "\n",
    "With the training pipeline finalized, it's time to create the validation pipeline.\n",
    "\n",
    "FFCV and fastxtend currently have one validation image decoder: [`CenterCropRGBImageDecoder`](https://docs.ffcv.io/api/decoders.html#ffcv.fields.decoders.CenterCropRGBImageDecoder), which resizes and center crops the validation image. This is identical to `fastai.vision.augment.Resize` valdiation behavior.\n",
    "\n",
    "```python\n",
    "valid_pipe = [\n",
    "    CenterCropRGBImageDecoder(output_size=(224,224), ratio=1),\n",
    "    ft.ToTensorImage(),\n",
    "    ft.ToDevice()\n",
    "]\n",
    "```\n",
    "Like `train_pipe`, we use `ToTensorImage` to convert to the correct tensor type and `ToDevice` asynchronously transfer each batch to the GPU."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Pipeline\n",
    "\n",
    "Now we have our image pipeline for our training and validaton datasets but we need to create our label pipeline.\n",
    "\n",
    "Since this is a single label dataset, we have integers as labels, so we'll use an [`IntDecoder`](https://docs.ffcv.io/api/decoders.html#ffcv.fields.decoders.IntDecoder) and convert to `TensorCategory`, followed by squeezing the extra dimension[^dim] with `Squeeze` and using `ToDevice` to transfer to the GPU.\n",
    "\n",
    "```python\n",
    "label_pipe = [\n",
    "    IntDecoder(), ft.ToTensorCategory(),\n",
    "    ft.Squeeze(), ft.ToDevice()\n",
    "]\n",
    "```\n",
    "\n",
    "[^dim]: Since this is a single label problem, we need to remove the added dimension after the batch is created. If this were a multi-labeled dataset, we'd skip the squeezing step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Required & Optional Batch Transforms\n",
    "\n",
    "After `Loader` finishes processing the pipelines, the images are batched on the GPU, but will be in `uint8` format and unnormalized.\n",
    "\n",
    "FFCV has operations to handle both[^normalize], but using them will convert the images from `TensorImage` to `Tensor`, limiting compatibility with other fastai features, such as callbacks, plotting, etc.\n",
    "\n",
    "Since fastxtend's `Loader` supports fastai GPU batch transforms, we'll use them instead.\n",
    "\n",
    "[^normalize]: CuPy is required to use for FFCV's [`NormalizeImage`](https://docs.ffcv.io/api/transforms.html#ffcv.transforms.NormalizeImage) on the GPU. Add `cupy` to the conda installation script if using."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required Batch Transform\n",
    "\n",
    "To convert the `uint8` tensors to `float` and normalize the images, we'll use `fastai.data.transforms.IntToFloatTensor` and `fastai.data.transforms.Normalize` to preserve tensor types and metadata.\n",
    "\n",
    ":::{.callout-important collapse=\"true\"}\n",
    "#### Important: IntToFloatTensor is a Required Batch Transform\n",
    "`fastai.data.transforms.IntToFloatTensor` is a required batch transform (`batch_tfms`) when training on image data for fastai feature compatibility.\n",
    ":::\n",
    "\n",
    "Unlike the FFCV transforms we've used so far, the fastai transforms in `Loader` will automatically reorder themselves into and use [type dispatch](https://fastcore.fast.ai/dispatch#typedispatch) to apply to the correct tensor types.\n",
    "\n",
    "```python\n",
    "batch_tfms = [\n",
    "    IntToFloatTensor,\n",
    "    Normalize.from_stats(*imagenet_stats)\n",
    "]\n",
    "```\n",
    "\n",
    "It is recommended, but not required, to normalize an image batch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Batch Transforms\n",
    "\n",
    "It's also possible to add any fastai batch transform to `Loader`'s `batch_tfms`, such as `fastai.vision.augment.aug_transforms` or `affine_transforms`:\n",
    "\n",
    "```python\n",
    "batch_tfms = [\n",
    "    IntToFloatTensor,\n",
    "    *aug_transforms(),\n",
    "    Normalize.from_stats(*imagenet_stats)\n",
    "]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Loader\n",
    "\n",
    "With the image pipelines, label pipeline, and batch transforms set up, we can now create our dataloaders, one `Loader` for train and one for valid.\n",
    "\n",
    "#### Training Loader\n",
    "\n",
    "Starting with the training `Loader`, we can manually set all the training specific arguments:\n",
    "\n",
    "```python\n",
    "Loader(path/'imagenette_320_train.ffcv',\n",
    "    batch_size=64,\n",
    "    num_workers=num_cpus(),\n",
    "    os_cache=True,\n",
    "    order=OrderOption.RANDOM,\n",
    "    drop_last=True,\n",
    "    pipelines={'image': train_pipeline, 'label': label_pipeline},\n",
    "    batch_tfms=batch_tfms,\n",
    "    batches_ahead=2,\n",
    "    device='cuda',\n",
    "    split_idx=0,\n",
    "    n_inp=1\n",
    ")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loader Arguments\n",
    "\n",
    "There are a handful of important `Loader` arguments which need further explanation:\n",
    "\n",
    "- `order`: Controls how much memory is used for dataset caching and whether the dataset is randomly shuffled. Can be one of `RANDOM`, `QUASI_RANDOM`, or `SEQUENTIAL`. See the note below for more details. Defaults to `SEQUENTIAL`, which is unrandomized.\n",
    "\n",
    "- `os_cache`: By default, FFCV will attempt to cache the entire dataset into RAM using the operating system's caching. This can be changed by setting `os_cache=False` or setting the enviroment variable 'FFCV_DEFAULT_CACHE_PROCESS' to \"True\" or \"1\". If `os_cache=False` then `order` must be set to `QUASI_RANDOM` for the training `Loader`.\n",
    "\n",
    "- `num_workers`: If not set, will use all CPU cores up to 16 by default.\n",
    "\n",
    "- `batches_ahead`: Controls the number of batches ahead the `Loader` works. Increasing uses more RAM, both CPU and GPU. Defaults to 3.\n",
    "\n",
    "- `n_inp`: Controls which inputs to pass to the model. By default, set to number of pipelines minus 1.\n",
    "\n",
    "- `drop_last`: Whether to drop the last partial batch. By default, will set to True if `order` is `RANDOM` or `QUASI_RANDOM`, False if `SEQUENTIAL`.\n",
    "\n",
    "- `device`: The device to place the processed batches of data on. Defaults to `fastai.torch_core.default_device` if not set.\n",
    "\n",
    "- `split_idx`: This tells the fastai batch transforms what dataset they are operating on. By default will use 0 (train) if `order` is `RANDOM` or `QUASI_RANDOM`, 1 (valid) if `SEQUENTIAL`.\n",
    "\n",
    ":::{.callout-note collapse=\"false\"}\n",
    "#### Note: Order Memory Usage\n",
    "Each `order` option requires differing amounts of system memory.\n",
    "\n",
    "- `RANDOM` caches the entire dataset in memory for fast random sampling. `RANDOM` uses the most memory.\n",
    "\n",
    "- `QUASI_RANDOM` caches a subset of the dataset at a time in memory and randomly samples from the subset. Use when the entire dataset cannot fit into memory.\n",
    "\n",
    "- `SEQUENTIAL` requires least memory. It loads a few samples ahead of time. As the name suggests, it is not random, and primarly is for validation.\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplified Training Loader\n",
    "\n",
    "Since Imagenette 320 is small enough to load both the training and validation images into RAM[^assuming], the only arguments for the training `Loader` that must be set are:\n",
    "\n",
    "[^assuming]: Assuming your machine has more than 8GB of RAM.\n",
    "\n",
    "```python\n",
    "Loader(path/'imagenette_320_train.ffcv',\n",
    "    batch_size=64,\n",
    "    order=OrderOption.RANDOM,\n",
    "    pipelines={'image': train_pipeline, 'label': label_pipeline},\n",
    "    batch_tfms=batch_tfms,\n",
    "    batches_ahead=2\n",
    ")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Loader\n",
    "\n",
    "Next is the validation `Loader`.\n",
    "\n",
    "```python\n",
    "Loader(path/'imagenette_320_valid.ffcv',\n",
    "    batch_size=64,\n",
    "    pipelines={'image': valid_pipeline, 'label': label_pipeline},\n",
    "    batch_tfms=batch_tfms,\n",
    "    batches_ahead=2\n",
    ")\n",
    "```\n",
    "\n",
    "This example only sets the required arguments, relying on the `Loader` defaults for the rest."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all Together\n",
    "\n",
    "The last step is to wrap both the training and validation `Loader` in a `fastai.data.core.DataLoaders`.\n",
    "\n",
    "The example below shows all the steps covered so far in a single codeblock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|slow\n",
    "loaders = {}\n",
    "for name in ['train', 'valid']:\n",
    "    label_pipe = [\n",
    "        IntDecoder(), fx.ToTensorCategory(),\n",
    "        fx.Squeeze(), fx.ToDevice()\n",
    "    ]\n",
    "\n",
    "    if name=='train':\n",
    "        image_pipe = [\n",
    "            RandomResizedCropRGBImageDecoder(output_size=(224,224), scale=(0.35, 1)),\n",
    "            fx.RandomHorizontalFlip(), fx.ToTensorImage(), fx.ToDevice()\n",
    "        ]\n",
    "        order = OrderOption.RANDOM\n",
    "    else:\n",
    "        image_pipe = [\n",
    "            CenterCropRGBImageDecoder(output_size=(224,224), ratio=1),\n",
    "            fx.ToTensorImage(), fx.ToDevice()\n",
    "        ]\n",
    "        order = OrderOption.SEQUENTIAL\n",
    "\n",
    "    batch_tfms = [IntToFloatTensor, *aug_transforms(), Normalize.from_stats(*imagenet_stats)]\n",
    "\n",
    "    loaders[name] = Loader(path/f'imagenette_320_{name}.ffcv',\n",
    "                        batch_size=64 if name=='train' else 128,\n",
    "                        order=order,\n",
    "                        pipelines={'image': image_pipe, 'label': label_pipe},\n",
    "                        batch_tfms=batch_tfms,\n",
    "                        batches_ahead=1,\n",
    "                        seed=42\n",
    "                    )\n",
    "\n",
    "dls = DataLoaders(loaders['train'], loaders['valid'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with a fastxtend Loader\n",
    "\n",
    "With the `DataLoaders` created, the only thing left to do is create a `fastai.learner.Learner` with our model, optimizer, loss function, metrics, and callbacks of choice.\n",
    "\n",
    "Here we create the setup which should train Imagenette to ~92.5% in ~226 seconds on a 3080 Ti, depending on randomness and hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|slow\n",
    "with less_random():\n",
    "    learn = Learner(dls, xresnext50(n_out=10), opt_func=ranger(foreach=True),\n",
    "                    loss_func=nn.CrossEntropyLoss(label_smoothing=0.1), metrics=Accuracy(),\n",
    "                    cbs=ProgressiveResize(increase_by=16)).to_channelslast()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first batch will be slower, as Numba needs to compile and FFCV needs to allocate memory for each transform and operation.\n",
    "\n",
    "Once the compilation is over, we will benefit from FFCV's accelerated data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively increase the initial image size of [112, 112] by 16 pixels every 0.8333 epochs for 7 resizes. \n",
      "Starting at epoch 10 and finishing at epoch 15 for a final training size of [224, 224].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.739842</td>\n",
       "      <td>1.804805</td>\n",
       "      <td>0.515924</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.483479</td>\n",
       "      <td>1.330434</td>\n",
       "      <td>0.662930</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.317108</td>\n",
       "      <td>1.193714</td>\n",
       "      <td>0.724076</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.215840</td>\n",
       "      <td>1.404873</td>\n",
       "      <td>0.640255</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.114017</td>\n",
       "      <td>1.206947</td>\n",
       "      <td>0.715924</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.068557</td>\n",
       "      <td>0.985433</td>\n",
       "      <td>0.817070</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.017356</td>\n",
       "      <td>1.111050</td>\n",
       "      <td>0.760255</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.985094</td>\n",
       "      <td>1.069334</td>\n",
       "      <td>0.767898</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.960843</td>\n",
       "      <td>0.889831</td>\n",
       "      <td>0.847389</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.911082</td>\n",
       "      <td>1.085570</td>\n",
       "      <td>0.755159</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.895687</td>\n",
       "      <td>0.927597</td>\n",
       "      <td>0.836688</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.867939</td>\n",
       "      <td>0.798519</td>\n",
       "      <td>0.885860</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.870780</td>\n",
       "      <td>0.957848</td>\n",
       "      <td>0.807388</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.843433</td>\n",
       "      <td>0.894144</td>\n",
       "      <td>0.846624</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.820972</td>\n",
       "      <td>0.793127</td>\n",
       "      <td>0.884076</td>\n",
       "      <td>00:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.797982</td>\n",
       "      <td>0.948408</td>\n",
       "      <td>0.824204</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.766880</td>\n",
       "      <td>0.769455</td>\n",
       "      <td>0.896815</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.716646</td>\n",
       "      <td>0.749211</td>\n",
       "      <td>0.906242</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.671945</td>\n",
       "      <td>0.715331</td>\n",
       "      <td>0.923057</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.649383</td>\n",
       "      <td>0.699832</td>\n",
       "      <td>0.926115</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|slow\n",
    "#|cuda\n",
    "with less_random():\n",
    "    learn.fit_flat_cos(20, 8e-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with FFCV and fastai\n",
    "\n",
    "While the fastxtend `Loader` and custom FFCV file format are great for accelerated training, they are not as useful for inference. This tutorial we will use the fastai DataBlock for inference.\n",
    "\n",
    "Since FFCV uses OpenCV for resizing and fastai uses Pillow, we cannot use the default fastai pipeline.\n",
    "\n",
    ":::{.callout-important collapse=\"true\"}\n",
    "#### Important: Unlike fastai, FFCV uses OpenCV\n",
    "FFCV is hardcoded to use OpenCV's `INTER_AREA` when resizing images, while fastai uses Pillow. This means we cannot use `fastai.vision.data.ImageBlock` for inference.\n",
    ":::\n",
    "\n",
    "fastxtend's [FFCV Inference module](ffcv.inference.html) provides a `FFAIImageBlock` and `FFAICenterCrop` item transform which use OpenCV for resizing images.\n",
    "\n",
    ":::{.callout-tip collapse=\"true\"}\n",
    "#### Tip: FFAI is for Inference\n",
    "Any class or method with the prefix FFAI is intended for inference after training with the fastxtend `Loader`.\n",
    ":::\n",
    "\n",
    "We can create an inference dataloader using the fastai DataBlock API which will create images identically to the `Loader` pipeline we created earlier in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|slow\n",
    "inference_dblock = DataBlock(blocks=(FFAIImageBlock, CategoryBlock),\n",
    "                             splitter=GrandparentSplitter(valid_name='val'),\n",
    "                             get_items=get_image_files, get_y=parent_label,\n",
    "                             item_tfms=[FFAICenterCrop(224, ratio=1)],\n",
    "                             batch_tfms=[*aug_transforms(), Normalize(*imagenet_stats)])\n",
    "inference_dls = dblock.dataloaders(source, bs=64, num_workers=num_cpus())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then assuming we had a folder with test images in `'/test'`, we'd set the `learn` dataloader to the new `inference_dls` and perform inference like normal.\n",
    "\n",
    "```python\n",
    "learn.dls = inference_dls\n",
    "test_dl = dls.test_dl('/test')\n",
    "preds, _ = learn.get_preds(dl=test_dl)\n",
    "```\n",
    "\n",
    "For more details on inference, check out my [Inference with fastai](https://benjaminwarner.dev/2021/10/01/inference-with-fastai) tutorial."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility and Other Limitations\n",
    "\n",
    "One downside of FFCV is it provides less reproducibility than most dataloader solutions.\n",
    "\n",
    "While `Loader` has a `seed` argument, it currently only affects the order data is loaded. With a couple exceptions, the Numba transforms are neither seeded nor reproducible. These transforms are also independent across pipelines.\n",
    "\n",
    "This means many image-to-image training tasks, such as image segmentation, cannot easily use `Loader` as the inputs and outputs will not be identically resized, flipped, etc.\n",
    "\n",
    "However, this FFCV limitation might be resolved in the near future. As MetaAI recently announced [FFCV-SSL](https://github.com/facebookresearch/FFCV-SSL), a fork of FFCV which, among other things, has reproducible transforms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identical fastai and FFCV Dataloaders\n",
    "\n",
    "The [fastai dataloader](#creating-a-fastxtend-loader) and [FFCV dataloader](#putting-it-all-together) we created in this tutorial do not produce identical validation images[^ignoring].\n",
    "\n",
    "This is due to `fastai.vision.augment.RandomResizedCrop` adding padding and squishing validation images, while FFCV's [`CenterCropRGBImageDecoder`](https://docs.ffcv.io/api/decoders.html#ffcv.fields.decoders.CenterCropRGBImageDecoder) creates validation images via center crop.\n",
    "\n",
    "There's an easy way to create a fastai dataloader with identical behavior via the [DataBlock API](https://docs.fast.ai/tutorial.datablock.html), we just have to create it twice.\n",
    "\n",
    "First, create a dataloader like we did before using `RandomResizedCrop`. Then create a second dataloader except with `fastai.vision.augment.Resize`. `Resize` creates a center crop during validation, just like `CenterCropRGBImageDecoder` with `ratio=1`. Finally, set the first `DataLoaders`' valid dataloader to our second valid dataloader. Then we can delete the second `DataLoaders`.\n",
    "\n",
    "[^ignoring]: Ignoring the OpenCV vs Pillow resizing differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|slow\n",
    "dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "                   splitter=GrandparentSplitter(valid_name='val'),\n",
    "                   get_items=get_image_files, get_y=parent_label,\n",
    "                   item_tfms=[RandomResizedCrop(224), FlipItem(0.5)],\n",
    "                   batch_tfms=[*aug_transforms(do_flip=False), \n",
    "                               Normalize.from_stats(*imagenet_stats)])\n",
    "dls = dblock.dataloaders(source, bs=64, num_workers=num_cpus())\n",
    "\n",
    "vblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "                   splitter=GrandparentSplitter(valid_name='val'),\n",
    "                   get_items=get_image_files, get_y=parent_label,\n",
    "                   item_tfms=Resize(224),\n",
    "                   batch_tfms=[*aug_transforms(), Normalize.from_stats(*imagenet_stats)])\n",
    "vls = vblock.dataloaders(source, bs=64, num_workers=num_cpus())\n",
    "\n",
    "dls.valid = vls.valid\n",
    "vls = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignoring differences due to randomness and Pillow vs OpenCV resizing, both the fastai dataloader and FFCV dataloader will produce identically modified images, ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
