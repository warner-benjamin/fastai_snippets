{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp optimizer.eightbit\n",
    "#|default_cls_lvl 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "# Contains code from:\n",
    "# bitsandbytes - MIT License - Copyright (c) Facebook, Inc. and its affiliates.\n",
    "# fastai - Apache License 2.0 - Copyright (c) 2023 fast.ai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8-Bit Optimizers\n",
    "> bitsandbytes 8-bit optimizers with full fastai compatibility"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[bitsandbytes](https://github.com/TimDettmers/bitsandbytes) 8-bit optimizers can reduce optimizer memory usage up to 75% compared to 32-bit optimizers.\n",
    "\n",
    "While it is possible to use bitsandbytes optimizers[^PyTorch] with fastai via `fastai.optimizer.OptimWrapper`, this doesn't provide compatibility with all fastai optimizer features. fastxtend adds full fastai compatibility to bitsandbytes 8-bit optimizers, including per-parameter weight decay, automatic weight decay exclusion for normalization and bias terms, and discriminative learning rate support.\n",
    "\n",
    "To use 8-bit optimizers, install bitsandbytes on a machine with a Cuda device\n",
    "\n",
    "```bash\n",
    "pip install bitandbytes\n",
    "```\n",
    "\n",
    "then import fastxtend optimizers after importing fastai\n",
    "\n",
    "```python\n",
    "from fastxtend.vision.all import *\n",
    "# or just import fastxtend optimizers\n",
    "from fastxtend.optimizer.all import *\n",
    "\n",
    "opt_func = adam(eightbit=True)\n",
    "Learner(..., opt_func=opt_func)\n",
    "```\n",
    "\n",
    "If training NLP models, you may need to replace the PyTorch embedding layer with a bitsandbytes layer : `torch.nn.Embedding(..) -> bnb.nn.Embedding(..)`.\n",
    "\n",
    "Check out the bitsandbytes [readme](https://github.com/TimDettmers/bitsandbytes#using-the-8-bit-optimizers) for more details on using 8-bit optimizers.\n",
    "\n",
    ":::{.callout-note collapse=\"true\"}\n",
    "#### Note: Modification of Synchronize\n",
    "bitsandbytes calls [`torch.cuda.synchronize`](https://pytorch.org/docs/stable/generated/torch.cuda.synchronize.html) after each optimizer step. This prevents starting the next optimizer step until the current step finishes, which may increase optimizer wallclock time.\n",
    "\n",
    "fastxtend adds `sync_each_step=False` as an argument to both all 8-bit optimizers, disabling the per-step `torch.cuda.synchronize`. Set to `sync_each_step=True` to match bitsandbytes behavior.\n",
    ":::\n",
    "\n",
    ":::{.callout-note collapse=\"true\"}\n",
    "#### Note: Supress Import Warnings\n",
    "fastxtend supresses the bitsandbytes import warning message. To view message import bitsandbytes seperately.\n",
    ":::\n",
    "\n",
    "[^PyTorch]: Or any PyTorch-compatible optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "\n",
    "from bitsandbytes.optim.optimizer import Optimizer1State, Optimizer2State, MockArgs\n",
    "import bitsandbytes.functional as BF\n",
    "\n",
    "from fastcore.basics import even_mults\n",
    "\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastai and bitsandbytes Compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def _convert_params(o:list, **defaults) -> list:\n",
    "    \"Convert fastai param_lists to PyTorch param_groups, adding defaults if group doesn't have it\"\n",
    "    splitter = []\n",
    "    for group in o:\n",
    "        if isinstance(group, dict):\n",
    "            splitter.append({**defaults, **group})\n",
    "        else:\n",
    "            splitter.append({'params':group, **defaults})\n",
    "    return splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class EightBitFastaiAdapter:\n",
    "    \"Base for adding fastai optimizer functionality to EightBit Optimizers\"\n",
    "    _keep_on_clear = ['force_train', 'do_wd']\n",
    "    def get_config(self, gindex, pindex, group):\n",
    "        config = {}\n",
    "        config[\"mom\"] = group[\"mom\"]\n",
    "        config[\"sqr_mom\"] = group[\"sqr_mom\"]\n",
    "        config[\"wd\"] = group[\"wd\"]\n",
    "        config[\"eps\"] = group[\"eps\"]\n",
    "        config[\"lr\"] = group[\"lr\"]\n",
    "        config[\"optim_bits\"] = self.args.optim_bits\n",
    "        config[\"min_8bit_size\"] = self.args.min_8bit_size\n",
    "        config[\"percentile_clipping\"] = self.args.percentile_clipping\n",
    "        config[\"block_wise\"] = self.args.block_wise\n",
    "        config[\"max_unorm\"] = self.args.max_unorm\n",
    "        config[\"skip_zeros\"] = self.args.skip_zeros\n",
    "\n",
    "        if (gindex, pindex) in self.mng.index2config:\n",
    "            config.update(self.mng.index2config[(gindex, pindex)])\n",
    "        return config\n",
    "\n",
    "    def get_params(self,\n",
    "        n:slice|int=slice(None), # Extended slicing over the optimizer `param_lists`\n",
    "        with_grad:bool=False # Get all param tuples. If `True` select only those with a gradient\n",
    "    ):\n",
    "        \"Slice of parameters and parameter states\"\n",
    "        return L((p, self.state[p]) for pg in self.param_groups[n] for p in pg['params']\n",
    "                    if (hasattr(p, 'grad') and p.grad is not None) or with_grad==False)\n",
    "\n",
    "    def clear_state(self):\n",
    "        \"Reset the state of the optimizer\"\n",
    "        for p,state in self.get_params():\n",
    "            self.state[p] = {k: state[k] for k in self._keep_on_clear if k in state}\n",
    "\n",
    "    def _set_require_grad(self,\n",
    "        rg:bool, # Requires grad: if `True` sets gradient for parameters, else uses state `state[\"force_train\"]`\n",
    "        p:Tensor, # Parameter to set gradient\n",
    "        state:dict, # Parameter's state dict\n",
    "    ):\n",
    "        p.requires_grad_(rg or state.get('force_train', False))\n",
    "\n",
    "    def freeze_to(self, n:int):\n",
    "        \"Freeze parameter groups up to `n`\"\n",
    "        self.frozen_idx = n if n >= 0 else len(self.param_groups) + n\n",
    "        if self.frozen_idx >= len(self.param_groups):\n",
    "            warn(f\"Freezing {self.frozen_idx} groups; model has {len(self.param_groups)}; whole model is frozen.\")\n",
    "        for o in self.get_params(slice(n, None)):\n",
    "            self._set_require_grad(True, *o)\n",
    "        for o in self.get_params(slice(None, n)):\n",
    "            self._set_require_grad(False, *o)\n",
    "\n",
    "    def freeze(self):\n",
    "        \"Freeze up to last parameter group\"\n",
    "        assert(len(self.param_groups) > 1)\n",
    "        self.freeze_to(-1)\n",
    "\n",
    "    def unfreeze(self):\n",
    "        \"Unfreeze the entire model\"\n",
    "        self.freeze_to(0)\n",
    "\n",
    "    @property\n",
    "    def hypers(self):\n",
    "        return [{k:v for k,v in pg.items() if k != 'params'} for pg in self.param_groups]\n",
    "\n",
    "    def set_hypers(self, **kwargs):\n",
    "        \"`set_hyper` for all `kwargs`\"\n",
    "        L(kwargs.items()).starmap(self.set_hyper)\n",
    "\n",
    "    def _set_hyper(self, k, v):\n",
    "        \"Set the value(s) in `v` for hyper-parameter `k`\"\n",
    "        for v_,h in zip(v, self.param_groups):\n",
    "            h[k] = v_\n",
    "\n",
    "    def set_hyper(self, k, v):\n",
    "        \"Set the value(s) in `v` for hyper-parameter `k`\"\n",
    "        if isinstance(v, slice):\n",
    "            if v.start:\n",
    "                v = even_mults(v.start, v.stop, len(self.param_groups))\n",
    "            else:\n",
    "                v = [v.stop/10]*(len(self.param_groups)-1) + [v.stop]\n",
    "        v = L(v, use_list=None)\n",
    "        if len(v)==1:\n",
    "            v = v*len(self.param_groups)\n",
    "        assert len(v) == len(self.param_groups), f\"Trying to set {len(v)} values for {k} but there are {len(self.param_groups)} parameter groups.\"\n",
    "        self._set_hyper(k, v)\n",
    "\n",
    "    @property\n",
    "    def param_lists(self):\n",
    "        return [pg['params'] for pg in self.param_groups]\n",
    "\n",
    "    @param_lists.setter\n",
    "    def param_lists(self, v):\n",
    "        for pg,v_ in zip(self.param_groups,v):\n",
    "            pg['params'] = v_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(EightBitFastaiAdapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class EightBitCommon:\n",
    "    \"Common changes to EightBit Optimizers\"\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        if not self.initialized:\n",
    "            self.check_overrides()\n",
    "            self.to_gpu()  # needed for fairseq pure fp16 training\n",
    "            self.initialized = True\n",
    "\n",
    "        #if self.is_paged: self.page_mng.prefetch_all()\n",
    "        for gindex, group in enumerate(self.param_groups):\n",
    "            for pindex, p in enumerate(group[\"params\"]):\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state = self.state[p]\n",
    "                if 'step' not in state:\n",
    "                    self.init_state(group, p, gindex, pindex)\n",
    "\n",
    "                self.prefetch_state(p)\n",
    "                self.update_step(group, p, gindex, pindex)\n",
    "                if self.sync_each_step:\n",
    "                    torch.cuda.synchronize()\n",
    "        if self.is_paged or not self.sync_each_step:\n",
    "            # all paged operation are asynchronous, we need\n",
    "            # to sync to make sure all tensors are in the right state\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(EightBitCommon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class EightBit1StateOptimizer(EightBitCommon, EightBitFastaiAdapter, Optimizer1State):\n",
    "    \"Adds fastai optimizer functionality & compatability to `Optimizer1State`\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer_name,\n",
    "        params,\n",
    "        lr=1e-3,\n",
    "        mom=0.9,\n",
    "        sqr_mom=0.0,\n",
    "        eps=1e-8,\n",
    "        wd=0.0,\n",
    "        optim_bits=8,\n",
    "        args=None,\n",
    "        min_8bit_size=4096,\n",
    "        percentile_clipping=100,\n",
    "        block_wise=True,\n",
    "        max_unorm=0.0,\n",
    "        skip_zeros=False,\n",
    "        is_paged=False,\n",
    "        sync_each_step=False\n",
    "    ):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(f\"Invalid epsilon value: {eps}\")\n",
    "        if not 0.0 <= mom < 1.0:\n",
    "            raise ValueError(f\"Invalid mom value: {mom}\")\n",
    "        if not 0.0 <= sqr_mom < 1.0:\n",
    "            raise ValueError(f\"Invalid sqr_mom value: {sqr_mom}\")\n",
    "        if not 0.0 <= wd:\n",
    "            raise ValueError(f\"Invalid weight_decay value: {wd}\")\n",
    "        defaults = dict(lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)\n",
    "        params = L(params)\n",
    "        params = _convert_params(params, **defaults) if isinstance(params[0], (L,list)) else params\n",
    "        super(Optimizer1State, self).__init__(params, defaults, optim_bits, is_paged)\n",
    "\n",
    "        if args is None:\n",
    "            args = {}\n",
    "            args[\"optim_bits\"] = optim_bits\n",
    "            args[\"percentile_clipping\"] = 100\n",
    "            args[\"min_8bit_size\"] = min_8bit_size\n",
    "            args[\"percentile_clipping\"] = percentile_clipping\n",
    "            args[\"block_wise\"] = block_wise\n",
    "            args[\"max_unorm\"] = max_unorm\n",
    "            args[\"skip_zeros\"] = skip_zeros\n",
    "\n",
    "            self.args = MockArgs(args)\n",
    "        else:\n",
    "            self.args = args\n",
    "\n",
    "        self.optimizer_name = optimizer_name\n",
    "        self.sync_each_step = sync_each_step\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_step(self, group, p, gindex, pindex):\n",
    "        state = self.state[p]\n",
    "        grad = p.grad\n",
    "\n",
    "        config = self.get_config(gindex, pindex, group)\n",
    "\n",
    "        state[\"step\"] += 1\n",
    "        step = state[\"step\"]\n",
    "\n",
    "        if config[\"percentile_clipping\"] < 100:\n",
    "            current_gnorm, clip_value, gnorm_scale = BF.percentile_clipping(\n",
    "                grad, state[\"gnorm_vec\"], step, config[\"percentile_clipping\"]\n",
    "            )\n",
    "        else:\n",
    "            gnorm_scale = 1.0\n",
    "\n",
    "        if state[\"state1\"].dtype == torch.float:\n",
    "            BF.optimizer_update_32bit(\n",
    "                self.optimizer_name,\n",
    "                grad,\n",
    "                p,\n",
    "                state[\"state1\"],\n",
    "                config[\"mom\"],\n",
    "                config[\"eps\"],\n",
    "                step,\n",
    "                config[\"lr\"],\n",
    "                None,\n",
    "                config['sqr_mom'],\n",
    "                state.get('wd', config['wd']) if state.get('do_wd', True) else 0.0,\n",
    "                gnorm_scale,\n",
    "                state[\"unorm_vec\"] if config[\"max_unorm\"] > 0.0 else None,\n",
    "                max_unorm=config[\"max_unorm\"],\n",
    "                skip_zeros=config[\"skip_zeros\"],\n",
    "            )\n",
    "\n",
    "        elif state[\"state1\"].dtype == torch.uint8 and not config[\"block_wise\"]:\n",
    "            BF.optimizer_update_8bit(\n",
    "                self.optimizer_name,\n",
    "                grad,\n",
    "                p,\n",
    "                state[\"state1\"],\n",
    "                None,\n",
    "                config[\"mom\"],\n",
    "                config['sqr_mom'],\n",
    "                config[\"eps\"],\n",
    "                step,\n",
    "                config[\"lr\"],\n",
    "                state[\"qmap1\"],\n",
    "                None,\n",
    "                state[\"max1\"],\n",
    "                None,\n",
    "                state[\"new_max1\"],\n",
    "                None,\n",
    "                state.get('wd', config['wd']) if state.get('do_wd', True) else 0.0,\n",
    "                gnorm_scale,\n",
    "                state[\"unorm_vec\"] if config[\"max_unorm\"] > 0.0 else None,\n",
    "                max_unorm=config[\"max_unorm\"],\n",
    "            )\n",
    "\n",
    "            state[\"max1\"], state[\"new_max1\"] = state[\"new_max1\"], state[\"max1\"]\n",
    "        elif state[\"state1\"].dtype == torch.uint8 and config[\"block_wise\"]:\n",
    "            BF.optimizer_update_8bit_blockwise(\n",
    "                self.optimizer_name,\n",
    "                grad,\n",
    "                p,\n",
    "                state[\"state1\"],\n",
    "                None,\n",
    "                config[\"mom\"],\n",
    "                config['sqr_mom'],\n",
    "                config[\"eps\"],\n",
    "                step,\n",
    "                config[\"lr\"],\n",
    "                state[\"qmap1\"],\n",
    "                None,\n",
    "                state[\"absmax1\"],\n",
    "                None,\n",
    "                state.get('wd', config['wd']) if state.get('do_wd', True) else 0.0,\n",
    "                gnorm_scale=gnorm_scale,\n",
    "                skip_zeros=config[\"skip_zeros\"],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(EightBit1StateOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class EightBit2StateOptimizer(EightBitCommon, EightBitFastaiAdapter, Optimizer2State):\n",
    "    \"Adds fastai optimizer functionality & compatability to `Optimizer2State`\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer_name,\n",
    "        params,\n",
    "        lr=1e-3,\n",
    "        mom=0.9,\n",
    "        sqr_mom=0.999,\n",
    "        eps=1e-8,\n",
    "        wd=0.0,\n",
    "        optim_bits=8,\n",
    "        args=None,\n",
    "        min_8bit_size=4096,\n",
    "        percentile_clipping=100,\n",
    "        block_wise=True,\n",
    "        max_unorm=0.0,\n",
    "        skip_zeros=False,\n",
    "        is_paged=False,\n",
    "        sync_each_step=False\n",
    "    ):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(f\"Invalid epsilon value: {eps}\")\n",
    "        if not 0.0 <= mom < 1.0:\n",
    "            raise ValueError(f\"Invalid mom value: {mom}\")\n",
    "        if not 0.0 <= sqr_mom < 1.0:\n",
    "            raise ValueError(f\"Invalid sqr_mom value: {sqr_mom}\")\n",
    "        if not 0.0 <= wd:\n",
    "            raise ValueError(f\"Invalid weight_decay value: {wd}\")\n",
    "        defaults = dict(lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)\n",
    "        params = L(params)\n",
    "        params = _convert_params(params, **defaults) if isinstance(params[0], (L,list)) else params\n",
    "        super(Optimizer2State, self).__init__(params, defaults, optim_bits, is_paged)\n",
    "\n",
    "        if args is None:\n",
    "            args = {}\n",
    "            args[\"optim_bits\"] = optim_bits\n",
    "            args[\"percentile_clipping\"] = 100\n",
    "            args[\"min_8bit_size\"] = min_8bit_size\n",
    "            args[\"percentile_clipping\"] = percentile_clipping\n",
    "            args[\"block_wise\"] = block_wise\n",
    "            args[\"max_unorm\"] = max_unorm\n",
    "            args[\"skip_zeros\"] = skip_zeros\n",
    "\n",
    "            self.args = MockArgs(args)\n",
    "        else:\n",
    "            self.args = args\n",
    "\n",
    "        self.optimizer_name = optimizer_name\n",
    "        self.sync_each_step = sync_each_step\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_step(self, group, p, gindex, pindex):\n",
    "        state = self.state[p]\n",
    "        grad = p.grad\n",
    "\n",
    "        config = self.get_config(gindex, pindex, group)\n",
    "\n",
    "        state[\"step\"] += 1\n",
    "        step = state[\"step\"]\n",
    "\n",
    "        if config[\"percentile_clipping\"] < 100:\n",
    "            current_gnorm, clip_value, gnorm_scale = BF.percentile_clipping(\n",
    "                grad, state[\"gnorm_vec\"], step, config[\"percentile_clipping\"]\n",
    "            )\n",
    "        else:\n",
    "            gnorm_scale = 1.0\n",
    "\n",
    "        if state[\"state1\"].dtype == torch.float:\n",
    "            BF.optimizer_update_32bit(\n",
    "                self.optimizer_name,\n",
    "                grad,\n",
    "                p,\n",
    "                state[\"state1\"],\n",
    "                config[\"mom\"],\n",
    "                config[\"eps\"],\n",
    "                step,\n",
    "                config[\"lr\"],\n",
    "                state[\"state2\"],\n",
    "                config[\"sqr_mom\"],\n",
    "                state.get('wd', config['wd']) if state.get('do_wd', True) else 0.0,\n",
    "                gnorm_scale,\n",
    "                state[\"unorm_vec\"] if config[\"max_unorm\"] > 0.0 else None,\n",
    "                max_unorm=config[\"max_unorm\"],\n",
    "                skip_zeros=config[\"skip_zeros\"],\n",
    "            )\n",
    "\n",
    "        elif state[\"state1\"].dtype == torch.uint8 and not config[\"block_wise\"]:\n",
    "            BF.optimizer_update_8bit(\n",
    "                self.optimizer_name,\n",
    "                grad,\n",
    "                p,\n",
    "                state[\"state1\"],\n",
    "                state[\"state2\"],\n",
    "                config[\"mom\"],\n",
    "                config['sqr_mom'],\n",
    "                config[\"eps\"],\n",
    "                step,\n",
    "                config[\"lr\"],\n",
    "                state[\"qmap1\"],\n",
    "                state[\"qmap2\"],\n",
    "                state[\"max1\"],\n",
    "                state[\"max2\"],\n",
    "                state[\"new_max1\"],\n",
    "                state[\"new_max2\"],\n",
    "                state.get('wd', config['wd']) if state.get('do_wd', True) else 0.0,\n",
    "                gnorm_scale=gnorm_scale,\n",
    "                unorm_vec=state[\"unorm_vec\"] if config[\"max_unorm\"] > 0.0 else None,\n",
    "                max_unorm=config[\"max_unorm\"],\n",
    "            )\n",
    "\n",
    "            # swap maxes\n",
    "            state[\"max1\"], state[\"new_max1\"] = state[\"new_max1\"], state[\"max1\"]\n",
    "            state[\"max2\"], state[\"new_max2\"] = state[\"new_max2\"], state[\"max2\"]\n",
    "        elif state[\"state1\"].dtype == torch.uint8 and config[\"block_wise\"]:\n",
    "            BF.optimizer_update_8bit_blockwise(\n",
    "                self.optimizer_name,\n",
    "                grad,\n",
    "                p,\n",
    "                state[\"state1\"],\n",
    "                state[\"state2\"],\n",
    "                config[\"mom\"],\n",
    "                config['sqr_mom'],\n",
    "                config[\"eps\"],\n",
    "                step,\n",
    "                config[\"lr\"],\n",
    "                state[\"qmap1\"],\n",
    "                state[\"qmap2\"],\n",
    "                state[\"absmax1\"],\n",
    "                state[\"absmax2\"],\n",
    "                state.get('wd', config['wd']) if state.get('do_wd', True) else 0.0,\n",
    "                gnorm_scale=gnorm_scale,\n",
    "                skip_zeros=config[\"skip_zeros\"],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(EightBit2StateOptimizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8-bit Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class SGD8bitOptimizer(EightBit1StateOptimizer):\n",
    "    \"A fastai-compatible bitsandbytes 8-bit SGD optimizer\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr,\n",
    "        mom,\n",
    "        wd=0,\n",
    "        args=None,\n",
    "        min_8bit_size=4096,\n",
    "        percentile_clipping=100,\n",
    "        block_wise=True,\n",
    "        sync_each_step=False\n",
    "    ):\n",
    "        if mom == 0:\n",
    "            raise NotImplementedError(f\"8-bit SGD without momentum {mom=} is not supported\")\n",
    "        super().__init__(\"momentum\", params, lr, mom, 0.0, 0.0, wd, 8, args,\n",
    "                         min_8bit_size, percentile_clipping, block_wise,\n",
    "                         sync_each_step=sync_each_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class RMSProp8bitOptimizer(EightBit1StateOptimizer):\n",
    "    \"A fastai-compatible bitsandbytes 8-bit RMSProb optimizer\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr=1e-2,\n",
    "        sqr_mom=0.99,\n",
    "        eps=1e-8,\n",
    "        wd=0,\n",
    "        args=None,\n",
    "        min_8bit_size=4096,\n",
    "        percentile_clipping=100,\n",
    "        block_wise=True,\n",
    "        sync_each_step=False\n",
    "    ):\n",
    "        if sqr_mom == 0:\n",
    "            raise NotImplementedError(f\"8-bit RMSProp with {sqr_mom=} is not supported\")\n",
    "        super().__init__(\"rmsprop\", params, lr, sqr_mom, 0, eps, wd, 8, args,\n",
    "                         min_8bit_size, percentile_clipping, block_wise,\n",
    "                         sync_each_step=sync_each_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AdamW8bitOptimizer(EightBit2StateOptimizer):\n",
    "    \"A fastai-compatible bitsandbytes 8-bit AdamW optimizer\"\n",
    "    def __init__(self,\n",
    "        params,\n",
    "        lr=1e-3,\n",
    "        mom=0.9,\n",
    "        sqr_mom=0.99,\n",
    "        eps=1e-8,\n",
    "        wd=1e-2,\n",
    "        args=None,\n",
    "        min_8bit_size=4096,\n",
    "        percentile_clipping=100,\n",
    "        block_wise=True,\n",
    "        is_paged=False,\n",
    "        sync_each_step=False\n",
    "    ):\n",
    "        super().__init__(\"adam\", params, lr, mom, sqr_mom, eps, wd, 8, args,\n",
    "                         min_8bit_size, percentile_clipping, block_wise, is_paged=is_paged,\n",
    "                         sync_each_step=sync_each_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LARS8bitOptimizer(EightBit1StateOptimizer):\n",
    "    \"A fastai-compatible bitsandbytes 8-bit LARS optimizer\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr,\n",
    "        mom=0,\n",
    "        wd=0,\n",
    "        args=None,\n",
    "        min_8bit_size=4096,\n",
    "        percentile_clipping=100,\n",
    "        trust_coeff=0.02,\n",
    "        sync_each_step=False\n",
    "    ):\n",
    "        if mom == 0:\n",
    "            raise NotImplementedError(f\"8-bit LARS without momentum {mom=} is not supported\")\n",
    "        super().__init__(\"lars\", params, lr, mom, 0.0, 0.0, wd, 8, args,\n",
    "                         min_8bit_size, percentile_clipping, max_unorm=trust_coeff, block_wise=False,\n",
    "                         sync_each_step=sync_each_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LAMB8bitOptimizer(EightBit2StateOptimizer):\n",
    "    \"A fastai-compatible bitsandbytes 8-bit LAMB optimizer\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr=1e-3,\n",
    "        mom=0.9,\n",
    "        sqr_mom=0.999,\n",
    "        eps=1e-8,\n",
    "        wd=0,\n",
    "        args=None,\n",
    "        min_8bit_size=4096,\n",
    "        percentile_clipping=100,\n",
    "        block_wise=False,\n",
    "        sync_each_step=False\n",
    "    ):\n",
    "        super().__init__(\"lamb\", params, lr, mom, sqr_mom, eps, wd, 8, args,\n",
    "                         min_8bit_size, percentile_clipping, block_wise, max_unorm=1.0,\n",
    "                         sync_each_step=sync_each_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Lion8bitOptimizer(EightBit1StateOptimizer):\n",
    "    \"A fastai-compatible bitsandbytes 8-bit Lion optimizer\"\n",
    "    def __init__(self,\n",
    "        params,\n",
    "        lr=1e-4,\n",
    "        beta1=0.9,\n",
    "        beta2=0.99,\n",
    "        wd=0,\n",
    "        args=None,\n",
    "        min_8bit_size=4096,\n",
    "        percentile_clipping=100,\n",
    "        block_wise=True,\n",
    "        is_paged=False,\n",
    "        sync_each_step=False\n",
    "    ):\n",
    "        super().__init__(\"lion\", params, lr, beta1, beta2, 0., wd, 8, args,\n",
    "                         min_8bit_size, percentile_clipping, block_wise, is_paged=is_paged,\n",
    "                         sync_each_step=sync_each_step)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastai Compatiblity Tests -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastxtend.test_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params = [tst_params(), tst_params(), tst_params()]\n",
    "for opt in [EightBit1StateOptimizer('momentum', params, lr=0.1), EightBit2StateOptimizer('adam', params, lr=0.1)]:\n",
    "    #Freezing the first layer\n",
    "    opt.freeze_to(1)\n",
    "    req_grad = Self.requires_grad()\n",
    "    test_eq(L(params[0]).map(req_grad), [False]*4)\n",
    "    for i in {1,2}:\n",
    "        test_eq(L(params[i]).map(req_grad), [True]*4)\n",
    "\n",
    "    #Unfreezing\n",
    "    opt.unfreeze()\n",
    "    for i in range(2):\n",
    "        test_eq(L(params[i]).map(req_grad), [True]*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params = [tst_params(), tst_params(), tst_params()]\n",
    "for opt in [EightBit1StateOptimizer('momentum', params, lr=0.1), EightBit2StateOptimizer('adam', params, lr=0.1)]:\n",
    "    for p in L(params[1])[[1,3]]:\n",
    "        opt.state[p] = {'force_train': True}\n",
    "    opt.freeze()\n",
    "    test_eq(L(params[0]).map(req_grad), [False]*4)\n",
    "    test_eq(L(params[1]).map(req_grad), [False, True, False, True])\n",
    "    test_eq(L(params[2]).map(req_grad), [True]*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params = [tst_params(), tst_params(), tst_params()]\n",
    "for opt in [EightBit1StateOptimizer('momentum', params), EightBit2StateOptimizer('adam', params)]:\n",
    "    opt.set_hypers(lr=3e-3, mom=0.98, sqr_mom=0.9999, eps=1e-6, wd=0.01)\n",
    "    test_eq(opt.hypers, [dict(lr=3e-3, mom=0.98, sqr_mom=0.9999, eps=1e-6, wd=0.01)]*3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8-bit Optimizer Tests -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "from fastai.optimizer import (weight_decay, l2_reg, average_grad, momentum_step,\n",
    "                              average_sqr_grad, rms_prop_step, step_stat, adam_step,\n",
    "                              larc_layer_lr, larc_step, lamb_step, Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "def assert_most_approx_close(a, b, rtol=1e-3, atol=1e-3, max_error_count=5):\n",
    "    idx = torch.isclose(a, b, rtol=rtol, atol=atol)\n",
    "    error_count = (idx == 0).sum().item()\n",
    "    if error_count > max_error_count:\n",
    "        print(f\"Too many values not close: assert {error_count} < {max_error_count}\")\n",
    "        torch.testing.assert_close(a, b, rtol=rtol, atol=atol)\n",
    "\n",
    "def tst_param(shape):\n",
    "    \"Create a tensor with `val` and a gradient of `grad` for testing\"\n",
    "    param = torch.rand(shape, device='cuda', dtype=torch.float32) * 0.1\n",
    "    param.grad = torch.rand(shape, device='cuda', dtype=param.dtype) * 0.01\n",
    "    return param\n",
    "\n",
    "def tst_params():\n",
    "    params1, params2 = [], []\n",
    "    for i in range(4):\n",
    "        param1 = tst_param(4096)\n",
    "        param2 = param1.clone()\n",
    "        param2.grad = param1.grad.clone()\n",
    "        params1.append(param1)\n",
    "        params2.append(param2)\n",
    "    return params1, params2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "def SGD(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    mom:float=0.9, # Gradient moving average (β1) coefficient\n",
    "    wd:float=0., # Optional L2 weight decay\n",
    "    eightbit:bool=False, # Use fused 8-bit implementation\n",
    "    **eightbitargs\n",
    ") -> Optimizer|SGD8bitOptimizer:\n",
    "    if eightbit:\n",
    "        return SGD8bitOptimizer(params, lr=lr, mom=mom, wd=wd, **eightbitargs)\n",
    "    else:\n",
    "        cbs = [l2_reg, average_grad, momentum_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "params_org, params_8bit = tst_params()\n",
    "opt_org = SGD(params_org, lr=0.01)\n",
    "opt_org.step()\n",
    "\n",
    "opt_8bit = SGD(params_8bit, lr=0.01, eightbit=True)\n",
    "opt_8bit.step()\n",
    "for p, e in zip(params_org, params_8bit):\n",
    "    assert_most_approx_close(p, e)\n",
    "\n",
    "opt_org.step()\n",
    "opt_8bit.step()\n",
    "for p, e in zip(params_org, params_8bit):\n",
    "    assert_most_approx_close(p, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "def RMSProp(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    sqr_mom:float=0.99, # Gradient squared moving average (β2) coefficient\n",
    "    eps:float=1e-8, # Added for numerical stability\n",
    "    wd:float=0., # Optional weight decay (true or L2)\n",
    "    eightbit:bool=False, # Use fused 8-bit implementation\n",
    "    **eightbitargs\n",
    ") -> Optimizer|RMSProp8bitOptimizer:\n",
    "    \"A fastai RMSProp/RMSPropW optimizer with fused TorchScript and 8-bit implementations\"\n",
    "    if eightbit:\n",
    "        return RMSProp8bitOptimizer(params, lr=lr, sqr_mom=sqr_mom, eps=eps, wd=wd, **eightbitargs)\n",
    "    else:\n",
    "        cbs = [l2_reg, average_sqr_grad, rms_prop_step]\n",
    "        return Optimizer(params, cbs, lr=lr, sqr_mom=sqr_mom, wd=wd, eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "params_org, params_8bit = tst_params()\n",
    "opt_org = RMSProp(params_org, lr=1e-3)\n",
    "opt_org.step()\n",
    "\n",
    "opt_8bit = RMSProp(params_8bit, lr=1e-3, eightbit=True)\n",
    "opt_8bit.step()\n",
    "for p, e in zip(params_org, params_8bit):\n",
    "    assert_most_approx_close(p, e)\n",
    "\n",
    "opt_org.step()\n",
    "opt_8bit.step()\n",
    "for p, e in zip(params_org, params_8bit):\n",
    "    assert_most_approx_close(p, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "def Adam(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    mom:float=0.9, # Gradient moving average (β1) coefficient\n",
    "    sqr_mom:float=0.99, # Gradient squared moving average (β2) coefficient\n",
    "    eps:float=1e-5, # Added for numerical stability\n",
    "    wd:float=0.01, # Optional weight decay (true or L2)\n",
    "    eightbit:bool=False, # Use fused 8-bit implementation\n",
    "    **eightbitargs\n",
    ") -> Optimizer|AdamW8bitOptimizer:\n",
    "    if eightbit:\n",
    "        return AdamW8bitOptimizer(params, lr=lr, mom=mom, sqr_mom=sqr_mom,\n",
    "                                eps=eps, wd=wd, **eightbitargs)\n",
    "    else:\n",
    "        cbs = [weight_decay, partial(average_grad, dampening=True), average_sqr_grad, step_stat, adam_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "params_org, params_8bit = tst_params()\n",
    "opt_org = Adam(params_org, lr=1e-3)\n",
    "opt_org.step()\n",
    "\n",
    "opt_8bit = Adam(params_8bit, lr=1e-3, eightbit=True)\n",
    "opt_8bit.step()\n",
    "for p, e in zip(params_org, params_8bit):\n",
    "    assert_most_approx_close(p, e)\n",
    "\n",
    "opt_org.step()\n",
    "opt_8bit.step()\n",
    "for p, e in zip(params_org, params_8bit):\n",
    "    assert_most_approx_close(p, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "def Lars(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    mom:float=0.9, # Gradient moving average (β1) coefficient\n",
    "    trust_coeff:float=0.02, # Trust coeffiecnet for calculating layerwise LR\n",
    "    eps:float=1e-8, # Added for numerical stability\n",
    "    wd:float=0., # Optional weight decay (true or L2)\n",
    "    eightbit:bool=False, # Use fused 8-bit implementation. Only supports LARS: `clip=False`\n",
    "    **eightbitargs\n",
    ") -> Optimizer|LARS8bitOptimizer:\n",
    "    if eightbit:\n",
    "        return LARS8bitOptimizer(params, lr=lr, mom=mom, wd=wd, trust_coeff=trust_coeff, **eightbitargs)\n",
    "    else:\n",
    "        cbs = [l2_reg, average_grad,partial(larc_layer_lr, clip=False), larc_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, trust_coeff=trust_coeff, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "params_org, params_8bit = tst_params()\n",
    "opt_org = Lars(params_org, lr=0.01)\n",
    "opt_org.step()\n",
    "\n",
    "opt_8bit = Lars(params_8bit, lr=0.01, eightbit=True)\n",
    "opt_8bit.step()\n",
    "for p, e in zip(params_org, params_8bit):\n",
    "    assert_most_approx_close(p, e)\n",
    "\n",
    "opt_org.step()\n",
    "opt_8bit.step()\n",
    "for p, e in zip(params_org, params_8bit):\n",
    "    assert_most_approx_close(p, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "def Lamb(\n",
    "    params:Listified[Tensor], # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    mom:float=0.9, # Gradient moving average (β1) coefficient\n",
    "    sqr_mom:float=0.99, # Gradient squared moving average (β2) coefficient\n",
    "    eps:float=1e-5, # Added for numerical stability\n",
    "    wd:float=0., # Optional weight decay (true or L2)\n",
    "    eightbit:bool=False, # Use fused 8-bit implementation. Only supports true weight decay\n",
    "    **eightbitargs\n",
    ") -> Optimizer|LAMB8bitOptimizer:\n",
    "    if eightbit:\n",
    "        return LAMB8bitOptimizer(params, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd, **eightbitargs)\n",
    "    else:\n",
    "        cbs = [weight_decay, partial(average_grad, dampening=True), average_sqr_grad, step_stat, lamb_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "params_org, params_8bit = tst_params()\n",
    "opt_org = Lamb(params_org, lr=0.01)\n",
    "opt_org.step()\n",
    "\n",
    "opt_8bit = Lamb(params_8bit, lr=0.01, eightbit=True)\n",
    "opt_8bit.step()\n",
    "for p, e in zip(params_org, params_8bit):\n",
    "    assert_most_approx_close(p, e)\n",
    "\n",
    "opt_org.step()\n",
    "opt_8bit.step()\n",
    "for p, e in zip(params_org, params_8bit):\n",
    "    assert_most_approx_close(p, e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8-bit Training Test -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "from packaging.version import parse\n",
    "import fastai\n",
    "\n",
    "from fastcore.basics import num_cpus\n",
    "\n",
    "if parse(fastai.__version__) < parse('2.7.11'):\n",
    "    from fastxtend.callback.channelslast import *\n",
    "else:\n",
    "    from fastai.callback.channelslast import *\n",
    "from fastai.data.external import URLs, untar_data\n",
    "from fastai.data.block import DataBlock, CategoryBlock\n",
    "from fastai.data.transforms import GrandparentSplitter, get_image_files, parent_label, Normalize\n",
    "from fastai.learner import Learner\n",
    "from fastai.vision.augment import Resize\n",
    "from fastai.vision.core import imagenet_stats\n",
    "from fastai.vision.data import ImageBlock\n",
    "from fastxtend.metrics import *\n",
    "from fastxtend.vision.models.xresnet import xresnext34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.959734</td>\n",
       "      <td>2.834240</td>\n",
       "      <td>0.320764</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.543875</td>\n",
       "      <td>1.487750</td>\n",
       "      <td>0.585223</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.225693</td>\n",
       "      <td>1.226562</td>\n",
       "      <td>0.696306</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.037938</td>\n",
       "      <td>1.019766</td>\n",
       "      <td>0.786242</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.917183</td>\n",
       "      <td>0.970842</td>\n",
       "      <td>0.808408</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "imagenette = untar_data(URLs.IMAGENETTE_160)\n",
    "\n",
    "with less_random():\n",
    "    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "                       splitter=GrandparentSplitter(valid_name='val'),\n",
    "                       get_items=get_image_files, get_y=parent_label,\n",
    "                       item_tfms=Resize(128),\n",
    "                       batch_tfms=[Normalize.from_stats(*imagenet_stats)])\n",
    "\n",
    "    dls = dblock.dataloaders(imagenette, bs=64,\n",
    "                             num_workers=num_cpus())\n",
    "\n",
    "    learn = Learner(dls, xresnext34(n_out=dls.c), opt_func=Adam,\n",
    "                    loss_func=nn.CrossEntropyLoss(label_smoothing=0.1),\n",
    "                    metrics=Accuracy()).to_channelslast()\n",
    "\n",
    "    learn.fit_one_cycle(5, 3e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
