{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp callback.amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "# Contains code from:\n",
    "# fastai - Apache License 2.0 - Copyright (c) 2023 fast.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from fastai.callback.core import Callback, CancelStepException\n",
    "from fastai.learner import Learner\n",
    "from fastai.torch_basics import ismin_torch\n",
    "\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Mixed Precision\n",
    "\n",
    "> Mixed precision training using PyTorch's AMP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With supported hardware, fastxtend supports training in both float16 and bfloat16 automatic mixed precision.\n",
    "\n",
    "For details on float16 mixed precision training, please see the fastai [mixed precision documentation](https://docs.fast.ai/callback.fp16.html).\n",
    "\n",
    ":::{.callout-note}\n",
    "BF16 mixed precision support has been [upstreamed](https://github.com/fastai/fastai/pull/3929) into fastai 2.7.13.\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MixedPrecision Callback\n",
    "fastxtend's <code>MixedPrecision</code> is a drop in replacement for `fastai.callback.fp16.MixedPrecision` for float16 mixed precision. Set `amp_mode` to train in bfloat16 mixed precision or use `Learner.to_bf16`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AMPMode(str, Enum):\n",
    "    \"Automatic mixed precision modes for ease of completion\"\n",
    "    FP16 = 'fp16'\n",
    "    BF16 = 'bf16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@delegates(GradScaler)\n",
    "class MixedPrecision(Callback):\n",
    "    \"Mixed precision training using Pytorch's Automatic Mixed Precision (AMP)\"\n",
    "    order = 10\n",
    "    def __init__(self,\n",
    "        amp_mode:str|AMPMode=AMPMode.FP16, # Mixed Precision training mode. Supports fp16 and bf16.\n",
    "        **kwargs\n",
    "    ):\n",
    "        amp_mode = AMPMode(amp_mode)\n",
    "        store_attr(names='amp_mode')\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def before_fit(self):\n",
    "        if self.amp_mode == AMPMode.BF16:\n",
    "            if not ismin_torch(\"1.10\"):\n",
    "                raise ValueError(\"PyTorch 1.10 or greater required for bfloat16 mixed precision training.\")\n",
    "            if torch.cuda.is_available() and not torch.cuda.is_bf16_supported():\n",
    "                raise ValueError(\"Unsuported GPU for bfloat16 mixed precision training.\")\n",
    "            dtype = torch.bfloat16\n",
    "        elif self.amp_mode == AMPMode.FP16:\n",
    "            dtype = torch.float16\n",
    "        else:\n",
    "            raise ValueError(f\"Unrecognized precision: {self.amp_mode=}\")\n",
    "\n",
    "        # `autocast` dtype should not be set before PyTorch 1.10.\n",
    "        self.autocast = autocast(dtype=dtype) if ismin_torch(\"1.10\") else autocast()\n",
    "\n",
    "        # `GradScaler` is not needed for bfloat16 as fp32 and bf16 have the same range\n",
    "        self.kwargs['enabled'] = dtype == torch.float16\n",
    "        self.learn.scaler,self.scales = GradScaler(**self.kwargs),L()\n",
    "\n",
    "    def before_batch(self):\n",
    "        self.autocast.__enter__()\n",
    "\n",
    "    def after_pred(self):\n",
    "        self.learn.pred = to_float(self.pred)\n",
    "\n",
    "    def after_loss(self):\n",
    "        self.autocast.__exit__(None, None, None)\n",
    "\n",
    "    def before_backward(self):\n",
    "        self.learn.loss_grad = self.scaler.scale(self.loss_grad)\n",
    "\n",
    "    def before_step(self):\n",
    "        \"Use `self` as a fake optimizer. `self.skipped` will be set to True `after_step` if gradients overflow.\"\n",
    "        self.skipped=True\n",
    "        self.scaler.step(self)\n",
    "        if self.skipped:\n",
    "            raise CancelStepException()\n",
    "        self.scales.append(self.scaler.get_scale())\n",
    "\n",
    "    def after_step(self):\n",
    "        self.learn.scaler.update()\n",
    "\n",
    "    def after_fit(self):\n",
    "        self.autocast,self.learn.scaler,self.scales = None,None,None\n",
    "\n",
    "    @property\n",
    "    def param_groups(self):\n",
    "        \"Pretend to be an optimizer for `GradScaler`\"\n",
    "        return self.opt.param_groups\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        \"Fake optimizer step to detect whether this batch was skipped from `GradScaler`\"\n",
    "        self.skipped=False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`amp_mode` accepts both <code>AMPMode</code> enums and 'fp16' or 'bf16' strings.\n",
    "\n",
    "Passing `GradScaler` arguments to <code>MixedPrecision</code> when training in bfloat16 has no effect, as bfloat16 mixed precision does not used a gradient scaler."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convenience Methods\n",
    "In addition to the fastai convenience methods, fastxtend adds <code>Learner.to_bf16</code> for training in bfloat16 mixed precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "@delegates(GradScaler)\n",
    "def to_fp16(self:Learner, **kwargs):\n",
    "    \"Set `Learner` to float16 mixed precision using PyTorch AMP\"\n",
    "    return self.add_cb(MixedPrecision(**kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def to_bf16(self:Learner):\n",
    "    \"Set `Learner` to bfloat16 mixed precision using PyTorch AMP\"\n",
    "    return self.add_cb(MixedPrecision(amp_mode=AMPMode.BF16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def to_fp32(self:Learner):\n",
    "    \"Set `Learner` to float32 precision\"\n",
    "    return self.remove_cb(MixedPrecision)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "from fastxtend.test_utils import *\n",
    "from fastai.optimizer import SGD\n",
    "from fastcore.basics import listify, flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "class FP16TestCallback(Callback):\n",
    "    \"Asserts that predictions are `float16` values\"\n",
    "    order = 9\n",
    "    def after_pred(self):\n",
    "        assert listify(flatten(self.pred))[0].dtype==torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "class BF16TestCallback(Callback):\n",
    "    \"Asserts that predictions are `bfloat16` values\"\n",
    "    order = 9\n",
    "    def after_pred(self):\n",
    "        assert listify(flatten(self.pred))[0].dtype==torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "set_seed(99, True)\n",
    "learn = synth_learner(cbs=[MixedPrecision,FP16TestCallback], cuda=True)\n",
    "learn.model = nn.Sequential(nn.Linear(1,1), nn.Linear(1,1)).cuda()\n",
    "learn.opt_func = partial(SGD, mom=0.)\n",
    "learn.splitter = lambda m: [list(m[0].parameters()), list(m[1].parameters())]\n",
    "learn.fit(3)\n",
    "assert learn.recorder.values[-1][-1]<learn.recorder.values[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "#Multioutput version\n",
    "set_seed(99, True)\n",
    "learn = synth_learner(cbs=[MixedPrecision('fp16'),FP16TestCallback], cuda=True)\n",
    "class MultiOutputModel(Module):\n",
    "    def __init__(self): self.linear1, self.linear2 = nn.Linear(1,1), nn.Linear(1,1)\n",
    "    def forward(self,x): return self.linear1(x), self.linear2(x)\n",
    "def multioutputloss(pred, val): return ((val-pred[0]).abs() + 0.5 * (val-pred[1]).abs()).sum()\n",
    "learn.model = MultiOutputModel()\n",
    "learn.opt_func = partial(SGD, mom=0.)\n",
    "learn.splitter = lambda m: [list(m.linear1.parameters()), list(m.linear2.parameters())]\n",
    "learn.loss_func=multioutputloss\n",
    "learn.fit(3)\n",
    "assert learn.recorder.values[-1][-1]<learn.recorder.values[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "if torch.cuda.is_bf16_supported():\n",
    "    set_seed(99, True)\n",
    "    learn = synth_learner(cbs=[MixedPrecision(amp_mode=AMPMode.BF16),BF16TestCallback], cuda=True)\n",
    "    learn.model = nn.Sequential(nn.Linear(1,1), nn.Linear(1,1)).cuda()\n",
    "    learn.opt_func = partial(SGD, mom=0.)\n",
    "    learn.splitter = lambda m: [list(m[0].parameters()), list(m[1].parameters())]\n",
    "    learn.fit(3)\n",
    "    assert learn.recorder.values[-1][-1]<learn.recorder.values[0][-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
