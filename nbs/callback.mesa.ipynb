{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp callback.mesa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "from types import FunctionType\n",
    "\n",
    "try:\n",
    "    import timm\n",
    "except ImportError:\n",
    "    raise ImportError(\"timm is required to use MESACallback. Install via `pip install timm`.\")\n",
    "\n",
    "from timm.utils.model_ema import ModelEmaV2\n",
    "\n",
    "from fastai.callback.core import Callback\n",
    "from fastai.callback.mixup import reduce_loss\n",
    "from fastai.callback.fp16 import MixedPrecision\n",
    "from fastai.layers import NoneReduce\n",
    "\n",
    "from fastxtend.multiloss import MultiLoss, MultiLossCallback, MixHandlerX\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastxtend.test_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory-Efficient Sharpness-Aware Training\n",
    "> A callback to add Memory-Efficient Sharpness-Aware Training from [Sharpness-Aware Training for Free](https://arxiv.org/abs/2205.14083) to fastai. EMA implementation from [timm](https://github.com/rwightman/pytorch-image-models/blob/master/timm/utils/model_ema.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory-Efficient Sharpness-Aware Training (MESA) adds a Kullback-Leibler divergence loss between the model's predictions and the exponential moving average (EMA) predictions to penalize the sharpness of current model weights. This process encourages the model to converge to a flat minimum.\n",
    "\n",
    "Unlike [Sharpness-Aware Minimization](https://exeter-ecml.github.io/papers/0068-sharpness-aware-minimisation-for-efficiently-improving-generalization) which can double training computation, MESA only requires a minimal ~15% of additional computation. A second forward pass over the EMA weights and the EMA calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MESALoss -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class MESALoss(MultiLoss):\n",
    "    \"Loss function for MESA. Automatically added to `Learner` by `MESACallback`\"\n",
    "    def __init__(self,\n",
    "        orig_loss:nn.Module|FunctionType, # Original loss function from `Learner.loss_func`\n",
    "        temp:Number=5, # Soften MESA targets by this temperature. τ in paper\n",
    "        weight:float=0.8, # Weight of MESA loss. λ in paper\n",
    "        reduction:str='mean' # PyTorch loss reduction\n",
    "    ):\n",
    "        store_attr(but='reduction')\n",
    "        self.temp = 1/temp\n",
    "        if hasattr(self.orig_loss, 'reduction'): self.orig_loss.reduction = reduction\n",
    "        else: self.orig_loss = partial(self.orig_loss, reduction=reduction)\n",
    "        self._mesa_loss = nn.KLDivLoss(log_target=True, reduction='batchmean' if reduction=='mean' else reduction)\n",
    "        self.mesa_loss = False\n",
    "        self.loss_names = L('orig_loss', 'mesa_loss')\n",
    "        self.loss_funcs = self.loss_names # compatibility with MultiLossCallback\n",
    "        self._zero, self._loss = torch.tensor(0., requires_grad=False), {}\n",
    "        if getattr(self.orig_loss, 'y_int', False): self.y_int = True\n",
    "\n",
    "    def forward(self, pred, *targs):\n",
    "        \"Add MESA loss to `orig_loss` if `mesa_loss==True`\"\n",
    "        targ, mesa_targ = targs\n",
    "        self._loss[0] = self.orig_loss(pred, targ)\n",
    "        if self.mesa_loss:\n",
    "            self._loss[1] = self.weight*self._mesa_loss(self.temp*F.log_softmax(pred, dim=1), self.temp*F.log_softmax(mesa_targ, dim=1))\n",
    "        else:\n",
    "            self._loss[1] = self._zero\n",
    "        return self._loss[0] + self._loss[1]\n",
    "\n",
    "    def forward_mixup(self, pred, *targs):\n",
    "        \"Used by `MixHandlerX` for MixUp, CutMix, etc. Otherwise, same as `forward`.\"\n",
    "        targ1, targ2, mesa_targ, lam = targs\n",
    "        with NoneReduce(self.orig_loss) as ol:\n",
    "            loss = torch.lerp(ol(pred, targ1), ol(pred, targ2), lam)\n",
    "        self._loss[0] = reduce_loss(loss, getattr(self.orig_loss, 'reduction', 'mean'))\n",
    "        if self.mesa_loss:\n",
    "            self._loss[1] = self.weight*self._mesa_loss(self.temp*F.log_softmax(pred, dim=1), self.temp*F.log_softmax(mesa_targ, dim=1))\n",
    "        else:\n",
    "            self._loss[1] = self._zero\n",
    "        return self._loss[0] + self._loss[1]\n",
    "\n",
    "    @property\n",
    "    def reduction(self): return self._reduction\n",
    "\n",
    "    @reduction.setter\n",
    "    def reduction(self, r):\n",
    "        if hasattr(self.orig_loss, 'reduction'): self.orig_loss.reduction = r\n",
    "        else: self.orig_loss = partial(self.orig_loss, reduction=r)\n",
    "        self._mesa_loss.reduction = 'batchmean' if r=='mean' else r\n",
    "        self._reduction = r\n",
    "\n",
    "    @delegates(Module.to)\n",
    "    def to(self, *args, **kwargs):\n",
    "        device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(*args, **kwargs)\n",
    "        self._zero.to(device)\n",
    "        super(Module, self).to(*args, **kwargs)\n",
    "\n",
    "    def activation(self, pred):\n",
    "        \"Returns `orig_loss` `activation`\"\n",
    "        return getattr(self.orig_loss, 'activation', noop)(pred)\n",
    "\n",
    "    def decodes(self, pred):\n",
    "        \"Returns `orig_loss` `decodes`\"\n",
    "        return getattr(self.orig_loss, 'decodes', noop)(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MESACallback -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class MESACallback(Callback):\n",
    "    order = MixedPrecision.order+1\n",
    "    \"Callback to implement Memory-Efficient Sharpness-Aware Training from https://arxiv.org/abs/2205.14083\"\n",
    "    def __init__(self,\n",
    "        start_epoch:int=4, # Epoch to start MESA (index 0)\n",
    "        temp:Number=5, # Soften MESA targets by this temperature. τ in paper\n",
    "        weight:float=0.8, # Weight of MESA loss. λ in paper\n",
    "        decay:float=0.9998, # EMA decay. β in paper\n",
    "        reduction:str='mean', # PyTorch loss reduction\n",
    "        cleanup:bool=True # Remove `MESACallback` after training \n",
    "    ):\n",
    "        store_attr()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def before_fit(self):\n",
    "        if hasattr(self.learn, 'lr_finder') or hasattr(self, \"gather_preds\"): return\n",
    "        self.start_epoch = max(self.start_epoch, 0)\n",
    "        self._ema_forward = lambda x: 0\n",
    "        self.orig_loss = self.learn.loss_func\n",
    "        self.orig_loss_reduction = self.orig_loss.reduction if hasattr(self.orig_loss, 'reduction') else None\n",
    "        self.learn.loss_func = MESALoss(self.orig_loss, self.temp, self.weight, self.reduction)\n",
    "        self.learn.loss_func.to(getattr(self.dls, 'device', default_device()))\n",
    "        self.ema_model = ModelEmaV2(self.learn.model, self.decay)\n",
    "        mix = self.learn._grab_cbs(MixHandlerX)\n",
    "        self._mixup = len(mix) > 0 and mix[0].stack_y\n",
    "\n",
    "    def before_train(self):\n",
    "        \"Start calculating MESA if `start_epoch` is reached\"\n",
    "        if self.start_epoch == self.epoch:\n",
    "            if self._mixup: self.learn.loss_func_mixup.mesa_loss = True\n",
    "            else:           self.learn.loss_func.mesa_loss = True\n",
    "            self._ema_forward = self.ema_model.module\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def after_pred(self):\n",
    "        \"Create MESA targets from EMA prediction\"\n",
    "        self.learn.yb = tuple([self.y, self._ema_forward(*self.xb)])\n",
    "\n",
    "    def after_loss(self):\n",
    "        \"Remove MESA targets `yb` for metrics compatibility\"\n",
    "        y, _ = self.yb\n",
    "        self.learn.yb = tuple([y])\n",
    "\n",
    "    def after_batch(self):\n",
    "        \"Update model's EMA\"\n",
    "        self.ema_model.update(self.learn.model)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def after_fit(self):\n",
    "        \"Optionally remove `MESACallback` from `Learner` post fit\"\n",
    "        if self.cleanup:\n",
    "            if hasattr(self.orig_loss, 'reduction'):\n",
    "                self.orig_loss.reduction = self.orig_loss_reduction\n",
    "            self.learn.loss_func = self.orig_loss\n",
    "            self.ema_model = None\n",
    "            self.remove_cb(MESACallback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, `MESACallback` is incompatible with multi-loss or multi-target training via `MultiLoss` and `MultiTargetLoss`, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: <code>MESACallback</code> defaults to the [reported hyperparameters](https://arxiv.org/abs/2205.14083) for training ResNets on ImageNet for 90 epochs with SGD.\n",
    "\n",
    "Du et al keep `weight` and `temp` constant across all reported CIFAR and ImageNet experiments. \n",
    "\n",
    "The MESA `start_epoch` is 5 for 90 epochs of ImageNet training using ResNets and SGD and for 200 epochs of CIFAR10 & CIFAR100 training using ResNets and SGD. Du et al report conflicting start epochs when training ViT on 300 epochs of ImageNet using AdamW, the paper body states the start epoch is 5 and the appendix reports it as 100. (Following fastai convention, `MESACallback` indexes from 0 so its default is 4).\n",
    "\n",
    "On CIFAR10 and CIFAR100, Du et al reduce MESA's EMA `decay` from ImageNet's 0.9998 to 0.9995."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|slow\n",
    "from fastcore.basics import num_cpus\n",
    "\n",
    "from fastai.data.external import URLs, untar_data\n",
    "from fastai.data.block import DataBlock, CategoryBlock\n",
    "from fastai.data.transforms import GrandparentSplitter, get_image_files, parent_label, Normalize\n",
    "from fastai.learner import Learner, Recorder\n",
    "from fastai.vision.augment import Resize\n",
    "from fastai.vision.core import imagenet_stats\n",
    "from fastai.vision.data import ImageBlock\n",
    "from fastai.vision.models.xresnet import xresnet18\n",
    "\n",
    "from fastxtend.callback.cutmixup import MixUp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To log both the original loss and MESA loss in addition to the combined loss, add both `MESACallback` and `MultiLossCallback` to the `Learner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_orig_loss</th>\n",
       "      <th>train_mesa_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_orig_loss</th>\n",
       "      <th>valid_mesa_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.764903</td>\n",
       "      <td>1.764903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.589739</td>\n",
       "      <td>1.589739</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.248024</td>\n",
       "      <td>1.248024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.415202</td>\n",
       "      <td>1.415202</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.036043</td>\n",
       "      <td>1.036043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.170383</td>\n",
       "      <td>1.170383</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.853409</td>\n",
       "      <td>0.853409</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.038804</td>\n",
       "      <td>1.038804</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.431591</td>\n",
       "      <td>1.149464</td>\n",
       "      <td>0.282127</td>\n",
       "      <td>1.517223</td>\n",
       "      <td>1.239665</td>\n",
       "      <td>0.277558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.400509</td>\n",
       "      <td>1.099069</td>\n",
       "      <td>0.301440</td>\n",
       "      <td>1.482520</td>\n",
       "      <td>1.173118</td>\n",
       "      <td>0.309402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.358856</td>\n",
       "      <td>1.035686</td>\n",
       "      <td>0.323169</td>\n",
       "      <td>1.493900</td>\n",
       "      <td>1.170899</td>\n",
       "      <td>0.323002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.304363</td>\n",
       "      <td>0.961582</td>\n",
       "      <td>0.342781</td>\n",
       "      <td>1.458709</td>\n",
       "      <td>1.110780</td>\n",
       "      <td>0.347929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.265853</td>\n",
       "      <td>0.914811</td>\n",
       "      <td>0.351041</td>\n",
       "      <td>1.451359</td>\n",
       "      <td>1.115306</td>\n",
       "      <td>0.336053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.248852</td>\n",
       "      <td>0.889899</td>\n",
       "      <td>0.358953</td>\n",
       "      <td>1.450576</td>\n",
       "      <td>1.114508</td>\n",
       "      <td>0.336068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|slow\n",
    "#|cuda\n",
    "with no_random():\n",
    "    imagenette = untar_data(URLs.IMAGENETTE_160)\n",
    "\n",
    "    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "                        splitter=GrandparentSplitter(valid_name='val'),\n",
    "                        get_items=get_image_files, get_y=parent_label,\n",
    "                        item_tfms=Resize(64),\n",
    "                        batch_tfms=Normalize.from_stats(*imagenet_stats))\n",
    "    dls =  dblock.dataloaders(imagenette, bs=64, num_workers=num_cpus())\n",
    "\n",
    "    learn = Learner(dls, xresnet18(n_out=dls.c), cbs=[MESACallback, MultiLossCallback])\n",
    "    learn.remove_cb(Recorder)\n",
    "    learn.add_cb(Recorder(add_time=False))\n",
    "    learn.fit_one_cycle(10, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MESACallback` works with `MixUp`, `CutMix`, `CutMixUp`, and `CutMixUpAugment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_orig_loss</th>\n",
       "      <th>train_mesa_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_orig_loss</th>\n",
       "      <th>valid_mesa_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.936363</td>\n",
       "      <td>1.936363</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.637596</td>\n",
       "      <td>1.637596</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.596632</td>\n",
       "      <td>1.596632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.494622</td>\n",
       "      <td>1.494622</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.423184</td>\n",
       "      <td>1.423184</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.211276</td>\n",
       "      <td>1.211276</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.326761</td>\n",
       "      <td>1.326761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.170963</td>\n",
       "      <td>1.170963</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.678020</td>\n",
       "      <td>1.489676</td>\n",
       "      <td>0.188344</td>\n",
       "      <td>1.522724</td>\n",
       "      <td>1.293728</td>\n",
       "      <td>0.228995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.656398</td>\n",
       "      <td>1.450418</td>\n",
       "      <td>0.205980</td>\n",
       "      <td>1.495232</td>\n",
       "      <td>1.249833</td>\n",
       "      <td>0.245400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.631453</td>\n",
       "      <td>1.404200</td>\n",
       "      <td>0.227253</td>\n",
       "      <td>1.512141</td>\n",
       "      <td>1.249805</td>\n",
       "      <td>0.262335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.598414</td>\n",
       "      <td>1.358582</td>\n",
       "      <td>0.239832</td>\n",
       "      <td>1.456735</td>\n",
       "      <td>1.160747</td>\n",
       "      <td>0.295988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.568128</td>\n",
       "      <td>1.318318</td>\n",
       "      <td>0.249810</td>\n",
       "      <td>1.449251</td>\n",
       "      <td>1.170657</td>\n",
       "      <td>0.278594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.565907</td>\n",
       "      <td>1.313766</td>\n",
       "      <td>0.252141</td>\n",
       "      <td>1.451141</td>\n",
       "      <td>1.162868</td>\n",
       "      <td>0.288273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|hide\n",
    "#|slow\n",
    "#|cuda\n",
    "\n",
    "# mixup test\n",
    "with no_random():\n",
    "    imagenette = untar_data(URLs.IMAGENETTE_160)\n",
    "\n",
    "    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "                        splitter=GrandparentSplitter(valid_name='val'),\n",
    "                        get_items=get_image_files, get_y=parent_label,\n",
    "                        item_tfms=Resize(64),\n",
    "                        batch_tfms=Normalize.from_stats(*imagenet_stats))\n",
    "    dls =  dblock.dataloaders(imagenette, bs=64, num_workers=num_cpus())\n",
    "\n",
    "    learn = Learner(dls, xresnet18(n_out=dls.c), cbs=[MESACallback, MultiLossCallback, MixUp])\n",
    "    learn.remove_cb(Recorder)\n",
    "    learn.add_cb(Recorder(add_time=False))\n",
    "    learn.fit_one_cycle(10, 1e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
