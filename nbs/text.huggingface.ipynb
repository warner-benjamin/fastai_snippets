{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp text.huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "# Contains code from:\n",
    "# blurr - Apache License 2.0 - Copyright (c) Wayde Gilliam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "\n",
    "import inspect, warnings\n",
    "from typing import Dict, Iterable, Sequence\n",
    "\n",
    "import torch._dynamo as dynamo\n",
    "from torch.utils.data import Sampler, Dataset\n",
    "from torch.utils.data import DataLoader as _DataLoader\n",
    "from torch.utils.data.dataloader import _worker_init_fn_t, _collate_fn_t\n",
    "\n",
    "from transformers import PreTrainedModel\n",
    "from transformers.modeling_utils import ModuleUtilsMixin\n",
    "from transformers import logging as hf_logging\n",
    "\n",
    "from fastai.callback.core import Callback\n",
    "from fastai.losses import BaseLoss\n",
    "\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers Compatibility\n",
    "> Train Hugging Face Transformers models using fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastxtend provides basic compatibility for training Hugging Face [Transformers](https://huggingface.co/docs/transformers/index) models using the `fastai.learner.Learner`.\n",
    "\n",
    ":::{.callout-tip collapse=\"true\"}\n",
    "#### Tip: Use blurr For a Complete Transformers Integration\n",
    "\n",
    "[blurr](https://ohmeow.github.io/blurr) provides a complete Hugging Face Transformers integration with fastai, including working fastai datablocks, dataloaders, and other fastai methods.\n",
    "\n",
    "In contrast, fastxtend only provides basic `Learner` compatibility.\n",
    ":::\n",
    "\n",
    ":::{.callout-important collapse=\"true\"}\n",
    "#### Important: Requires PyTorch 2.0+\n",
    "\n",
    "fastxtend's Transformers compatibility requires a minimum of PyTorch 2.0.\n",
    ":::\n",
    "\n",
    "To use fastxend's compatibility, setup the Hugging Face dataset, dataloader, and model per the [Transformers documentation](https://huggingface.co/docs/transformers/index), exchanging the PyTorch `Dataloader` for the `HuggingFaceLoader`. Then wrap the dataloaders in `fastai.data.core.DataLoaders` and create a `Learner` with the Hugging Face model, `HuggingFaceLoss`, and `HuggingFaceCallback`. This will automatically setup the compatibility and use the Hugging Face model's built in loss.\n",
    "\n",
    "Jump to the [example](#example) section for a full tutorial.\n",
    "\n",
    "```python\n",
    "from fastai.text.all import *\n",
    "from fastxtend.text.all import *\n",
    "\n",
    "# load a task specific AutoModel\n",
    "hf_model = AutoModel.from_pretrained(\"model-name\")\n",
    "\n",
    "# setup dataset and then dataloaders\n",
    "train_dataset = dataset['train'].with_format('torch')\n",
    "train_dataloader = HuggingFaceLoader(\n",
    "    train_dataset, batch_size=batch_size,\n",
    "    collate_fn=data_collator, shuffle=True,\n",
    "    drop_last=True, num_workers=num_cpus()\n",
    ")\n",
    "\n",
    "# valid_dataloader definition cut for brevity\n",
    "dls = DataLoaders(train_dataloader, valid_dataloader)\n",
    "\n",
    "learn = Learner(dls, hf_model, loss_func=HuggingFaceLoss(), ...,\n",
    "                cbs=HuggingFaceCallback()).to_bf16()\n",
    "\n",
    "# save the model after training using Transformers\n",
    "learn.hf_model.save_pretrained(\"trained-model-name\")\n",
    "```\n",
    "\n",
    "To train with a different loss, pass in a PyTorch compatible loss to `Learner` as normal, and `HuggingFaceCallback` will use it instead of the model's built in loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "warnings.simplefilter(\"ignore\")\n",
    "hf_logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class HuggingFaceLoss(BaseLoss):\n",
    "    \"To use the Hugging Face model's built in loss function, pass this loss to `Learner`\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.func = None\n",
    "\n",
    "    def __call__(self, inp:Tensor, targ:Tensor|None=None, **kwargs):\n",
    "        return tensor(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class HuggingFaceWrapper(nn.Module, ModuleUtilsMixin):\n",
    "    \"A minimal compatibility wrapper between a Hugging Face model and `Learner`\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: PreTrainedModel, # Hugging Face compatible model\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hf_model = model\n",
    "        self._forward_args = inspect.getfullargspec(self.hf_model.forward).args\n",
    "\n",
    "    def forward(self, x:Dict):\n",
    "        return self.hf_model(**{k:v for k,v in x.items() if k in self._forward_args})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, you won't need to use the <code>HuggingFaceWrapper</code> as `HuggingFaceCallback` will automatically add it for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class HuggingFaceCallback(Callback):\n",
    "    \"Provides compatibility between fastai's `Learner`, the Transformers model, & `HuggingFaceLoader`\"\n",
    "    run_valid = True\n",
    "    def __init__(self,\n",
    "        labels:str|None='labels', # Input batch labels key. Set to None if input doesn't contain labels\n",
    "        loss:str='loss', # Model output loss key\n",
    "        logits:str='logits', # Model output logits key\n",
    "        unwrap:bool=False, # After training completes, unwrap the Transformers model\n",
    "    ):\n",
    "        self._label_key, self._loss_key = labels, loss\n",
    "        self._logit_key, self.unwrap = logits, unwrap\n",
    "\n",
    "    def after_create(self):\n",
    "        self._model_loss = isinstance(self.learn.loss_func, HuggingFaceLoss)\n",
    "        if not isinstance(self.model, HuggingFaceWrapper) and not isinstance(self.model, dynamo.OptimizedModule):\n",
    "            self.learn.model = HuggingFaceWrapper(self.learn.model)\n",
    "            self.learn.hf_model = self.learn.model.hf_model\n",
    "\n",
    "    def before_batch(self):\n",
    "        self._loss = None\n",
    "        if self._label_key is not None:\n",
    "            if not self._model_loss:\n",
    "                self.learn.yb = (self.xb[0].pop(self._label_key),)\n",
    "            else:\n",
    "                self.learn.yb = (self.xb[0][self._label_key],)\n",
    "        else:\n",
    "            self.learn.yb = (1,)\n",
    "\n",
    "    def after_pred(self):\n",
    "        outputs = self.learn.pred\n",
    "        if self._model_loss:\n",
    "            self._loss = to_float(outputs[self._loss_key])\n",
    "        self.learn.pred = outputs.get(self._logit_key, None)\n",
    "\n",
    "    def after_loss(self):\n",
    "        if self._model_loss:\n",
    "            self.learn.loss_grad = self._loss\n",
    "            self.learn.loss = self.learn.loss_grad.clone()\n",
    "        else:\n",
    "            self.xb[0][self._label_key] = self.learn.yb[0]\n",
    "\n",
    "    def after_fit(self):\n",
    "        if self.unwrap:\n",
    "            if isinstance(self.learn.model, dynamo.OptimizedModule) and hasattr(self.learn, 'compiler'):\n",
    "                self.learn.compiler._reset_compiled()\n",
    "            if isinstance(self.model, HuggingFaceWrapper):\n",
    "                self.learn.model = self.learn.model.hf_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>HuggingFaceCallback</code> automatically wraps a Transformer model with the `HuggingFaceWrapper` for compatibility with `fastai.learner.Learner`. The original Transformers model is accessable via `Learner.hf_model`.\n",
    "\n",
    "If `HuggingFaceLoss` is passed to `Learner`, then <code>HuggingFaceCallback</code> will use the Hugging Face model's built in loss.\n",
    "\n",
    "If any other loss function is passed to `Learner`, <code>HuggingFaceCallback</code> will prevent the built-in loss from being calculated and will use the `Learner` loss function instead.\n",
    "\n",
    "If `labels=None`, then <code>HuggingFaceCallback</code> will not attempt to assign a fastai target from the Hugging Face input batch. The default fastai and fastxtend metrics will not work without labels.\n",
    "\n",
    "After training, the <code>HuggingFaceCallback</code> can automatically unwrap model if `unwrap=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class HuggingFaceLoader(_DataLoader):\n",
    "    \"A minimal compatibility DataLoader between a Hugging Face and `Learner`\"\n",
    "    def __init__(self,\n",
    "        dataset:Dataset, # dataset from which to load the data\n",
    "        batch_size:int, # Batch size\n",
    "        shuffle:bool|None = None, # Randomize the order of data at each epoch (default: False)\n",
    "        sampler:Sampler|Iterable|None = None, # Determines how to draw samples from the dataset. Cannot be used with shuffle.\n",
    "        batch_sampler:Sampler[Sequence]|Iterable[Sequence]|None = None, # Rreturns a batch of indices at a time. Cannot be used with batch_size, shuffle, sampler, or drop_last.\n",
    "        num_workers:int=0, # Number of processes to use for data loading. 0 means using the main process (default: 0).\n",
    "        collate_fn:_collate_fn_t|None = None, # Function that merges a list of samples into a mini-batch of Tensors. Used for map-style datasets.\n",
    "        pin_memory:bool=False, # Copy Tensors into device/CUDA pinned memory before returning them\n",
    "        drop_last:bool=False, # Drop the last incomplete batch if the dataset size is not divisible by the batch size\n",
    "        timeout:float=0, # Timeout value for collecting a batch from workers\n",
    "        worker_init_fn:_worker_init_fn_t|None = None, # called on each worker subprocess with the worker id as input\n",
    "        multiprocessing_context=None,\n",
    "        generator=None,\n",
    "        prefetch_factor:int|None=None, # number of batches loaded in advance by each worker\n",
    "        persistent_workers:bool=False, # if True, the data loader will not shutdown the worker processes after a dataset has been consumed once\n",
    "        pin_memory_device:str= \"\", # the data loader will copy Tensors into device pinned memory before returning them if pin_memory is set to true\n",
    "    ):\n",
    "        super().__init__(\n",
    "            dataset=dataset, batch_size=batch_size, shuffle=shuffle, sampler=sampler,\n",
    "            batch_sampler=batch_sampler, num_workers=num_workers, collate_fn=collate_fn,\n",
    "            pin_memory=pin_memory, drop_last=drop_last, timeout=timeout,\n",
    "            worker_init_fn=worker_init_fn, multiprocessing_context=multiprocessing_context,\n",
    "            generator=generator, prefetch_factor=prefetch_factor, persistent_workers=persistent_workers,\n",
    "            pin_memory_device=pin_memory_device\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def bs(self) -> int:\n",
    "        \"Number of items a batch\"\n",
    "        return self.batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        for b in super().__iter__():\n",
    "            yield (b,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face Datasets, and thus DataLoaders, return dictionary objects while the `fastai.learner.Learner` expects tuples. <code>HuggingFaceLoader</code> is a PyTorch DataLoader which wraps the Hugging Face batch dictionary in a tuple for `Learner` compatibility. It is otherwise identical to a PyTorch DataLoader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers.utils import logging as hf_logging\n",
    "\n",
    "from fastcore.basics import num_cpus\n",
    "\n",
    "from fastai.data.core import DataLoaders\n",
    "from fastai.learner import Learner\n",
    "\n",
    "from fastxtend.callback import compiler\n",
    "from fastxtend.callback import amp\n",
    "from fastxtend.metrics import Accuracy\n",
    "from fastxtend.optimizer.stableadam import stableadam\n",
    "from fastxtend import schedulers\n",
    "from fastxtend.utils import less_random\n",
    "\n",
    "from fastxtend.imports import *\n",
    "from fastxtend.utils import *\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# silence hugging face warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "hf_logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll use Hugging Face [Transformers](https://huggingface.co/docs/transformers) along with fastai & fastxtend to train a [DistilRoBERTa](https://huggingface.co/distilroberta-base) on a [IMDb](https://huggingface.co/datasets/imdb) subset.\n",
    "\n",
    ":::{.callout-note collapse=\"true\"}\n",
    "#### Note: Transformers Documentation\n",
    "\n",
    "This example is based on the Transformers documentation [sequence classification example](https://huggingface.co/docs/transformers/tasks/sequence_classification).\n",
    ":::\n",
    "\n",
    "### Setup Transformer Objects\n",
    "\n",
    "First, we'll grab the DistilRoBERTa tokenizer and model from the Transformers Auto methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|cuda\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilroberta-base', num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, download IMDb using [Dataset's](https://huggingface.co/docs/datasets) `load_dataset`. In this example, we'll use a subset of IMDb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|cuda\n",
    "imdb = load_dataset('imdb')\n",
    "with less_random():\n",
    "    imdb['train'] = imdb['train'].shuffle().select(range(5000))\n",
    "    imdb['test'] = imdb['test'].shuffle().select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll tokenize the data using Dataset's `map` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|cuda\n",
    "def tokenize_data(batch, tokenizer):\n",
    "    return tokenizer(batch['text'], truncation=True)\n",
    "\n",
    "imdb['train'] = imdb['train'].map(\n",
    "    partial(tokenize_data, tokenizer=tokenizer),\n",
    "    remove_columns='text', batched=True, batch_size=512, num_proc=num_cpus(),\n",
    ")\n",
    "\n",
    "imdb['test'] = imdb['test'].map(\n",
    "    partial(tokenize_data, tokenizer=tokenizer),\n",
    "    remove_columns='text', batched=True, batch_size=512, num_proc=num_cpus(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the DataLoader\n",
    "\n",
    "We need to use fastxtend's `HuggingFaceLoader` instead of the PyTorch `DataLoader`. <code>HuggingFaceLoader</code> is a simple wrapper around a PyTorch `DataLoader` which returns Transformer's dictionary batches in tuples as the `fastai.learner.Learner` expects. It is otherwise identical to the PyTorch DataLoader.\n",
    "\n",
    "After creating the train and valid <code>HuggingFaceLoader</code>, we need to wrap them in `fastai.data.core.DataLoaders`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|cuda\n",
    "with less_random():\n",
    "    train_dataloader = HuggingFaceLoader(\n",
    "        imdb['train'].with_format('torch'), batch_size=16,\n",
    "        collate_fn=DataCollatorWithPadding(tokenizer), shuffle=True,\n",
    "        drop_last=True, num_workers=num_cpus()\n",
    "    )\n",
    "\n",
    "    valid_dataloader = HuggingFaceLoader(\n",
    "        imdb['test'].with_format('torch'), batch_size=16,\n",
    "        collate_fn=DataCollatorWithPadding(tokenizer), shuffle=False,\n",
    "        drop_last=False, num_workers=num_cpus()\n",
    "    )\n",
    "\n",
    "    dls = DataLoaders(train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Learner and Train\n",
    "\n",
    "Finally, we'll create the `Learner` to train DistilRoBERTa on IMDb. We'll pass in the `HuggingFaceCallback` to `cbs` to handle loss function compatibility between Transformers and fastai.\n",
    "\n",
    "Transformer models contain an internal loss method, which we'll use by passing `HuggingFaceLoss` to `loss_func`.\n",
    "\n",
    ":::{.callout-note collapse=\"true\"}\n",
    "#### Note: Non-Default Transformers Keys\n",
    "\n",
    "<code>HuggingFaceCallback</code> expects the Transformer model to have and output logits and loss keys. If these exist but are named differently, you'll need to pass the non-standard key names to <code>HuggingFaceCallback</code>.\n",
    "\n",
    "If your input doesn't have a label key, perhaps because you are pretraining a causal language model, you should set it to `None`.\n",
    ":::\n",
    "\n",
    "We now can use any fastai and/or fastxtend callbacks, optimizers, or metrics to train our Transformers model like usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.691708</td>\n",
       "      <td>0.690203</td>\n",
       "      <td>0.492000</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.510412</td>\n",
       "      <td>0.409681</td>\n",
       "      <td>0.854000</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.282954</td>\n",
       "      <td>0.300484</td>\n",
       "      <td>0.873000</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|cuda\n",
    "with less_random():\n",
    "    learn = Learner(dls, model, loss_func=HuggingFaceLoss(),\n",
    "                    opt_func=stableadam(foreach=True),\n",
    "                    metrics=Accuracy(), cbs=HuggingFaceCallback).to_bf16()\n",
    "\n",
    "    learn.fit_flat_warmup(3, lr=8e-4, wd=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "model = None\n",
    "free_gpu_memory(learn, dls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to use our own loss, such as `nn.CrossEntropyLoss` with label smoothing, we could pass in any PyTorch compatible loss function to `Learner` and <code>HuggingFaceCallback</code> will automatically use it instead of DistilRoBERTa's internal loss function.\n",
    "\n",
    "In this example, we use fastxtend's `CompilerCallback` via the `Learner.compile` convenience method to accelerate training throughput using `torch.compile`. After compiling the model in the first epoch, training speed is increased, and memory usage is reduced. In this small example it's an overall loss, but we'd want to compile DistilRoBERTa if training on the entirety of IMDb.\n",
    "\n",
    ":::{.callout-warning collapse=\"true\"}\n",
    "#### Warning: Dynamic Requires PyTorch 2.1+\n",
    "\n",
    "Compiling the model with `compile(dynamic=True)` requires a minimum of Pytorch 2.1. Dynamic shapes does not work in PyTorch 2.0.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.686346</td>\n",
       "      <td>0.677865</td>\n",
       "      <td>0.658000</td>\n",
       "      <td>01:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.423131</td>\n",
       "      <td>0.383354</td>\n",
       "      <td>0.886000</td>\n",
       "      <td>00:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.355547</td>\n",
       "      <td>0.374400</td>\n",
       "      <td>0.887000</td>\n",
       "      <td>00:27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|cuda\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilroberta-base', num_labels=2)\n",
    "\n",
    "with less_random():\n",
    "    learn = Learner(dls, model, loss_func=nn.CrossEntropyLoss(label_smoothing=0.1),\n",
    "                    opt_func=stableadam(foreach=True), metrics=Accuracy(),\n",
    "                    cbs=HuggingFaceCallback).to_bf16().compile(dynamic=True)\n",
    "\n",
    "    learn.fit_flat_warmup(3, lr=8e-4, wd=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing and Saving the Model\n",
    "\n",
    "The the original Transformers model is accessable via `Learner.model.hf_model` or `Learner.hf_model` (both point to the same object).\n",
    "\n",
    "We can use any Transformers method to save the model, such as `save_pretrained`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "temp_path = tempfile.TemporaryDirectory(dir=learn.path)\n",
    "model_path = temp_path.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|cuda\n",
    "learn.hf_model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "temp_path.cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
