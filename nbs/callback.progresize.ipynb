{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp callback.progresize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progressive Resizing\n",
    "> A callback to add automatic progressive resizing of images during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ProgressiveResize` is inspired by MosaicML’s [Progressive Resizing algorithm for Composer](https://docs.mosaicml.com/en/stable/method_cards/progressive_resizing.html) which in turn was inspired by [fastai’s](https://github.com/fastai/fastbook/blob/780b76bef3127ce5b64f8230fce60e915a7e0735/07_sizing_and_tta.ipynb) manual progressive resizing.\n",
    "\n",
    "![progressive resizing illustrated](images/progressive_resizing.png)\n",
    "\n",
    "Progressive Resizing decreases model training time by training on smaller images then gradually increasing to the full image size. This allows training on more samples for the same compute budget, often leading to higher performance then training on full sized images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "from fastcore.basics import detuplify\n",
    "from fastcore.transform import Pipeline, Transform\n",
    "\n",
    "from fastai.callback.core import Callback\n",
    "from fastai.callback.fp16 import MixedPrecision\n",
    "from fastai.learner import _cast_tensor\n",
    "from fastai.vision.augment import AffineCoordTfm, RandomResizedCropGPU\n",
    "\n",
    "from fastxtend.callback.cutmixup import CutMixUpAugment\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "_resize_augs = (AffineCoordTfm, RandomResizedCropGPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def _to_size(t:Tensor):\n",
    "    \"Convert Tensor to size compatible values\"\n",
    "    if sum(t.shape)==2: return tuple(t.tolist())\n",
    "    else:               return tuple(t.item(),t.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def _num_steps(final_size, current_size, increase_by):\n",
    "    \"Convert Tensor to size compatible values\"\n",
    "    steps = (final_size - current_size) / increase_by\n",
    "    if sum(steps.shape)==2: \n",
    "        steps = steps[0].item()\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def _evenly_divisible(final_size, current_size, increase_by, steps):\n",
    "    increase_by = tensor(increase_by)\n",
    "    return (((final_size-current_size) % increase_by).sum() == 0) and (((final_size-current_size) - (increase_by*steps)).sum() == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class IncreaseMode(Enum):\n",
    "    \"Increase mode for `ProgressiveResize`\"\n",
    "    Epoch = 'epoch'\n",
    "    Batch = 'batch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ProgressiveResize(Callback):\n",
    "    order = MixedPrecision.order+1 # Needs to run after MixedPrecision\n",
    "    \"Progressively increase the size of input images during training. Starting from `initial_size` and ending at the valid image size or `final_size`.\"\n",
    "    def __init__(self,\n",
    "        initial_size:float|tuple[int,int]=0.5, # Staring size to increase from. Image shape must be square\n",
    "        start:Number=0.5, # Earliest upsizing epoch in percent of training time or epoch (index 0)\n",
    "        finish:Number=0.75, # Last upsizing epoch in percent of training time or epoch (index 0)\n",
    "        increase_by:int=4, # Progressivly increase image size by `increase_by`, or minimum increase per upsizing epoch\n",
    "        increase_mode:IncreaseMode=IncreaseMode.Batch, # Increase image size by training percent or before an epoch starts\n",
    "        resize_mode:str='bilinear', # PyTorch interpolate mode string for upsizing. Resets to existing fastai DataLoader mode at `final_size`.\n",
    "        resize_valid:bool=True, # Apply progressive resizing to valid dataset\n",
    "        final_size:tuple[int,int]|None=None, # Final image size. Set if using a non-fastai DataLoaders, automatically detected from fastai DataLoader with batch_tfms\n",
    "        add_resize:bool=False, # Add a seperate resize step. Use for non-fastai DataLoaders or fastai DataLoader without batch_tfms\n",
    "        resize_targ:bool=False, # Applies the seperate resize step to targets\n",
    "        empty_cache:bool=False, # Call `torch.cuda.empty_cache()` before a resizing epoch. May prevent cuda & magma errors. Don't use with multiple GPUs\n",
    "        verbose:str=True, # Print a summary of the progressive resizing schedule\n",
    "        logger_callback:str='wandb', # Log image size to `logger_callback` using `Callback.name` if avalible\n",
    "    ):\n",
    "        store_attr()\n",
    "        self.run_valid = resize_valid\n",
    "        if resize_targ and not add_resize:\n",
    "            warn(f'`resize_targ` requires `add_resize` set to True')\n",
    "        if empty_cache and increase_mode==IncreaseMode.Batch:\n",
    "            warn(f'`empty_cache` requires `increase_mode` set to Epoch')\n",
    "\n",
    "    def before_fit(self):\n",
    "        \"Sets up Progressive Resizing\"\n",
    "        if hasattr(self.learn, 'lr_finder') and not hasattr(self, \"gather_preds\"):\n",
    "            self.run = False\n",
    "            return\n",
    "\n",
    "        self._resize, self.remove_resize, self.null_resize, self.remove_cutmix = [], True, True, False\n",
    "        self._log_after_resize = getattr(self, f'_{self.logger_callback}_log_after_resize', noop)\n",
    "        self.has_logger = hasattr(self.learn, self.logger_callback) and self._log_after_resize != noop\n",
    "        self.increase_by = tensor(self.increase_by)\n",
    "        self.resize_batch = self.increase_mode == IncreaseMode.Batch\n",
    "\n",
    "        # Dry run at full resolution to pre-allocate memory\n",
    "        # See https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#pre-allocate-memory-in-case-of-variable-input-length\n",
    "        try:\n",
    "            states = get_random_states()\n",
    "            path = self.path/self.model_dir\n",
    "            path.mkdir(parents=True, exist_ok=True)\n",
    "            tmp_d = TemporaryDirectory(dir=path)\n",
    "            tmp_p = Path(tmp_d.name).stem\n",
    "            self.learn.save(f'{tmp_p}/_tmp')\n",
    "\n",
    "            b = self.dls.valid.one_batch()\n",
    "            i = getattr(self.dls, 'n_inp', 1 if len(b)==1 else len(b)-1)\n",
    "            self.learn.xb, self.learn.yb = b[:i],b[i:]\n",
    "\n",
    "            if hasattr(self.learn, 'mixed_precision'):\n",
    "                self.learn.mixed_precision.autocast.__enter__()\n",
    "\n",
    "            self.learn.pred = self.learn.model(*_cast_tensor(self.learn.xb))\n",
    "            self.learn.loss = self.learn.loss_func(self.learn.pred, *_cast_tensor(self.learn.yb))\n",
    "\n",
    "            if hasattr(self.learn, 'mixed_precision'):\n",
    "                self.learn.mixed_precision.autocast.__exit__(None, None, None)\n",
    "\n",
    "            self.learn.loss.backward()\n",
    "            self.learn.opt.zero_grad()\n",
    "\n",
    "        finally:\n",
    "            self.learn.load(f'{tmp_p}/_tmp', with_opt=True)\n",
    "            tmp_d.cleanup()\n",
    "            set_random_states(**states)\n",
    "\n",
    "        # Try to automatically determine the input size\n",
    "        try:\n",
    "            for n in range(i):\n",
    "                x = detuplify(self.learn.xb[n])\n",
    "                if isinstance(x, TensorImageBase):\n",
    "                    self.final_size = x.shape[-2:]\n",
    "        finally:\n",
    "            if self.final_size is None: \n",
    "                raise ValueError(f'Could not determine image size from DataLoader. Set `final_size`: {self.final_size}')\n",
    "            self.final_size = tensor(self.final_size)\n",
    "            if self.final_size[0] != self.final_size[1]:\n",
    "                raise ValueError(f'`ProgressiveResize` does not support non-square images: `final_size` = {self.final_size.tolist()}')\n",
    "            if not self.resize_batch:\n",
    "                if self.final_size[0] % 2 != 0:\n",
    "                    raise ValueError(f\"In Epoch mode, input image size must be even: {self.final_size.tolist()}\")\n",
    "                if self.increase_by.item() % 2 != 0:\n",
    "                    raise ValueError(f\"In Epoch Mode, `increase_by` must be even: {self.increase_by}\")\n",
    "\n",
    "        # Set the initial size\n",
    "        if isinstance(self.initial_size, float): \n",
    "            self.current_size = (tensor(self.initial_size) * self.final_size).int()\n",
    "        elif isinstance(self.initial_size, tuple): \n",
    "            self.current_size = tensor(self.initial_size)\n",
    "\n",
    "        start_epoch  = int(self.n_epoch*self.start)  if self.start < 1  else self.start\n",
    "        finish_epoch = int(self.n_epoch*self.finish) if self.finish < 1 else self.finish\n",
    "        max_steps = finish_epoch - start_epoch \n",
    "\n",
    "        if self.resize_batch:\n",
    "            # Set when the progressive resizing step is applied in training percent\n",
    "            n_steps = ((self.final_size-self.current_size) / self.increase_by).int()\n",
    "            if sum(n_steps.shape)==2:\n",
    "                n_steps = n_steps[0].item()\n",
    "            pct = (self.finish - self.start) / (n_steps-1)\n",
    "            self.step_pcts = [self.start + pct*i for i in range(n_steps)]\n",
    "        else:\n",
    "            # Automatically determine the number of steps, increasing `increase_by` as needed\n",
    "            count = 10000 # prevent infinite loop\n",
    "            steps = _num_steps(self.final_size, self.current_size, self.increase_by)\n",
    "            while ((steps > max_steps) or not _evenly_divisible(self.final_size, self.current_size, self.increase_by, steps)) and count > 0:\n",
    "                self.increase_by += 2\n",
    "                steps = _num_steps(self.final_size, self.current_size, self.increase_by)\n",
    "                count -= 1\n",
    "            n_steps = _num_steps(self.final_size, self.current_size, self.increase_by)\n",
    "\n",
    "            # Set when per epoch progressive resizing steps are applied\n",
    "            step_size = int(max_steps / n_steps)\n",
    "            start_epoch = finish_epoch - ((self.final_size-self.current_size) / self.increase_by)*step_size\n",
    "            if isinstance(start_epoch, torch.Tensor):\n",
    "                if sum(start_epoch.shape)==2: start_epoch = int(start_epoch[0].item())\n",
    "                else:                         start_epoch = int(start_epoch.item())\n",
    "            self.step_epochs = [i for i in range(start_epoch+step_size, finish_epoch+step_size, step_size)]\n",
    "\n",
    "\n",
    "        # Double check that the step size works\n",
    "        if not _evenly_divisible(self.final_size, self.current_size, self.increase_by, n_steps):\n",
    "            raise ValueError(f'Resize amount {self.final_size-self.current_size} not evenly divisible by `increase_by` {self.increase_by}')\n",
    "\n",
    "        if self.verbose:\n",
    "            if self.resize_batch:\n",
    "                msg = f'Progressively increase the initial image size of {self.current_size.tolist()} by {self.increase_by} '\\\n",
    "                      f'pixels every {pct*self.n_epoch:.4g} epochs for {len(self.step_pcts)} resizes. \\nStarting at epoch '\\\n",
    "                      f'{self.step_pcts[0]*self.n_epoch:.4g} and finishing at epoch {self.step_pcts[-1]*self.n_epoch:.4g} '\\\n",
    "                      f'for a final training size of {(self.current_size+(len(self.step_pcts))*self.increase_by).tolist()}.'\n",
    "                print(msg)\n",
    "            else:\n",
    "                msg = f'Progressively increase the initial image size of {self.current_size.tolist()} by {self.increase_by} '\\\n",
    "                      f'pixels every {step_size} epoch{\"s\" if step_size > 1 else \"\"} for {len(self.step_epochs)} resizes.\\nStarting '\\\n",
    "                      f'at epoch {start_epoch+step_size} and finishing at epoch {finish_epoch} for a final training size of '\\\n",
    "                      f'{(self.current_size+(len(self.step_epochs))*self.increase_by).tolist()}.'\n",
    "                print(msg)\n",
    "\n",
    "\n",
    "        # If `add_resize`, add a seperate resize\n",
    "        if self.add_resize:\n",
    "            self._added_resize = partial(F.interpolate, mode=self.resize_mode, recompute_scale_factor=True)\n",
    "            self.remove_resize = True\n",
    "        else:\n",
    "            if hasattr(self.learn, 'cut_mix_up_augment'):\n",
    "                self._has_cutmixupaug = True\n",
    "                # Modify the `CutMixUpAugment` augmentation pipeline \n",
    "                self._process_pipeline(self.learn.cut_mix_up_augment._orig_pipe, False)\n",
    "\n",
    "                # If `CutMixUpAugment` has an Affine Transform for Augmentations then use it\n",
    "                if len(self._resize) > 0:\n",
    "                    # Check for pre-mixup augment pipeline and modify it\n",
    "                    if self.learn.cut_mix_up_augment._docutmixaug:\n",
    "                        self._process_pipeline(self.learn.cut_mix_up_augment._cutmixaugs_pipe, False)\n",
    "                        self.learn.cut_mix_up_augment._size = _to_size(self.current_size)\n",
    "                    else:\n",
    "                        # There isn't one, then add it a pre-mixup augment pipeline for resizing\n",
    "                        self.learn.cut_mix_up_augment._cutmixaugs_pipe = Pipeline(AffineCoordTfm(size=_to_size(self.current_size)))\n",
    "                        self.learn.cut_mix_up_augment._docutmixaug = True\n",
    "                        self.learn.cut_mix_up_augment._size = _to_size(self.current_size)\n",
    "                        self._resize.append(self.learn.cut_mix_up_augment._cutmixaugs_pipe[0])\n",
    "                        self.remove_cutmix, self.remove_resize = True, True\n",
    "            else:\n",
    "                self._has_cutmixupaug = False\n",
    "                # If no `CutMixUpAugment` check the train dataloader pipeline for Affine Transforms\n",
    "                self._process_pipeline(self.dls.train.after_batch.fs, False)\n",
    "\n",
    "            # If `resize_valid` check the valid dataloader pipeline for Affine Transforms\n",
    "            if self.resize_valid:\n",
    "                self._process_pipeline(self.dls.valid.after_batch.fs, False)\n",
    "\n",
    "        # Set created or detected resize to the first size and store original interpolation\n",
    "        self._orig_modes = []\n",
    "        for resize in self._resize:\n",
    "            resize.size = _to_size(self.current_size)\n",
    "            self._orig_modes.append(resize.mode)\n",
    "            resize.mode = self.resize_mode\n",
    "\n",
    "    def before_batch(self):\n",
    "        \"Increases the image size before a batch if set to ProgSizeMode.Batch and applies optional additional resize\"\n",
    "        if self.resize_batch and len(self.step_pcts) > 0 and self.pct_train >= self.step_pcts[0]:\n",
    "            _ = self.step_pcts.pop(0)\n",
    "            self._increase_size()\n",
    "        if self.add_resize:\n",
    "            self.learn.xb = (self._added_resize(self.x, scale_factor=(self.current_size/self.final_size)[0]),)\n",
    "            if self.resize_targ:\n",
    "                self.learn.yb = (self._added_resize(self.y, scale_factor=(self.current_size/self.final_size)[0]),)\n",
    "        \n",
    "    def before_train(self):\n",
    "        \"Increases the image size before the training epoch if set to ProgSizeMode.Epoch\"\n",
    "        if not self.resize_batch and len(self.step_epochs) > 0 and self.epoch >= self.step_epochs[0]:\n",
    "            _ = self.step_epochs.pop(0)\n",
    "            self._increase_size()\n",
    "\n",
    "    def after_epoch(self):\n",
    "        \"Calls `torch.cuda.empty_cache()` if `empty_cache=True` before a resizing epoch if set to ProgSizeMode.Epoch. May slightly increase single GPU training speed.\"\n",
    "        if not self.resize_batch and self.empty_cache and len(self.step_epochs) > 0 and self.epoch+1 >= self.step_epochs[0]:\n",
    "            del self.learn.xb\n",
    "            del self.learn.yb\n",
    "            del self.learn.pred\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    def _increase_size(self):\n",
    "        \"Increase the input size\"\n",
    "        if self.has_logger: self._log_after_resize(step=0)\n",
    "\n",
    "        self.current_size += self.increase_by\n",
    "        for i, resize in enumerate(self._resize):\n",
    "            if (self.current_size < self.final_size).all():\n",
    "                resize.size = _to_size(self.current_size)\n",
    "                if self._has_cutmixupaug:\n",
    "                    self.learn.cut_mix_up_augment._size = _to_size(self.current_size)\n",
    "            else:\n",
    "                # Reset everything after progressive resizing is done\n",
    "                if self.null_resize: \n",
    "                    resize.size = None\n",
    "                    if self._has_cutmixupaug:\n",
    "                        self.learn.cut_mix_up_augment._size = None\n",
    "                else:\n",
    "                    resize.size = _to_size(self.current_size)\n",
    "                    resize.mode = self._orig_modes[i]\n",
    "\n",
    "        if (self.current_size == self.final_size).all() and self.remove_resize:\n",
    "            self.add_resize = False\n",
    "            if self.remove_cutmix:\n",
    "                self.learn.cut_mix_up_augment._cutmixaugs_pipe = Pipeline([])\n",
    "                self.learn.cut_mix_up_augment._docutmixaug = False\n",
    "                    \n",
    "        if self.has_logger: self._log_after_resize()\n",
    "\n",
    "    def _process_pipeline(self, pipe, remove_resize=False, null_resize=None):\n",
    "        'Helper method for processing augmentation pipelines'\n",
    "        for p in pipe:\n",
    "            if isinstance(p, _resize_augs):\n",
    "                self._resize.append(p)\n",
    "                if null_resize is None:\n",
    "                    self.null_resize = self.null_resize and p.size is None\n",
    "                else:\n",
    "                    self.null_resize = null_resize\n",
    "        self.remove_resize = remove_resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Progressive Resizing initially trains on downsampled images then gradually increases the image size over to the full size for the remainder of training. This can significantly reduce training time at the possible expense of lower model performance, but allows training on more samples in the same compute budget often leading to increased performance. The model must be capable of variable image sizes.\n",
    "\n",
    "::: {.callout-important}\n",
    "<code>ProgressiveResize</code> should increase GPU throughput which may cause other parts of the training pipeline become a bottleneck. An easy way to increase fastai’s DataLoader throughput is by [replacing pillow with pillow-simd](https://docs.fast.ai/dev/performance.html#pillow-simd).\n",
    ":::\n",
    "\n",
    "When testing Composer's [Progressive Resizing](https://docs.mosaicml.com/en/stable/method_cards/progressive_resizing.html) callback MosiacML [found]( https://docs.mosaicml.com/en/stable/method_cards/progressive_resizing.html#technical-details):\n",
    "\n",
    "> In our experiments, Progressive Resizing improves the attainable tradeoffs between training speed and the final quality of the trained model. In some cases, it leads to slightly lower quality than the original model for the same number of training steps. However, Progressive Resizing increases training speed so much (via improved throughput during the early part of training) that it is possible to train for more steps, recover accuracy, and still complete training in less time.\n",
    "\n",
    "`ProgressiveResize` modifies the fastai batch augmentation pipeline by changing the `batch_tfms` size during training. Specifically, it modifies `AffineCoordTfm` size, which is set by any rotate, warp, or resize batch augmentation, and/or `RandomResizedCropGPU` size. This modification prevents unnecessarily resizing images a second time on the GPU, speeding up the process. If there are no `batch_tfms` or if training with a non-fastai DataLoader, set `add_resize=True` to resize the batch on the GPU using PyTorch’s `interpolate`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "If training with <code>ProgressiveResize</code> results in CUDA or Magma errors, try setting <code>increase_mode=IncreaseMode.Epoch</code> and <code>empty_cache=True</code>. This will upsize once per epoch and call <code>torch.cuda.empty_cache()</code> before a resizing epoch. <code>empty_cache=True</code> may interfere with training multiple models on multi-GPU systems.\n",
    ":::\n",
    "\n",
    "Progressive resizing works best when the resize steps are small, 8 or less pixels, and spread out over multiple epochs.\n",
    "\n",
    "`ProgressiveResize` fully compatible with `CutMixUpAugment`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastxtend.test_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "class SyncthProgResizeTest(Callback):\n",
    "    order = ProgressiveResize.order+1\n",
    "    def __init__(self, final_size, start_size, increase, step_size, first_epoch, last_epoch, total_resizes):\n",
    "        store_attr()\n",
    "\n",
    "    def before_fit(self):\n",
    "        prog = self.learn.progressive_resize\n",
    "        if isinstance(self.start_size, tuple):\n",
    "            assert torch.equal(prog.current_size, tensor(self.start_size).int())\n",
    "        else:\n",
    "            assert torch.equal(prog.current_size, tensor([self.start_size,self.start_size]).int())\n",
    "        assert prog.increase_by==self.increase\n",
    "        assert prog.step_epochs[1]-prog.step_epochs[0]==self.step_size\n",
    "        assert prog.step_epochs[0]==self.first_epoch\n",
    "        assert prog.step_epochs[-1]==self.last_epoch\n",
    "        assert len(prog.step_epochs)==self.total_resizes\n",
    "        size = self.start_size\n",
    "        for i in prog.step_epochs:\n",
    "            size += self.increase\n",
    "        assert size==self.final_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively increase the initial image size of [128, 128] by 32 pixels every 1 epoch for 4 resizes.\n",
      "Starting at epoch 12 and finishing at epoch 15 for a final training size of [256, 256].\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "test = SyncthProgResizeTest(final_size=256, start_size=128, increase=32, step_size=1, first_epoch=12, last_epoch=15, total_resizes=4)\n",
    "learn = synth_learner(cbs=[ProgressiveResize(final_size=[256,256], increase_mode=IncreaseMode.Epoch), test])\n",
    "learn('after_create')\n",
    "learn.create_opt()\n",
    "learn.n_epoch=20\n",
    "learn('before_fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively increase the initial image size of [128, 128] by 8 pixels every 1 epoch for 16 resizes.\n",
      "Starting at epoch 34 and finishing at epoch 49 for a final training size of [256, 256].\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "test = SyncthProgResizeTest(final_size=256, start_size=128, increase=8, step_size=1, first_epoch=34, last_epoch=49, total_resizes=16)\n",
    "learn = synth_learner(cbs=[ProgressiveResize(final_size=[256,256], increase_mode=IncreaseMode.Epoch), test])\n",
    "learn('after_create')\n",
    "learn.create_opt()\n",
    "learn.n_epoch=66\n",
    "learn('before_fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively increase the initial image size of [192, 192] by 6 pixels every 2 epochs for 32 resizes.\n",
      "Starting at epoch 163 and finishing at epoch 225 for a final training size of [384, 384].\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "test = SyncthProgResizeTest(final_size=384, start_size=192, increase=6, step_size=2, first_epoch=163, last_epoch=225, total_resizes=32)\n",
    "learn = synth_learner(cbs=[ProgressiveResize(final_size=[384,384], increase_by=6, increase_mode=IncreaseMode.Epoch), test])\n",
    "learn('after_create')\n",
    "learn.create_opt()\n",
    "learn.n_epoch=300\n",
    "learn('before_fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively increase the initial image size of [192, 192] by 8 pixels every 3 epochs for 24 resizes.\n",
      "Starting at epoch 156 and finishing at epoch 225 for a final training size of [384, 384].\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "test = SyncthProgResizeTest(final_size=384, start_size=192, increase=8, step_size=3, first_epoch=156, last_epoch=225, total_resizes=24)\n",
    "learn = synth_learner(cbs=[ProgressiveResize(final_size=[384,384], increase_by=8, increase_mode=IncreaseMode.Epoch), test])\n",
    "learn('after_create')\n",
    "learn.create_opt()\n",
    "learn.n_epoch=300\n",
    "learn('before_fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively increase the initial image size of [128, 128] by 4 pixels every 0.6452 epochs for 32 resizes. \n",
      "Starting at epoch 40 and finishing at epoch 60 for a final training size of [256, 256].\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "learn = synth_learner(cbs=[ProgressiveResize(final_size=[256,256], increase_by=4, increase_mode=IncreaseMode.Batch)])\n",
    "learn('after_create')\n",
    "learn.create_opt()\n",
    "learn.n_epoch=80\n",
    "learn('before_fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively increase the initial image size of [192, 192] by 4 pixels every 1.596 epochs for 48 resizes. \n",
      "Starting at epoch 150 and finishing at epoch 225 for a final training size of [384, 384].\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "learn = synth_learner(cbs=[ProgressiveResize(final_size=[384,384], increase_by=4, increase_mode=IncreaseMode.Batch)])\n",
    "learn('after_create')\n",
    "learn.create_opt()\n",
    "learn.n_epoch=300\n",
    "learn('before_fit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, a ResNet50 is trained for 20 & 25 epochs on Imagenette at an image size of 224 pixels on a SageMaker Studio Lab Tesla T4 instance. Due to the short training run, `start` and `final` are set to 0.2 and 0.8, respectively.\n",
    "\n",
    "Despite increasing the image size early relative to the default hyperparameters, `ProgressiveResize` yields significant training time savings compared to training at full size. At a similar compute budget of roughly 14 minutes progressive resizing results with 87.8% accuracy compared to 86.2% accuracy with full sized training.\n",
    "\n",
    "| Mode              | Epochs | Time (Mins) | Accuracy |\n",
    "|:----------------- |:------:|:-----------:|:--------:|\n",
    "| Full Size         | 20     | 14.3        | 86.2%    |\n",
    "| Progressive Batch | 20     | 11.5        | 85.8%    |\n",
    "| Progressive Epoch | 20     | 10.5        | 85.6%    |\n",
    "| Progressive Batch | 25     | 13.9        | 87.8%    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|slow\n",
    "import time\n",
    "\n",
    "from fastcore.basics import num_cpus\n",
    "\n",
    "from fastai.data.external import URLs, untar_data\n",
    "from fastai.data.block import DataBlock, CategoryBlock\n",
    "from fastai.data.transforms import GrandparentSplitter, get_image_files, parent_label, Normalize\n",
    "from fastai.learner import Learner\n",
    "from fastai.vision.augment import Resize, aug_transforms\n",
    "from fastai.vision.core import imagenet_stats\n",
    "from fastai.vision.data import ImageBlock\n",
    "from fastai.vision.models import resnet50\n",
    "from fastxtend.callback.channelslast import *\n",
    "from fastxtend.metrics import *\n",
    "from fastxtend.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|slow\n",
    "free_gpu_memory(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|slow\n",
    "class ProgressiveResizeTest(Callback):\n",
    "    run_valid, order = True, ProgressiveResize.order-1\n",
    "    \n",
    "    def before_train(self):\n",
    "        self.progsize = self.learn.progressive_resize.current_size\n",
    "\n",
    "    def before_batch(self):\n",
    "        assert L(self.x.shape[-2:]) == L(self.progsize.tolist())\n",
    "            \n",
    "    def after_batch(self):\n",
    "        self.progsize = self.learn.progressive_resize.current_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progressive Resizing\n",
    "`ProgressiveResize` with the default `increase_mode=IncreaseMode.Batch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively increase the initial image size of [112, 112] by 4 pixels every 0.4444 epochs for 28 resizes. \n",
      "Starting at epoch 4 and finishing at epoch 16 for a final training size of [224, 224].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.122014</td>\n",
       "      <td>2.182029</td>\n",
       "      <td>0.227516</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.905108</td>\n",
       "      <td>2.192691</td>\n",
       "      <td>0.363057</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.690208</td>\n",
       "      <td>1.839630</td>\n",
       "      <td>0.487389</td>\n",
       "      <td>00:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.433910</td>\n",
       "      <td>1.606834</td>\n",
       "      <td>0.480764</td>\n",
       "      <td>00:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.358350</td>\n",
       "      <td>1.460259</td>\n",
       "      <td>0.555669</td>\n",
       "      <td>00:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.260721</td>\n",
       "      <td>1.169747</td>\n",
       "      <td>0.634904</td>\n",
       "      <td>00:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.092935</td>\n",
       "      <td>1.578764</td>\n",
       "      <td>0.542166</td>\n",
       "      <td>00:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.009341</td>\n",
       "      <td>1.090838</td>\n",
       "      <td>0.666242</td>\n",
       "      <td>00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.927261</td>\n",
       "      <td>1.372288</td>\n",
       "      <td>0.608917</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.873940</td>\n",
       "      <td>1.454040</td>\n",
       "      <td>0.623185</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.843020</td>\n",
       "      <td>0.977987</td>\n",
       "      <td>0.692994</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.768998</td>\n",
       "      <td>0.878254</td>\n",
       "      <td>0.725605</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.697043</td>\n",
       "      <td>0.772750</td>\n",
       "      <td>0.757962</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.609062</td>\n",
       "      <td>0.762754</td>\n",
       "      <td>0.765095</td>\n",
       "      <td>00:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.556327</td>\n",
       "      <td>0.709908</td>\n",
       "      <td>0.787771</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.482874</td>\n",
       "      <td>0.562079</td>\n",
       "      <td>0.822675</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.428716</td>\n",
       "      <td>0.499115</td>\n",
       "      <td>0.837707</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.372526</td>\n",
       "      <td>0.468393</td>\n",
       "      <td>0.857580</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.335535</td>\n",
       "      <td>0.467523</td>\n",
       "      <td>0.856306</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.303651</td>\n",
       "      <td>0.456422</td>\n",
       "      <td>0.858344</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training time: 687.7 s\n"
     ]
    }
   ],
   "source": [
    "#|slow\n",
    "#|cuda\n",
    "imagenette = untar_data(URLs.IMAGENETTE_320)\n",
    "\n",
    "with less_random():\n",
    "    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "                        splitter=GrandparentSplitter(valid_name='val'),\n",
    "                        get_items=get_image_files, get_y=parent_label,\n",
    "                        item_tfms=Resize(224),\n",
    "                        batch_tfms=[*aug_transforms(), Normalize.from_stats(*imagenet_stats)])\n",
    "    dls =  dblock.dataloaders(imagenette, bs=128, num_workers=num_cpus(), pin_memory=True)\n",
    "\n",
    "    # ProgressiveResizeTest is for additional testing and shouldn't be used\n",
    "    cbs = [ProgressiveResize(start=0.2, finish=0.8), ProgressiveResizeTest]\n",
    "    learn = Learner(dls, resnet50(num_classes=dls.c), metrics=Accuracy(), cbs=cbs).to_channelslast()\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    learn.fit_one_cycle(20, 3e-3)\n",
    "    total = time.perf_counter() - start\n",
    "    print(f'Total training time: {scale_time(total)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|slow\n",
    "#|cuda\n",
    "free_gpu_memory(learn, dls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ProgressiveResize` with `increase_mode=IncreaseMode.Epoch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively increase the initial image size of [112, 112] by 14 pixels every 1 epoch for 8 resizes.\n",
      "Starting at epoch 9 and finishing at epoch 16 for a final training size of [224, 224].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.122014</td>\n",
       "      <td>2.182029</td>\n",
       "      <td>0.227516</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.905108</td>\n",
       "      <td>2.192691</td>\n",
       "      <td>0.363057</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.690208</td>\n",
       "      <td>1.839630</td>\n",
       "      <td>0.487389</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.433910</td>\n",
       "      <td>1.606834</td>\n",
       "      <td>0.480764</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.286285</td>\n",
       "      <td>3.116283</td>\n",
       "      <td>0.494522</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.200042</td>\n",
       "      <td>1.254510</td>\n",
       "      <td>0.626242</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.105527</td>\n",
       "      <td>1.396334</td>\n",
       "      <td>0.572484</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.106385</td>\n",
       "      <td>1.339236</td>\n",
       "      <td>0.564586</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.982895</td>\n",
       "      <td>0.885877</td>\n",
       "      <td>0.723822</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.953809</td>\n",
       "      <td>1.116866</td>\n",
       "      <td>0.637197</td>\n",
       "      <td>00:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.843665</td>\n",
       "      <td>1.307829</td>\n",
       "      <td>0.638471</td>\n",
       "      <td>00:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.802480</td>\n",
       "      <td>1.135976</td>\n",
       "      <td>0.643822</td>\n",
       "      <td>00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.725916</td>\n",
       "      <td>0.727306</td>\n",
       "      <td>0.765350</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.624359</td>\n",
       "      <td>0.705979</td>\n",
       "      <td>0.771465</td>\n",
       "      <td>00:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.564622</td>\n",
       "      <td>0.571569</td>\n",
       "      <td>0.814267</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.488334</td>\n",
       "      <td>0.559648</td>\n",
       "      <td>0.821911</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.438291</td>\n",
       "      <td>0.491797</td>\n",
       "      <td>0.842548</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.381706</td>\n",
       "      <td>0.471103</td>\n",
       "      <td>0.853503</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.343013</td>\n",
       "      <td>0.457413</td>\n",
       "      <td>0.855032</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.308196</td>\n",
       "      <td>0.461995</td>\n",
       "      <td>0.855796</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training time: 632.1 s\n"
     ]
    }
   ],
   "source": [
    "#|slow\n",
    "#|cuda\n",
    "imagenette = untar_data(URLs.IMAGENETTE_320)\n",
    "\n",
    "with less_random():\n",
    "    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "                        splitter=GrandparentSplitter(valid_name='val'),\n",
    "                        get_items=get_image_files, get_y=parent_label,\n",
    "                        item_tfms=Resize(224),\n",
    "                        batch_tfms=[*aug_transforms(), Normalize.from_stats(*imagenet_stats)])\n",
    "    dls =  dblock.dataloaders(imagenette, bs=128, num_workers=num_cpus(), pin_memory=True)\n",
    "\n",
    "    # ProgressiveResizeTest is for additional testing and shouldn't be used\n",
    "    cbs = [ProgressiveResize(start=0.2, finish=0.8, increase_mode=IncreaseMode.Epoch), ProgressiveResizeTest]\n",
    "    learn = Learner(dls, resnet50(num_classes=dls.c), metrics=Accuracy(), cbs=cbs).to_channelslast()\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    learn.fit_one_cycle(20, 3e-3)\n",
    "    total = time.perf_counter() - start\n",
    "    print(f'Total training time: {scale_time(total)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|slow\n",
    "#|cuda\n",
    "free_gpu_memory(learn, dls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.016391</td>\n",
       "      <td>2.256088</td>\n",
       "      <td>0.241274</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.769153</td>\n",
       "      <td>3.686334</td>\n",
       "      <td>0.311083</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.529073</td>\n",
       "      <td>1.638564</td>\n",
       "      <td>0.471847</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.348534</td>\n",
       "      <td>1.439297</td>\n",
       "      <td>0.554650</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.181351</td>\n",
       "      <td>1.534368</td>\n",
       "      <td>0.530446</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.127666</td>\n",
       "      <td>1.914330</td>\n",
       "      <td>0.532994</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.025626</td>\n",
       "      <td>3.243782</td>\n",
       "      <td>0.461911</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.951557</td>\n",
       "      <td>1.247780</td>\n",
       "      <td>0.625987</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.887196</td>\n",
       "      <td>0.973012</td>\n",
       "      <td>0.694777</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.855779</td>\n",
       "      <td>1.063800</td>\n",
       "      <td>0.663694</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.786207</td>\n",
       "      <td>0.939424</td>\n",
       "      <td>0.721019</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.734812</td>\n",
       "      <td>0.883735</td>\n",
       "      <td>0.711083</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.650073</td>\n",
       "      <td>0.719170</td>\n",
       "      <td>0.777580</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.573434</td>\n",
       "      <td>0.835425</td>\n",
       "      <td>0.745732</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.527194</td>\n",
       "      <td>0.652105</td>\n",
       "      <td>0.806115</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.460990</td>\n",
       "      <td>0.541590</td>\n",
       "      <td>0.826752</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.402692</td>\n",
       "      <td>0.477356</td>\n",
       "      <td>0.851465</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.361319</td>\n",
       "      <td>0.463299</td>\n",
       "      <td>0.861146</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.321333</td>\n",
       "      <td>0.447778</td>\n",
       "      <td>0.863949</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.293849</td>\n",
       "      <td>0.447774</td>\n",
       "      <td>0.862420</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training time: 860.5 s\n"
     ]
    }
   ],
   "source": [
    "#|slow\n",
    "#|cuda\n",
    "imagenette = untar_data(URLs.IMAGENETTE_320)\n",
    "\n",
    "with less_random():\n",
    "    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "                        splitter=GrandparentSplitter(valid_name='val'),\n",
    "                        get_items=get_image_files, get_y=parent_label,\n",
    "                        item_tfms=Resize(224),\n",
    "                        batch_tfms=[*aug_transforms(),Normalize.from_stats(*imagenet_stats)])\n",
    "    dls =  dblock.dataloaders(imagenette, bs=128, num_workers=num_cpus(), pin_memory=True)\n",
    "\n",
    "    learn = Learner(dls, resnet50(num_classes=dls.c), metrics=Accuracy()).to_channelslast()\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    learn.fit_one_cycle(20, 3e-3)\n",
    "    total = time.perf_counter() - start\n",
    "    print(f'Total training time: {scale_time(total)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|slow\n",
    "#|cuda\n",
    "free_gpu_memory(learn, dls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progressive Resizing with Normalized Compute Budget\n",
    "`ProgressiveResize` with the default `increase_mode=IncreaseMode.Batch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively increase the initial image size of [112, 112] by 4 pixels every 0.5556 epochs for 28 resizes. \n",
      "Starting at epoch 5 and finishing at epoch 20 for a final training size of [224, 224].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.140554</td>\n",
       "      <td>2.323328</td>\n",
       "      <td>0.215541</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.924575</td>\n",
       "      <td>1.785451</td>\n",
       "      <td>0.406115</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.716462</td>\n",
       "      <td>2.180465</td>\n",
       "      <td>0.432357</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.513920</td>\n",
       "      <td>1.579820</td>\n",
       "      <td>0.514650</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.322239</td>\n",
       "      <td>3.293287</td>\n",
       "      <td>0.423949</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.261505</td>\n",
       "      <td>1.819019</td>\n",
       "      <td>0.530446</td>\n",
       "      <td>00:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.116962</td>\n",
       "      <td>1.576113</td>\n",
       "      <td>0.546497</td>\n",
       "      <td>00:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.065735</td>\n",
       "      <td>1.277910</td>\n",
       "      <td>0.610446</td>\n",
       "      <td>00:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.023897</td>\n",
       "      <td>1.102415</td>\n",
       "      <td>0.662420</td>\n",
       "      <td>00:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.937714</td>\n",
       "      <td>1.181165</td>\n",
       "      <td>0.627771</td>\n",
       "      <td>00:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.887837</td>\n",
       "      <td>1.122100</td>\n",
       "      <td>0.672102</td>\n",
       "      <td>00:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.846309</td>\n",
       "      <td>1.031982</td>\n",
       "      <td>0.667771</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.786773</td>\n",
       "      <td>0.884016</td>\n",
       "      <td>0.722293</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.693110</td>\n",
       "      <td>1.067082</td>\n",
       "      <td>0.687134</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.653226</td>\n",
       "      <td>0.762704</td>\n",
       "      <td>0.766624</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.614739</td>\n",
       "      <td>0.634725</td>\n",
       "      <td>0.803312</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.551392</td>\n",
       "      <td>0.664690</td>\n",
       "      <td>0.788790</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.509965</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.812994</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.454482</td>\n",
       "      <td>0.551515</td>\n",
       "      <td>0.826242</td>\n",
       "      <td>00:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.391981</td>\n",
       "      <td>0.463849</td>\n",
       "      <td>0.853248</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.354720</td>\n",
       "      <td>0.431224</td>\n",
       "      <td>0.864968</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.304196</td>\n",
       "      <td>0.443742</td>\n",
       "      <td>0.863439</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.261721</td>\n",
       "      <td>0.404501</td>\n",
       "      <td>0.880764</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.248861</td>\n",
       "      <td>0.414708</td>\n",
       "      <td>0.877962</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.242153</td>\n",
       "      <td>0.407861</td>\n",
       "      <td>0.878981</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training time: 832.8 s\n"
     ]
    }
   ],
   "source": [
    "#|slow\n",
    "#|cuda\n",
    "imagenette = untar_data(URLs.IMAGENETTE_320)\n",
    "\n",
    "with less_random():\n",
    "    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "                        splitter=GrandparentSplitter(valid_name='val'),\n",
    "                        get_items=get_image_files, get_y=parent_label,\n",
    "                        item_tfms=Resize(224),\n",
    "                        batch_tfms=[*aug_transforms(), Normalize.from_stats(*imagenet_stats)])\n",
    "    dls =  dblock.dataloaders(imagenette, bs=128, num_workers=num_cpus(), pin_memory=True)\n",
    "\n",
    "    # ProgressiveResizeTest is for additional testing and shouldn't be used\n",
    "    cbs = [ProgressiveResize(start=0.2, finish=0.8), ProgressiveResizeTest]\n",
    "    learn = Learner(dls, resnet50(num_classes=dls.c), metrics=Accuracy(), cbs=cbs).to_channelslast()\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    learn.fit_one_cycle(25, 3e-3)\n",
    "    total = time.perf_counter() - start\n",
    "    print(f'Total training time: {scale_time(total)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progressive Resizing Wandb Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exports\n",
    "try:\n",
    "    import wandb\n",
    "\n",
    "    @patch\n",
    "    def _wandb_log_after_resize(self:ProgressiveResize):\n",
    "        size = _to_size(self.current_size, step=1)\n",
    "        wandb.log({'progressive_resize_size': size[0]}, self.learn.wandb._wandb_step+step)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extend to other Loggers\n",
    "\n",
    "To extend to new loggers, follow the Weights & Biases code above and create patches for `ProgressiveResize` to add a `_{Callback.name}_log_after_resize`, where `Callback.name` is the [name of the logger callback](https://docs.fast.ai/callback.core.html#Callback.name).\n",
    "\n",
    "Then to use, pass `logger_callback='{Callback.name}'` to `Learner.profile()`. \n",
    "\n",
    "`ProgressiveResize` sets its `_log_after_resize` method to `f'_{self.logger_callback}_log_after_resize'`, which should match the patched method.\n",
    "\n",
    "```python\n",
    "self._log_after_resize = getattr(self, f'_{self.logger_callback}_log_after_resize', noop)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('fastai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "13823a492421f5f84f9059b75c88ebb0eec72237eae812caa6d0bf5be399ae62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
