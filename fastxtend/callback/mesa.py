# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/callback.mesa.ipynb (unless otherwise specified).


from __future__ import annotations


__all__ = ['MESALoss', 'MESACallback']

# Cell
#nbdev_comment from __future__ import annotations
from types import FunctionType

try:
    import timm
except ImportError:
    raise ImportError("timm is required to use MESACallback. Install via `pip install timm`.")

from timm.utils.model_ema import ModelEmaV2

from fastai.callback.core import Callback
from fastai.callback.mixup import reduce_loss
from fastai.callback.fp16 import MixedPrecision
from fastai.layers import NoneReduce

from ..multiloss import MultiLoss, MultiLossCallback, MixHandlerX
from ..imports import *

# Cell
class MESALoss(MultiLoss):
    def __init__(self,
        orig_loss:nn.Module|FunctionType,
        temp:Number=3, # Soften MESA targets by this temperature. τ in paper
        weight:float=0.8, # Weight of MESA loss. λ in paper
        reduction:str='mean' # PyTorch loss reduction
    ):
        store_attr(but='reduction')
        self.temp = 1/temp
        if hasattr(self.orig_loss, 'reduction'): self.orig_loss.reduction = reduction
        else: self.orig_loss = partial(self.orig_loss, reduction=reduction)
        self._mesa_loss = nn.KLDivLoss(log_target=True, reduction='batchmean' if reduction=='mean' else reduction)
        self.mesa_loss = False
        self.loss_names = L('orig_loss', 'mesa_loss')
        self.loss_funcs = self.loss_names # compatibility with MultiLossCallback
        self._zero, self._loss = torch.tensor(0., requires_grad=False), {}
        if getattr(self.orig_loss, 'y_int', False): self.y_int = True

    def forward(self, pred, *targs):
        targ, mesa_targ = targs
        self._loss[0] = self.orig_loss(pred, targ)
        if self.mesa_loss:
            self._loss[1] = self.weight*self._mesa_loss(F.log_softmax(self.temp*pred, dim=1), F.log_softmax(self.temp*mesa_targ, dim=1))
        else:
            self._loss[1] = self._zero
        return self._loss[0] + self._loss[1]

    def forward_mixup(self, pred, *targs):
        targ1, targ2, mesa_targ, lam = targs
        with NoneReduce(self.orig_loss) as ol:
            loss = torch.lerp(ol(pred, targ1), ol(pred, targ2), lam)
        self._loss[0] = reduce_loss(loss, getattr(self.orig_loss, 'reduction', 'mean'))
        if self.mesa_loss:
            self._loss[1] = self.weight*self._mesa_loss(F.log_softmax(self.temp*pred, dim=1), F.log_softmax(self.temp*mesa_targ, dim=1))
        else:
            self._loss[1] = self._zero
        return self._loss[0] + self._loss[1]

    @property
    def reduction(self): return self._reduction

    @reduction.setter
    def reduction(self, r):
        if hasattr(self.orig_loss, 'reduction'): self.orig_loss.reduction = r
        else: self.orig_loss = partial(self.orig_loss, reduction=r)
        self._mesa_loss.reduction = 'batchmean' if r=='mean' else r
        self._reduction = r

    @delegates(Module.to)
    def to(self, *args, **kwargs):
        device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(*args, **kwargs)
        self._zero.to(device)
        super(Module, self).to(*args, **kwargs)

    def activation(self, pred):
        "Returns `orig_loss` `activation`"
        return getattr(self.orig_loss, 'activation', noop)(pred)

    def decodes(self, pred):
        "Returns `orig_loss` `decodes`"
        return getattr(self.orig_loss, 'decodes', noop)(pred)

# Cell
class MESACallback(Callback):
    order = MixedPrecision.order+1
    "Callback to implment Memory-Efficient Sharpness-Aware training from https://arxiv.org/abs/2205.14083"
    def __init__(self,
        start_epoch:int=5, # Epoch to start MESA. Defaults to `start_pct` if None (index 1)
        temp:Number=3, # Soften MESA targets by this temperature. τ in paper
        weight:float=0.8, # Weight of MESA loss. λ in paper
        decay:float=0.9998, # EMA decay. β in paper
        reduction:str='mean', # PyTorch loss reduction
        cleanup:bool=True # Remove `MESACallback` after training
    ):
        store_attr()

    @torch.no_grad()
    def before_fit(self):
        if hasattr(self.learn, 'lr_finder') or hasattr(self, "gather_preds"): return
        self.start_epoch = max(self.start_epoch-1, 0)
        self._ema_forward = lambda x: 0
        self.orig_loss = self.learn.loss_func
        self.orig_loss_reduction = self.orig_loss.reduction if hasattr(self.orig_loss, 'reduction') else None
        self.learn.loss_func = MESALoss(self.orig_loss, self.temp, self.weight, self.reduction)
        self.learn.loss_func.to(getattr(self.dls, 'device', default_device()))
        self.ema_model = ModelEmaV2(self.learn.model, self.decay)
        mix = self.learn._grab_cbs(MixHandlerX)
        self._mixup = len(mix) > 0 and mix[0].stack_y

    def before_train(self):
        if self.start_epoch == self.epoch:
            if self._mixup: self.learn.loss_func_mixup.mesa_loss = True
            else:           self.learn.loss_func.mesa_loss = True
            self._ema_forward = self.ema_model.module

    @torch.no_grad()
    def after_pred(self):
        self.learn.yb = tuple([self.y, self._ema_forward(*self.xb)])

    def after_loss(self):
        y, _ = self.yb
        self.learn.yb = tuple([y])

    def after_batch(self):
        self.ema_model.update(self.learn.model)

    @torch.no_grad()
    def after_fit(self):
        if self.cleanup:
            if hasattr(self.orig_loss, 'reduction'):
                self.orig_loss.reduction = self.orig_loss_reduction
            self.learn.loss_func = self.orig_loss
            self.ema_model = None
            self.remove_cb(MESACallback)