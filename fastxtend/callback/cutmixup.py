# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/callback.cutmixup.ipynb (unless otherwise specified).

__all__ = ['CutMixUp', 'CutMixUpAugment']

# Cell
from fastai.basics import *
from fastai.data.transforms import IntToFloatTensor, Normalize
from fastai.callback.mixup import CutMix, MixUp, reduce_loss
from torch.distributions.beta import Beta

# Cell
class CutMixUp(MixUp, CutMix):
    "Combo implementation of https://arxiv.org/abs/1710.09412 and https://arxiv.org/abs/1905.04899"
    run_valid = False
    def __init__(self, mix_alpha=.4, cut_alpha=1., cutmix_ratio=1, mixup_ratio=1):
        MixUp.__init__(self, mix_alpha)
        CutMix.__init__(self, cut_alpha)
        self.mix_distrib = Beta(tensor(mix_alpha), tensor(mix_alpha))
        self.cut_distrib = Beta(tensor(cut_alpha), tensor(cut_alpha))
        self.ratio = mixup_ratio / (cutmix_ratio + mixup_ratio)

    def before_batch(self):
        if torch.rand(1) <= self.ratio: #mixup
            self.distrib = self.mix_distrib
            MixUp.before_batch(self)
        else:
            self.distrib = self.cut_distrib
            CutMix.before_batch(self)

# Cell
class CutMixUpAugment(MixUp, CutMix):
    "Combo implementation of https://arxiv.org/abs/1710.09412 and https://arxiv.org/abs/1905.04899 plus Augmentation"
    run_valid = False
    def __init__(self, mix_alpha=.4, cut_alpha=1., augment_ratio=1, cutmix_ratio=1, mixup_ratio=1, augs_only=None):
        MixUp.__init__(self, mix_alpha)
        CutMix.__init__(self, cut_alpha)
        self.mix_distrib = Beta(tensor(mix_alpha), tensor(mix_alpha))
        self.cut_distrib = Beta(tensor(cut_alpha), tensor(cut_alpha))
        self.aug_cutmix_ratio = augment_ratio / (augment_ratio + cutmix_ratio + mixup_ratio)
        if self.aug_cutmix_ratio == 1: self.cut_mix_ratio = 0
        else: self.cut_mix_ratio = mixup_ratio / (cutmix_ratio + mixup_ratio)
        self.augs_only = augs_only

    def before_fit(self):
        if self.augs_only is None: self.augs_only = (self.learn.n_epoch + 1)/self.learn.n_epoch
        elif self.augs_only >=1: self.augs_only = self.augs_only/self.learn.n_epoch
        else: self.augs_only = self.augs_only

        self._inttofloat_pipe = Pipeline([])
        self._norm_pipe = Pipeline([])

        # first copy transforms
        self._orig_pipe = self.dls.train.after_batch
        self._orig_pipe.split_idx = 0 # need to manually set split_idx for training augmentations to run

        # loop through existing transforms looking for IntToFloatTensor and Normalize
        for i in range(len(self.dls.train.after_batch.fs)):
            if isinstance(self.dls.train.after_batch[i], IntToFloatTensor):
                self._inttofloat_pipe = Pipeline([self.dls.train.after_batch[i]])
            elif isinstance(self.dls.train.after_batch[i], Normalize):
                self._norm_pipe = Pipeline([self.dls.train.after_batch[i]])

        # set existing transforms to an empty Pipeline
        self.dls.train.after_batch = Pipeline([])

    def before_batch(self):
        if self.augs_only >= self.learn.pct_train and torch.rand(1) >= self.aug_cutmix_ratio: # augs or mixup/cutmix
            self._aug = False
            self.learn.xb = self._inttofloat_pipe(self.xb) # apply inttofloat first
            if self.cut_mix_ratio > 0 and torch.rand(1) <= self.cut_mix_ratio: # mixup or cutmix
                self.distrib = self.mix_distrib
                MixUp.before_batch(self)
            else:
                self.distrib = self.cut_distrib
                CutMix.before_batch(self)
            self.learn.xb = self._norm_pipe(self.xb) # now normalize
        else:
            self._aug = True
            self.learn.xb = self._orig_pipe(self.xb) # original transforms

    def after_fit(self):
        self.dls.train.after_batch = self._orig_pipe

    def after_cancel_fit(self):
        self.after_fit()
        MixUp.after_cancel_fit(self)

    def lf(self, pred, *yb):
        if not self.training or self._aug: return self.old_lf(pred, *yb)
        with NoneReduce(self.old_lf) as lf:
            loss = torch.lerp(lf(pred,*self.yb1), lf(pred,*yb), self.lam)
        return reduce_loss(loss, getattr(self.old_lf, 'reduction', 'mean'))