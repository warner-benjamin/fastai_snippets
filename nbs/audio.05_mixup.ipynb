{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp audio.mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "# Contains code from:\n",
    "# fastai - Apache License 2.0 - Copyright (c) 2023 fast.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio MixUp & Friends\n",
    "\n",
    "> Apply MixUp, CutMix, and combination of both to audio waveforms before converting to Spectrogram or MelSpectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "\n",
    "from torch.distributions import Bernoulli\n",
    "from torch.distributions.beta import Beta\n",
    "\n",
    "from fastcore.transform import Pipeline\n",
    "\n",
    "from fastai.callback.mixup import reduce_loss\n",
    "from fastai.layers import NoneReduce\n",
    "\n",
    "from fastxtend.audio.data import MelSpectrogram, Spectrogram\n",
    "from fastxtend.audio.augment import AmplitudeToDB, AudioNormalize\n",
    "from fastxtend.multiloss import MixHandlerX\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AudioMixHandler -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AudioMixHandler(MixHandlerX):\n",
    "    \"Mixup base for `TensorAudio`\"\n",
    "    def __init__(self,\n",
    "        alpha:float=0.5, # Alpha & beta parametrization for `Beta` distribution\n",
    "        interp_label:bool|None=None # Blend or stack labels. Defaults to `loss_func.y_int` if None\n",
    "    ):\n",
    "        super().__init__(alpha, interp_label)\n",
    "\n",
    "    def before_fit(self):\n",
    "        waveforms, wave, spec = True, [], []\n",
    "        self._wave_pipe = Pipeline([])\n",
    "        self._spec_pipe = Pipeline([])\n",
    "\n",
    "        # first copy transforms\n",
    "        self._orig_pipe = self.dls.train.after_batch\n",
    "\n",
    "        # loop through existing transforms appending to pre_spec/post_spec until Spec/Mel is found\n",
    "        for i in range(len(self.dls.train.after_batch.fs)):\n",
    "            if isinstance(self.dls.train.after_batch[i], (Spectrogram, MelSpectrogram)):\n",
    "                waveforms = False\n",
    "            if waveforms:\n",
    "                wave.append(self.dls.train.after_batch[i])\n",
    "            else:\n",
    "                spec.append(self.dls.train.after_batch[i])\n",
    "\n",
    "        self._wave_pipe.add(wave)\n",
    "        self._spec_pipe.add(spec)\n",
    "\n",
    "        # set existing transforms to an empty Pipeline\n",
    "        self.dls.train.after_batch = Pipeline([])\n",
    "\n",
    "    def after_fit(self):\n",
    "        self.dls.train.after_batch = self._orig_pipe\n",
    "\n",
    "    def after_cancel_fit(self):\n",
    "        self.after_fit()\n",
    "        super().after_cancel_fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AudioMixUp -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AudioMixUp(AudioMixHandler):\n",
    "    \"Implementation of https://arxiv.org/abs/1710.09412 for `TensorAudio`\"\n",
    "    def __init__(self, alpha=0.5, stack_y=True):\n",
    "        super().__init__(alpha, stack_y)\n",
    "\n",
    "    def before_batch(self,\n",
    "        wave:bool=True # Apply waveform augmentations, used by `AudioCutMixAug`\n",
    "    ):\n",
    "        if wave: self.learn.xb = self._wave_pipe(self.xb)\n",
    "\n",
    "        lam = self.distrib.sample((self.y.size(0),)).squeeze().to(self.x.device)\n",
    "        lam = torch.stack([lam, 1-lam], 1)\n",
    "        self.lam = lam.max(1)[0]\n",
    "        shuffle = torch.randperm(self.y.size(0)).to(self.x.device)\n",
    "        xb1,self.yb1 = tuple(L(self.xb).itemgot(shuffle)),tuple(L(self.yb).itemgot(shuffle))\n",
    "        nx_dims = len(self.x.size())\n",
    "        self.learn.xb = tuple(L(xb1,self.xb).map_zip(torch.lerp,weight=unsqueeze(self.lam, n=nx_dims-1)))\n",
    "\n",
    "        if not self.stack_y:\n",
    "            ny_dims = len(self.y.size())\n",
    "            self.learn.yb = tuple(L(self.yb1,self.yb).map_zip(torch.lerp,weight=unsqueeze(self.lam, n=ny_dims-1)))\n",
    "\n",
    "        self.learn.xb = self._spec_pipe(self.xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AudioCutMix -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AudioCutMix(AudioMixHandler):\n",
    "    \"Implementation of https://arxiv.org/abs/1710.09412 for `TensorAudio`\"\n",
    "    def __init__(self,\n",
    "        alpha:float=1., # Alpha & beta parametrization for `Beta` distribution\n",
    "        uniform:bool=True, # Uniform patches across batch. True matches fastai CutMix\n",
    "        p:float=1., # Per Image probablily of applying CutMix if `uniform` is False\n",
    "        interp_label:bool|None=None # Blend or stack labels. Defaults to loss' `y_int` if None\n",
    "    ):\n",
    "        super().__init__(alpha, interp_label)\n",
    "        store_attr(but='alpha,interp_label')\n",
    "        if not uniform: self.bernoulli = Bernoulli(p)\n",
    "\n",
    "    def before_batch(self,\n",
    "        wave:bool=True # Apply waveform augmentations, used by `AudioCutMixAug`\n",
    "    ):\n",
    "        \"Add clips and blend labels from another random item in batch\"\n",
    "        if wave:\n",
    "            self.learn.xb = self._wave_pipe(self.xb)\n",
    "        if self.uniform:\n",
    "            self._uniform_cutmix()\n",
    "        else:\n",
    "            self._multi_cutmix()\n",
    "        self.learn.xb = self._spec_pipe(self.xb)\n",
    "\n",
    "    def _uniform_cutmix(self):\n",
    "        \"Add uniform clip and blend labels from another random item in batch\"\n",
    "        bs, _, X = self.x.size()\n",
    "        self.lam = self.distrib.sample((1,)).to(self.x.device)\n",
    "        shuffle = torch.randperm(bs).to(self.x.device)\n",
    "        xb1,self.yb1 = self.x[shuffle], tuple((self.y[shuffle],))\n",
    "        x1, x2 = self.rand_cut(X, self.lam)\n",
    "        self.learn.xb[0][..., x1:x2] = xb1[..., x1:x2]\n",
    "        self.lam = (1 - (x2-x1)/float(X))\n",
    "        if not self.stack_y:\n",
    "            ny_dims = len(self.y.size())\n",
    "            self.learn.yb = tuple(L(self.yb1,self.yb).map_zip(torch.lerp,weight=unsqueeze(self.lam, n=ny_dims-1)))\n",
    "\n",
    "    def _multi_cutmix(self):\n",
    "        \"Add random clip and blend labels from another random item in batch\"\n",
    "        bs, _, X = self.x.size()\n",
    "        samples = self.bernoulli.sample((bs,)).sum().int()\n",
    "        idxes = torch.multinomial(torch.ones(bs, device=self.x.device), samples).to(self.x.device)\n",
    "        self.lam = self.distrib.sample((samples,)).to(self.x.device)\n",
    "        shuffle = torch.randperm(bs).to(self.x.device)\n",
    "        xb1,self.yb1 = self.x[idxes][shuffle], tuple((self.y[idxes][shuffle],))\n",
    "        for i, idx in enumerate(idxes):\n",
    "            if 1 > self.lam[i] > 0:\n",
    "                x1, x2 = self.rand_cut(X, self.lam[i])\n",
    "                self.learn.xb[0][idx, ..., x1:x2] = xb1[i, ..., x1:x2]\n",
    "                self.lam[i] = (1 - (x2-x1)/float(X))\n",
    "        if not self.stack_y:\n",
    "            ny_dims = len(self.y.size())\n",
    "            self.learn.yb = tuple(L(self.yb1,self.yb).map_zip(torch.lerp,weight=unsqueeze(self.lam, n=ny_dims-1)))\n",
    "\n",
    "    def rand_cut(self,\n",
    "        X:int, # Input audio length\n",
    "        lam:Tensor # Lambda sample from Beta distribution\n",
    "    ) -> tuple[Tensor, Tensor]:\n",
    "        \"Return random audio clip coordinates\"\n",
    "        cut_rat = torch.sqrt(1. - lam).to(self.x.device)\n",
    "        cut_x = torch.round(X * cut_rat).type(torch.long).to(self.x.device)\n",
    "        cut_x = torch.div(cut_x, 2, rounding_mode='floor')\n",
    "        # uniform\n",
    "        cx = torch.randint(0, X, (1,)).to(self.x.device)\n",
    "        x1 = torch.clamp(cx - cut_x, 0, X)\n",
    "        x2 = torch.clamp(cx + cut_x, 0, X)\n",
    "        return x1, x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AudioCutMixUp -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AudioCutMixUp(AudioMixUp, AudioCutMix):\n",
    "    \"Implementation of Mixup or CutMix for `TensorAudio`\"\n",
    "    def __init__(self, mix_alpha=.4, cut_alpha=1., stack_y=True, cut_ratio=1, mix_ratio=1): \n",
    "        AudioMixUp.__init__(self, mix_alpha, stack_y)\n",
    "        AudioCutMix.__init__(self, cut_alpha, stack_y)\n",
    "        self.mix_distrib = Beta(tensor(mix_alpha), tensor(mix_alpha))\n",
    "        self.cut_distrib = Beta(tensor(cut_alpha), tensor(cut_alpha))\n",
    "        self.ratio = mix_ratio / (cut_ratio + mix_ratio)\n",
    "\n",
    "    def before_batch(self):\n",
    "        if torch.rand(1) <= self.ratio: #mixup\n",
    "            self.distrib = self.mix_distrib\n",
    "            AudioMixUp.before_batch(self)\n",
    "        else:\n",
    "            self.distrib = self.cut_distrib\n",
    "            AudioCutMix.before_batch(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AudioCutMixUpAugment -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AudioCutMixUpAugment(AudioMixUp, AudioCutMix):\n",
    "    \"Implementation of Mixup, CutMix, or Augment for `TensorAudio`\"\n",
    "    def __init__(self, mix_alpha=.4, cut_alpha=1., stack_y=True, aug_ratio=1, cut_ratio=1, mix_ratio=1, augs_only=None, wave_augs=False): \n",
    "        AudioMixUp.__init__(self, mix_alpha, stack_y)\n",
    "        AudioCutMix.__init__(self, cut_alpha, stack_y)\n",
    "        self.mix_distrib = Beta(tensor(mix_alpha), tensor(mix_alpha))\n",
    "        self.cut_distrib = Beta(tensor(cut_alpha), tensor(cut_alpha))\n",
    "        self.aug_cutmix_ratio = aug_ratio / (aug_ratio + cut_ratio + mix_ratio)\n",
    "        if self.aug_cutmix_ratio == 1: self.cut_mix_ratio = 0\n",
    "        else: self.cut_mix_ratio = mix_ratio / (cut_ratio + mix_ratio)\n",
    "        self.augs_only = augs_only\n",
    "        self.wave_augs = wave_augs\n",
    "\n",
    "    def before_fit(self):\n",
    "        if self.augs_only is None: self.augs_only = (self.learn.n_epoch + 1)/self.learn.n_epoch\n",
    "        elif self.augs_only >=1: self.augs_only = self.augs_only/self.learn.n_epoch\n",
    "        else: self.augs_only = self.augs_only\n",
    "\n",
    "        waveforms, wave, spec, norm = True, [], [], []\n",
    "        self._wave_pipe = Pipeline([])\n",
    "        self._spec_pipe = Pipeline([])\n",
    "        self._norm_pipe = Pipeline([])\n",
    "        \n",
    "        # first copy transforms\n",
    "        self._orig_pipe = self.dls.train.after_batch\n",
    "        self._orig_pipe.split_idx = 0 # need to manually set split_idx for training augmentations to run\n",
    "        \n",
    "        # loop through existing transforms appending to pre_spec/post_spec until Spec/Mel is found\n",
    "        for i in range(len(self.dls.train.after_batch.fs)):\n",
    "            if isinstance(self.dls.train.after_batch[i], (Spectrogram, MelSpectrogram)):\n",
    "                waveforms = False\n",
    "            \n",
    "            if waveforms: \n",
    "                wave.append(self.dls.train.after_batch[i])\n",
    "            else:\n",
    "                if isinstance(self.dls.train.after_batch[i], (AmplitudeToDB, AudioNormalize)):\n",
    "                    norm.append(self.dls.train.after_batch[i])\n",
    "                elif isinstance(self.dls.train.after_batch[i], (Spectrogram, MelSpectrogram)):\n",
    "                    spec.append(self.dls.train.after_batch[i])\n",
    "\n",
    "        self._wave_pipe.add(wave)\n",
    "        self._spec_pipe.add(spec)\n",
    "        self._norm_pipe.add(norm)\n",
    "\n",
    "        # set existing transforms to an empty Pipeline\n",
    "        self.dls.train.after_batch = Pipeline([])\n",
    "\n",
    "    def before_batch(self):\n",
    "        if self.augs_only >= self.learn.pct_train and torch.rand(1) >= self.aug_cutmix_ratio: # augs or mixup/cutmix\n",
    "            self._doaugs = False\n",
    "            if self.cut_mix_ratio > 0 and torch.rand(1) <= self.cut_mix_ratio: # mixup or cutmix\n",
    "                self.distrib = self.mix_distrib\n",
    "                AudioMixUp.before_batch(self, self.wave_augs)\n",
    "            else:\n",
    "                self.distrib = self.cut_distrib\n",
    "                AudioCutMix.before_batch(self, self.wave_augs)\n",
    "            self.learn.xb = self._norm_pipe(self.xb) # now normalize\n",
    "        else:\n",
    "            self._doaugs = True\n",
    "            self.learn.xb = self._orig_pipe(self.xb) # original transforms\n",
    "\n",
    "    def after_cancel_fit(self):\n",
    "        self.after_fit()\n",
    "        AudioMixUp.after_cancel_fit(self)\n",
    "\n",
    "    def solo_lf(self, pred, *yb):\n",
    "        \"`norm_lf` applies the original loss function on both outputs based on `self.lam` if applicable\"\n",
    "        if not self.training or self._doaugs: \n",
    "            return self.old_lf(pred, *yb)\n",
    "        with NoneReduce(self.old_lf) as lf:\n",
    "            loss = torch.lerp(lf(pred,*self.yb1), lf(pred,*yb), self.lam)\n",
    "        return reduce_loss(loss, getattr(self.old_lf, 'reduction', 'mean'))\n",
    "\n",
    "    def multi_lf(self, pred, *yb):\n",
    "        \"`norm_lf` applies the original loss function on both outputs based on `self.lam` if applicable\"\n",
    "        if not self.training or self._doaugs:\n",
    "            return self.learn.loss_func_mixup(pred, *yb)\n",
    "        else:\n",
    "            return self.learn.loss_func_mixup.forward_mixup(pred, *self.yb1, *yb, self.lam)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
