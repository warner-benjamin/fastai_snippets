{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp optimizer.finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "# Contains code from:\n",
    "# fastai - Apache License 2.0 - Copyright (c) 2023 fast.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Weight Decay\n",
    "> Optimizers with fine-tuning weight decay from Katherine Crowson's [AdamWFineTune](https://gist.github.com/crowsonkb/f646976de8033b371ea17cb9b1c1561f)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`FineTuneOpt` adds and additional optional weight decay `ft_wd` towards the starting value, to prevent overfitting to the new dataset during fine-tuning. This version uses fastai splitters to only apply the fine-tuning weight decay to the pre-trained model body and not the new head.\n",
    "\n",
    "All fastai optimizers are replicated here with the suffix FT to indicate they are `FineTuneOpt`.\n",
    "\n",
    "Early experimental results suggest `AdamFT` without weight decay might be equivalent to `AdamW` in vision fine-tuning performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "\n",
    "from fastcore.basics import GetAttr\n",
    "\n",
    "from fastai.optimizer import (Optimizer, _update, weight_decay, l2_reg, average_grad, sgd_step,\n",
    "                              momentum_step, average_sqr_grad, rms_prop_step, step_stat, adam_step,\n",
    "                              radam_step, qhadam_step, larc_layer_lr, larc_step, lamb_step)\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastxtend.test_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def tst_param(val, grad=None):\n",
    "    \"Create a tensor with `val` and a gradient of `grad` for testing\"\n",
    "    res = tensor([val]).float()\n",
    "    res.grad = tensor([val/10 if grad is None else grad]).float()\n",
    "    return res\n",
    "\n",
    "r = L.range(4)\n",
    "def tst_params(): return r.map(tst_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FineTuneOpt -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class FineTuneOpt(Optimizer):\n",
    "    \"\"\"\n",
    "    Modification of the base optimizer class for the fastai library, updating `params` with `cbs`\n",
    "\n",
    "    In combination with the `fine_tune_wd` callback, adds optional weight decay `ft_wd` towards the starting value, \n",
    "    to prevent overfitting to the new dataset during fine-tuning.\n",
    "\n",
    "    By default, will not apply to the fine-tuning head, just the pretrained body.\n",
    "\n",
    "    From: https://gist.github.com/crowsonkb/f646976de8033b371ea17cb9b1c1561f\n",
    "    \"\"\"\n",
    "    _keep_on_clear = ['force_train', 'do_wd']\n",
    "    def __init__(self,\n",
    "        params:Tensor, # Parameters and hyper parameters\n",
    "        cbs:list, # `Optimizer` callbacks\n",
    "        train_bn:bool=True, # Batch normalization is always trained\n",
    "        wd_ft_head:bool=False, # Apply fine tuning weight decay to model head\n",
    "        **defaults # Default values to set on hyper parameters\n",
    "    ):\n",
    "        super().__init__(params, cbs, train_bn, **defaults)\n",
    "        self.wd_ft_head, self.set_orig_p = wd_ft_head, True\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        if self.set_orig_p:\n",
    "            self.set_orig_p = False\n",
    "            n = slice(None) if self.wd_ft_head or len(self.param_lists)<=1 else slice(None, -1)\n",
    "            for p,pg,state,hyper in self.all_params(n):\n",
    "                state['orig_p'] = p.detach().clone()\n",
    "                self.state[p] = state\n",
    "        super().step(closure)\n",
    "\n",
    "    def clear_state(self):\n",
    "        self.set_orig_p = True\n",
    "        super().clear_state()\n",
    "\n",
    "    def state_dict(self):\n",
    "        state = super().state_dict()\n",
    "        state.update({'set_orig_p': self.set_orig_p})\n",
    "        return state\n",
    "\n",
    "    def load_state_dict(self, sd):\n",
    "        self.set_orig_p = sd.pop('set_orig_p')\n",
    "        super().load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# Test the initializtion of the FineTuneOpt optimizer\n",
    "opt = FineTuneOpt([1,2,3], noop)\n",
    "test_eq(opt.param_lists, [[1,2,3]])\n",
    "opt = FineTuneOpt(range(3), noop)\n",
    "test_eq(opt.param_lists, [[0,1,2]])\n",
    "opt = FineTuneOpt([[1,2],[3]], noop)\n",
    "test_eq(opt.param_lists, [[1,2],[3]])\n",
    "opt = FineTuneOpt(([o,o+1] for o in range(0,4,2)), noop)\n",
    "test_eq(opt.param_lists, [[0,1],[2,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# Test that callbacks have not changed\n",
    "def tst_arg(p, lr=0, **kwargs): return p\n",
    "tst_arg.defaults = dict(lr=1e-2)\n",
    "\n",
    "def tst_arg2(p, lr2=0, **kwargs): return p\n",
    "tst_arg2.defaults = dict(lr2=1e-3)\n",
    "\n",
    "def tst_arg3(p, mom=0, **kwargs): return p\n",
    "tst_arg3.defaults = dict(mom=0.9)\n",
    "\n",
    "def tst_arg4(p, **kwargs): return p\n",
    "\n",
    "opt = FineTuneOpt([1,2,3], [tst_arg,tst_arg2, tst_arg3])\n",
    "test_eq(opt.hypers, [{'lr2': 1e-3, 'mom': 0.9, 'lr': 1e-2}])\n",
    "opt = FineTuneOpt([1,2,3], tst_arg, lr=0.1)\n",
    "test_eq(opt.hypers, [{'lr': 0.1}])\n",
    "opt = FineTuneOpt([[1,2],[3]], tst_arg)\n",
    "test_eq(opt.hypers, [{'lr': 1e-2}, {'lr': 1e-2}])\n",
    "opt = FineTuneOpt([[1,2],[3]], tst_arg, lr=0.1)\n",
    "test_eq(opt.hypers, [{'lr': 0.1}, {'lr': 0.1}])\n",
    "\n",
    "opt = FineTuneOpt([[1,2],[3]], tst_arg, lr=[0.1,0.2])\n",
    "test_eq(opt.hypers, [{'lr': 0.1}, {'lr': 0.2}])\n",
    "opt = FineTuneOpt([[1,2],[3],[4]], tst_arg, lr=slice(1e-2))\n",
    "test_eq(opt.hypers, [{'lr': 1e-3}, {'lr': 1e-3}, {'lr': 1e-2}])\n",
    "opt = FineTuneOpt([[1,2],[3],[4]], tst_arg, lr=slice(1e-4,1e-2))\n",
    "test_eq(opt.hypers, [{'lr': 1e-4}, {'lr': 1e-3}, {'lr': 1e-2}])\n",
    "test_eq(opt.param_groups, [{'params': [1,2], 'lr': 1e-4}, {'params': [3], 'lr': 1e-3}, {'params': [4], 'lr': 1e-2}])\n",
    "test_fail(lambda: FineTuneOpt([[1,2],[3],[4]], tst_arg, lr=np.array([0.1,0.2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune WD Step -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def fine_tune_wd(p, lr, ft_wd, orig_p=None, do_wd=True, **kwargs):\n",
    "    \"Weight decay `p` towards the starting value `orig_p`\"\n",
    "    if do_wd and ft_wd !=0 and orig_p is not None:\n",
    "        p.lerp_(orig_p, lr*ft_wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "p = tst_param(1., 0.1)\n",
    "fine_tune_wd(p, 1., 0.5, tensor([0.5]))\n",
    "test_eq(p, tensor([0.75]))\n",
    "test_eq(p.grad, tensor([0.1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def SGDFT(params, lr, mom=0., wd=0., ft_wd=0., decouple_wd=True, wd_ft_head=False):\n",
    "    \"A `Optimizer` for SGD with `lr` and `mom` and `params`\"\n",
    "    cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "    cbs += [fine_tune_wd]\n",
    "    if mom != 0: cbs.append(average_grad)\n",
    "    cbs.append(sgd_step if mom==0 else momentum_step)\n",
    "    return FineTuneOpt(params, cbs, lr=lr, mom=mom, wd=wd, ft_wd=ft_wd, wd_ft_head=wd_ft_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#Vanilla SGD\n",
    "params = tst_params()\n",
    "opt = SGDFT(params, lr=0.1)\n",
    "opt.step()\n",
    "test_close([p.item() for p in params], [i*0.99 for i in range(4)])\n",
    "opt.step()\n",
    "test_close([p.item() for p in params], [i*0.98 for i in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#Vanilla SGD with FT_WD\n",
    "params = tst_params()\n",
    "opt = SGDFT(params, lr=0.1, ft_wd=0.1)\n",
    "opt.step()\n",
    "test_close([p.item() for p in params], [i*0.99 for i in range(4)])\n",
    "opt.step()\n",
    "test_close([p.item() for p in params], [i*0.9801 for i in range(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSPropFT -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def RMSPropFT(params, lr, sqr_mom=0.99, mom=0., wd=0., ft_wd=0., decouple_wd=True, wd_ft_head=False):\n",
    "    \"A `FineTuneOpt` for RMSProp with `lr`, `sqr_mom`, `mom` and `params`\"\n",
    "    cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "    cbs += [fine_tune_wd] + [average_sqr_grad] if mom==0. else [average_grad, average_sqr_grad]\n",
    "    cbs.append(rms_prop_step)\n",
    "    return FineTuneOpt(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, wd=wd, ft_wd=ft_wd, wd_ft_head=wd_ft_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdamFT -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def AdamFT(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0.01, ft_wd=0., decouple_wd=True, wd_ft_head=False):\n",
    "    \"A `FineTuneOpt` for Adam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n",
    "    cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "    cbs += [fine_tune_wd, partial(average_grad, dampening=True), average_sqr_grad, step_stat, adam_step]\n",
    "    return FineTuneOpt(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd, ft_wd=ft_wd, wd_ft_head=wd_ft_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAdamFT -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def RAdamFT(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0., ft_wd=0., beta=0., decouple_wd=True, wd_ft_head=False):\n",
    "    \"A `FineTuneOpt` for Adam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n",
    "    cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "    cbs += [fine_tune_wd, partial(average_grad, dampening=True), average_sqr_grad, step_stat, radam_step]\n",
    "    return FineTuneOpt(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd, ft_wd=ft_wd, beta=beta, wd_ft_head=wd_ft_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QHAdamFT -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def QHAdamFT(params, lr, mom=0.999, sqr_mom=0.999, nu_1=0.7, nu_2 = 1.0, eps=1e-8, wd=0., ft_wd=0., decouple_wd=True, wd_ft_head=False):\n",
    "    \"An `FineTuneOpt` for Adam with `lr`, `mom`, `sqr_mom`, `nus`, eps` and `params`\"\n",
    "    cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "    cbs += [fine_tune_wd, partial(average_grad, dampening=True), partial(average_sqr_grad, dampening=True), step_stat, qhadam_step]\n",
    "    return FineTuneOpt(params, cbs, lr=lr, nu_1=nu_1, nu_2=nu_2 ,\n",
    "                       mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd,\n",
    "                       ft_wd=ft_wd, wd_ft_head=wd_ft_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LarcFT -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def LarcFT(params, lr, mom=0.9, clip=True, trust_coeff=0.02, eps=1e-8, wd=0., ft_wd=0., decouple_wd=True, wd_ft_head=False):\n",
    "    \"A `FineTuneOpt` for Adam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n",
    "    cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "    cbs += [fine_tune_wd]\n",
    "    if mom!=0.: cbs.append(average_grad)\n",
    "    cbs += [partial(larc_layer_lr, clip=clip), larc_step]\n",
    "    return FineTuneOpt(params, cbs, lr=lr, mom=mom, trust_coeff=trust_coeff, eps=eps, wd=wd, ft_wd=ft_wd, wd_ft_head=wd_ft_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LambFT -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def LambFT(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0., ft_wd=0., decouple_wd=True, wd_ft_head=False):\n",
    "    \"A `FineTuneOpt` for Adam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n",
    "    cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "    cbs += [fine_tune_wd, partial(average_grad, dampening=True), average_sqr_grad, step_stat, lamb_step]\n",
    "    return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd, ft_wd=ft_wd, wd_ft_head=wd_ft_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LookaheadFT -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LookaheadFT(FineTuneOpt, GetAttr):\n",
    "    \"Wrap a `FineTuneOpt` `opt` in a Lookahead optimizer\"\n",
    "    _default='opt'\n",
    "    def __init__(self, opt, k=6, alpha=0.5):\n",
    "        store_attr('opt,k,alpha')\n",
    "        self._init_state()\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        if closure is not None: raise NotImplementedError(\"fastai optimizers currently do not support closure\")\n",
    "        if self.slow_weights is None: self._copy_weights()\n",
    "        self.opt.step()\n",
    "        self.count += 1\n",
    "        if self.count%self.k != 0: return\n",
    "        for slow_pg,fast_pg in zip(self.slow_weights,self.param_lists):\n",
    "            for slow_p,fast_p in zip(slow_pg,fast_pg):\n",
    "                slow_p.data.add_(fast_p.data-slow_p.data, alpha=self.alpha)\n",
    "                fast_p.data.copy_(slow_p.data)\n",
    "\n",
    "    def clear_state(self):\n",
    "        self.opt.clear_state()\n",
    "        self._init_state()\n",
    "\n",
    "    def state_dict(self):\n",
    "        state = self.opt.state_dict()\n",
    "        state.update({'count': self.count, 'slow_weights': self.slow_weights})\n",
    "        return state\n",
    "\n",
    "    def load_state_dict(self, sd):\n",
    "        self.count = sd.pop('count')\n",
    "        self.slow_weights = sd.pop('slow_weights')\n",
    "        self.opt.load_state_dict(sd)\n",
    "\n",
    "    def _init_state(self): self.count,self.slow_weights = 0,None\n",
    "    def _copy_weights(self): self.slow_weights = L(L(p.clone().detach() for p in pg) for pg in self.param_lists)\n",
    "\n",
    "    @property\n",
    "    def param_lists(self): return self.opt.param_lists\n",
    "    @param_lists.setter\n",
    "    def param_lists(self, v): self.opt.param_lists = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rangerFT -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@delegates(RAdamFT)\n",
    "def rangerFT(p, lr, mom=0.95, wd=0.01, ft_wd=0., eps=1e-6, **kwargs):\n",
    "    \"Convenience method for `LookaheadFT` with `RAdamFT`\"\n",
    "    return LookaheadFT(RAdamFT(p, lr=lr, mom=mom, wd=wd, ft_wd=ft_wd, eps=eps, **kwargs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('fastai')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
