{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp optimizer.torchscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "# Contains code from:\n",
    "# fastai - Apache License 2.0 - Copyright (c) 2023 fast.ai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchScript Optimizers\n",
    "> Fused fastai optimizers compiled with TorchScript for improved performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastxtend TorchScript optimizers are adapted from [fastai optimizers](https://docs.fast.ai/optimizer.html) and are modified to be compiled with TorchScript. They are 10 to 137 percent faster relative to fastai native optimizers depending on the model and optimizer, with complex optimizers like `QHAdam` recieving the largest performance increase.\n",
    "\n",
    "Unlike fastai optimizers, which are made of [multiple stepper callbacks](https://docs.fast.ai/optimizer.html#basic-steppers), TorchScript optimizers require a per-optimizer step so TorchScript can fuse the operation into as few CUDA calls as possible. All fastai optimizers have TorchScript implementations.\n",
    "\n",
    ":::{.callout-important}\n",
    "TorchScript optimizers have only been tested on PyTorch 1.12+ with NVFuser and are not guaranteed to work on older versions.\n",
    ":::\n",
    "\n",
    "TorchScript optimizers are faster due to vertical fusion across multiple Cuda calls. Using `xresnet50` and `SGD` with momentum as an example, a TorchScript fused `SGD` step would (hopefully) fuse all three Cuda calls (`mul`, `add`, and `add`) into one or two Cuda kernels resulting in 167 or 334 Cuda calls.\n",
    "\n",
    "```python\n",
    "@torch.jit.script\n",
    "def sgd_momentum_jit(param:Tensor, grad:Tensor, grad_avg:Tensor, lr:float):\n",
    "    grad_avg = grad_avg.mul(mom).add(grad)\n",
    "    param = param.add(grad_avg, alpha=-lr)\n",
    "```\n",
    "\n",
    "In contrast, a standard PyTorch optimizer would call the `SGD` with momentum step 167 times for a total of 501 inplace Cuda kernel calls:\n",
    "\n",
    "```python\n",
    "def simple_momentum_standard(param:Tensor, grad_avg:Tensor, lr:float):\n",
    "    grad_avg.mul_(mom).add_(grad)\n",
    "    param.add_(param.grad, alpha=-lr)\n",
    "```\n",
    "\n",
    "TorchScript optimizers are tested to be equal to fastai optimizers for 25 steps using [nbdev's GitHub CI](https://nbdev.fast.ai/tutorials/tutorial.html#check-out-your-workflows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "from typing import Optional, Dict\n",
    "\n",
    "from packaging.version import parse\n",
    "\n",
    "import fastai\n",
    "from fastai.optimizer import Optimizer\n",
    "\n",
    "from fastxtend.imports import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Utils -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "from fastai.optimizer import (weight_decay, l2_reg, average_grad, sgd_step, momentum_step,\n",
    "                              average_sqr_grad, rms_prop_step, step_stat, adam_step, radam_step,\n",
    "                              qhadam_step, larc_layer_lr, larc_step, lamb_step, Lookahead)\n",
    "\n",
    "from fastxtend.test_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# tests are copied with light modifications from fastai\n",
    "test_steps = 25\n",
    "\n",
    "def tst_param(val, grad=None):\n",
    "    \"Create a tensor with `val` and a gradient of `grad` for testing\"\n",
    "    res = tensor([val]).float()\n",
    "    res.grad = tensor([val/10 if grad is None else grad]).float()\n",
    "    return res\n",
    "\n",
    "def tst_params():\n",
    "    r = L.range(4)\n",
    "    return r.map(tst_param)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JitOptimizer -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "def _update(\n",
    "    state:dict,\n",
    "    new=None # New values to update `state` dict\n",
    "):\n",
    "    if isinstance(new, dict): state.update(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class JitOptimizer(Optimizer):\n",
    "    \"An `Optimizer` with a modified step for TorchScript optimizers\"\n",
    "    def __init__(self,\n",
    "        params:Listified[Tensor], # Model parameters\n",
    "        opt_step:Callable, # `JitOptimizer` optimizer step\n",
    "        decouple_wd:bool=False, # Use decoupled weight decay or L2 regularization, if applicable\n",
    "        **defaults\n",
    "    ):\n",
    "        if notmax_torch('1.12'):\n",
    "            warn(f'TorchScript optimizers are untested on PyTorch {torch.__verson__}, recommended to use 1.12 or newer')\n",
    "        if parse(fastai.__version__) < parse('2.7.11'):\n",
    "            super().__init__(params, [None], True, **defaults)\n",
    "        else:\n",
    "            super().__init__(params, [None], **defaults)\n",
    "        self.opt_step = opt_step\n",
    "        self.decouple_wd = decouple_wd\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        if closure is not None:\n",
    "            raise NotImplementedError(\"fastai optimizers currently do not support closure\")\n",
    "        for pg, hyper in zip(self.param_lists, self.hypers):\n",
    "            for p in pg:\n",
    "                if hasattr(p, 'grad') and p.grad is not None:\n",
    "                    state = self.state[p]\n",
    "                    _update(state, self.opt_step(p=p, g=p.grad, decouple_wd=self.decouple_wd, **{**state, **hyper}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(JitOptimizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@torch.jit.script\n",
    "def sgd_jit_step(p:Tensor, g:Tensor, decouple_wd:bool, lr:float, wd:float, mom:float,\n",
    "                 grad_avg:Optional[Tensor]=None, do_wd:bool=True, force_train:Optional[bool]=None):\n",
    "    \"SGD TorchScript compiled `JitOptimizer` step\"\n",
    "    param = p\n",
    "    grad = g\n",
    "    if do_wd and wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            param = param.mul(1-lr*wd)\n",
    "        else:\n",
    "            # l2_reg\n",
    "            grad = grad.add(param, alpha=wd)\n",
    "\n",
    "    if mom != 0:\n",
    "        if grad_avg is None:\n",
    "            grad_avg = torch.zeros_like(param, memory_format=torch.preserve_format)\n",
    "\n",
    "        # average_grad, dampening=False\n",
    "        grad_avg = grad_avg.mul(mom).add(grad)\n",
    "\n",
    "        # momentum_step\n",
    "        param = param.add(grad_avg, alpha=-lr)\n",
    "        p.set_(param)\n",
    "        g.set_(grad)\n",
    "        return {'grad_avg': grad_avg}\n",
    "    else:\n",
    "        # sgd_step\n",
    "        param = param.add(grad, alpha=-lr)\n",
    "        p.set_(param)\n",
    "        g.set_(grad)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(sgd_jit_step.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def SGD(params, lr, mom=0., wd=0., decouple_wd=True, jit=False):\n",
    "    \"A `Optimizer` or `JitOptimizer` for SGD with `lr` and `mom` and `params`\"\n",
    "    if jit:\n",
    "        return JitOptimizer(params, sgd_jit_step, lr=lr, mom=mom, wd=wd, decouple_wd=decouple_wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        if mom != 0: cbs.append(average_grad)\n",
    "        cbs.append(sgd_step if mom==0 else momentum_step)\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# Vanilla SGD\n",
    "params_org = tst_params()\n",
    "opt_org = SGD(params_org, lr=0.1, jit=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = tst_params()\n",
    "opt_jit = SGD(params_jit, lr=0.1, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "test_close([p.item() for p in params_org], [i*0.99 for i in range(4)])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_jit])\n",
    "\n",
    "opt_org.step()\n",
    "opt_jit.step()\n",
    "test_close([p.item() for p in params_org], [i*0.98 for i in range(4)])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_jit])\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_jit.step()\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_jit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# SGD with momentum\n",
    "params_org = tst_params()\n",
    "opt_org = SGD(params_org, lr=0.1, mom=0.9, jit=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = tst_params()\n",
    "opt_jit = SGD(params_jit, lr=0.1, mom=0.9, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "test_close([p.item() for p in params_org], [i*0.99 for i in range(4)])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_jit])\n",
    "\n",
    "opt_org.step()\n",
    "opt_jit.step()\n",
    "\n",
    "test_close([p.item() for p in params_org], [i*(1 - 0.1 * (0.1 + 0.1*1.9)) for i in range(4)])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_jit])\n",
    "\n",
    "for i,p in enumerate(params_org):\n",
    "    test_close(opt_org.state[p]['grad_avg'].item(), i*0.19)\n",
    "for i,p in enumerate(params_jit):\n",
    "    test_close(opt_jit.state[p]['grad_avg'].item(), i*0.19)\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_jit.step()\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_jit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# Weight decay\n",
    "params_org = tst_params()\n",
    "opt_org = SGD(params_org, lr=0.1, mom=0.9, wd=0.1, jit=False)\n",
    "opt_org.step()\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = tst_params()\n",
    "opt_jit = SGD(params_jit, lr=0.1, mom=0.9, wd=0.1, jit=True)\n",
    "opt_jit.step()\n",
    "opt_jit.step()\n",
    "\n",
    "test_close([p.item() for p in params_org], [i*0.9512 for i in range(4)])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_jit])\n",
    "\n",
    "opt_org.step()\n",
    "opt_org.step()\n",
    "opt_jit.step()\n",
    "opt_jit.step()\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_jit])\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_jit.step()\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_jit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# L2 reg\n",
    "params_org = tst_params()\n",
    "opt_org = SGD(params_org, lr=0.1, mom=0.9, wd=0.1, decouple_wd=False, jit=False)\n",
    "opt_org.step()\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = tst_params()\n",
    "opt_jit = SGD(params_jit, lr=0.1, mom=0.9, wd=0.1, decouple_wd=False, jit=True)\n",
    "opt_jit.step()\n",
    "opt_jit.step()\n",
    "\n",
    "test_close([p.item() for p in params_org], [i*0.9322 for i in range(4)])\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_jit])\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_jit.step()\n",
    "test_close([p.item() for p in params_org], [p.item() for p in params_jit])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@torch.jit.script\n",
    "def rmsprop_jit_step(p:Tensor, g:Tensor, lr:float, wd:float, mom:float, sqr_mom:float, eps:float, decouple_wd:bool,\n",
    "                     grad_avg:Optional[Tensor]=None, sqr_avg:Optional[Tensor]=None, do_wd:bool=True, force_train:Optional[bool]=None):\n",
    "    \"SGD TorchScript compiled `JitOptimizer` step\"\n",
    "    param = p\n",
    "    grad = g\n",
    "    if do_wd and wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            param = param.mul(1 - lr*wd)\n",
    "        else:\n",
    "            # l2_reg\n",
    "            grad = grad.add(param, alpha=wd)\n",
    "\n",
    "    if sqr_avg is None:\n",
    "        sqr_avg = torch.zeros_like(param, memory_format=torch.preserve_format)\n",
    "\n",
    "    if mom != 0:\n",
    "        if grad_avg is None:\n",
    "            grad_avg = torch.zeros_like(param, memory_format=torch.preserve_format)\n",
    "\n",
    "        # average_grad, dampening=False\n",
    "        grad_avg = grad_avg.mul(mom).add(grad)\n",
    "\n",
    "        # average_sqr_grad\n",
    "        sqr_avg = sqr_avg.mul(sqr_mom).addcmul(grad, grad, value=1-sqr_mom)\n",
    "\n",
    "        # rms_prop_step\n",
    "        param = param.addcdiv(grad_avg, sqr_avg.sqrt().add(eps), value=-lr)\n",
    "        p.set_(param)\n",
    "        g.set_(grad)\n",
    "        return {'grad_avg': grad_avg, 'sqr_avg': sqr_avg}\n",
    "    else:\n",
    "        # average_sqr_grad\n",
    "        sqr_avg = sqr_avg.mul(sqr_mom).addcmul(grad, grad, value=1-sqr_mom)\n",
    "\n",
    "        # rms_prop_step\n",
    "        param = param.addcdiv(grad, sqr_avg.sqrt().add(eps), value=-lr)\n",
    "        p.set_(param)\n",
    "        g.set_(grad)\n",
    "        return {'sqr_avg': sqr_avg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(rmsprop_jit_step.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def RMSProp(params, lr, sqr_mom=0.99, mom=0., eps=1e-8, wd=0., decouple_wd=True, jit=False):\n",
    "    if jit:\n",
    "        return JitOptimizer(params, rmsprop_jit_step, lr=lr, mom=mom, sqr_mom=sqr_mom,\n",
    "                            eps=eps, wd=wd, decouple_wd=decouple_wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        cbs += ([average_sqr_grad] if mom==0. else [average_grad, average_sqr_grad])\n",
    "        cbs.append(rms_prop_step)\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, wd=wd, eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# Without momentum\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = RMSProp(params_org, lr=0.1, jit=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = RMSProp(params_jit, lr=0.1, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "test_close(params_org[0], tensor([0.,1.,2.]))\n",
    "test_close(params_org[0], params_jit[0])\n",
    "\n",
    "opt_org.step()\n",
    "opt_jit.step()\n",
    "step = - 0.1 * 0.1 / (math.sqrt((0.01*0.99+0.01) * 0.1**2) + 1e-8)\n",
    "test_close(params_org[0], tensor([step, 1+step, 2+step]))\n",
    "test_close(params_org[0], params_jit[0])\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_jit.step()\n",
    "test_close(params_org[0], params_jit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# With momentum\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = RMSProp(params_org, lr=0.1, mom=0.9, jit=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = RMSProp(params_jit, lr=0.1, mom=0.9, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "test_close(params_org[0], tensor([0.,1.,2.]))\n",
    "test_close(params_org[0], params_jit[0])\n",
    "\n",
    "opt_org.step()\n",
    "opt_jit.step()\n",
    "step = - 0.1 * (0.1 + 0.9*0.1) / (math.sqrt((0.01*0.99+0.01) * 0.1**2) + 1e-8)\n",
    "test_close(params_org[0], tensor([step, 1+step, 2+step]))\n",
    "test_close(params_org[0], params_jit[0])\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_jit.step()\n",
    "test_close(params_org[0], params_jit[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@torch.jit.script\n",
    "def adam_jit_step(p:Tensor, g:Tensor, lr:float, wd:float, mom:float, sqr_mom:float, eps:float,\n",
    "                  decouple_wd:bool, grad_avg:Optional[Tensor]=None, sqr_avg:Optional[Tensor]=None,\n",
    "                  do_wd:bool=True, step:int=0, force_train:Optional[bool]=None):\n",
    "    \"Adam TorchScript compiled `JitOptimizer` step\"\n",
    "    param = p\n",
    "    grad = g\n",
    "    step += 1\n",
    "    if do_wd and wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            param = param.mul(1 - lr*wd)\n",
    "        else:\n",
    "            # l2_reg\n",
    "            grad = grad.add(param, alpha=wd)\n",
    "\n",
    "    if grad_avg is None:\n",
    "        grad_avg = torch.zeros_like(param, memory_format=torch.preserve_format)\n",
    "    if sqr_avg is None:\n",
    "        sqr_avg  = torch.zeros_like(param, memory_format=torch.preserve_format)\n",
    "\n",
    "    # average_grad, dampening=True\n",
    "    grad_avg = grad_avg.mul(mom).add(grad, alpha=1-mom)\n",
    "\n",
    "    # average_sqr_grad\n",
    "    sqr_avg = sqr_avg.mul(sqr_mom).addcmul(grad, grad, value=1-sqr_mom)\n",
    "\n",
    "    # adam_step\n",
    "    debias1 = 1-mom**step\n",
    "    debias2 = 1-sqr_mom**step\n",
    "    param = torch.addcdiv(param, grad_avg, torch.sqrt(sqr_avg/debias2) + eps, value = -lr / debias1)\n",
    "    p.set_(param)\n",
    "    g.set_(grad)\n",
    "\n",
    "    return torch.jit.annotate(Dict[str, Union[Tensor, int]], {'grad_avg': grad_avg, 'sqr_avg': sqr_avg, 'step': step})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(adam_jit_step.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def Adam(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0.01, decouple_wd=True, jit=False):\n",
    "    if jit:\n",
    "        return JitOptimizer(params, adam_jit_step, lr=lr, mom=mom, sqr_mom=sqr_mom,\n",
    "                            eps=eps, wd=wd, decouple_wd=decouple_wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, adam_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# fastai test\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = Adam(params_org, lr=0.1, wd=0, jit=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = Adam(params_jit, lr=0.1, wd=0, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "step = -0.1 * 0.1 / (math.sqrt(0.1**2) + 1e-8)\n",
    "test_close(params_org[0], tensor([1+step, 2+step, 3+step]))\n",
    "test_close([p[0].item() for p in params_org], [p[0].item() for p in params_jit])\n",
    "\n",
    "opt_org.step()\n",
    "opt_jit.step()\n",
    "test_close(params_org[0], tensor([1+2*step, 2+2*step, 3+2*step]), eps=1e-3)\n",
    "test_close([p[0].item() for p in params_org], [p[0].item() for p in params_jit])\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_jit.step()\n",
    "test_close([p[0].item() for p in params_org], [p[0].item() for p in params_jit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# test with weight decay\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = Adam(params_org, lr=0.1, wd=0.1, jit=False)\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = Adam(params_jit, lr=0.1, wd=0.1, jit=True)\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_jit.step()\n",
    "\n",
    "test_close([p[0].item() for p in params_org], [p[0].item() for p in params_jit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# test with l2 reg\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = Adam(params_org, lr=0.1, wd=0.1, decouple_wd=False, jit=False)\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = Adam(params_jit, lr=0.1, wd=0.1, decouple_wd=False, jit=True)\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_jit.step()\n",
    "\n",
    "test_close([p[0].item() for p in params_org], [p[0].item() for p in params_jit])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAdam -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@torch.jit.script\n",
    "def radam_jit_step(p:Tensor, g:Tensor, lr:float, wd:float, mom:float, sqr_mom:float, eps:float,\n",
    "                   decouple_wd:bool, grad_avg:Optional[Tensor]=None, sqr_avg:Optional[Tensor]=None,\n",
    "                   do_wd:bool=True, step:int=0, force_train:Optional[bool]=None):\n",
    "    \"RAdam TorchScript compiled `JitOptimizer` step\"\n",
    "    param = p\n",
    "    grad = g\n",
    "    step += 1\n",
    "    if do_wd and wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            param = param.mul(1 - lr*wd)\n",
    "        else:\n",
    "            # l2_reg\n",
    "            grad = grad.add(param, alpha=wd)\n",
    "\n",
    "    if grad_avg is None:\n",
    "        grad_avg = torch.zeros_like(param, memory_format=torch.preserve_format)\n",
    "    if sqr_avg is None:\n",
    "        sqr_avg  = torch.zeros_like(param, memory_format=torch.preserve_format)\n",
    "\n",
    "    # average_grad, dampening=True\n",
    "    grad_avg = grad_avg.mul(mom).add(grad, alpha=1-mom)\n",
    "\n",
    "    # average_sqr_grad\n",
    "    sqr_avg = sqr_avg.mul(sqr_mom).addcmul(grad, grad, value=1-sqr_mom)\n",
    "\n",
    "    # radam_step\n",
    "    debias1 = 1-mom**step\n",
    "    debias2 = 1-sqr_mom**step\n",
    "    r_inf = 2/(1-sqr_mom) - 1\n",
    "    r = r_inf - 2*step*sqr_mom**step/(1-sqr_mom**step)\n",
    "\n",
    "    if r > 5:\n",
    "        v = math.sqrt(((r-4)*(r-2)*r_inf)/((r_inf-4)*(r_inf-2)*r))\n",
    "        denom = torch.sqrt(sqr_avg/debias2).add(eps)\n",
    "        param = param.addcdiv(grad_avg, denom, value=-lr*v/debias1)\n",
    "    else:\n",
    "        param = param.add(grad_avg, alpha=-lr/debias1)\n",
    "    p.set_(param)\n",
    "    g.set_(grad)\n",
    "\n",
    "    return torch.jit.annotate(Dict[str, Union[Tensor, int]], {'grad_avg': grad_avg, 'sqr_avg': sqr_avg, 'step': step})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(radam_jit_step.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def RAdam(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0., beta=0., decouple_wd=True, jit=False):\n",
    "    if jit:\n",
    "        if beta != 0: warn('TorchScript RAdam does not use beta, set jit=False if beta!=0')\n",
    "        return JitOptimizer(params, radam_jit_step, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps,\n",
    "                            wd=wd, decouple_wd=decouple_wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, radam_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd, beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = RAdam(params_org, lr=0.1, jit=False)\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = RAdam(params_jit, lr=0.1, jit=True)\n",
    "\n",
    "#The r factor is lower than 5 during the first 5 steps so updates use the average of gradients (all the same)\n",
    "r_inf = 2/(1-0.99) - 1\n",
    "for i in range(5):\n",
    "    r = r_inf - 2*(i+1)*0.99**(i+1)/(1-0.99**(i+1))\n",
    "    assert r <= 5\n",
    "    opt_org.step()\n",
    "    opt_jit.step()\n",
    "p = tensor([0.95, 1.9, 2.85])\n",
    "test_close(params_org[0], p)\n",
    "test_close(params_org[0], params_jit[0])\n",
    "\n",
    "#The r factor is greater than 5 for the sixth step so we update with RAdam\n",
    "r = r_inf - 2*6*0.99**6/(1-0.99**6)\n",
    "assert r > 5\n",
    "opt_org.step()\n",
    "opt_jit.step()\n",
    "v = math.sqrt(((r-4) * (r-2) * r_inf)/((r_inf-4)*(r_inf-2)*r))\n",
    "step = -0.1*0.1*v/(math.sqrt(0.1**2) + 1e-8)\n",
    "test_close(params_org[0], p+step)\n",
    "test_close(params_org[0], params_jit[0])\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_jit.step()\n",
    "\n",
    "test_close(params_org[0], params_jit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# test with weight decay\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = RAdam(params_org, lr=0.1, wd=0.1, jit=False)\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = RAdam(params_jit, lr=0.1, wd=0.1, jit=True)\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_jit.step()\n",
    "\n",
    "test_close(params_org[0], params_jit[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QHAdam -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@torch.jit.script\n",
    "def qhadam_jit_step(p:Tensor, g:Tensor, lr:float, wd:float, mom:float, sqr_mom:float, eps:float,\n",
    "                    nu_1:float, nu_2:float, decouple_wd:bool, grad_avg:Optional[Tensor]=None,\n",
    "                    sqr_avg:Optional[Tensor]=None, do_wd:bool=True, step:int=0, force_train:Optional[bool]=None):\n",
    "    \"QHAdam TorchScript compiled `JitOptimizer` step\"\n",
    "    param = p\n",
    "    grad = g\n",
    "    step += 1\n",
    "    if do_wd and wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            param = param.mul(1 - lr*wd)\n",
    "        else:\n",
    "            # l2_reg\n",
    "            grad = grad.add(param, alpha=wd)\n",
    "\n",
    "    if grad_avg is None:\n",
    "        grad_avg = torch.zeros_like(param, memory_format=torch.preserve_format)\n",
    "    if sqr_avg is None:\n",
    "        sqr_avg  = torch.zeros_like(param, memory_format=torch.preserve_format)\n",
    "\n",
    "    # average_grad, dampening=True\n",
    "    grad_avg = grad_avg.mul(mom).add(grad, alpha=1-mom)\n",
    "\n",
    "    # average_sqr_grad\n",
    "    sqr_avg = sqr_avg.mul(sqr_mom).addcmul(grad, grad, value=1-sqr_mom)\n",
    "\n",
    "    # qhadam_step\n",
    "    debias1 = 1-mom**step\n",
    "    debias2 = 1-sqr_mom**step\n",
    "    param = param.addcdiv(((1-nu_1) * grad) + (nu_1 * (grad_avg / debias1)),\n",
    "                          torch.sqrt(((1 - nu_2) * (grad)**2) + (nu_2 * (sqr_avg / debias2))) + eps,\n",
    "                          value = -lr)\n",
    "    p.set_(param)\n",
    "    g.set_(grad)\n",
    "\n",
    "    return torch.jit.annotate(Dict[str, Union[Tensor, int]], {'grad_avg': grad_avg, 'sqr_avg': sqr_avg, 'step': step})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(qhadam_jit_step.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def QHAdam(params, lr, mom=0.999, sqr_mom=0.999, nu_1=0.7, nu_2=1.0, eps=1e-8, wd=0., decouple_wd=True, jit=True):\n",
    "    if jit:\n",
    "        return JitOptimizer(params, qhadam_jit_step, lr=lr, nu_1=nu_1, nu_2=nu_2, mom=mom,\n",
    "                            sqr_mom=sqr_mom, eps=eps, wd=wd, decouple_wd=decouple_wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, qhadam_step]\n",
    "        return Optimizer(params, cbs, lr=lr, nu_1=nu_1, nu_2=nu_2, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = QHAdam(params_org, lr=0.1, jit=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = QHAdam(params_jit, lr=0.1, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "step = -0.1 * (((1-0.7) * 0.1) + (0.7 * 0.1)) / (\n",
    "     math.sqrt(((1-1.0) * 0.1**2) + (1.0 * 0.1**2)) + 1e-8)\n",
    "\n",
    "test_close(params_org[0], tensor([1+step, 2+step, 3+step]))\n",
    "test_close(params_org[0], params_jit[0])\n",
    "\n",
    "opt_org.step()\n",
    "opt_jit.step()\n",
    "test_close(params_org[0], tensor([1+2*step, 2+2*step, 3+2*step]), eps=1e-3)\n",
    "test_close(params_org[0], params_jit[0])\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_jit.step()\n",
    "\n",
    "test_close(params_org[0], params_jit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# test with weight decay\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = QHAdam(params_org, lr=0.1, wd=0.1, jit=False)\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = QHAdam(params_jit, lr=0.1, wd=0.1, jit=True)\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_jit.step()\n",
    "\n",
    "test_close(params_org[0], params_jit[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LARS/LARC -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@torch.jit.script\n",
    "def larc_jit_step(p:Tensor, g:Tensor, lr:float, wd:float, mom:float, eps:float, trust_coeff:float, decouple_wd:bool,\n",
    "                  clip:bool, grad_avg:Optional[Tensor]=None, do_wd:bool=True, dampening:bool=False, force_train:Optional[bool]=None):\n",
    "    \"LARC TorchScript compiled `JitOptimizer` step\"\n",
    "    param = p\n",
    "    grad = g\n",
    "    if do_wd and wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            param = param.mul(1 - lr*wd)\n",
    "        else:\n",
    "            # l2_reg\n",
    "            grad = grad.add(param, alpha=wd)\n",
    "\n",
    "    # larc_layer_lr\n",
    "    p_norm = torch.norm(param)\n",
    "    g_norm = torch.norm(grad)\n",
    "    local_lr = lr*trust_coeff * (p_norm) / (g_norm + p_norm * wd + eps)\n",
    "    if clip:\n",
    "        lr = min(local_lr, lr)\n",
    "    else:\n",
    "        lr = local_lr\n",
    "\n",
    "    if mom != 0:\n",
    "        if grad_avg is None:\n",
    "            grad_avg = torch.zeros_like(param, memory_format=torch.preserve_format)\n",
    "\n",
    "        # average_grad, dampening=False\n",
    "        grad_avg = grad_avg.mul(mom).add(grad)\n",
    "\n",
    "        # larc_step\n",
    "        param = torch.add(param, grad_avg, alpha=-lr)\n",
    "    else:\n",
    "        # larc_step\n",
    "        param = torch.add(param, grad, alpha=-lr)\n",
    "\n",
    "    p.set_(param)\n",
    "    g.set_(grad)\n",
    "    return {'grad_avg': grad_avg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(larc_jit_step.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def Larc(params, lr, mom=0.9, clip=True, trust_coeff=0.02, eps=1e-8, wd=0., decouple_wd=True, jit=False):\n",
    "    if jit:\n",
    "        cb = partial(larc_jit_step, clip=clip)\n",
    "        return JitOptimizer(params, cb, lr=lr, mom=mom, trust_coeff=trust_coeff,\n",
    "                            eps=eps, wd=wd, decouple_wd=decouple_wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        if mom!=0.: cbs.append(average_grad)\n",
    "        cbs += [partial(larc_layer_lr, clip=clip), larc_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, trust_coeff=trust_coeff, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params_org = [tst_param([1,2,3], [0.1,0.2,0.3]), tst_param([1,2,3], [0.01,0.02,0.03])]\n",
    "opt_org = Larc(params_org, lr=0.1, jit=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = [tst_param([1,2,3], [0.1,0.2,0.3]), tst_param([1,2,3], [0.01,0.02,0.03])]\n",
    "opt_jit = Larc(params_jit, lr=0.1, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "#First param local lr is 0.02 < lr so it's not clipped\n",
    "test_close(opt_org.state[params_org[0]]['local_lr'], 0.02)\n",
    "#Second param local lr is 0.2 > lr so it's clipped\n",
    "test_eq(opt_org.state[params_org[1]]['local_lr'], 0.1)\n",
    "\n",
    "test_close(params_org[0], tensor([0.998,1.996,2.994]))\n",
    "test_close(params_org[0], params_jit[0])\n",
    "test_close(params_org[1], tensor([0.999,1.998,2.997]))\n",
    "test_close(params_org[1], params_jit[1])\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_jit.step()\n",
    "\n",
    "test_close(params_org[0], params_jit[0])\n",
    "test_close(params_org[1], params_jit[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params_org = [tst_param([1,2,3], [0.1,0.2,0.3]), tst_param([1,2,3], [0.01,0.02,0.03])]\n",
    "opt_org = Larc(params_org, lr=0.1, clip=False, jit=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = [tst_param([1,2,3], [0.1,0.2,0.3]), tst_param([1,2,3], [0.01,0.02,0.03])]\n",
    "opt_jit = Larc(params_jit, lr=0.1, clip=False, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "#No clipping\n",
    "test_close(opt_org.state[params_org[0]]['local_lr'], 0.02)\n",
    "test_close(opt_org.state[params_org[1]]['local_lr'], 0.2)\n",
    "test_close(params_org[0], tensor([0.998,1.996,2.994]))\n",
    "test_close(params_org[0], params_jit[0])\n",
    "test_close(params_org[1], tensor([0.998,1.996,2.994]))\n",
    "test_close(params_org[1], params_jit[1])\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_jit.step()\n",
    "\n",
    "test_close(params_org[0], params_jit[0])\n",
    "test_close(params_org[1], params_jit[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# test with weight decay\n",
    "params_org = [tst_param([1,2,3], [0.1,0.2,0.3]), tst_param([1,2,3], [0.01,0.02,0.03])]\n",
    "opt_org = Larc(params_org, lr=0.1, wd=0.1, jit=False)\n",
    "\n",
    "params_jit = [tst_param([1,2,3], [0.1,0.2,0.3]), tst_param([1,2,3], [0.01,0.02,0.03])]\n",
    "opt_jit = Larc(params_jit, lr=0.1, wd=0.1, jit=True)\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_jit.step()\n",
    "\n",
    "test_close(params_org[0], params_jit[0])\n",
    "test_close(params_org[1], params_jit[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAMB -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@torch.jit.script\n",
    "def lamb_jit_step(p:Tensor, g:Tensor, lr:float, wd:float, mom:float, sqr_mom:float, eps:float,\n",
    "                  decouple_wd:bool, grad_avg:Optional[Tensor]=None, sqr_avg:Optional[Tensor]=None,\n",
    "                  do_wd:bool=True, step:int=0, force_train:Optional[bool]=None):\n",
    "    \"LAMB TorchScript compiled `JitOptimizer` step\"\n",
    "    param = p\n",
    "    grad = g\n",
    "    step += 1\n",
    "    if do_wd and wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            param = param.mul(1 - lr*wd)\n",
    "        else:\n",
    "            # l2_reg\n",
    "            grad = grad.add(param, alpha=wd)\n",
    "\n",
    "    if grad_avg is None:\n",
    "        grad_avg = torch.zeros_like(param, memory_format=torch.preserve_format)\n",
    "    if sqr_avg is None:\n",
    "        sqr_avg  = torch.zeros_like(param, memory_format=torch.preserve_format)\n",
    "\n",
    "    # average_grad, dampening=True\n",
    "    grad_avg = grad_avg.mul(mom).add(grad, alpha=1-mom)\n",
    "\n",
    "    # average_sqr_grad\n",
    "    sqr_avg = sqr_avg.mul(sqr_mom).addcmul(grad, grad, value=1-sqr_mom)\n",
    "\n",
    "    # lamb_step\n",
    "    debias1 = 1-mom**step\n",
    "    debias2 = 1-sqr_mom**step\n",
    "\n",
    "    r1 = param.norm(2)\n",
    "    lstep = (grad_avg/debias1) / ((sqr_avg/debias2).sqrt()+eps)\n",
    "    r2 = lstep.norm(2)\n",
    "\n",
    "    if r1 == 0 or r2 == 0:\n",
    "        param = param.add(lstep, alpha=-lr)\n",
    "    else:\n",
    "        q = min(r1/r2, 10.)\n",
    "        param = param.add(lstep, alpha=-lr*q)\n",
    "\n",
    "    p.set_(param)\n",
    "    g.set_(grad)\n",
    "\n",
    "    return torch.jit.annotate(Dict[str, Union[Tensor, int]], {'grad_avg': grad_avg, 'sqr_avg': sqr_avg, 'step': step})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(lamb_jit_step.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def Lamb(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0., decouple_wd=True, jit=False):\n",
    "    if jit:\n",
    "        return JitOptimizer(params, lamb_jit_step, lr=lr, mom=mom, sqr_mom=sqr_mom,\n",
    "                            eps=eps, wd=wd, decouple_wd=decouple_wd)\n",
    "    else:\n",
    "        cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "        cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, lamb_step]\n",
    "        return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = Lamb(params_org, lr=0.1, jit=False)\n",
    "opt_org.step()\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = Lamb(params_jit, lr=0.1, jit=True)\n",
    "opt_jit.step()\n",
    "\n",
    "test_close(params_org[0], tensor([0.7840,1.7840,2.7840]), eps=1e-3)\n",
    "test_close(params_org[0], params_jit[0])\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_jit.step()\n",
    "\n",
    "test_close(params_org[0], params_jit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# test with weight decay\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = Lamb(params_org, lr=0.1, wd=0.1, jit=False)\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = Lamb(params_jit, lr=0.1, wd=0.1, jit=True)\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_jit.step()\n",
    "\n",
    "test_close(params_org[0], params_jit[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookahead -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class JitLookahead(JitOptimizer):\n",
    "    \"An `JitOptimizer` with a modified step for Lookahead TorchScript optimizers\"\n",
    "    def __init__(self,\n",
    "        params:Listified[Tensor], # Model parameters\n",
    "        opt_step:Callable, # `JitLookahead` optimizer step\n",
    "        decouple_wd:bool=False, # Use decoupled weight decay or L2 regularization, if applicable\n",
    "        **defaults\n",
    "    ):\n",
    "        super().__init__(params, opt_step, decouple_wd, **defaults)\n",
    "        self._init_state()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        self.count += 1\n",
    "        if closure is not None: raise NotImplementedError(\"fastai optimizers currently do not support closure\")\n",
    "        for pg, hyper in zip(self.param_lists, self.hypers):\n",
    "            for p in pg:\n",
    "                if hasattr(p, 'grad') and p.grad is not None:\n",
    "                    _update(self.state[p], self.opt_step(p, p.grad, decouple_wd=self.decouple_wd, **{**self.state[p], **hyper}, count=self.count))\n",
    "\n",
    "    def clear_state(self):\n",
    "        super().clear_state()\n",
    "        self._init_state()\n",
    "\n",
    "    def state_dict(self):\n",
    "        state = super().state_dict()\n",
    "        state.update({'count': self.count})\n",
    "        return state\n",
    "\n",
    "    def load_state_dict(self, sd):\n",
    "        self.count = sd.pop('count')\n",
    "        super().load_state_dict(sd)\n",
    "\n",
    "    def _init_state(self):\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(JitLookahead)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ranger -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "@torch.jit.script\n",
    "def ranger_jit_step(p:Tensor, g:Tensor, lr:float, wd:float, mom:float, sqr_mom:float, eps:float, decouple_wd:bool,\n",
    "                    count:int, k:int, alpha:float, grad_avg:Optional[Tensor]=None, sqr_avg:Optional[Tensor]=None,\n",
    "                    slow_p:Optional[Tensor]=None, do_wd:bool=True, step:int=0, force_train:Optional[bool]=None):\n",
    "    \"ranger TorchScript compiled `JitOptimizer` step\"\n",
    "    param = p\n",
    "    grad = g\n",
    "    step += 1\n",
    "    if slow_p is None:\n",
    "        slow_p = param.detach().clone()\n",
    "\n",
    "    if do_wd and wd != 0:\n",
    "        if decouple_wd:\n",
    "            # weight_decay\n",
    "            param = param.mul(1 - lr*wd)\n",
    "        else:\n",
    "            # l2_reg\n",
    "            grad = grad.add(param, alpha=wd)\n",
    "\n",
    "    if grad_avg is None:\n",
    "        grad_avg = torch.zeros_like(param, memory_format=torch.preserve_format)\n",
    "    if sqr_avg is None:\n",
    "        sqr_avg  = torch.zeros_like(param, memory_format=torch.preserve_format)\n",
    "\n",
    "    # average_grad, dampening=True\n",
    "    grad_avg = grad_avg.mul(mom).add(grad, alpha=1-mom)\n",
    "\n",
    "    # average_sqr_grad\n",
    "    sqr_avg = sqr_avg.mul(sqr_mom).addcmul(grad, grad, value=1-sqr_mom)\n",
    "\n",
    "    # radam_step\n",
    "    debias1 = 1-mom**step\n",
    "    debias2 = 1-sqr_mom**step\n",
    "    r_inf = 2/(1-sqr_mom)-1\n",
    "    r = r_inf - 2*step*sqr_mom**step/(1-sqr_mom**step)\n",
    "\n",
    "    if r > 5:\n",
    "        v = math.sqrt(((r-4)*(r-2)*r_inf)/((r_inf-4)*(r_inf-2)*r))\n",
    "        denom = torch.sqrt(sqr_avg/debias2).add(eps)\n",
    "        param = param.addcdiv(grad_avg, denom, value=-lr*v/debias1)\n",
    "    else:\n",
    "        param = param.add(grad_avg, alpha=-lr/debias1)\n",
    "\n",
    "    # lookahead step\n",
    "    if count % k != 0:\n",
    "        p.set_(param)\n",
    "        g.set_(grad)\n",
    "    else:\n",
    "        slow_p = slow_p.add(param.sub(slow_p), alpha=alpha)\n",
    "        p.set_(slow_p)\n",
    "        g.set_(grad)\n",
    "    return torch.jit.annotate(Dict[str, Union[Tensor, int]], {'grad_avg': grad_avg, 'sqr_avg': sqr_avg, 'step': step, 'slow_p': slow_p})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ranger_jit_step.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "def Ranger(params, lr, mom=0.95, sqr_mom=0.99, eps=1e-6, wd=0.01, k=6, alpha=0.5, decouple_wd=True, jit=False):\n",
    "    if jit:\n",
    "        return JitLookahead(params, ranger_jit_step, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps,\n",
    "                            wd=wd, k=k, alpha=alpha, decouple_wd=decouple_wd)\n",
    "    else:\n",
    "        return Lookahead(RAdam(params, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd,\n",
    "                               decouple_wd=decouple_wd, jit=False),\n",
    "                         k=k, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "po = tensor([1,2,3])\n",
    "\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = Ranger(params_org, lr=0.1, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0., jit=False)\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = Ranger(params_jit, lr=0.1, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0., jit=True)\n",
    "\n",
    "#The first 5 steps are normal RAdam steps\n",
    "#The r factor is lower than 5 during the first 5 steps so updates use the average of gradients (all the same)\n",
    "r_inf = 2/(1-0.99) - 1\n",
    "for i in range(5):\n",
    "    r = r_inf - 2*(i+1)*0.99**(i+1)/(1-0.99**(i+1))\n",
    "    assert r <= 5\n",
    "    opt_org.step()\n",
    "    opt_jit.step()\n",
    "p = tensor([0.95, 1.9, 2.85])\n",
    "test_close(params_org[0], p)\n",
    "test_close(params_org[0], params_jit[0])\n",
    "\n",
    "#The r factor is greater than 5 for the sixth step so we update with RAdam\n",
    "r = r_inf - 2*6*0.99**6/(1-0.99**6)\n",
    "assert r > 5\n",
    "opt_org.step()\n",
    "opt_jit.step()\n",
    "v = math.sqrt(((r-4) * (r-2) * r_inf)/((r_inf-4)*(r_inf-2)*r))\n",
    "step = -0.1*0.1*v/(math.sqrt(0.1**2) + 1e-8)\n",
    "\n",
    "#Since k=6, sixth step is a moving average of the 6 RAdam steps with the initial weight\n",
    "test_close(params_org[0], po+((p+step)-po)*0.5)\n",
    "test_close(params_org[0], params_jit[0])\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_jit.step()\n",
    "\n",
    "test_close(params_org[0], params_jit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# test with weight decay\n",
    "params_org = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_org = Ranger(params_org, lr=0.1, wd=0.1, jit=False)\n",
    "\n",
    "params_jit = tst_param([1,2,3], [0.1,0.2,0.3])\n",
    "opt_jit = Ranger(params_jit, lr=0.1, wd=0.1, jit=True)\n",
    "\n",
    "for i in range(test_steps):\n",
    "    opt_org.step()\n",
    "    opt_jit.step()\n",
    "\n",
    "test_close(params_org[0], params_jit[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
