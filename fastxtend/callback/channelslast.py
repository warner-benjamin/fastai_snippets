# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/callback.channelslast.ipynb (unless otherwise specified).

__all__ = ['ChannelsLastTfm', 'ChannelsLastCallback']

# Cell
import torch
from fastai.torch_core import TensorImage, TensorMask
from fastai.learner import Learner
from fastai.vision.augment import DisplayedTransform
from fastai.callback.core import Callback
from fastai.callback.fp16 import MixedPrecision
from fastai.callback.mixup import MixHandler
from fastai.basics import Pipeline
from fastcore.foundation import patch
from fastcore.meta import delegates
from torch.cuda.amp import GradScaler

# Cell
class ChannelsLastTfm(DisplayedTransform):
    "Sets image inputs to `channels_last` format. For use in ChannelsLastCallback"
    def encodes(self, x:(TensorImage, TensorMask)):
        return x.to(memory_format=torch.channels_last)

# Cell
class ChannelsLastCallback(Callback):
    "Channels last training using PyTorch's Channels Last Memory Format (beta)"
    order = MixHandler.order+1
    def __init__(self):
        self._channels_last = Pipeline([ChannelsLastTfm()])

    def before_fit(self):
        self.learn.model.to(memory_format=torch.channels_last)

    def before_batch(self):
        self.learn.xb = self._channels_last(self.xb)

# Cell
@patch
@delegates(GradScaler)
def to_channelslast(self:Learner, to_fp16=True, **kwargs):
    "Set `Learner` and inputs to `channels_last` format and Mixed Precision by default"
    if to_fp16 and not hasattr(self, 'mixed_precision'):
        return self.add_cbs([ChannelsLastCallback(), MixedPrecision(**kwargs)])
    else:
        return self.add_cb(ChannelsLastCallback())

# Cell
@patch
def to_contiguous(self:Learner, to_fp32=False):
    "Set `Learner` and inputs to `contiguous_format` (default format), optionally to single precision"
    self.model.to(memory_format=torch.contiguous_format)
    if to_fp32: return self.remove_cbs([ChannelsLastCallback, MixedPrecision])
    else:       return self.remove_cb(ChannelsLastCallback())